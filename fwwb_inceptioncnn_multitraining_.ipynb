{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.utils import shuffle\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "特征如下：['ITEM_NAME' 'TYPE'] \n",
      "\n",
      "数据数目:  499447\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('data/train/trainx.txt', sep='\\t')\n",
    "features = data.columns.values\n",
    "print(\"特征如下：{} \\n\".format(features))\n",
    "print(\"数据数目: \", len(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据清洗\n",
    "* 替换一些字符 类似于 （ ） ‘ ’ _"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ITEM_NAME</th>\n",
       "      <th>TYPE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>腾讯QQ黄钻三个月QQ黄钻3个月季卡官方自动充值可查时间可续费</td>\n",
       "      <td>本地生活--游戏充值--QQ充值</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>腾讯q币充值30元qq币30qb30q币自动充值</td>\n",
       "      <td>本地生活--游戏充值--QQ充值</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>腾讯QQ红钻包月卡 QQ红钻一个月QQ红钻1个月</td>\n",
       "      <td>本地生活--游戏充值--QQ充值</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>腾讯QQ蓝钻贵族31个月直充</td>\n",
       "      <td>本地生活--游戏充值--QQ充值</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>腾讯QQ币148元148QQ币148个直充148Q币148个Q币148个QQB★自动充值</td>\n",
       "      <td>本地生活--游戏充值--QQ充值</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      ITEM_NAME              TYPE\n",
       "0               腾讯QQ黄钻三个月QQ黄钻3个月季卡官方自动充值可查时间可续费  本地生活--游戏充值--QQ充值\n",
       "1                      腾讯q币充值30元qq币30qb30q币自动充值  本地生活--游戏充值--QQ充值\n",
       "2                      腾讯QQ红钻包月卡 QQ红钻一个月QQ红钻1个月  本地生活--游戏充值--QQ充值\n",
       "3                                腾讯QQ蓝钻贵族31个月直充  本地生活--游戏充值--QQ充值\n",
       "4  腾讯QQ币148元148QQ币148个直充148Q币148个Q币148个QQB★自动充值  本地生活--游戏充值--QQ充值"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "from zhon.hanzi import punctuation\n",
    "from string import punctuation as english_punc\n",
    "def  rm_punc(strs):\n",
    "    return re.sub(r\"[{}]+\".format(punctuation + english_punc),\" \",strs)\n",
    "data[features[0]]  = data[features[0]].apply(rm_punc)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练数据数目:   399558\n",
      "测试数据数目:   99889\n",
      "(399558,) (99889,)\n"
     ]
    }
   ],
   "source": [
    "# 因为有的label里本身就存在空格，所以先要把空格去掉，再去掉- - ， 再以空格划分\n",
    "data = shuffle(data)\n",
    "data_y = data[features[1]]\n",
    "\n",
    "# 取训练数据和测试数据\n",
    "train = data.sample(frac= 0.8).reset_index()\n",
    "test = data.sample(frac= 0.2).reset_index()\n",
    "\n",
    "\n",
    "print(\"训练数据数目:  \", len(train))\n",
    "print(\"测试数据数目:  \", len(test))\n",
    "\n",
    "train_x  = train[features[0]]\n",
    "train_y  = train[features[1]]\n",
    "\n",
    "test_x  = test[features[0]]\n",
    "test_y  = test[features[1]]\n",
    "\n",
    "print(train_x.shape, test_x.shape)\n",
    "\n",
    "# 分出数据集 训练集 测试集 数据集用来训练词向量\n",
    "data_y = data_y.apply(lambda x: x.replace(\" \", \"\").replace(\"--\", \" \").replace(\"/\", \"\")).tolist()\n",
    "data_y= [each.split() for each in data_y]\n",
    "\n",
    "train_y = train_y.apply(lambda x: x.replace(\" \", \"\").replace(\"--\", \" \").replace(\"/\", \"\")).tolist()\n",
    "train_y = [each.split() for each in train_y]\n",
    "\n",
    "test_y = test_y.apply(lambda x: x.replace(\" \", \"\").replace(\"--\", \" \").replace(\"/\", \"\")).tolist()\n",
    "test_y = [each.split() for each in test_y]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 分词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba\n",
    "from random import shuffle\n",
    "def participle(data, doshuffle = False):\n",
    "    words = []\n",
    "    for i in range(len(data)):\n",
    "        result=[]\n",
    "        seg_list = jieba.cut(data.iloc[i])\n",
    "        for w in seg_list :#读取每一行分词\n",
    "            if w != \" \":\n",
    "                result.append(w)\n",
    "        # 打乱每条数据的分词\n",
    "        if doshuffle:\n",
    "            shuffle(result)\n",
    "        words.append(result)#将该行分词写入列表形式的总分词列表\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\Richado\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 1.674 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "# 每条数据的分词结果是个长度不等的词列表\n",
    "word_data = participle(data[features[0]], doshuffle = False)\n",
    "word_data_train = participle(train_x,doshuffle = False)\n",
    "word_data_test = participle(test_x, doshuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'腾讯QQ黄钻三个月QQ黄钻3个月季卡官方自动充值可查时间可续费'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[features[0]][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['凌度',\n",
       " 'BL950',\n",
       " '行车',\n",
       " '记录仪',\n",
       " '安霸',\n",
       " 'A7L70',\n",
       " '方案',\n",
       " '高清',\n",
       " '广角',\n",
       " '170',\n",
       " '度',\n",
       " 'HDR',\n",
       " '高',\n",
       " '动态',\n",
       " 'BL950',\n",
       " '升级版',\n",
       " '标配',\n",
       " '64G',\n",
       " '卡']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from random import shuffle\n",
    "word_data[8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 标签分离(一个标签分成三个标签)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y = pd.DataFrame(train_y, columns=['label1', 'label2', 'label3'])\n",
    "test_y = pd.DataFrame(test_y, columns=['label1', 'label2', 'label3'])\n",
    "data_y = pd.DataFrame(data_y, columns=['label1', 'label2', 'label3'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_y[\"label1\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "191"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_y[\"label2\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1192"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_y[\"label3\"].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 标签onn_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import to_categorical\n",
    "# 先生成唯一数组\n",
    "y_label1 = data_y['label1'].unique().tolist()\n",
    "y_label2 = data_y['label2'].unique().tolist()\n",
    "y_label3 = data_y['label3'].unique().tolist()\n",
    "\n",
    "# 获取在唯一数组中的索引(训练集和测试集各有3个标签需要处理)\n",
    "train_y_label1_map = train_y['label1'].apply(lambda x: y_label1.index(x))\n",
    "train_y_label2_map = train_y['label2'].apply(lambda x: y_label2.index(x))\n",
    "train_y_label3_map = train_y['label3'].apply(lambda x: y_label3.index(x))\n",
    "test_y_label1_map = test_y['label1'].apply(lambda x: y_label1.index(x))\n",
    "test_y_label2_map = test_y['label2'].apply(lambda x: y_label2.index(x))\n",
    "test_y_label3_map = test_y['label3'].apply(lambda x: y_label3.index(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生成对应one-hot(用做训练模型的标签)\n",
    "train_y_label1_one_hot = to_categorical(train_y_label1_map, 22)\n",
    "train_y_label2_one_hot = to_categorical(train_y_label2_map, 191)\n",
    "train_y_label3_one_hot = to_categorical(train_y_label3_map, 1192)\n",
    "test_y_label1_one_hot = to_categorical(test_y_label1_map, 22)\n",
    "test_y_label2_one_hot = to_categorical(test_y_label2_map, 191)\n",
    "test_y_label3_one_hot = to_categorical(test_y_label3_map, 1192)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import *\n",
    "from keras.preprocessing import sequence\n",
    "tok = Tokenizer()\n",
    "tok.fit_on_texts(word_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'新款': 1,\n",
       " '1': 2,\n",
       " '汽车': 3,\n",
       " '时尚': 4,\n",
       " '黑色': 5,\n",
       " '鞋': 6,\n",
       " '2016': 7,\n",
       " '休闲': 8,\n",
       " '款': 9,\n",
       " '专用': 10,\n",
       " '2': 11,\n",
       " '5': 12,\n",
       " '男士': 13,\n",
       " '跟': 14,\n",
       " '手机': 15,\n",
       " '适用': 16,\n",
       " '女': 17,\n",
       " '套装': 18,\n",
       " '男': 19,\n",
       " '韩版': 20,\n",
       " '夏季': 21,\n",
       " '儿童': 22,\n",
       " '3': 23,\n",
       " '包': 24,\n",
       " '于': 25,\n",
       " '透气': 26,\n",
       " '壳': 27,\n",
       " '套': 28,\n",
       " '休闲鞋': 29,\n",
       " '米': 30,\n",
       " '户外': 31,\n",
       " '男鞋': 32,\n",
       " '真皮': 33,\n",
       " '垫': 34,\n",
       " '红色': 35,\n",
       " '6': 36,\n",
       " '4': 37,\n",
       " '脚垫': 38,\n",
       " '女鞋': 39,\n",
       " '男女': 40,\n",
       " '白色': 41,\n",
       " '全': 42,\n",
       " '蓝色': 43,\n",
       " '创意': 44,\n",
       " '宝宝': 45,\n",
       " '防水': 46,\n",
       " '简约': 47,\n",
       " '四季': 48,\n",
       " '女士': 49,\n",
       " '8': 50,\n",
       " '保护套': 51,\n",
       " '凉鞋': 52,\n",
       " '牛皮': 53,\n",
       " '四件套': 54,\n",
       " '皮鞋': 55,\n",
       " '系列': 56,\n",
       " '专车': 57,\n",
       " '情侣': 58,\n",
       " '包围': 59,\n",
       " '灯': 60,\n",
       " '通用': 61,\n",
       " '纯棉': 62,\n",
       " '加厚': 63,\n",
       " '小': 64,\n",
       " '运动': 65,\n",
       " '单鞋': 66,\n",
       " '商务': 67,\n",
       " '学生': 68,\n",
       " '大': 69,\n",
       " '2015': 70,\n",
       " '送': 71,\n",
       " '潮流': 72,\n",
       " '客厅': 73,\n",
       " '婴儿': 74,\n",
       " '带': 75,\n",
       " '新': 76,\n",
       " '卡通': 77,\n",
       " '号': 78,\n",
       " '全棉': 79,\n",
       " '英伦': 80,\n",
       " '7': 81,\n",
       " '0': 82,\n",
       " '寸': 83,\n",
       " '可': 84,\n",
       " '现代': 85,\n",
       " '鞋子': 86,\n",
       " 'led': 87,\n",
       " '单肩': 88,\n",
       " '玩具': 89,\n",
       " '黑': 90,\n",
       " '被': 91,\n",
       " '座垫': 92,\n",
       " '39': 93,\n",
       " '礼物': 94,\n",
       " '苹果': 95,\n",
       " '40': 96,\n",
       " '版': 97,\n",
       " '的': 98,\n",
       " '摆件': 99,\n",
       " '保暖': 100,\n",
       " '礼品': 101,\n",
       " '码': 102,\n",
       " '38': 103,\n",
       " '保护': 104,\n",
       " '改装': 105,\n",
       " '灰色': 106,\n",
       " '春季': 107,\n",
       " '床上用品': 108,\n",
       " '舒适': 109,\n",
       " '卧室': 110,\n",
       " '可爱': 111,\n",
       " '印花': 112,\n",
       " '拖鞋': 113,\n",
       " '装饰': 114,\n",
       " '多功能': 115,\n",
       " '高': 116,\n",
       " '坐垫': 117,\n",
       " '棕色': 118,\n",
       " '被套': 119,\n",
       " '茶具': 120,\n",
       " '进口': 121,\n",
       " '系带': 122,\n",
       " '米床': 123,\n",
       " '10': 124,\n",
       " '防': 125,\n",
       " '背包': 126,\n",
       " '金属': 127,\n",
       " '防滑': 128,\n",
       " '杯': 129,\n",
       " '复古': 130,\n",
       " '座套': 131,\n",
       " '经典': 132,\n",
       " '正版': 133,\n",
       " '秋冬': 134,\n",
       " '系': 135,\n",
       " '贴': 136,\n",
       " '春夏': 137,\n",
       " '12': 138,\n",
       " '汽车坐垫': 139,\n",
       " '奥迪': 140,\n",
       " '不锈钢': 141,\n",
       " '陶瓷': 142,\n",
       " '粉色': 143,\n",
       " '36': 144,\n",
       " '床单': 145,\n",
       " '欧美': 146,\n",
       " '床': 147,\n",
       " '定制': 148,\n",
       " '双人': 149,\n",
       " '生日礼物': 150,\n",
       " '手表': 151,\n",
       " '35': 152,\n",
       " '孕妇': 153,\n",
       " '板鞋': 154,\n",
       " '宝马': 155,\n",
       " '女款': 156,\n",
       " '车载': 157,\n",
       " '大众': 158,\n",
       " '套件': 159,\n",
       " '实木': 160,\n",
       " '年': 161,\n",
       " '女包': 162,\n",
       " '夹': 163,\n",
       " '枕': 164,\n",
       " '空调': 165,\n",
       " '37': 166,\n",
       " '办公': 167,\n",
       " '膜': 168,\n",
       " 'l': 169,\n",
       " '红': 170,\n",
       " '风': 171,\n",
       " '手机套': 172,\n",
       " '欧式': 173,\n",
       " '15': 174,\n",
       " '平底': 175,\n",
       " '车': 176,\n",
       " '尖头': 177,\n",
       " '男款': 178,\n",
       " '装': 179,\n",
       " '个性': 180,\n",
       " '手链': 181,\n",
       " '42': 182,\n",
       " '运动鞋': 183,\n",
       " '靴': 184,\n",
       " '41': 185,\n",
       " 'm': 186,\n",
       " '英寸': 187,\n",
       " '钻': 188,\n",
       " '皮革': 189,\n",
       " '色': 190,\n",
       " '防晒': 191,\n",
       " '新品': 192,\n",
       " '金': 193,\n",
       " '家用': 194,\n",
       " '水晶': 195,\n",
       " '懒人': 196,\n",
       " '吊坠': 197,\n",
       " '厚底': 198,\n",
       " '中': 199,\n",
       " '家居': 200,\n",
       " '配件': 201,\n",
       " '43': 202,\n",
       " '黄色': 203,\n",
       " '组合': 204,\n",
       " '粗': 205,\n",
       " '全包': 206,\n",
       " '书包': 207,\n",
       " '200': 208,\n",
       " '架': 209,\n",
       " '高跟鞋': 210,\n",
       " '饰品': 211,\n",
       " '三星': 212,\n",
       " '米色': 213,\n",
       " '公仔': 214,\n",
       " '硅胶': 215,\n",
       " '绿色': 216,\n",
       " '内': 217,\n",
       " '百搭': 218,\n",
       " '丝圈': 219,\n",
       " '后备箱': 220,\n",
       " '吸顶灯': 221,\n",
       " '皮套': 222,\n",
       " '透明': 223,\n",
       " '电脑': 224,\n",
       " '钱包': 225,\n",
       " 's': 226,\n",
       " '三件套': 227,\n",
       " '16': 228,\n",
       " '手工': 229,\n",
       " '蓝': 230,\n",
       " '家具': 231,\n",
       " '厘米': 232,\n",
       " '中国': 233,\n",
       " '个': 234,\n",
       " '高清': 235,\n",
       " '正品': 236,\n",
       " '毛绒玩具': 237,\n",
       " '型': 238,\n",
       " '9': 239,\n",
       " '春秋': 240,\n",
       " '台': 241,\n",
       " '冬季': 242,\n",
       " '挂件': 243,\n",
       " '奔驰': 244,\n",
       " '金色': 245,\n",
       " '跑步': 246,\n",
       " '银': 247,\n",
       " '包邮': 248,\n",
       " '智能': 249,\n",
       " '抱': 250,\n",
       " '3d': 251,\n",
       " '斜挎包': 252,\n",
       " 'a': 253,\n",
       " '双肩包': 254,\n",
       " 'xl': 255,\n",
       " '福克斯': 256,\n",
       " '纯色': 257,\n",
       " '岁': 258,\n",
       " '婚庆': 259,\n",
       " '天然': 260,\n",
       " '级': 261,\n",
       " '潮': 262,\n",
       " '礼盒': 263,\n",
       " '增高': 264,\n",
       " '白': 265,\n",
       " '20': 266,\n",
       " '灯具': 267,\n",
       " '一': 268,\n",
       " '帆布鞋': 269,\n",
       " '家纺': 270,\n",
       " '华为': 271,\n",
       " '银色': 272,\n",
       " '紫色': 273,\n",
       " '迷你': 274,\n",
       " '双层': 275,\n",
       " '丰田': 276,\n",
       " '斜': 277,\n",
       " '单': 278,\n",
       " '本田': 279,\n",
       " '单人': 280,\n",
       " '卡': 281,\n",
       " '支架': 282,\n",
       " '短袖': 283,\n",
       " '手提包': 284,\n",
       " '戒指': 285,\n",
       " '皮': 286,\n",
       " '用品': 287,\n",
       " '件套': 288,\n",
       " '钻石': 289,\n",
       " '圆头': 290,\n",
       " '片': 291,\n",
       " '14': 292,\n",
       " '双': 293,\n",
       " '项链': 294,\n",
       " '耐磨': 295,\n",
       " '车型': 296,\n",
       " '立体': 297,\n",
       " '月': 298,\n",
       " '玻璃': 299,\n",
       " '水钻': 300,\n",
       " '性感': 301,\n",
       " 't恤': 302,\n",
       " '餐厅': 303,\n",
       " '方向盘': 304,\n",
       " '边框': 305,\n",
       " '高尔夫': 306,\n",
       " '贴纸': 307,\n",
       " '贴膜': 308,\n",
       " '头层': 309,\n",
       " '外壳': 310,\n",
       " '式': 311,\n",
       " '镂空': 312,\n",
       " '原装': 313,\n",
       " '盒': 314,\n",
       " '冰丝': 315,\n",
       " '豆豆': 316,\n",
       " '100': 317,\n",
       " '益智': 318,\n",
       " '椅': 319,\n",
       " '扣': 320,\n",
       " '头': 321,\n",
       " '磨砂': 322,\n",
       " '包包': 323,\n",
       " '彩色': 324,\n",
       " '44': 325,\n",
       " '床品': 326,\n",
       " '一对': 327,\n",
       " '别克': 328,\n",
       " '摔': 329,\n",
       " '玫瑰': 330,\n",
       " '棉': 331,\n",
       " '册': 332,\n",
       " '帕萨特': 333,\n",
       " '把': 334,\n",
       " '充电': 335,\n",
       " '收纳': 336,\n",
       " '十字绣': 337,\n",
       " '旅行': 338,\n",
       " '折叠': 339,\n",
       " '公主': 340,\n",
       " '浅口': 341,\n",
       " '功夫': 342,\n",
       " '环保': 343,\n",
       " '韩国': 344,\n",
       " '人': 345,\n",
       " '小米': 346,\n",
       " '车用': 347,\n",
       " '多': 348,\n",
       " '不': 349,\n",
       " '粉': 350,\n",
       " '衣': 351,\n",
       " '吊灯': 352,\n",
       " '彩绘': 353,\n",
       " '钥匙包': 354,\n",
       " '手': 355,\n",
       " '条': 356,\n",
       " '夏': 357,\n",
       " '春夏季': 358,\n",
       " '大容量': 359,\n",
       " '凉': 360,\n",
       " '笔': 361,\n",
       " '布鞋': 362,\n",
       " '24': 363,\n",
       " '厨房': 364,\n",
       " '斜纹': 365,\n",
       " '茶杯': 366,\n",
       " '水杯': 367,\n",
       " '与': 368,\n",
       " '结婚': 369,\n",
       " '途观': 370,\n",
       " '专用汽车': 371,\n",
       " '被子': 372,\n",
       " '衣服': 373,\n",
       " '本': 374,\n",
       " '925': 375,\n",
       " '猫': 376,\n",
       " '灯饰': 377,\n",
       " '和': 378,\n",
       " '座椅': 379,\n",
       " '套餐': 380,\n",
       " '子': 381,\n",
       " '拉': 382,\n",
       " '无线': 383,\n",
       " '遥控': 384,\n",
       " '细': 385,\n",
       " '小号': 386,\n",
       " '卡罗': 387,\n",
       " '沙发': 388,\n",
       " '低帮': 389,\n",
       " '便携': 390,\n",
       " '办公室': 391,\n",
       " '花': 392,\n",
       " '无': 393,\n",
       " '手提': 394,\n",
       " '蝴蝶结': 395,\n",
       " '潮鞋': 396,\n",
       " '低': 397,\n",
       " '13': 398,\n",
       " '标准': 399,\n",
       " '纹': 400,\n",
       " '福特': 401,\n",
       " '笔记本': 402,\n",
       " '桌': 403,\n",
       " '钢化': 404,\n",
       " '全新': 405,\n",
       " '袋': 406,\n",
       " 'diy': 407,\n",
       " '坡': 408,\n",
       " 'plus': 409,\n",
       " '30': 410,\n",
       " 'iphone6': 411,\n",
       " '双肩': 412,\n",
       " '女单': 413,\n",
       " '零食': 414,\n",
       " '6s': 415,\n",
       " '北京': 416,\n",
       " '粉红色': 417,\n",
       " '短靴': 418,\n",
       " '咖啡色': 419,\n",
       " '凯美瑞': 420,\n",
       " '芯': 421,\n",
       " '套脚': 422,\n",
       " '驾车': 423,\n",
       " '起亚': 424,\n",
       " '工具': 425,\n",
       " 'usb': 426,\n",
       " '秋季': 427,\n",
       " '书籍': 428,\n",
       " '罩': 429,\n",
       " '60': 430,\n",
       " '备注': 431,\n",
       " '钥匙': 432,\n",
       " '50': 433,\n",
       " '均码': 434,\n",
       " '坠': 435,\n",
       " '后': 436,\n",
       " '老': 437,\n",
       " '器': 438,\n",
       " '颜色': 439,\n",
       " '女童': 440,\n",
       " '工艺品': 441,\n",
       " '塑料': 442,\n",
       " '模型': 443,\n",
       " '230cm': 444,\n",
       " '情人节': 445,\n",
       " '防风': 446,\n",
       " '大码': 447,\n",
       " '11': 448,\n",
       " '爱': 449,\n",
       " '线': 450,\n",
       " '徒步': 451,\n",
       " '鲜花': 452,\n",
       " '克': 453,\n",
       " '雅阁': 454,\n",
       " '酒': 455,\n",
       " '柜': 456,\n",
       " '34': 457,\n",
       " '尾箱': 458,\n",
       " '活性': 459,\n",
       " '18': 460,\n",
       " '外套': 461,\n",
       " '积木': 462,\n",
       " '标配': 463,\n",
       " '手串': 464,\n",
       " '新生儿': 465,\n",
       " '拉链': 466,\n",
       " '紫': 467,\n",
       " '花花公子': 468,\n",
       " '教材': 469,\n",
       " 'b': 470,\n",
       " '长': 471,\n",
       " '迈腾': 472,\n",
       " '中式': 473,\n",
       " '滤清器': 474,\n",
       " '绿': 475,\n",
       " 't': 476,\n",
       " '宽': 477,\n",
       " '150': 478,\n",
       " '田园': 479,\n",
       " '灰': 480,\n",
       " '孕妇装': 481,\n",
       " '甜美': 482,\n",
       " '玩偶': 483,\n",
       " '女式': 484,\n",
       " '壶': 485,\n",
       " 'c': 486,\n",
       " '现货': 487,\n",
       " '马丁': 488,\n",
       " '安全': 489,\n",
       " '亚麻': 490,\n",
       " '脚': 491,\n",
       " '遮阳': 492,\n",
       " '自动': 493,\n",
       " '圆形': 494,\n",
       " '沙滩鞋': 495,\n",
       " '长安': 496,\n",
       " '裤': 497,\n",
       " '座': 498,\n",
       " '女友': 499,\n",
       " '豪华版': 500,\n",
       " '虎': 501,\n",
       " '温馨': 502,\n",
       " '咖啡': 503,\n",
       " '女孩': 504,\n",
       " '美': 505,\n",
       " '长袖': 506,\n",
       " '帆布': 507,\n",
       " '软壳': 508,\n",
       " '女生': 509,\n",
       " '松糕': 510,\n",
       " '条纹': 511,\n",
       " '茶盘': 512,\n",
       " '长款': 513,\n",
       " '迪士尼': 514,\n",
       " '美式': 515,\n",
       " '熊': 516,\n",
       " '两用': 517,\n",
       " '用': 518,\n",
       " '马自达': 519,\n",
       " '标致': 520,\n",
       " '游戏': 521,\n",
       " '日产': 522,\n",
       " '含': 523,\n",
       " '滤': 524,\n",
       " '香水': 525,\n",
       " '美国': 526,\n",
       " '板': 527,\n",
       " '专业': 528,\n",
       " '冲锋衣': 529,\n",
       " '速腾': 530,\n",
       " '布娃娃': 531,\n",
       " '挡': 532,\n",
       " '空气': 533,\n",
       " '韩版潮': 534,\n",
       " '蓝牙': 535,\n",
       " '装饰品': 536,\n",
       " '白光': 537,\n",
       " '一字': 538,\n",
       " '科鲁兹': 539,\n",
       " 'crv': 540,\n",
       " '音乐': 541,\n",
       " '服': 542,\n",
       " '成人': 543,\n",
       " '车衣': 544,\n",
       " '双人床': 545,\n",
       " '学院': 546,\n",
       " '平板': 547,\n",
       " '斤': 548,\n",
       " '隔热': 549,\n",
       " '网面': 550,\n",
       " '凉席': 551,\n",
       " '靴子': 552,\n",
       " '优雅': 553,\n",
       " '垫子': 554,\n",
       " '键盘': 555,\n",
       " '度': 556,\n",
       " '件': 557,\n",
       " '棕': 558,\n",
       " '日本': 559,\n",
       " '盖': 560,\n",
       " '皮肤': 561,\n",
       " '配': 562,\n",
       " '黄': 563,\n",
       " '蕾丝': 564,\n",
       " '证书': 565,\n",
       " '德国': 566,\n",
       " '联想': 567,\n",
       " '翡翠': 568,\n",
       " '茶': 569,\n",
       " '速干': 570,\n",
       " '轻便': 571,\n",
       " '拖': 572,\n",
       " '三': 573,\n",
       " '捷达': 574,\n",
       " '全套': 575,\n",
       " '深蓝色': 576,\n",
       " '糖果': 577,\n",
       " '碗': 578,\n",
       " '茶壶': 579,\n",
       " '仿真': 580,\n",
       " '小包': 581,\n",
       " '英朗': 582,\n",
       " 'polo': 583,\n",
       " '内衣': 584,\n",
       " '180': 585,\n",
       " '达': 586,\n",
       " '书房': 587,\n",
       " '请': 588,\n",
       " '上衣': 589,\n",
       " '办公桌': 590,\n",
       " '橙色': 591,\n",
       " '挎': 592,\n",
       " '合金': 593,\n",
       " '小学生': 594,\n",
       " '钥匙扣': 595,\n",
       " '宝': 596,\n",
       " '逸': 597,\n",
       " '监控': 598,\n",
       " '儿童玩具': 599,\n",
       " '钢化玻璃': 600,\n",
       " '格': 601,\n",
       " '蒙迪欧': 602,\n",
       " '水壶': 603,\n",
       " '浅': 604,\n",
       " '汽车用品': 605,\n",
       " '特产': 606,\n",
       " '妈妈': 607,\n",
       " '婴幼儿': 608,\n",
       " '枕套': 609,\n",
       " '耳机': 610,\n",
       " '玫': 611,\n",
       " '箱': 612,\n",
       " '天籁': 613,\n",
       " '狗': 614,\n",
       " '鱼': 615,\n",
       " '浮雕': 616,\n",
       " '2014': 617,\n",
       " '比亚迪': 618,\n",
       " '钱': 619,\n",
       " '椅子': 620,\n",
       " '清新': 621,\n",
       " '200cm': 622,\n",
       " '适合': 623,\n",
       " '直径': 624,\n",
       " '插': 625,\n",
       " '坐套': 626,\n",
       " '登山': 627,\n",
       " '下': 628,\n",
       " '紫砂': 629,\n",
       " 'ipad': 630,\n",
       " '80': 631,\n",
       " '被罩': 632,\n",
       " '修复': 633,\n",
       " '电池': 634,\n",
       " '明锐': 635,\n",
       " '滤芯': 636,\n",
       " '娃娃': 637,\n",
       " '迷彩': 638,\n",
       " '卡其色': 639,\n",
       " '盘': 640,\n",
       " '置物架': 641,\n",
       " '绣': 642,\n",
       " '18k': 643,\n",
       " '货到付款': 644,\n",
       " '味': 645,\n",
       " '移动': 646,\n",
       " '随机': 647,\n",
       " '脚蹬': 648,\n",
       " '茶几': 649,\n",
       " '男包': 650,\n",
       " '瓶': 651,\n",
       " 'rav4': 652,\n",
       " '玉石': 653,\n",
       " 'xxl': 654,\n",
       " '开关': 655,\n",
       " '珠宝': 656,\n",
       " '博世': 657,\n",
       " '机油': 658,\n",
       " '浪漫': 659,\n",
       " '高档': 660,\n",
       " '雨刮器': 661,\n",
       " '书': 662,\n",
       " '双面': 663,\n",
       " '佛珠': 664,\n",
       " '翼': 665,\n",
       " '睡袋': 666,\n",
       " '加绒': 667,\n",
       " '拼色': 668,\n",
       " '夏装': 669,\n",
       " '电动': 670,\n",
       " '链': 671,\n",
       " '超薄': 672,\n",
       " 'u': 673,\n",
       " '景德镇': 674,\n",
       " '餐具': 675,\n",
       " '短款': 676,\n",
       " '软底': 677,\n",
       " '沙滩': 678,\n",
       " '男式': 679,\n",
       " '秋冬季': 680,\n",
       " '考试': 681,\n",
       " '长裤': 682,\n",
       " '喷漆': 683,\n",
       " '镶': 684,\n",
       " '提花': 685,\n",
       " '和田玉': 686,\n",
       " '室内': 687,\n",
       " '加大': 688,\n",
       " '花瓶': 689,\n",
       " '全国': 690,\n",
       " '探路者': 691,\n",
       " '吉普': 692,\n",
       " '精品': 693,\n",
       " '荣耀': 694,\n",
       " '阳光': 695,\n",
       " '日': 696,\n",
       " '高帮': 697,\n",
       " '雪佛兰': 698,\n",
       " '保温杯': 699,\n",
       " '手套': 700,\n",
       " 'cd': 701,\n",
       " '护板': 702,\n",
       " '电源': 703,\n",
       " '哈弗': 704,\n",
       " '飞度': 705,\n",
       " '有': 706,\n",
       " '4g': 707,\n",
       " '户外运动': 708,\n",
       " '帮': 709,\n",
       " '一体机': 710,\n",
       " '夏凉': 711,\n",
       " '留言': 712,\n",
       " '画': 713,\n",
       " '后盖': 714,\n",
       " '帐篷': 715,\n",
       " '45': 716,\n",
       " '汉兰达': 717,\n",
       " '机': 718,\n",
       " '皇冠': 719,\n",
       " '电子': 720,\n",
       " '衣柜': 721,\n",
       " '网': 722,\n",
       " 'a6l': 723,\n",
       " 'pu': 724,\n",
       " '生日': 725,\n",
       " '上': 726,\n",
       " 'q5': 727,\n",
       " '世界': 728,\n",
       " '220': 729,\n",
       " '平': 730,\n",
       " '旅游': 731,\n",
       " '拼接': 732,\n",
       " '内饰': 733,\n",
       " '手镯': 734,\n",
       " '气质': 735,\n",
       " '摆设': 736,\n",
       " '面板': 737,\n",
       " 'dvd': 738,\n",
       " '帽': 739,\n",
       " '跨': 740,\n",
       " '灯泡': 741,\n",
       " '特价': 742,\n",
       " '鳄鱼': 743,\n",
       " '餐桌': 744,\n",
       " '磨毛': 745,\n",
       " '拍': 746,\n",
       " '木': 747,\n",
       " '长城': 748,\n",
       " '雪铁龙': 749,\n",
       " '杯子': 750,\n",
       " '元': 751,\n",
       " '支': 752,\n",
       " '动物': 753,\n",
       " '春': 754,\n",
       " '靠垫': 755,\n",
       " '罗马': 756,\n",
       " '玫红': 757,\n",
       " '夏天': 758,\n",
       " '早教': 759,\n",
       " '防雨': 760,\n",
       " '拉杆箱': 761,\n",
       " '其他': 762,\n",
       " '干': 763,\n",
       " '食品': 764,\n",
       " '男孩': 765,\n",
       " '鱼嘴': 766,\n",
       " '充电器': 767,\n",
       " '日常': 768,\n",
       " '头盔': 769,\n",
       " '升级版': 770,\n",
       " '毯': 771,\n",
       " '纯': 772,\n",
       " '罐': 773,\n",
       " '豪华': 774,\n",
       " '旅行包': 775,\n",
       " '泳衣': 776,\n",
       " 'x5': 777,\n",
       " '思域': 778,\n",
       " '嘴': 779,\n",
       " '实用': 780,\n",
       " '光盘': 781,\n",
       " '木质': 782,\n",
       " '升级': 783,\n",
       " '行李箱': 784,\n",
       " '趾': 785,\n",
       " '两件套': 786,\n",
       " 'e': 787,\n",
       " '单个': 788,\n",
       " '钢笔': 789,\n",
       " '马': 790,\n",
       " '凯越': 791,\n",
       " '支装': 792,\n",
       " '米奇': 793,\n",
       " '保温': 794,\n",
       " '抓': 795,\n",
       " '摩托车': 796,\n",
       " '款式': 797,\n",
       " '男表': 798,\n",
       " 'ix35': 799,\n",
       " '乐福鞋': 800,\n",
       " '正装': 801,\n",
       " '第': 802,\n",
       " '记录仪': 803,\n",
       " '黄金': 804,\n",
       " '网鞋': 805,\n",
       " '纸': 806,\n",
       " '连衣裙': 807,\n",
       " '片装': 808,\n",
       " '毛毯': 809,\n",
       " '划痕': 810,\n",
       " '宝来': 811,\n",
       " '居家': 812,\n",
       " '简易': 813,\n",
       " '男童': 814,\n",
       " '连体': 815,\n",
       " '内胆': 816,\n",
       " '230': 817,\n",
       " '大号': 818,\n",
       " '拼': 819,\n",
       " '皮带': 820,\n",
       " '之': 821,\n",
       " '拼装': 822,\n",
       " '三合一': 823,\n",
       " '松糕鞋': 824,\n",
       " '层': 825,\n",
       " '调光': 826,\n",
       " '儿童节': 827,\n",
       " '修身': 828,\n",
       " '附': 829,\n",
       " 'oppo': 830,\n",
       " '19': 831,\n",
       " '设计': 832,\n",
       " '包头': 833,\n",
       " '我': 834,\n",
       " '编织': 835,\n",
       " '单件': 836,\n",
       " '阳台': 837,\n",
       " '音箱': 838,\n",
       " '登山鞋': 839,\n",
       " '咖色': 840,\n",
       " '贡缎': 841,\n",
       " '镜': 842,\n",
       " '上海': 843,\n",
       " '安装': 844,\n",
       " '棉鞋': 845,\n",
       " '钓鱼': 846,\n",
       " '锐志': 847,\n",
       " '桌椅': 848,\n",
       " '训练': 849,\n",
       " '漆皮': 850,\n",
       " '宠物': 851,\n",
       " '游泳': 852,\n",
       " '仕': 853,\n",
       " '硬盘': 854,\n",
       " '骑行': 855,\n",
       " '逍客': 856,\n",
       " '索纳塔': 857,\n",
       " '漆': 858,\n",
       " '英语': 859,\n",
       " '故事': 860,\n",
       " '帽子': 861,\n",
       " '路': 862,\n",
       " '选': 863,\n",
       " '兔': 864,\n",
       " '方形': 865,\n",
       " '前后': 866,\n",
       " '土豪': 867,\n",
       " '开': 868,\n",
       " '约': 869,\n",
       " 'a3': 870,\n",
       " '摄像头': 871,\n",
       " '夜光': 872,\n",
       " '25': 873,\n",
       " 'a4l': 874,\n",
       " '导航': 875,\n",
       " '120': 876,\n",
       " '铆钉': 877,\n",
       " '绣花': 878,\n",
       " '睿': 879,\n",
       " '风格': 880,\n",
       " 'cc': 881,\n",
       " '学习': 882,\n",
       " '旅行箱': 883,\n",
       " '打火机': 884,\n",
       " '自': 885,\n",
       " '东风': 886,\n",
       " '桑塔纳': 887,\n",
       " '8g': 888,\n",
       " '整套': 889,\n",
       " 'h6': 890,\n",
       " '铝合金': 891,\n",
       " '长方形': 892,\n",
       " '软': 893,\n",
       " '扳手': 894,\n",
       " '仿古': 895,\n",
       " '貔貅': 896,\n",
       " '斯柯达': 897,\n",
       " '珍珠': 898,\n",
       " '行车': 899,\n",
       " 'd': 900,\n",
       " 's6': 901,\n",
       " '张': 902,\n",
       " '底': 903,\n",
       " '水洗': 904,\n",
       " '球': 905,\n",
       " '卷': 906,\n",
       " '点': 907,\n",
       " '速递': 908,\n",
       " '鼠标': 909,\n",
       " '咖': 910,\n",
       " '出版社': 911,\n",
       " '配饰': 912,\n",
       " '免': 913,\n",
       " '黑白': 914,\n",
       " '5mm': 915,\n",
       " '裙': 916,\n",
       " '图书': 917,\n",
       " '22': 918,\n",
       " '车垫': 919,\n",
       " '大红': 920,\n",
       " '宝骏': 921,\n",
       " '雕花': 922,\n",
       " '定做': 923,\n",
       " '盒装': 924,\n",
       " '电镀': 925,\n",
       " '超高': 926,\n",
       " '好': 927,\n",
       " '翻盖': 928,\n",
       " '音响': 929,\n",
       " '手绘': 930,\n",
       " '台灯': 931,\n",
       " '骆驼': 932,\n",
       " '圆领': 933,\n",
       " 'v': 934,\n",
       " '浅蓝色': 935,\n",
       " '古典': 936,\n",
       " '拉手': 937,\n",
       " '年级': 938,\n",
       " 'pro': 939,\n",
       " '酒杯': 940,\n",
       " '台湾': 941,\n",
       " '吊': 942,\n",
       " '天': 943,\n",
       " '品': 944,\n",
       " '笔记本电脑': 945,\n",
       " '镜头': 946,\n",
       " '绒': 947,\n",
       " '精装': 948,\n",
       " '世家': 949,\n",
       " '指环': 950,\n",
       " '后视镜': 951,\n",
       " '铜': 952,\n",
       " '幼儿园': 953,\n",
       " '生肖': 954,\n",
       " '烤': 955,\n",
       " '网布': 956,\n",
       " '可折叠': 957,\n",
       " '标签': 958,\n",
       " '朗逸': 959,\n",
       " '必备': 960,\n",
       " '边': 961,\n",
       " '共': 962,\n",
       " '魅族': 963,\n",
       " '电脑包': 964,\n",
       " '补漆笔': 965,\n",
       " '小熊': 966,\n",
       " '轻薄': 967,\n",
       " '网络': 968,\n",
       " '静音': 969,\n",
       " '泰迪熊': 970,\n",
       " '艺术': 971,\n",
       " '标准版': 972,\n",
       " '卡宴': 973,\n",
       " '腰包': 974,\n",
       " '文具': 975,\n",
       " '500ml': 976,\n",
       " '电视': 977,\n",
       " '头枕': 978,\n",
       " '风衣': 979,\n",
       " '年份': 980,\n",
       " '插座': 981,\n",
       " '男装': 982,\n",
       " '电视柜': 983,\n",
       " '90': 984,\n",
       " '刹车片': 985,\n",
       " '超': 986,\n",
       " '奶瓶': 987,\n",
       " '冬': 988,\n",
       " '昂科威': 989,\n",
       " '主机': 990,\n",
       " '富贵': 991,\n",
       " '自由': 992,\n",
       " '仪表': 993,\n",
       " '开业': 994,\n",
       " '工装': 995,\n",
       " '耳钉': 996,\n",
       " 'vivo': 997,\n",
       " '羊皮': 998,\n",
       " '书桌': 999,\n",
       " '机械': 1000,\n",
       " ...}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "274420"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tok.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = tok.texts_to_sequences(word_data_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = tok.texts_to_sequences(word_data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = sequence.pad_sequences(x_train, maxlen=20)\n",
    "x_test  = sequence.pad_sequences(x_test, maxlen=20) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   155,   1487,    105, ...,      9,      2,    557],\n",
       "       [     0,      0,      0, ..., 162550, 108802, 162551],\n",
       "       [    91,    278,    149, ...,     11,     37,    548],\n",
       "       ...,\n",
       "       [     0,      0,      0, ...,    366,   7531, 152760],\n",
       "       [     0,      0,      0, ...,    151,     47,     35],\n",
       "       [     0,      0,      0, ...,  10689,     75,    565]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## GRU Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.contrib import rnn\n",
    "import tensorflow.contrib.layers as layers\n",
    "class GruModel(object):\n",
    "    \"\"\"\n",
    "    A CNN for text classification.\n",
    "    Uses an embedding layer, followed by a convolutional, max-pooling and softmax layer.\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 sequence_length, \n",
    "                 num_classes, \n",
    "                 embedding_size,  \n",
    "                 num_filters, \n",
    "                 l2_reg_lambda=0.0, \n",
    "                 n_layer = 1, \n",
    "                 hidden_size = 32, \n",
    "                 batch_size = 256, \n",
    "                 vac_size = 27440):\n",
    "        \"\"\"\n",
    "        sequence_length : 一个句子的长度（词的个数）\n",
    "        embedding_size : 词向量的长度\n",
    "        num_classes : 三个标签的类别数\n",
    "        vac_size : 词的个数\n",
    "        \"\"\"\n",
    "        # Placeholders for input, output and dropout\n",
    "        self.input_x = tf.placeholder(tf.int32, [None, sequence_length], name=\"input_x\")\n",
    "        self.input_y1 = tf.placeholder(tf.float32, [None, num_classes[0]], name=\"input_y1\")\n",
    "        self.input_y2 = tf.placeholder(tf.float32, [None, num_classes[1]], name=\"input_y2\")\n",
    "        self.input_y3 = tf.placeholder(tf.float32, [None, num_classes[2]], name=\"input_y3\")\n",
    "        self.keep_prob = tf.placeholder(tf.float32, name=\"keep_prob\")\n",
    "        self.n_layer = n_layer\n",
    "        self.batch_size = batch_size\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        \n",
    "        with tf.name_scope(\"embedding\"):\n",
    "            W = tf.Variable(\n",
    "                tf.random_uniform([274420, embedding_size], -1.0, 1.0),\n",
    "                name=\"W\")\n",
    "            self.embedded_chars = tf.nn.embedding_lookup(W, self.input_x)\n",
    "\n",
    "        l2_loss = tf.constant(0.0)\n",
    "        \n",
    "        self.embedded_input = tf.layers.batch_normalization(self.embedded_chars, training=is_training)\n",
    "        self.embedded_input = tf.expand_dims(self.embedded_input, -1)\n",
    "        \n",
    "        with tf.variable_scope(\"cnn1\"):\n",
    "            filter_shape = [3, 3, 1, num_filters]\n",
    "            W = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.1), name=\"W\")\n",
    "            b = tf.Variable(tf.constant(0.1, shape=[num_filters]), name=\"b\")\n",
    "            self.conv_output = tf.nn.conv2d(\n",
    "                    self.embedded_input,\n",
    "                    W,\n",
    "                    strides=[1, 1, 1, 1],\n",
    "                    padding=\"SAME\",\n",
    "                    name=\"conv\")\n",
    "            self.conv_output = tf.nn.dropout(self.conv_output, self.keep_prob)\n",
    "            \n",
    "        #gru模型\n",
    "        with tf.variable_scope('inception_text'):\n",
    "            self.output = self.text_inception(self.conv_output)\n",
    "        \n",
    "        with tf.variable_scope(\"cnn2\"):\n",
    "            filter_shape = [5, 5, 16, num_filters]\n",
    "            W = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.1), name=\"W\")\n",
    "            b = tf.Variable(tf.constant(0.1, shape=[num_filters]), name=\"b\")\n",
    "            self.conv_output = tf.nn.conv2d(\n",
    "                    self.output,\n",
    "                    W,\n",
    "                    strides=[1, 1, 1, 1],\n",
    "                    padding=\"VALID\",\n",
    "                    name=\"conv\")\n",
    "            self.conv_output = tf.nn.dropout(self.conv_output, self.keep_prob)\n",
    "        print(self.conv_output)\n",
    "        self.h_drop = tf.reshape(self.conv_output,(-1, 16 * 20 * 4))\n",
    "        print(self.h_drop.shape)\n",
    "        num_filters_total = self.h_drop.shape[1]\n",
    "        \n",
    "        \n",
    "        with tf.name_scope(\"output1\"):\n",
    "            W1 = tf.get_variable(\n",
    "                \"W11\",\n",
    "                shape=[num_filters_total, 64],\n",
    "                initializer=tf.contrib.layers.xavier_initializer())\n",
    "            b1 = tf.Variable(tf.constant(0.1, shape=[64]), name=\"b11\")\n",
    "            self.output = tf.nn.xw_plus_b(self.h_drop, W1, b1, name=\"output1\")\n",
    "            self.output = tf.layers.batch_normalization(self.output, training=is_training)\n",
    "            self.output = tf.nn.relu(self.output)\n",
    "            self.output = tf.nn.dropout(self.output, self.keep_prob)\n",
    "            W2 = tf.get_variable(\n",
    "                \"W12\",\n",
    "                shape=[64, num_classes[0]],\n",
    "                initializer=tf.contrib.layers.xavier_initializer())\n",
    "            b2 = tf.Variable(tf.constant(0.1, shape=[num_classes[0]]), name=\"b12\")\n",
    "            l2_loss += tf.nn.l2_loss(W1)\n",
    "            l2_loss += tf.nn.l2_loss(b1)\n",
    "            self.scores1 = tf.nn.xw_plus_b(self.output, W2, b2, name=\"scores\")\n",
    "            self.predictions1 = tf.argmax(self.scores1, 1, name=\"predictions\")\n",
    "            \n",
    "        with tf.name_scope(\"output2\"):\n",
    "            W1 = tf.get_variable(\n",
    "                \"W21\",\n",
    "                shape=[num_filters_total, 128],\n",
    "                initializer=tf.contrib.layers.xavier_initializer())\n",
    "            b1 = tf.Variable(tf.constant(0.1, shape=[128]), name=\"b21\")\n",
    "            self.output = tf.nn.xw_plus_b(self.h_drop, W1, b1, name=\"output2\")\n",
    "            self.output = tf.layers.batch_normalization(self.output, training=is_training)\n",
    "            self.output = tf.nn.relu(self.output)\n",
    "            self.output = tf.nn.dropout(self.output, self.keep_prob)\n",
    "            W2 = tf.get_variable(\n",
    "                \"W22\",\n",
    "                shape=[128, num_classes[1]],\n",
    "                initializer=tf.contrib.layers.xavier_initializer())\n",
    "            b2 = tf.Variable(tf.constant(0.1, shape=[num_classes[1]]), name=\"b22\")\n",
    "            l2_loss += tf.nn.l2_loss(W1)\n",
    "            l2_loss += tf.nn.l2_loss(b1)\n",
    "            self.scores2 = tf.nn.xw_plus_b(self.output, W2, b2, name=\"scores\")\n",
    "            self.predictions2 = tf.argmax(self.scores2, 1, name=\"predictions\")\n",
    "            \n",
    "        with tf.name_scope(\"output3\"):\n",
    "            W1 = tf.get_variable(\n",
    "                \"W31\",\n",
    "                shape=[num_filters_total, 256],\n",
    "                initializer=tf.contrib.layers.xavier_initializer())\n",
    "            b1 = tf.Variable(tf.constant(0.1, shape=[256]), name=\"b31\")\n",
    "            self.output = tf.nn.xw_plus_b(self.h_drop, W1, b1, name=\"output3\")\n",
    "            self.output = tf.layers.batch_normalization(self.output, training=is_training)\n",
    "            self.output = tf.nn.relu(self.output)\n",
    "            self.output = tf.nn.dropout(self.output, self.keep_prob)\n",
    "            W2 = tf.get_variable(\n",
    "                \"W32\",\n",
    "                shape=[256, num_classes[2]],\n",
    "                initializer=tf.contrib.layers.xavier_initializer())\n",
    "            b2 = tf.Variable(tf.constant(0.1, shape=[num_classes[2]]), name=\"b32\")\n",
    "            l2_loss += tf.nn.l2_loss(W1)\n",
    "            l2_loss += tf.nn.l2_loss(b1)\n",
    "            self.scores3 = tf.nn.xw_plus_b(self.output, W2, b2, name=\"scores\")\n",
    "            self.predictions3 = tf.argmax(self.scores3, 1, name=\"predictions\")\n",
    "        # Calculate mean cross-entropy loss\n",
    "        \n",
    "        with tf.name_scope(\"loss\"):\n",
    "            losses1 = tf.nn.softmax_cross_entropy_with_logits(logits=self.scores1, labels=self.input_y1)\n",
    "            losses2 = tf.nn.softmax_cross_entropy_with_logits(logits=self.scores2, labels=self.input_y2)\n",
    "            losses3 = tf.nn.softmax_cross_entropy_with_logits(logits=self.scores3, labels=self.input_y3)\n",
    "            self.loss = 0.1 * tf.reduce_mean(losses1) + 0.2 * tf.reduce_mean(losses2) + 0.7*tf.reduce_mean(losses3) + l2_reg_lambda * l2_loss\n",
    "            tf.summary.scalar('loss1',tf.reduce_mean(losses1))\n",
    "            tf.summary.scalar('loss2',tf.reduce_mean(losses2))\n",
    "            tf.summary.scalar('loss3',tf.reduce_mean(losses3))\n",
    "            tf.summary.scalar('loss',self.loss)\n",
    "        # Accuracy\n",
    "        with tf.name_scope(\"accuracy\"):\n",
    "            correct_predictions1 = tf.equal(self.predictions1, tf.argmax(self.input_y1, 1))\n",
    "            correct_predictions2 = tf.equal(self.predictions2, tf.argmax(self.input_y2, 1))\n",
    "            correct_predictions3 = tf.equal(self.predictions3, tf.argmax(self.input_y3, 1))\n",
    "#           单独的准确率\n",
    "            self.accuracy1 = tf.reduce_mean(tf.cast(correct_predictions1, \"float\"), name=\"accuracy1\")\n",
    "            self.accuracy2 = tf.reduce_mean(tf.cast(correct_predictions2, \"float\"), name=\"accuracy2\")\n",
    "            self.accuracy3 = tf.reduce_mean(tf.cast(correct_predictions3, \"float\"), name=\"accuracy3\")\n",
    "#           一起的准确率\n",
    "            self.acc = tf.reduce_mean(tf.cast(correct_predictions1, \"float\")*tf.cast(correct_predictions2, \"float\")*tf.cast(correct_predictions3, \"float\"))\n",
    "            tf.summary.scalar('acc1',self.accuracy1)\n",
    "            tf.summary.scalar('acc2',self.accuracy2)\n",
    "            tf.summary.scalar('acc3',self.accuracy3)\n",
    "            tf.summary.scalar('acc',self.acc)\n",
    "            \n",
    "    def text_inception(self, inputs):\n",
    "        output = []\n",
    "        with tf.name_scope(\"conv-1\"):\n",
    "            filter_shape = [1, 1, num_filters, num_filters]\n",
    "            W = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.1), name=\"W\")\n",
    "            b = tf.Variable(tf.constant(0.1, shape=[num_filters]), name=\"b\")\n",
    "            conv = tf.nn.conv2d(\n",
    "                    inputs,\n",
    "                    W,\n",
    "                    strides=[1, 1, 1, 1],\n",
    "                    padding=\"VALID\",\n",
    "                    name=\"conv\")\n",
    "            conv = tf.nn.dropout(conv, self.keep_prob)\n",
    "            output.append(conv)\n",
    "        with tf.name_scope(\"conv-2\"):\n",
    "            filter_shape1 = [1, 1, num_filters, num_filters]\n",
    "            W = tf.Variable(tf.truncated_normal(filter_shape1, stddev=0.1), name=\"W\")\n",
    "            b = tf.Variable(tf.constant(0.1, shape=[num_filters]), name=\"b\")\n",
    "            conv1 = tf.nn.conv2d(\n",
    "                    inputs,\n",
    "                    W,\n",
    "                    strides=[1, 1, 1, 1],\n",
    "                    padding=\"VALID\",\n",
    "                    name=\"conv\")\n",
    "            filter_shape2 = [3, 3, num_filters, 2 * num_filters]\n",
    "            W1 = tf.Variable(tf.truncated_normal(filter_shape2, stddev=0.1), name=\"W\")\n",
    "            b1 = tf.Variable(tf.constant(0.1, shape=[num_filters]), name=\"b\")\n",
    "            conv2 = tf.nn.conv2d(\n",
    "                    conv1,\n",
    "                    W1,\n",
    "                    strides=[1, 1, 1, 1],\n",
    "                    padding=\"SAME\",\n",
    "                    name=\"conv\")\n",
    "            conv2 = tf.nn.dropout(conv2, self.keep_prob)\n",
    "            output.append(conv2)\n",
    "        with tf.name_scope(\"conv-4\"):\n",
    "            filter_shape = [3, 3, num_filters, num_filters]\n",
    "            W = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.1), name=\"W\")\n",
    "            b = tf.Variable(tf.constant(0.1, shape=[num_filters]), name=\"b\")\n",
    "            conv = tf.nn.conv2d(\n",
    "                    inputs,\n",
    "                    W,\n",
    "                    strides=[1, 1, 1, 1],\n",
    "                    padding=\"SAME\",\n",
    "                    name=\"conv\")\n",
    "            conv = tf.nn.dropout(conv, self.keep_prob)\n",
    "            output.append(conv)\n",
    "        print(tf.concat(output, 3).shape)\n",
    "        return tf.concat(output, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# batch 生成函数\n",
    "import math\n",
    "def batch_iter(data, batch_size, num_epochs, shuffle=True):\n",
    "    \"\"\"\n",
    "    Generates a batch iterator for a dataset.\n",
    "    \"\"\"\n",
    "    data = np.array(data)\n",
    "    data_size = len(data)\n",
    "    num_batches_per_epoch = int((len(data)-1)/batch_size) + 1\n",
    "    for epoch in range(num_epochs):\n",
    "        # Shuffle the data at each epoch\n",
    "        if shuffle:\n",
    "            shuffle_indices = np.random.permutation(np.arange(data_size))\n",
    "            shuffled_data = data[shuffle_indices]\n",
    "        else:\n",
    "            shuffled_data = data\n",
    "  \n",
    "        for batch_num in range(num_batches_per_epoch):\n",
    "            start_index = batch_num * batch_size\n",
    "            end_index = np.min(((batch_num + 1) * batch_size, data_size))\n",
    "            yield shuffled_data[start_index:end_index]\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 特征数这里是第三级标签的特征数\n",
    "max_features = 1192\n",
    "\n",
    "# 句子填充的长度\n",
    "sequence_len = 20\n",
    "\n",
    "# batch 大小\n",
    "batch_size = 512 \n",
    "\n",
    "# 迭代次数\n",
    "epochs = 50\n",
    "\n",
    "is_training = True\n",
    "\n",
    "# 词向量长度\n",
    "embedding_dims = 24\n",
    "\n",
    "# gru  的filters\n",
    "num_filters = 4\n",
    "\n",
    "# filter 的大小\n",
    "filter_size = [1, 2, 3, 4, 5]\n",
    "\n",
    "# 三个标签的类数\n",
    "num_classes = [22, 191, 1192]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "from tensorflow.contrib import learn\n",
    "\n",
    "\n",
    "\n",
    "FLAGS = tf.flags.FLAGS\n",
    "\n",
    "def train(x_train, y_train, x_dev, y_dev):\n",
    "    # Training\n",
    "    # ==================================================\n",
    "    \n",
    "    with tf.Graph().as_default():\n",
    "        \n",
    "        \n",
    "        session_conf = tf.ConfigProto()\n",
    "        sess = tf.Session(config=session_conf)\n",
    "        \n",
    "        with sess.as_default():\n",
    "            cnn = GruModel(sequence_length = sequence_len, \n",
    "                          num_classes = num_classes, \n",
    "                          embedding_size = embedding_dims, \n",
    "                          num_filters = num_filters)\n",
    "            saver=tf.train.Saver(max_to_keep=4)\n",
    "            # Define Training procedure\n",
    "\n",
    "            global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "#           学习率衰减\n",
    "            learning_rate = tf.train.exponential_decay(\n",
    "                0.01,\n",
    "                global_step,\n",
    "                x_train.shape[0] / batch_size,\n",
    "                0.99,\n",
    "                staircase=True,)\n",
    "            optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "            grads_and_vars = optimizer.compute_gradients(cnn.loss)\n",
    "            train_op = optimizer.apply_gradients(grads_and_vars, global_step=global_step)\n",
    "\n",
    "            # Initialize all variables\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            \n",
    "            merged = tf.summary.merge_all()\n",
    "            \n",
    "            writer = tf.summary.FileWriter(\"fwwb\",sess.graph)\n",
    "            acc_sum = np.array([0.0,0.0,0.0,0.0])\n",
    "            loss_sum = 0\n",
    "            \n",
    "            def train_step(x_batch, y_batch1, y_batch2, y_batch3, loss_sum, acc_sum, interal = 10):\n",
    "  \n",
    "                \"\"\"\n",
    "                A single training step\n",
    "                \"\"\"\n",
    "                feed_dict = {\n",
    "                  cnn.input_x: x_batch,\n",
    "                  cnn.input_y1: y_batch1,\n",
    "                  cnn.input_y2: y_batch2,\n",
    "                  cnn.input_y3: y_batch3,\n",
    "                  cnn.keep_prob: 0.9\n",
    "                }\n",
    "\n",
    "#         三个标签单独的准确率， 一起的准确率共四个准确率\n",
    "                rs, _, step,  loss, accuracy1, accuracy2, accuracy3, acc = sess.run(\n",
    "                    [merged, train_op, global_step, cnn.loss, cnn.accuracy1, cnn.accuracy2, cnn.accuracy3, cnn.acc],\n",
    "                    feed_dict)\n",
    "                time_str = datetime.datetime.now().isoformat()\n",
    "                acc_sum += [accuracy1, accuracy2, accuracy3, acc]\n",
    "                loss_sum += loss\n",
    "                if step % interal == 0:\n",
    "                    saver.save(sess, \"model/inception_model\", global_step=step)\n",
    "                    print(\"{}: step {}, loss {:g}, acc {}\".format(time_str, step, loss_sum/interal, acc_sum/interal))\n",
    "                    loss_sum = 0\n",
    "                    acc_sum = np.array([0.0,0.0,0.0,0.0])\n",
    "                    writer.add_summary(rs, step)\n",
    "                return loss_sum, acc_sum\n",
    "            # 评估步骤\n",
    "            def dev_step(x_batch, y_batch):\n",
    "                \"\"\"\n",
    "                Evaluates model on a dev set\n",
    "                \"\"\"\n",
    "\n",
    "                dev_batches = batch_iter(\n",
    "                    list(zip(x_batch, y_batch[0], y_batch[1], y_batch[2])), batch_size, 1)\n",
    "                step = 0\n",
    "                loss = 0\n",
    "                accuracy = np.array([0.0, 0.0, 0.0,0.0])\n",
    "                for batch in dev_batches:\n",
    "                    x_batch_dev, y_batch1_dev, y_batch2_dev, y_batch3_dev = zip(*batch)\n",
    "                    feed_dict = {\n",
    "                      cnn.input_x: x_batch_dev,\n",
    "                      cnn.input_y1: y_batch1_dev,\n",
    "                      cnn.input_y2: y_batch2_dev,\n",
    "                      cnn.input_y3: y_batch3_dev,\n",
    "                      cnn.keep_prob: 1\n",
    "                    }\n",
    "                    if len(x_batch_dev) < batch_size:\n",
    "                        continue\n",
    "                    step,  temp_loss, temp_accuracy1, temp_accuracy2, temp_accuracy3 , acc= sess.run(\n",
    "                        [global_step,cnn.loss, cnn.accuracy1, cnn.accuracy2, cnn.accuracy3, cnn.acc],\n",
    "                        feed_dict)\n",
    "                    accuracy[0] += temp_accuracy1 * len(x_batch_dev)\n",
    "                    accuracy[1] += temp_accuracy2 * len(x_batch_dev)\n",
    "                    accuracy[2] += temp_accuracy3 * len(x_batch_dev)\n",
    "                    accuracy[3] += acc * len(x_batch_dev)\n",
    "                    loss += temp_loss * len(x_batch_dev)\n",
    "                accuracy /= x_batch.shape[0]\n",
    "                loss /= x_batch.shape[0]\n",
    "                time_str = datetime.datetime.now().isoformat()                    \n",
    "\n",
    "                print(\"{}: step {}, loss {:g}, acc {} \".format(time_str, step, loss, accuracy))\n",
    "\n",
    "\n",
    "            # Generate batches\n",
    "            batches = batch_iter(\n",
    "                list(zip(x_train, y_train[0],y_train[1], y_train[2] )), batch_size, epochs)\n",
    "            # Training loop. For each batch...\n",
    "            for batch in batches:\n",
    "                x_batch, y_batch1, y_batch2, y_batch3 = zip(*batch)\n",
    "                if len(x_batch) < batch_size:\n",
    "                    continue\n",
    "                loss_sum, acc_sum = train_step(x_batch, y_batch1, y_batch2, y_batch3,  loss_sum, acc_sum, 40)\n",
    "                current_step = tf.train.global_step(sess, global_step)\n",
    "                if current_step % 200 == 0:\n",
    "                    print(\"\\nEvaluation:\")\n",
    "                    dev_step(x_dev, y_dev)\n",
    "                    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 20, 24, 16)\n",
      "Tensor(\"cnn2/dropout/mul:0\", shape=(?, 16, 20, 4), dtype=float32)\n",
      "(?, 1280)\n",
      "2019-03-03T11:45:07.427926: step 40, loss 3.88368, acc [0.42802734 0.35566406 0.25712891 0.19150391]\n",
      "2019-03-03T11:45:11.918623: step 80, loss 2.54691, acc [0.63896484 0.56005859 0.47509766 0.37773438]\n",
      "2019-03-03T11:45:16.657132: step 120, loss 1.99194, acc [0.73476562 0.65048828 0.57255859 0.48413086]\n",
      "2019-03-03T11:45:21.488536: step 160, loss 1.69627, acc [0.78100586 0.69863281 0.62504883 0.53818359]\n",
      "2019-03-03T11:45:26.477262: step 200, loss 1.5102, acc [0.80976563 0.725      0.65458984 0.57172852]\n",
      "\n",
      "Evaluation:\n",
      "2019-03-03T11:45:35.418647: step 200, loss 1.32893, acc [0.83475658 0.75455756 0.69211825 0.62106939] \n",
      "\n",
      "2019-03-03T11:45:39.652730: step 240, loss 1.35429, acc [0.82792969 0.75043945 0.68232422 0.60224609]\n",
      "2019-03-03T11:45:44.238968: step 280, loss 1.28994, acc [0.84316406 0.76064453 0.69755859 0.62250977]\n",
      "2019-03-03T11:45:48.632506: step 320, loss 1.20029, acc [0.84672852 0.77426758 0.71503906 0.63574219]\n",
      "2019-03-03T11:45:52.972269: step 360, loss 1.13062, acc [0.859375   0.78813477 0.73041992 0.65527344]\n",
      "2019-03-03T11:45:57.543720: step 400, loss 1.05695, acc [0.86650391 0.79633789 0.74365234 0.67050781]\n",
      "\n",
      "Evaluation:\n",
      "2019-03-03T11:46:04.879066: step 400, loss 0.925649, acc [0.88714473 0.8267577  0.77277778 0.71361211] \n",
      "\n",
      "2019-03-03T11:46:08.724056: step 440, loss 1.05301, acc [0.87197266 0.80541992 0.74467773 0.67675781]\n",
      "2019-03-03T11:46:13.127473: step 480, loss 0.993603, acc [0.88095703 0.81479492 0.75634766 0.68642578]\n",
      "2019-03-03T11:46:17.410903: step 520, loss 0.983783, acc [0.88164062 0.82045898 0.76181641 0.69707031]\n",
      "2019-03-03T11:46:21.720696: step 560, loss 0.955019, acc [0.87958984 0.81967773 0.7640625  0.69829102]\n",
      "2019-03-03T11:46:25.946656: step 600, loss 0.916383, acc [0.8878418  0.82431641 0.77363281 0.70820313]\n",
      "\n",
      "Evaluation:\n",
      "2019-03-03T11:46:33.495063: step 600, loss 0.729452, acc [0.91095116 0.8550491  0.81164092 0.75905255] \n",
      "\n",
      "2019-03-03T11:46:37.207315: step 640, loss 0.895527, acc [0.88676758 0.82915039 0.77768555 0.71430664]\n",
      "2019-03-03T11:46:41.367595: step 680, loss 0.869026, acc [0.8925293  0.83369141 0.78310547 0.71801758]\n",
      "2019-03-03T11:46:45.526885: step 720, loss 0.852683, acc [0.8953125  0.84150391 0.78242188 0.72285156]\n",
      "2019-03-03T11:46:49.687966: step 760, loss 0.847818, acc [0.89511719 0.8394043  0.78603516 0.72446289]\n",
      "2019-03-03T11:46:53.933689: step 800, loss 0.738415, acc [0.90625    0.85507813 0.81171875 0.75087891]\n",
      "\n",
      "Evaluation:\n",
      "2019-03-03T11:47:01.141903: step 800, loss 0.606157, acc [0.9239656  0.87602238 0.83823044 0.78764429] \n",
      "\n",
      "2019-03-03T11:47:04.761710: step 840, loss 0.572338, acc [0.92089844 0.87890625 0.84360352 0.78330078]\n",
      "2019-03-03T11:47:08.871762: step 880, loss 0.59151, acc [0.92128906 0.87944336 0.83984375 0.78076172]\n",
      "2019-03-03T11:47:12.974434: step 920, loss 0.592353, acc [0.91972656 0.8765625  0.83945313 0.7784668 ]\n",
      "2019-03-03T11:47:17.099981: step 960, loss 0.588682, acc [0.92358398 0.87578125 0.83911133 0.78100586]\n",
      "2019-03-03T11:47:21.213897: step 1000, loss 0.586166, acc [0.92216797 0.87626953 0.83808594 0.78022461]\n",
      "\n",
      "Evaluation:\n",
      "2019-03-03T11:47:28.429518: step 1000, loss 0.540557, acc [0.93202455 0.89133939 0.85577992 0.80820711] \n",
      "\n",
      "2019-03-03T11:47:32.043869: step 1040, loss 0.608403, acc [0.91791992 0.87592773 0.8362793  0.77836914]\n",
      "2019-03-03T11:47:36.149626: step 1080, loss 0.583386, acc [0.92568359 0.88110352 0.84179688 0.78657227]\n",
      "2019-03-03T11:47:40.256388: step 1120, loss 0.585567, acc [0.92529297 0.87539062 0.84140625 0.7828125 ]\n",
      "2019-03-03T11:47:44.370087: step 1160, loss 0.568163, acc [0.92412109 0.88139648 0.84643555 0.78930664]\n",
      "2019-03-03T11:47:48.468897: step 1200, loss 0.580082, acc [0.92441406 0.884375   0.84438477 0.78925781]\n",
      "\n",
      "Evaluation:\n",
      "2019-03-03T11:47:55.702345: step 1200, loss 0.479195, acc [0.93872198 0.90157074 0.87007578 0.82539619] \n",
      "\n",
      "2019-03-03T11:47:59.311194: step 1240, loss 0.566478, acc [0.92695313 0.88183594 0.84550781 0.78911133]\n",
      "2019-03-03T11:48:03.428070: step 1280, loss 0.566931, acc [0.92758789 0.88427734 0.84501953 0.79287109]\n",
      "2019-03-03T11:48:07.537573: step 1320, loss 0.558406, acc [0.92270508 0.88652344 0.84780273 0.7925293 ]\n",
      "2019-03-03T11:48:11.662778: step 1360, loss 0.574239, acc [0.92148438 0.88232422 0.84790039 0.79121094]\n",
      "2019-03-03T11:48:15.790026: step 1400, loss 0.552618, acc [0.92666016 0.88769531 0.84882813 0.79584961]\n",
      "\n",
      "Evaluation:\n",
      "2019-03-03T11:48:23.005747: step 1400, loss 0.436727, acc [0.94354734 0.91094114 0.88043729 0.83821041] \n",
      "\n",
      "2019-03-03T11:48:26.618561: step 1440, loss 0.572131, acc [0.92753906 0.8840332  0.84628906 0.79101562]\n",
      "2019-03-03T11:48:30.753765: step 1480, loss 0.54123, acc [0.92856445 0.89106445 0.85092773 0.79760742]\n",
      "2019-03-03T11:48:34.861243: step 1520, loss 0.559499, acc [0.92280273 0.88398438 0.84882813 0.79331055]\n",
      "2019-03-03T11:48:39.000262: step 1560, loss 0.558759, acc [0.92475586 0.88452148 0.84907227 0.79482422]\n",
      "2019-03-03T11:48:43.240685: step 1600, loss 0.329661, acc [0.95087891 0.92358398 0.9043457  0.85546875]\n",
      "\n",
      "Evaluation:\n",
      "2019-03-03T11:48:50.468079: step 1600, loss 0.377201, acc [0.94851285 0.9214428  0.89629489 0.85591006] \n",
      "\n",
      "2019-03-03T11:48:54.077966: step 1640, loss 0.315243, acc [0.95063477 0.92568359 0.90722656 0.85400391]\n",
      "2019-03-03T11:48:58.186370: step 1680, loss 0.332171, acc [0.94638672 0.9237793  0.8996582  0.84829102]\n",
      "2019-03-03T11:49:02.313056: step 1720, loss 0.320736, acc [0.9496582  0.92358398 0.90375977 0.84960938]\n",
      "2019-03-03T11:49:06.431013: step 1760, loss 0.347436, acc [0.94609375 0.92260742 0.89755859 0.84428711]\n",
      "2019-03-03T11:49:10.568596: step 1800, loss 0.340833, acc [0.94648438 0.9203125  0.89936523 0.8449707 ]\n",
      "\n",
      "Evaluation:\n",
      "2019-03-03T11:49:17.781358: step 1800, loss 0.359375, acc [0.95229705 0.92624814 0.9045941  0.86468981] \n",
      "\n",
      "2019-03-03T11:49:21.389756: step 1840, loss 0.346846, acc [0.94614258 0.92036133 0.89956055 0.84697266]\n",
      "2019-03-03T11:49:25.496416: step 1880, loss 0.348699, acc [0.94453125 0.91948242 0.89863281 0.84394531]\n",
      "2019-03-03T11:49:29.609970: step 1920, loss 0.357104, acc [0.94462891 0.91982422 0.89589844 0.84389648]\n",
      "2019-03-03T11:49:33.822991: step 1960, loss 0.34916, acc [0.94604492 0.92036133 0.89873047 0.84604492]\n",
      "2019-03-03T11:49:37.948490: step 2000, loss 0.349141, acc [0.9472168  0.92211914 0.89760742 0.84624023]\n",
      "\n",
      "Evaluation:\n",
      "2019-03-03T11:49:45.187607: step 2000, loss 0.340816, acc [0.95448948 0.93025258 0.90958964 0.87087667] \n",
      "\n",
      "2019-03-03T11:49:48.811357: step 2040, loss 0.353405, acc [0.94755859 0.92036133 0.89628906 0.84838867]\n",
      "2019-03-03T11:49:52.951310: step 2080, loss 0.366697, acc [0.94228516 0.9171875  0.89448242 0.84072266]\n",
      "2019-03-03T11:49:57.080598: step 2120, loss 0.367905, acc [0.94423828 0.91865234 0.89321289 0.84169922]\n",
      "2019-03-03T11:50:01.221997: step 2160, loss 0.364653, acc [0.94423828 0.9159668  0.89125977 0.83959961]\n",
      "2019-03-03T11:50:05.351176: step 2200, loss 0.363012, acc [0.94321289 0.91757813 0.89487305 0.8409668 ]\n",
      "\n",
      "Evaluation:\n",
      "2019-03-03T11:50:12.576064: step 2200, loss 0.312657, acc [0.95748281 0.93520808 0.91680766 0.87866532] \n",
      "\n",
      "2019-03-03T11:50:16.197360: step 2240, loss 0.369907, acc [0.94394531 0.91914063 0.89350586 0.84140625]\n",
      "2019-03-03T11:50:20.313975: step 2280, loss 0.375033, acc [0.94296875 0.91635742 0.89223633 0.84013672]\n",
      "2019-03-03T11:50:24.454076: step 2320, loss 0.371223, acc [0.94433594 0.91704102 0.89140625 0.84072266]\n",
      "2019-03-03T11:50:28.709044: step 2360, loss 0.293803, acc [0.95283203 0.92880859 0.91723633 0.8668457 ]\n",
      "2019-03-03T11:50:32.829946: step 2400, loss 0.196894, acc [0.96318359 0.94628906 0.93945312 0.89121094]\n",
      "\n",
      "Evaluation:\n",
      "2019-03-03T11:50:40.085633: step 2400, loss 0.284282, acc [0.96195777 0.94200563 0.92690887 0.89175985] \n",
      "\n",
      "2019-03-03T11:50:43.698992: step 2440, loss 0.200324, acc [0.96376953 0.94663086 0.93896484 0.89238281]\n",
      "2019-03-03T11:50:47.815750: step 2480, loss 0.198825, acc [0.9605957  0.94868164 0.93886719 0.89130859]\n",
      "2019-03-03T11:50:51.940270: step 2520, loss 0.205609, acc [0.9625     0.94462891 0.9375     0.89082031]\n",
      "2019-03-03T11:50:56.053996: step 2560, loss 0.218205, acc [0.95844727 0.94272461 0.93408203 0.8840332 ]\n",
      "2019-03-03T11:51:00.205650: step 2600, loss 0.215135, acc [0.96113281 0.94165039 0.93432617 0.88564453]\n",
      "\n",
      "Evaluation:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-03T11:51:12.774769: step 2600, loss 0.287411, acc [0.96263853 0.94214578 0.9291013  0.89403238] \n",
      "\n",
      "2019-03-03T11:51:16.420319: step 2640, loss 0.218603, acc [0.96000977 0.94453125 0.93354492 0.88525391]\n",
      "2019-03-03T11:51:20.589602: step 2680, loss 0.227332, acc [0.95864258 0.94077148 0.93183594 0.88217773]\n",
      "2019-03-03T11:51:24.727869: step 2720, loss 0.228621, acc [0.95961914 0.94399414 0.93164062 0.88300781]\n",
      "2019-03-03T11:51:28.850603: step 2760, loss 0.2238, acc [0.95991211 0.94414062 0.93095703 0.88334961]\n",
      "2019-03-03T11:51:32.981490: step 2800, loss 0.242632, acc [0.95629883 0.93745117 0.92548828 0.87626953]\n",
      "\n",
      "Evaluation:\n",
      "2019-03-03T11:51:40.216222: step 2800, loss 0.283534, acc [0.96280872 0.94301675 0.92959185 0.89429266] \n",
      "\n",
      "2019-03-03T11:51:43.850871: step 2840, loss 0.234794, acc [0.95932617 0.94213867 0.92856445 0.87851563]\n",
      "2019-03-03T11:51:48.016877: step 2880, loss 0.237248, acc [0.95546875 0.940625   0.92617187 0.87617188]\n",
      "2019-03-03T11:51:52.149682: step 2920, loss 0.235487, acc [0.95810547 0.93950195 0.92885742 0.88012695]\n",
      "2019-03-03T11:51:56.281824: step 2960, loss 0.250039, acc [0.95561523 0.94033203 0.9246582  0.87568359]\n",
      "2019-03-03T11:52:00.466914: step 3000, loss 0.252105, acc [0.95600586 0.93842773 0.92514648 0.87573242]\n",
      "\n",
      "Evaluation:\n",
      "2019-03-03T11:52:07.711842: step 3000, loss 0.267126, acc [0.96559181 0.94635045 0.93481765 0.90114027] \n",
      "\n",
      "2019-03-03T11:52:11.333584: step 3040, loss 0.263545, acc [0.95561523 0.93745117 0.92094727 0.87319336]\n",
      "2019-03-03T11:52:15.490147: step 3080, loss 0.256752, acc [0.95380859 0.93759766 0.92387695 0.87416992]\n",
      "2019-03-03T11:52:19.658682: step 3120, loss 0.256474, acc [0.95673828 0.93730469 0.92128906 0.87128906]\n",
      "2019-03-03T11:52:23.961727: step 3160, loss 0.132255, acc [0.97006836 0.96171875 0.95908203 0.91738281]\n",
      "2019-03-03T11:52:28.109491: step 3200, loss 0.129554, acc [0.96933594 0.96181641 0.96108398 0.9190918 ]\n",
      "\n",
      "Evaluation:\n",
      "2019-03-03T11:52:35.391212: step 3200, loss 0.257311, acc [0.96771416 0.95167636 0.94087437 0.91031044] \n",
      "\n",
      "2019-03-03T11:52:39.007049: step 3240, loss 0.128594, acc [0.97119141 0.9621582  0.96079102 0.92075195]\n",
      "2019-03-03T11:52:43.159970: step 3280, loss 0.143032, acc [0.96914062 0.95805664 0.95703125 0.91352539]\n",
      "2019-03-03T11:52:47.278735: step 3320, loss 0.142829, acc [0.97055664 0.95678711 0.95595703 0.9112793 ]\n",
      "2019-03-03T11:52:51.394816: step 3360, loss 0.147995, acc [0.96958008 0.95917969 0.95493164 0.91459961]\n",
      "2019-03-03T11:52:55.558271: step 3400, loss 0.154828, acc [0.96801758 0.95742187 0.95195312 0.91142578]\n",
      "\n",
      "Evaluation:\n",
      "2019-03-03T11:53:02.792812: step 3400, loss 0.265093, acc [0.96835487 0.95273754 0.93926258 0.91041056] \n",
      "\n",
      "2019-03-03T11:53:06.412077: step 3440, loss 0.149523, acc [0.97036133 0.95834961 0.95585937 0.91499023]\n",
      "2019-03-03T11:53:10.567709: step 3480, loss 0.161796, acc [0.96923828 0.9565918  0.94912109 0.90673828]\n",
      "2019-03-03T11:53:14.714065: step 3520, loss 0.156189, acc [0.96762695 0.95761719 0.95166016 0.90976563]\n",
      "2019-03-03T11:53:18.862457: step 3560, loss 0.166667, acc [0.96665039 0.95341797 0.94824219 0.90400391]\n",
      "2019-03-03T11:53:22.981230: step 3600, loss 0.176582, acc [0.96499023 0.95151367 0.94536133 0.8984375 ]\n",
      "\n",
      "Evaluation:\n",
      "2019-03-03T11:53:30.408820: step 3600, loss 0.265457, acc [0.9692759  0.95182653 0.94011353 0.91100121] \n",
      "\n",
      "2019-03-03T11:53:34.111956: step 3640, loss 0.172599, acc [0.965625   0.95375977 0.94511719 0.90126953]\n",
      "2019-03-03T11:53:38.436123: step 3680, loss 0.186534, acc [0.96645508 0.95429688 0.94169922 0.90009766]\n",
      "2019-03-03T11:53:42.572473: step 3720, loss 0.173312, acc [0.96552734 0.95019531 0.94575195 0.89887695]\n",
      "2019-03-03T11:53:46.745399: step 3760, loss 0.176718, acc [0.96640625 0.95380859 0.94541016 0.90205078]\n",
      "2019-03-03T11:53:50.896635: step 3800, loss 0.189481, acc [0.96572266 0.95170898 0.94121094 0.89702148]\n",
      "\n",
      "Evaluation:\n",
      "2019-03-03T11:53:58.127342: step 3800, loss 0.250306, acc [0.9694561  0.95442942 0.94373755 0.91557629] \n",
      "\n",
      "2019-03-03T11:54:01.753088: step 3840, loss 0.186976, acc [0.96201172 0.95263672 0.94155273 0.89702148]\n",
      "2019-03-03T11:54:05.921992: step 3880, loss 0.192334, acc [0.96420898 0.95053711 0.94033203 0.89599609]\n",
      "2019-03-03T11:54:10.212841: step 3920, loss 0.148662, acc [0.96928711 0.95878906 0.95366211 0.91386719]\n",
      "2019-03-03T11:54:14.325107: step 3960, loss 0.103729, acc [0.975      0.96918945 0.96796875 0.93164062]\n",
      "2019-03-03T11:54:18.475331: step 4000, loss 0.0966837, acc [0.97817383 0.96987305 0.97026367 0.93540039]\n",
      "\n",
      "Evaluation:\n",
      "2019-03-03T11:54:25.843409: step 4000, loss 0.254764, acc [0.9716185  0.95662185 0.94798226 0.9214428 ] \n",
      "\n",
      "2019-03-03T11:54:29.531168: step 4040, loss 0.100792, acc [0.97421875 0.96660156 0.96801758 0.92988281]\n",
      "2019-03-03T11:54:33.749647: step 4080, loss 0.109103, acc [0.97431641 0.96591797 0.96572266 0.928125  ]\n",
      "2019-03-03T11:54:37.961712: step 4120, loss 0.1101, acc [0.97451172 0.96743164 0.96459961 0.92822266]\n",
      "2019-03-03T11:54:42.105953: step 4160, loss 0.117188, acc [0.97133789 0.96342773 0.9644043  0.9234375 ]\n",
      "2019-03-03T11:54:46.227014: step 4200, loss 0.113285, acc [0.97426758 0.96577148 0.96411133 0.92651367]\n",
      "\n",
      "Evaluation:\n",
      "2019-03-03T11:54:53.543592: step 4200, loss 0.259801, acc [0.97126811 0.95712241 0.94646057 0.92052178] \n",
      "\n",
      "2019-03-03T11:54:57.169348: step 4240, loss 0.116967, acc [0.97304687 0.96586914 0.96171875 0.92373047]\n",
      "2019-03-03T11:55:01.353011: step 4280, loss 0.121699, acc [0.97358398 0.96328125 0.9621582  0.92270508]\n",
      "2019-03-03T11:55:05.499680: step 4320, loss 0.122053, acc [0.97260742 0.96220703 0.9609375  0.92075195]\n",
      "2019-03-03T11:55:09.645015: step 4360, loss 0.125355, acc [0.97358398 0.96450195 0.96044922 0.92358398]\n",
      "2019-03-03T11:55:13.750461: step 4400, loss 0.127588, acc [0.96904297 0.96201172 0.95996094 0.91567383]\n",
      "\n",
      "Evaluation:\n",
      "2019-03-03T11:55:20.961878: step 4400, loss 0.259446, acc [0.97216911 0.95815355 0.94657069 0.92134269] \n",
      "\n",
      "2019-03-03T11:55:24.581728: step 4440, loss 0.13203, acc [0.97026367 0.96118164 0.9578125  0.9168457 ]\n",
      "2019-03-03T11:55:28.707127: step 4480, loss 0.134696, acc [0.97128906 0.96381836 0.95678711 0.91791992]\n",
      "2019-03-03T11:55:32.823479: step 4520, loss 0.137647, acc [0.96845703 0.95966797 0.95639648 0.91337891]\n",
      "2019-03-03T11:55:37.003691: step 4560, loss 0.144374, acc [0.96958008 0.95957031 0.95307617 0.91108398]\n",
      "2019-03-03T11:55:41.157573: step 4600, loss 0.144999, acc [0.96816406 0.95942383 0.95410156 0.91347656]\n",
      "\n",
      "Evaluation:\n",
      "2019-03-03T11:55:48.379364: step 4600, loss 0.255368, acc [0.97221916 0.95865411 0.94839272 0.92285437] \n",
      "\n",
      "2019-03-03T11:55:51.998628: step 4640, loss 0.142583, acc [0.97128906 0.96005859 0.95258789 0.91279297]\n",
      "2019-03-03T11:55:56.164346: step 4680, loss 0.149641, acc [0.96831055 0.9590332  0.95161133 0.90996094]\n",
      "2019-03-03T11:56:00.463833: step 4720, loss 0.0799751, acc [0.98095703 0.97495117 0.97451172 0.94414062]\n",
      "2019-03-03T11:56:04.611863: step 4760, loss 0.0789969, acc [0.9793457  0.97446289 0.97626953 0.9440918 ]\n",
      "2019-03-03T11:56:08.806527: step 4800, loss 0.0869929, acc [0.97944336 0.97226563 0.97333984 0.94082031]\n",
      "\n",
      "Evaluation:\n",
      "2019-03-03T11:56:16.017955: step 4800, loss 0.259135, acc [0.9730601  0.96117691 0.95073532 0.92742945] \n",
      "\n",
      "2019-03-03T11:56:19.638754: step 4840, loss 0.0889872, acc [0.97636719 0.97260742 0.97456055 0.93955078]\n",
      "2019-03-03T11:56:23.748192: step 4880, loss 0.0820959, acc [0.97885742 0.97358398 0.97504883 0.94189453]\n",
      "2019-03-03T11:56:27.852405: step 4920, loss 0.0921924, acc [0.9765625  0.97084961 0.97133789 0.93740234]\n",
      "2019-03-03T11:56:31.970693: step 4960, loss 0.0876254, acc [0.97836914 0.97392578 0.97114258 0.93823242]\n",
      "2019-03-03T11:56:36.132372: step 5000, loss 0.0944279, acc [0.97807617 0.97290039 0.97006836 0.93632812]\n",
      "\n",
      "Evaluation:\n",
      "2019-03-03T11:56:43.471148: step 5000, loss 0.264991, acc [0.97411126 0.96139715 0.95025478 0.92817027] \n",
      "\n",
      "2019-03-03T11:56:47.095371: step 5040, loss 0.0972791, acc [0.97583008 0.97124023 0.96879883 0.93442383]\n",
      "2019-03-03T11:56:51.220335: step 5080, loss 0.105718, acc [0.97470703 0.96972656 0.96733398 0.93085938]\n",
      "2019-03-03T11:56:55.328896: step 5120, loss 0.0995579, acc [0.97504883 0.97041016 0.96679688 0.93012695]\n",
      "2019-03-03T11:56:59.430288: step 5160, loss 0.105655, acc [0.97553711 0.96879883 0.96450195 0.92939453]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-03T11:57:03.601066: step 5200, loss 0.108239, acc [0.97348633 0.96767578 0.96533203 0.9284668 ]\n",
      "\n",
      "Evaluation:\n",
      "2019-03-03T11:57:11.002422: step 5200, loss 0.261295, acc [0.97465186 0.96229815 0.95015467 0.92843056] \n",
      "\n",
      "2019-03-03T11:57:14.609784: step 5240, loss 0.108462, acc [0.97641602 0.96821289 0.96518555 0.92919922]\n",
      "2019-03-03T11:57:18.730174: step 5280, loss 0.11659, acc [0.97314453 0.96552734 0.96147461 0.9230957 ]\n",
      "2019-03-03T11:57:22.850446: step 5320, loss 0.112098, acc [0.97524414 0.96572266 0.96191406 0.92548828]\n",
      "2019-03-03T11:57:26.955761: step 5360, loss 0.118677, acc [0.97382813 0.96811523 0.96279297 0.92504883]\n",
      "2019-03-03T11:57:31.147068: step 5400, loss 0.108798, acc [0.97397461 0.9675293  0.96445313 0.92763672]\n",
      "\n",
      "Evaluation:\n",
      "2019-03-03T11:57:38.509085: step 5400, loss 0.255201, acc [0.97470192 0.96264854 0.95263743 0.93055291] \n",
      "\n",
      "2019-03-03T11:57:42.165099: step 5440, loss 0.11786, acc [0.97285156 0.96230469 0.9628418  0.92280273]\n",
      "2019-03-03T11:57:46.402264: step 5480, loss 0.0901594, acc [0.9800293  0.97460938 0.97104492 0.93999023]\n",
      "2019-03-03T11:57:50.520925: step 5520, loss 0.0658175, acc [0.98310547 0.98178711 0.97851562 0.95454102]\n",
      "2019-03-03T11:57:54.687664: step 5560, loss 0.0687709, acc [0.98046875 0.97792969 0.97841797 0.9480957 ]\n",
      "2019-03-03T11:57:58.856333: step 5600, loss 0.066525, acc [0.98212891 0.97949219 0.97924805 0.95244141]\n",
      "\n",
      "Evaluation:\n",
      "2019-03-03T11:58:06.132327: step 5600, loss 0.267514, acc [0.97561293 0.96330927 0.95316802 0.93254513] \n",
      "\n",
      "2019-03-03T11:58:09.757133: step 5640, loss 0.0704315, acc [0.9824707  0.97890625 0.97719727 0.95097656]\n",
      "2019-03-03T11:58:13.934190: step 5680, loss 0.0725367, acc [0.9824707  0.97807617 0.97670898 0.94868164]\n",
      "2019-03-03T11:58:18.102744: step 5720, loss 0.0782661, acc [0.97973633 0.97612305 0.97470703 0.94335938]\n",
      "2019-03-03T11:58:22.220590: step 5760, loss 0.0817245, acc [0.97924805 0.97446289 0.97397461 0.94228516]\n",
      "2019-03-03T11:58:26.386917: step 5800, loss 0.0831669, acc [0.97949219 0.97314453 0.97480469 0.9425293 ]\n",
      "\n",
      "Evaluation:\n",
      "2019-03-03T11:58:33.664243: step 5800, loss 0.2686, acc [0.97586321 0.96352952 0.95470973 0.93426704] \n",
      "\n",
      "2019-03-03T11:58:37.283500: step 5840, loss 0.0759625, acc [0.97998047 0.9769043  0.97597656 0.94628906]\n",
      "2019-03-03T11:58:41.461643: step 5880, loss 0.0793966, acc [0.98017578 0.97421875 0.97456055 0.94233398]\n",
      "2019-03-03T11:58:45.620670: step 5920, loss 0.0915014, acc [0.97924805 0.97119141 0.97109375 0.93896484]\n",
      "2019-03-03T11:58:49.733232: step 5960, loss 0.0906205, acc [0.97900391 0.97207031 0.96962891 0.93754883]\n",
      "2019-03-03T11:58:53.855253: step 6000, loss 0.095402, acc [0.97841797 0.97089844 0.96928711 0.93540039]\n",
      "\n",
      "Evaluation:\n",
      "2019-03-03T11:59:01.435218: step 6000, loss 0.272633, acc [0.97481204 0.96318914 0.95296779 0.93140386] \n",
      "\n",
      "2019-03-03T11:59:05.069905: step 6040, loss 0.0964461, acc [0.97670898 0.97133789 0.9684082  0.93500977]\n",
      "2019-03-03T11:59:09.242321: step 6080, loss 0.0943514, acc [0.97587891 0.97226563 0.97016602 0.93554688]\n",
      "2019-03-03T11:59:13.399837: step 6120, loss 0.0927728, acc [0.97802734 0.97216797 0.96977539 0.9362793 ]\n",
      "2019-03-03T11:59:17.585097: step 6160, loss 0.0986121, acc [0.97675781 0.97109375 0.96801758 0.93374023]\n",
      "2019-03-03T11:59:21.772823: step 6200, loss 0.0987963, acc [0.97695312 0.96992188 0.96738281 0.93208008]\n",
      "\n",
      "Evaluation:\n",
      "2019-03-03T11:59:29.000084: step 6200, loss 0.263488, acc [0.97589324 0.9640501  0.95429927 0.93394668] \n",
      "\n",
      "2019-03-03T11:59:32.621372: step 6240, loss 0.100056, acc [0.97436523 0.96977539 0.96733398 0.93066406]\n",
      "2019-03-03T11:59:36.931427: step 6280, loss 0.0582485, acc [0.98369141 0.98120117 0.98168945 0.95668945]\n",
      "2019-03-03T11:59:41.108112: step 6320, loss 0.0563354, acc [0.98447266 0.98022461 0.98305664 0.95703125]\n",
      "2019-03-03T11:59:45.269498: step 6360, loss 0.0582086, acc [0.98300781 0.98134766 0.98120117 0.95463867]\n",
      "2019-03-03T11:59:49.409055: step 6400, loss 0.061126, acc [0.98320312 0.98056641 0.9793457  0.95332031]\n",
      "\n",
      "Evaluation:\n",
      "2019-03-03T11:59:56.686477: step 6400, loss 0.276686, acc [0.97703451 0.96552173 0.95536045 0.93707015] \n",
      "\n",
      "2019-03-03T12:00:00.295374: step 6440, loss 0.0593569, acc [0.984375   0.98139648 0.98017578 0.95605469]\n",
      "2019-03-03T12:00:04.418470: step 6480, loss 0.0596463, acc [0.9840332  0.97880859 0.98051758 0.95273438]\n",
      "2019-03-03T12:00:08.524466: step 6520, loss 0.0631385, acc [0.98320312 0.97973633 0.97978516 0.95322266]\n",
      "2019-03-03T12:00:12.646415: step 6560, loss 0.0675952, acc [0.9824707  0.97929687 0.97802734 0.95141602]\n",
      "2019-03-03T12:00:16.762976: step 6600, loss 0.0707472, acc [0.9828125  0.97929687 0.97646484 0.94990234]\n",
      "\n",
      "Evaluation:\n",
      "2019-03-03T12:00:24.007814: step 6600, loss 0.277631, acc [0.97655397 0.96502117 0.95574087 0.93694   ] \n",
      "\n",
      "2019-03-03T12:00:27.629559: step 6640, loss 0.0758012, acc [0.98027344 0.97719727 0.9765625  0.94672852]\n",
      "2019-03-03T12:00:31.831363: step 6680, loss 0.0721667, acc [0.98095703 0.97666016 0.97509766 0.94550781]\n",
      "2019-03-03T12:00:36.073773: step 6720, loss 0.071469, acc [0.98022461 0.97514648 0.97675781 0.94462891]\n",
      "2019-03-03T12:00:40.200646: step 6760, loss 0.0801691, acc [0.98017578 0.97714844 0.97543945 0.94560547]\n",
      "2019-03-03T12:00:44.420284: step 6800, loss 0.0810885, acc [0.9784668  0.97470703 0.97314453 0.94116211]\n",
      "\n",
      "Evaluation:\n",
      "2019-03-03T12:00:51.638355: step 6800, loss 0.273732, acc [0.97714463 0.96555176 0.95500005 0.93668973] \n",
      "\n",
      "2019-03-03T12:00:55.256134: step 6840, loss 0.0829045, acc [0.98007813 0.97451172 0.9734375  0.94135742]\n",
      "2019-03-03T12:00:59.361200: step 6880, loss 0.0862884, acc [0.97944336 0.97539062 0.97216797 0.94262695]\n",
      "2019-03-03T12:01:03.475023: step 6920, loss 0.0854117, acc [0.97822266 0.97387695 0.971875   0.93950195]\n",
      "2019-03-03T12:01:07.624956: step 6960, loss 0.0817478, acc [0.98022461 0.97583008 0.97265625 0.94331055]\n",
      "2019-03-03T12:01:11.738282: step 7000, loss 0.0871681, acc [0.98037109 0.97431641 0.97172852 0.94189453]\n",
      "\n",
      "Evaluation:\n",
      "2019-03-03T12:01:18.973378: step 7000, loss 0.272464, acc [0.97752505 0.96602228 0.95515022 0.93740051] \n",
      "\n",
      "2019-03-03T12:01:22.721648: step 7040, loss 0.0719982, acc [0.98149414 0.9777832  0.97719727 0.94916992]\n",
      "2019-03-03T12:01:26.882115: step 7080, loss 0.0459058, acc [0.98486328 0.98417969 0.98544922 0.96191406]\n",
      "2019-03-03T12:01:31.005717: step 7120, loss 0.054274, acc [0.98413086 0.98242188 0.98291016 0.95888672]\n",
      "2019-03-03T12:01:35.107027: step 7160, loss 0.0503558, acc [0.98510742 0.9831543  0.98364258 0.95874023]\n",
      "2019-03-03T12:01:39.266950: step 7200, loss 0.0553454, acc [0.98496094 0.98110352 0.98276367 0.95751953]\n",
      "\n",
      "Evaluation:\n",
      "2019-03-03T12:01:46.507517: step 7200, loss 0.287229, acc [0.97742494 0.96466077 0.95567079 0.93679985] \n",
      "\n",
      "2019-03-03T12:01:50.129759: step 7240, loss 0.0550556, acc [0.98486328 0.98193359 0.98134766 0.95605469]\n",
      "2019-03-03T12:01:54.289117: step 7280, loss 0.0587619, acc [0.98452148 0.98076172 0.98129883 0.95576172]\n",
      "2019-03-03T12:01:58.400093: step 7320, loss 0.0597371, acc [0.98354492 0.98022461 0.97973633 0.95297852]\n",
      "2019-03-03T12:02:02.519485: step 7360, loss 0.0566806, acc [0.98325195 0.98056641 0.98178711 0.95458984]\n",
      "2019-03-03T12:02:06.691000: step 7400, loss 0.0643069, acc [0.98193359 0.98007813 0.97915039 0.95175781]\n",
      "\n",
      "Evaluation:\n",
      "2019-03-03T12:02:13.991127: step 7400, loss 0.287803, acc [0.97782539 0.96598224 0.95578092 0.93788105] \n",
      "\n",
      "2019-03-03T12:02:17.605477: step 7440, loss 0.0593679, acc [0.9828125  0.98007813 0.98154297 0.95449219]\n",
      "2019-03-03T12:02:21.780376: step 7480, loss 0.0696127, acc [0.98334961 0.97973633 0.97763672 0.9519043 ]\n",
      "2019-03-03T12:02:26.117399: step 7520, loss 0.0751683, acc [0.98081055 0.97817383 0.975      0.9472168 ]\n",
      "2019-03-03T12:02:30.279383: step 7560, loss 0.0722716, acc [0.98144531 0.97788086 0.97597656 0.94785156]\n",
      "2019-03-03T12:02:34.400064: step 7600, loss 0.0680671, acc [0.98154297 0.97851562 0.97744141 0.94902344]\n",
      "\n",
      "Evaluation:\n",
      "2019-03-03T12:02:41.700982: step 7600, loss 0.284863, acc [0.9777453  0.96661294 0.95613131 0.93911241] \n",
      "\n",
      "2019-03-03T12:02:45.329716: step 7640, loss 0.0705507, acc [0.98173828 0.97612305 0.97700195 0.94804687]\n",
      "2019-03-03T12:02:49.456561: step 7680, loss 0.0740638, acc [0.98276367 0.97802734 0.97475586 0.94750977]\n",
      "2019-03-03T12:02:53.573168: step 7720, loss 0.0800064, acc [0.9793457  0.97705078 0.97573242 0.94750977]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-03T12:02:57.738900: step 7760, loss 0.0754239, acc [0.98149414 0.97578125 0.97548828 0.94555664]\n",
      "2019-03-03T12:03:01.851905: step 7800, loss 0.0809063, acc [0.98012695 0.97348633 0.97363281 0.94262695]\n",
      "\n",
      "Evaluation:\n",
      "2019-03-03T12:03:09.082110: step 7800, loss 0.280451, acc [0.97798556 0.96650282 0.95607124 0.93857181] \n",
      "\n",
      "2019-03-03T12:03:12.836286: step 7840, loss 0.0454402, acc [0.98769531 0.98452148 0.98481445 0.96313477]\n",
      "2019-03-03T12:03:16.984355: step 7880, loss 0.0462592, acc [0.98613281 0.98515625 0.9855957  0.96342773]\n",
      "2019-03-03T12:03:21.185456: step 7920, loss 0.0480802, acc [0.98666992 0.98295898 0.98481445 0.96245117]\n",
      "2019-03-03T12:03:25.380206: step 7960, loss 0.0521463, acc [0.98413086 0.98276367 0.9831543  0.95776367]\n",
      "2019-03-03T12:03:29.488617: step 8000, loss 0.049319, acc [0.98583984 0.98374023 0.98476562 0.96245117]\n",
      "\n",
      "Evaluation:\n",
      "2019-03-03T12:03:36.742882: step 8000, loss 0.290258, acc [0.97782539 0.96734375 0.95724254 0.94028371] \n",
      "\n",
      "2019-03-03T12:03:40.352809: step 8040, loss 0.0487387, acc [0.98500977 0.98339844 0.984375   0.9612793 ]\n",
      "2019-03-03T12:03:44.564798: step 8080, loss 0.0490761, acc [0.98535156 0.98515625 0.98354492 0.96074219]\n",
      "2019-03-03T12:03:48.730204: step 8120, loss 0.0487847, acc [0.98486328 0.98193359 0.98457031 0.95917969]\n",
      "2019-03-03T12:03:52.874758: step 8160, loss 0.0584903, acc [0.98491211 0.98261719 0.9809082  0.95727539]\n",
      "2019-03-03T12:03:56.994035: step 8200, loss 0.0602892, acc [0.98525391 0.9815918  0.98017578 0.95620117]\n",
      "\n",
      "Evaluation:\n",
      "2019-03-03T12:04:04.209050: step 8200, loss 0.299992, acc [0.97770525 0.96668302 0.95700227 0.93950285] \n",
      "\n",
      "2019-03-03T12:04:07.829356: step 8240, loss 0.0635914, acc [0.98447266 0.97993164 0.97973633 0.95410156]\n",
      "2019-03-03T12:04:12.002205: step 8280, loss 0.060912, acc [0.98388672 0.98046875 0.97949219 0.95424805]\n",
      "2019-03-03T12:04:16.116766: step 8320, loss 0.0614787, acc [0.98359375 0.98027344 0.98066406 0.95483398]\n",
      "2019-03-03T12:04:20.275823: step 8360, loss 0.0630642, acc [0.98330078 0.98066406 0.97949219 0.95332031]\n",
      "2019-03-03T12:04:24.380214: step 8400, loss 0.0641633, acc [0.98334961 0.97905273 0.97924805 0.95161133]\n",
      "\n",
      "Evaluation:\n",
      "2019-03-03T12:04:31.659698: step 8400, loss 0.289683, acc [0.97858623 0.96703341 0.95768303 0.94068416] \n",
      "\n",
      "2019-03-03T12:04:35.281489: step 8440, loss 0.0669295, acc [0.98320312 0.98022461 0.97773438 0.95200195]\n",
      "2019-03-03T12:04:39.420691: step 8480, loss 0.0696436, acc [0.98256836 0.97944336 0.97597656 0.94887695]\n",
      "2019-03-03T12:04:43.605099: step 8520, loss 0.0674149, acc [0.98325195 0.97866211 0.97797852 0.9496582 ]\n",
      "2019-03-03T12:04:47.738373: step 8560, loss 0.073525, acc [0.98173828 0.97714844 0.97612305 0.94819336]\n",
      "2019-03-03T12:04:52.037338: step 8600, loss 0.0559093, acc [0.98535156 0.98217773 0.98173828 0.95883789]\n",
      "\n",
      "Evaluation:\n",
      "2019-03-03T12:04:59.253611: step 8600, loss 0.287827, acc [0.97871638 0.96785432 0.95744276 0.94142498] \n",
      "\n",
      "2019-03-03T12:05:02.869947: step 8640, loss 0.0428015, acc [0.98769531 0.98618164 0.98608398 0.96630859]\n",
      "2019-03-03T12:05:06.989028: step 8680, loss 0.0438547, acc [0.98701172 0.98637695 0.98520508 0.96494141]\n",
      "2019-03-03T12:05:11.172112: step 8720, loss 0.0420909, acc [0.98632812 0.9859375  0.98652344 0.96464844]\n",
      "2019-03-03T12:05:15.297344: step 8760, loss 0.043246, acc [0.98647461 0.98588867 0.9859375  0.96450195]\n",
      "2019-03-03T12:05:19.476930: step 8800, loss 0.0452074, acc [0.98613281 0.98481445 0.98574219 0.96376953]\n",
      "\n",
      "Evaluation:\n",
      "2019-03-03T12:05:26.744731: step 8800, loss 0.298823, acc [0.97895664 0.96718357 0.95780316 0.94142498] \n",
      "\n",
      "2019-03-03T12:05:30.368508: step 8840, loss 0.0477592, acc [0.98662109 0.98461914 0.9847168  0.96279297]\n",
      "2019-03-03T12:05:34.492922: step 8880, loss 0.0476099, acc [0.98803711 0.98320312 0.9847168  0.96362305]\n",
      "2019-03-03T12:05:38.611890: step 8920, loss 0.0454937, acc [0.98720703 0.98325195 0.98452148 0.96186523]\n",
      "2019-03-03T12:05:42.728285: step 8960, loss 0.0536937, acc [0.98574219 0.9824707  0.98276367 0.96015625]\n",
      "2019-03-03T12:05:46.849761: step 9000, loss 0.0507247, acc [0.98520508 0.98271484 0.98383789 0.95947266]\n",
      "\n",
      "Evaluation:\n",
      "2019-03-03T12:05:54.109796: step 9000, loss 0.299912, acc [0.97847611 0.96789436 0.95781317 0.94221586] \n",
      "\n",
      "2019-03-03T12:05:57.731039: step 9040, loss 0.0540283, acc [0.98422852 0.98168945 0.98295898 0.95786133]\n",
      "2019-03-03T12:06:01.908067: step 9080, loss 0.0585078, acc [0.98549805 0.98129883 0.98110352 0.95615234]\n",
      "2019-03-03T12:06:06.042707: step 9120, loss 0.0576595, acc [0.98632812 0.98115234 0.98129883 0.95693359]\n",
      "2019-03-03T12:06:10.224057: step 9160, loss 0.0598862, acc [0.9847168  0.97998047 0.98085937 0.9559082 ]\n",
      "2019-03-03T12:06:14.450866: step 9200, loss 0.0580352, acc [0.98349609 0.98256836 0.98168945 0.95600586]\n",
      "\n",
      "Evaluation:\n",
      "2019-03-03T12:06:21.738765: step 9200, loss 0.299618, acc [0.97935709 0.96796444 0.95748281 0.94250618] \n",
      "\n",
      "2019-03-03T12:06:25.370475: step 9240, loss 0.0626489, acc [0.98520508 0.97973633 0.97905273 0.95415039]\n",
      "2019-03-03T12:06:29.567068: step 9280, loss 0.0643062, acc [0.98198242 0.9800293  0.97880859 0.95107422]\n",
      "2019-03-03T12:06:33.746478: step 9320, loss 0.0616918, acc [0.98447266 0.98110352 0.98071289 0.95595703]\n",
      "2019-03-03T12:06:37.918495: step 9360, loss 0.060083, acc [0.98417969 0.98071289 0.97954102 0.95395508]\n",
      "2019-03-03T12:06:42.226907: step 9400, loss 0.035643, acc [0.98764648 0.98691406 0.98945313 0.96904297]\n",
      "\n",
      "Evaluation:\n",
      "2019-03-03T12:06:49.483419: step 9400, loss 0.295266, acc [0.97980759 0.96899559 0.95941495 0.94476869] \n",
      "\n",
      "2019-03-03T12:06:53.100709: step 9440, loss 0.0356641, acc [0.9894043  0.98828125 0.98876953 0.97128906]\n",
      "2019-03-03T12:06:57.266100: step 9480, loss 0.0353007, acc [0.98774414 0.98793945 0.98847656 0.96948242]\n",
      "2019-03-03T12:07:01.435131: step 9520, loss 0.0376042, acc [0.98881836 0.98710937 0.98798828 0.96918945]\n",
      "2019-03-03T12:07:05.560692: step 9560, loss 0.043567, acc [0.98745117 0.98564453 0.98720703 0.96606445]\n",
      "2019-03-03T12:07:09.740524: step 9600, loss 0.0412704, acc [0.98769531 0.98632812 0.98632812 0.96679688]\n",
      "\n",
      "Evaluation:\n",
      "2019-03-03T12:07:16.991915: step 9600, loss 0.304899, acc [0.97942716 0.96948613 0.95887435 0.94437826] \n",
      "\n",
      "2019-03-03T12:07:20.610767: step 9640, loss 0.0407934, acc [0.98789063 0.98671875 0.98569336 0.96665039]\n",
      "2019-03-03T12:07:24.760618: step 9680, loss 0.0431474, acc [0.98691406 0.98354492 0.9859375  0.96342773]\n",
      "2019-03-03T12:07:28.872529: step 9720, loss 0.0470576, acc [0.98652344 0.98476562 0.98442383 0.96328125]\n",
      "2019-03-03T12:07:32.981669: step 9760, loss 0.0450632, acc [0.98671875 0.98549805 0.9859375  0.96542969]\n",
      "2019-03-03T12:07:37.155412: step 9800, loss 0.0434442, acc [0.98857422 0.98432617 0.98544922 0.96503906]\n",
      "\n",
      "Evaluation:\n",
      "2019-03-03T12:07:44.444053: step 9800, loss 0.307822, acc [0.97915686 0.9692759  0.95847391 0.94358738] \n",
      "\n",
      "2019-03-03T12:07:48.065347: step 9840, loss 0.0507304, acc [0.98676758 0.98398438 0.98344727 0.96171875]\n",
      "2019-03-03T12:07:52.258049: step 9880, loss 0.0527563, acc [0.98686523 0.98398438 0.98300781 0.9609375 ]\n",
      "2019-03-03T12:07:56.374342: step 9920, loss 0.0495914, acc [0.98525391 0.98305664 0.98310547 0.95947266]\n",
      "2019-03-03T12:08:00.486585: step 9960, loss 0.0526312, acc [0.98569336 0.9828125  0.98208008 0.95908203]\n",
      "2019-03-03T12:08:04.653558: step 10000, loss 0.0489132, acc [0.98681641 0.98325195 0.98417969 0.96230469]\n",
      "\n",
      "Evaluation:\n",
      "2019-03-03T12:08:11.873425: step 10000, loss 0.311458, acc [0.9794572  0.968465   0.95726256 0.94233599] \n",
      "\n",
      "2019-03-03T12:08:15.485252: step 10040, loss 0.0552356, acc [0.98486328 0.98222656 0.98198242 0.95732422]\n",
      "2019-03-03T12:08:19.665627: step 10080, loss 0.0563596, acc [0.98632812 0.98325195 0.98149414 0.95849609]\n",
      "2019-03-03T12:08:23.850358: step 10120, loss 0.0592063, acc [0.98510742 0.98178711 0.9809082  0.95712891]\n",
      "2019-03-03T12:08:28.096016: step 10160, loss 0.0486747, acc [0.98579102 0.98427734 0.98452148 0.96225586]\n",
      "2019-03-03T12:08:32.213310: step 10200, loss 0.0343324, acc [0.98833008 0.98891602 0.98911133 0.97202148]\n",
      "\n",
      "Evaluation:\n",
      "2019-03-03T12:08:39.474408: step 10200, loss 0.306862, acc [0.97984763 0.97005676 0.96010572 0.94633043] \n",
      "\n",
      "2019-03-03T12:08:43.101160: step 10240, loss 0.033077, acc [0.98959961 0.98808594 0.99042969 0.97304687]\n",
      "2019-03-03T12:08:47.207542: step 10280, loss 0.0355195, acc [0.98862305 0.9878418  0.9887207  0.97055664]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-03T12:08:51.347798: step 10320, loss 0.0338035, acc [0.98930664 0.98828125 0.98916016 0.97119141]\n",
      "2019-03-03T12:08:55.456800: step 10360, loss 0.0361197, acc [0.9875     0.98681641 0.98891602 0.96879883]\n",
      "2019-03-03T12:08:59.635312: step 10400, loss 0.0416031, acc [0.98701172 0.98676758 0.98632812 0.96591797]\n",
      "\n",
      "Evaluation:\n",
      "2019-03-03T12:09:06.958170: step 10400, loss 0.319137, acc [0.97908679 0.96921583 0.95902452 0.94403788] \n",
      "\n",
      "2019-03-03T12:09:10.574008: step 10440, loss 0.0417319, acc [0.98867187 0.98701172 0.98686523 0.96811523]\n",
      "2019-03-03T12:09:14.706676: step 10480, loss 0.0408126, acc [0.98857422 0.98603516 0.98720703 0.96728516]\n",
      "2019-03-03T12:09:18.835145: step 10520, loss 0.0437618, acc [0.98920898 0.98706055 0.98549805 0.9684082 ]\n",
      "2019-03-03T12:09:23.046674: step 10560, loss 0.0456022, acc [0.98833008 0.98510742 0.98554688 0.96538086]\n",
      "2019-03-03T12:09:27.161244: step 10600, loss 0.039812, acc [0.98740234 0.98510742 0.98740234 0.96591797]\n",
      "\n",
      "Evaluation:\n",
      "2019-03-03T12:09:34.441190: step 10600, loss 0.315244, acc [0.9799978  0.96921583 0.95994554 0.94627036] \n",
      "\n",
      "2019-03-03T12:09:38.054548: step 10640, loss 0.0448212, acc [0.98725586 0.98505859 0.98505859 0.96430664]\n",
      "2019-03-03T12:09:42.161593: step 10680, loss 0.0444532, acc [0.98662109 0.98632812 0.98579102 0.96547852]\n",
      "2019-03-03T12:09:46.260331: step 10720, loss 0.0489505, acc [0.98886719 0.98359375 0.98325195 0.96367187]\n",
      "2019-03-03T12:09:50.451129: step 10760, loss 0.047728, acc [0.98647461 0.98549805 0.98447266 0.96357422]\n",
      "2019-03-03T12:09:54.584391: step 10800, loss 0.049129, acc [0.98657227 0.98417969 0.98388672 0.96152344]\n",
      "\n",
      "Evaluation:\n",
      "2019-03-03T12:10:01.801853: step 10800, loss 0.311495, acc [0.97982761 0.96955621 0.95974532 0.94554956] \n",
      "\n",
      "2019-03-03T12:10:05.418685: step 10840, loss 0.049663, acc [0.98686523 0.98334961 0.98427734 0.96201172]\n",
      "2019-03-03T12:10:09.560955: step 10880, loss 0.0482525, acc [0.98706055 0.98310547 0.98417969 0.96293945]\n",
      "2019-03-03T12:10:13.721627: step 10920, loss 0.0536206, acc [0.98618164 0.98178711 0.98276367 0.95883789]\n",
      "2019-03-03T12:10:17.959448: step 10960, loss 0.0319035, acc [0.99003906 0.98881836 0.98994141 0.97387695]\n",
      "2019-03-03T12:10:22.091278: step 11000, loss 0.0319032, acc [0.99033203 0.98911133 0.99008789 0.97373047]\n",
      "\n",
      "Evaluation:\n",
      "2019-03-03T12:10:29.371060: step 11000, loss 0.318023, acc [0.97985764 0.96986655 0.96045611 0.94619027] \n",
      "\n",
      "2019-03-03T12:10:33.025586: step 11040, loss 0.0315281, acc [0.98886719 0.98793945 0.98989258 0.97075195]\n",
      "2019-03-03T12:10:37.363402: step 11080, loss 0.0319659, acc [0.98813477 0.98823242 0.99082031 0.97177734]\n",
      "2019-03-03T12:10:41.480523: step 11120, loss 0.0358764, acc [0.98886719 0.98842773 0.98818359 0.97089844]\n",
      "2019-03-03T12:10:45.672506: step 11160, loss 0.0334185, acc [0.98999023 0.98828125 0.9887207  0.97099609]\n",
      "2019-03-03T12:10:49.804184: step 11200, loss 0.0350809, acc [0.98950195 0.98808594 0.98847656 0.97104492]\n",
      "\n",
      "Evaluation:\n",
      "2019-03-03T12:10:57.142052: step 11200, loss 0.326185, acc [0.98000781 0.96932595 0.96011573 0.94598004] \n",
      "\n",
      "2019-03-03T12:11:00.757845: step 11240, loss 0.0358257, acc [0.98891602 0.98666992 0.98842773 0.96943359]\n",
      "2019-03-03T12:11:04.946068: step 11280, loss 0.041966, acc [0.98916016 0.98681641 0.98569336 0.96669922]\n",
      "2019-03-03T12:11:09.061700: step 11320, loss 0.0391599, acc [0.98808594 0.98706055 0.98759766 0.96845703]\n",
      "2019-03-03T12:11:13.237965: step 11360, loss 0.0407499, acc [0.98759766 0.98623047 0.98676758 0.96699219]\n",
      "2019-03-03T12:11:17.359972: step 11400, loss 0.0407286, acc [0.98876953 0.9859375  0.98769531 0.96743164]\n",
      "\n",
      "Evaluation:\n",
      "2019-03-03T12:11:24.656186: step 11400, loss 0.329577, acc [0.98009791 0.96901561 0.95982541 0.94571975] \n",
      "\n",
      "2019-03-03T12:11:28.276488: step 11440, loss 0.044629, acc [0.98828125 0.98564453 0.9855957  0.96582031]\n",
      "2019-03-03T12:11:32.382250: step 11480, loss 0.0455337, acc [0.98803711 0.98427734 0.98569336 0.96479492]\n",
      "2019-03-03T12:11:36.497346: step 11520, loss 0.0470102, acc [0.98823242 0.98481445 0.98476562 0.96450195]\n",
      "2019-03-03T12:11:40.697820: step 11560, loss 0.0496862, acc [0.98828125 0.98466797 0.98320312 0.96391602]\n",
      "2019-03-03T12:11:44.805505: step 11600, loss 0.0499095, acc [0.98662109 0.98515625 0.98413086 0.96298828]\n",
      "\n",
      "Evaluation:\n",
      "2019-03-03T12:11:52.043135: step 11600, loss 0.319866, acc [0.97974752 0.96937601 0.95952507 0.94519917] \n",
      "\n",
      "2019-03-03T12:11:55.649056: step 11640, loss 0.0496396, acc [0.98623047 0.9840332  0.98310547 0.96064453]\n",
      "2019-03-03T12:11:59.777477: step 11680, loss 0.0524041, acc [0.98662109 0.98344727 0.98198242 0.9597168 ]\n",
      "2019-03-03T12:12:04.019763: step 11720, loss 0.03928, acc [0.98823242 0.98676758 0.98662109 0.96801758]\n",
      "2019-03-03T12:12:08.259725: step 11760, loss 0.0316347, acc [0.99003906 0.98969727 0.98959961 0.97338867]\n",
      "2019-03-03T12:12:12.416598: step 11800, loss 0.032613, acc [0.99067383 0.98964844 0.99067383 0.97509766]\n",
      "\n",
      "Evaluation:\n",
      "2019-03-03T12:12:19.649091: step 11800, loss 0.326271, acc [0.98052839 0.96983652 0.96042607 0.94696113] \n",
      "\n",
      "2019-03-03T12:12:23.272324: step 11840, loss 0.0323649, acc [0.99101562 0.98925781 0.98920898 0.97397461]\n",
      "2019-03-03T12:12:27.425468: step 11880, loss 0.0333653, acc [0.98974609 0.98798828 0.98891602 0.97128906]\n",
      "2019-03-03T12:12:31.534119: step 11920, loss 0.032624, acc [0.98979492 0.98828125 0.98979492 0.97231445]\n",
      "2019-03-03T12:12:35.636648: step 11960, loss 0.0349149, acc [0.98964844 0.98647461 0.98847656 0.96953125]\n",
      "2019-03-03T12:12:39.751980: step 12000, loss 0.037346, acc [0.98930664 0.98808594 0.98920898 0.9715332 ]\n",
      "\n",
      "Evaluation:\n",
      "2019-03-03T12:12:46.993161: step 12000, loss 0.330617, acc [0.97993773 0.9701769  0.96016578 0.94669083] \n",
      "\n",
      "2019-03-03T12:12:50.617927: step 12040, loss 0.0349439, acc [0.98974609 0.98759766 0.98886719 0.97148437]\n",
      "2019-03-03T12:12:54.813674: step 12080, loss 0.0354316, acc [0.99077148 0.98842773 0.98774414 0.97202148]\n",
      "2019-03-03T12:12:58.919255: step 12120, loss 0.0412995, acc [0.98808594 0.98823242 0.98657227 0.96811523]\n",
      "2019-03-03T12:13:03.081685: step 12160, loss 0.0388981, acc [0.98696289 0.98608398 0.98725586 0.96650391]\n",
      "2019-03-03T12:13:07.206833: step 12200, loss 0.0381369, acc [0.98876953 0.9875     0.98779297 0.96870117]\n",
      "\n",
      "Evaluation:\n",
      "2019-03-03T12:13:14.487317: step 12200, loss 0.337997, acc [0.9799978  0.97025699 0.95977535 0.94641052] \n",
      "\n",
      "2019-03-03T12:13:18.110054: step 12240, loss 0.0375307, acc [0.99033203 0.9862793  0.98701172 0.96923828]\n",
      "2019-03-03T12:13:22.234294: step 12280, loss 0.0390369, acc [0.98901367 0.98623047 0.98735352 0.96899414]\n",
      "2019-03-03T12:13:26.400693: step 12320, loss 0.0409248, acc [0.98793945 0.98701172 0.98613281 0.96694336]\n",
      "2019-03-03T12:13:30.526862: step 12360, loss 0.0442501, acc [0.98798828 0.98662109 0.98457031 0.96518555]\n",
      "2019-03-03T12:13:34.697110: step 12400, loss 0.0429274, acc [0.98740234 0.98652344 0.98623047 0.96650391]\n",
      "\n",
      "Evaluation:\n",
      "2019-03-03T12:13:41.916202: step 12400, loss 0.326819, acc [0.98030814 0.96979647 0.96030594 0.94708126] \n",
      "\n",
      "2019-03-03T12:13:45.538489: step 12440, loss 0.0427934, acc [0.98842773 0.98588867 0.98613281 0.96723633]\n",
      "2019-03-03T12:13:49.726620: step 12480, loss 0.0463587, acc [0.98833008 0.9847168  0.98476562 0.96494141]\n",
      "2019-03-03T12:13:54.020262: step 12520, loss 0.0272663, acc [0.99091797 0.98959961 0.99199219 0.97553711]\n",
      "2019-03-03T12:13:58.191769: step 12560, loss 0.0275029, acc [0.99165039 0.99042969 0.99213867 0.97753906]\n",
      "2019-03-03T12:14:02.361394: step 12600, loss 0.0252862, acc [0.99125977 0.98984375 0.99248047 0.97661133]\n",
      "\n",
      "Evaluation:\n",
      "2019-03-03T12:14:09.649225: step 12600, loss 0.340754, acc [0.98039824 0.97027701 0.9608065  0.94798226] \n",
      "\n",
      "2019-03-03T12:14:13.272483: step 12640, loss 0.0305028, acc [0.99091797 0.98754883 0.99047852 0.97363281]\n",
      "2019-03-03T12:14:21.961804: step 12680, loss 0.0314401, acc [0.99033203 0.98999023 0.98935547 0.97358398]\n",
      "2019-03-03T12:14:26.113818: step 12720, loss 0.029653, acc [0.990625   0.98896484 0.99018555 0.97392578]\n",
      "2019-03-03T12:14:30.244265: step 12760, loss 0.0338744, acc [0.99042969 0.98935547 0.98969727 0.97412109]\n",
      "2019-03-03T12:14:34.404830: step 12800, loss 0.0322247, acc [0.99082031 0.98916016 0.98945313 0.97353516]\n",
      "\n",
      "Evaluation:\n",
      "2019-03-03T12:14:41.741407: step 12800, loss 0.339448, acc [0.98052839 0.97028702 0.96061628 0.94755178] \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-03T12:14:45.373613: step 12840, loss 0.0340745, acc [0.98964844 0.98896484 0.98896484 0.97226563]\n",
      "2019-03-03T12:14:49.503158: step 12880, loss 0.0341848, acc [0.98989258 0.98857422 0.9894043  0.97104492]\n",
      "2019-03-03T12:14:53.695407: step 12920, loss 0.0350309, acc [0.99023438 0.98769531 0.98945313 0.97207031]\n",
      "2019-03-03T12:14:57.805272: step 12960, loss 0.0369219, acc [0.98803711 0.98754883 0.98823242 0.9690918 ]\n",
      "2019-03-03T12:15:02.004440: step 13000, loss 0.0382528, acc [0.98896484 0.98808594 0.98764648 0.97001953]\n",
      "\n",
      "Evaluation:\n",
      "2019-03-03T12:15:09.267395: step 13000, loss 0.343203, acc [0.98037822 0.97000671 0.96032596 0.94759183] \n",
      "\n",
      "2019-03-03T12:15:12.886211: step 13040, loss 0.038292, acc [0.98881836 0.98637695 0.98808594 0.96918945]\n",
      "2019-03-03T12:15:16.991861: step 13080, loss 0.0369931, acc [0.98774414 0.98774414 0.98774414 0.96875   ]\n",
      "2019-03-03T12:15:21.159397: step 13120, loss 0.041564, acc [0.98852539 0.98613281 0.98657227 0.9668457 ]\n",
      "2019-03-03T12:15:25.281397: step 13160, loss 0.0445606, acc [0.98813477 0.98505859 0.98554688 0.96499023]\n",
      "2019-03-03T12:15:29.397344: step 13200, loss 0.0446177, acc [0.98891602 0.98686523 0.98583984 0.9675293 ]\n",
      "\n",
      "Evaluation:\n",
      "2019-03-03T12:15:36.620093: step 13200, loss 0.333061, acc [0.98084874 0.96950615 0.9599956  0.94692108] \n",
      "\n",
      "2019-03-03T12:15:40.247837: step 13240, loss 0.0427242, acc [0.98793945 0.98666992 0.98515625 0.96640625]\n",
      "2019-03-03T12:15:44.502778: step 13280, loss 0.0371883, acc [0.98916016 0.98793945 0.98769531 0.96933594]\n",
      "2019-03-03T12:15:48.640010: step 13320, loss 0.0266958, acc [0.99204102 0.99091797 0.99135742 0.97758789]\n",
      "2019-03-03T12:15:52.813278: step 13360, loss 0.0249172, acc [0.99155273 0.99160156 0.99169922 0.9777832 ]\n",
      "2019-03-03T12:15:56.989653: step 13400, loss 0.0268204, acc [0.99135742 0.99013672 0.99174805 0.97705078]\n",
      "\n",
      "Evaluation:\n",
      "2019-03-03T12:16:04.305182: step 13400, loss 0.345371, acc [0.98076865 0.97049725 0.96095666 0.94860295] \n",
      "\n",
      "2019-03-03T12:16:07.926480: step 13440, loss 0.0258204, acc [0.99140625 0.98999023 0.99155273 0.97714844]\n",
      "2019-03-03T12:16:12.131426: step 13480, loss 0.0279502, acc [0.99121094 0.98925781 0.99155273 0.97548828]\n",
      "2019-03-03T12:16:16.253600: step 13520, loss 0.0318723, acc [0.99038086 0.98959961 0.99013672 0.97480469]\n",
      "2019-03-03T12:16:20.376560: step 13560, loss 0.0309851, acc [0.99091797 0.99003906 0.98989258 0.97446289]\n",
      "2019-03-03T12:16:24.568694: step 13600, loss 0.0297274, acc [0.99047852 0.98989258 0.99057617 0.97490234]\n",
      "\n",
      "Evaluation:\n",
      "2019-03-03T12:16:31.809223: step 13600, loss 0.343569, acc [0.98069858 0.97072751 0.96069637 0.94837269] \n",
      "\n",
      "2019-03-03T12:16:35.431015: step 13640, loss 0.0298519, acc [0.99213867 0.98867187 0.99101562 0.97519531]\n",
      "2019-03-03T12:16:39.546430: step 13680, loss 0.033032, acc [0.98896484 0.98920898 0.98916016 0.9715332 ]\n",
      "2019-03-03T12:16:43.667114: step 13720, loss 0.0335552, acc [0.98959961 0.98740234 0.98969727 0.97119141]\n",
      "2019-03-03T12:16:47.787363: step 13760, loss 0.0343723, acc [0.99003906 0.98857422 0.98891602 0.97260742]\n",
      "2019-03-03T12:16:51.907368: step 13800, loss 0.0370919, acc [0.99047852 0.98779297 0.98813477 0.97099609]\n",
      "\n",
      "Evaluation:\n",
      "2019-03-03T12:16:59.173591: step 13800, loss 0.341657, acc [0.9811691  0.97094775 0.96094665 0.94912353] \n",
      "\n",
      "2019-03-03T12:17:02.801832: step 13840, loss 0.0351325, acc [0.9890625  0.98984375 0.98857422 0.971875  ]\n",
      "2019-03-03T12:17:06.980600: step 13880, loss 0.0396463, acc [0.9894043  0.98774414 0.98730469 0.96953125]\n",
      "2019-03-03T12:17:11.135187: step 13920, loss 0.036101, acc [0.98930664 0.98623047 0.98847656 0.96904297]\n",
      "2019-03-03T12:17:15.329021: step 13960, loss 0.0361966, acc [0.98842773 0.98720703 0.98735352 0.96777344]\n",
      "2019-03-03T12:17:19.508922: step 14000, loss 0.0423222, acc [0.98935547 0.9862793  0.98618164 0.96860352]\n",
      "\n",
      "Evaluation:\n",
      "2019-03-03T12:17:26.795526: step 14000, loss 0.337497, acc [0.98120914 0.971168   0.9610768  0.94911352] \n",
      "\n",
      "2019-03-03T12:17:30.417273: step 14040, loss 0.0416823, acc [0.98828125 0.98686523 0.9859375  0.96723633]\n",
      "2019-03-03T12:17:34.665017: step 14080, loss 0.0264893, acc [0.99125977 0.99165039 0.99150391 0.97739258]\n",
      "2019-03-03T12:17:38.841334: step 14120, loss 0.0264819, acc [0.99169922 0.99086914 0.99111328 0.97719727]\n",
      "2019-03-03T12:17:42.953135: step 14160, loss 0.0267963, acc [0.99306641 0.99174805 0.99121094 0.97875977]\n",
      "2019-03-03T12:17:47.054902: step 14200, loss 0.0225995, acc [0.99145508 0.99228516 0.99223633 0.97885742]\n",
      "\n",
      "Evaluation:\n",
      "2019-03-03T12:17:54.486710: step 14200, loss 0.350692, acc [0.98171971 0.97111794 0.96186767 0.95058515] \n",
      "\n",
      "2019-03-03T12:17:58.130820: step 14240, loss 0.0246359, acc [0.99243164 0.99199219 0.99228516 0.97963867]\n",
      "2019-03-03T12:18:02.300559: step 14280, loss 0.0283334, acc [0.99155273 0.99111328 0.99130859 0.97739258]\n",
      "2019-03-03T12:18:06.489919: step 14320, loss 0.0261175, acc [0.99072266 0.99082031 0.99091797 0.97636719]\n",
      "2019-03-03T12:18:10.667154: step 14360, loss 0.0304367, acc [0.99091797 0.98979492 0.99052734 0.97504883]\n",
      "2019-03-03T12:18:14.863191: step 14400, loss 0.0328933, acc [0.99038086 0.98964844 0.99033203 0.97412109]\n",
      "\n",
      "Evaluation:\n",
      "2019-03-03T12:18:22.082416: step 14400, loss 0.355536, acc [0.98126921 0.97055732 0.96078647 0.94857292] \n",
      "\n",
      "2019-03-03T12:18:25.706688: step 14440, loss 0.0298613, acc [0.99165039 0.98925781 0.99013672 0.97431641]\n",
      "2019-03-03T12:18:29.912741: step 14480, loss 0.0309401, acc [0.9909668  0.98891602 0.99042969 0.97456055]\n",
      "2019-03-03T12:18:34.028490: step 14520, loss 0.0307824, acc [0.99194336 0.99018555 0.98964844 0.97558594]\n",
      "2019-03-03T12:18:38.147593: step 14560, loss 0.034424, acc [0.98945313 0.98852539 0.98930664 0.97216797]\n",
      "2019-03-03T12:18:42.251918: step 14600, loss 0.0361947, acc [0.99008789 0.98857422 0.98813477 0.97109375]\n",
      "\n",
      "Evaluation:\n",
      "2019-03-03T12:18:49.531131: step 14600, loss 0.352155, acc [0.98128923 0.97083763 0.96059626 0.94880317] \n",
      "\n",
      "2019-03-03T12:18:53.153418: step 14640, loss 0.0345226, acc [0.99033203 0.98896484 0.98789063 0.9715332 ]\n",
      "2019-03-03T12:18:57.283652: step 14680, loss 0.0346935, acc [0.990625   0.98959961 0.98847656 0.9730957 ]\n",
      "2019-03-03T12:19:01.484449: step 14720, loss 0.0345568, acc [0.98896484 0.98740234 0.98842773 0.96987305]\n",
      "2019-03-03T12:19:05.600121: step 14760, loss 0.0373272, acc [0.9890625  0.9887207  0.98793945 0.97050781]\n",
      "2019-03-03T12:19:09.783568: step 14800, loss 0.0351127, acc [0.9890625  0.98789063 0.98808594 0.97041016]\n",
      "\n",
      "Evaluation:\n",
      "2019-03-03T12:19:17.019214: step 14800, loss 0.349441, acc [0.98072861 0.97106789 0.96050616 0.94818248] \n",
      "\n",
      "2019-03-03T12:19:20.776911: step 14840, loss 0.0293322, acc [0.99047852 0.99013672 0.99077148 0.9753418 ]\n",
      "2019-03-03T12:19:24.948019: step 14880, loss 0.0234927, acc [0.99160156 0.99233398 0.99165039 0.97822266]\n",
      "2019-03-03T12:19:29.088870: step 14920, loss 0.0225397, acc [0.99301758 0.99228516 0.99296875 0.98046875]\n",
      "2019-03-03T12:19:33.195806: step 14960, loss 0.0252462, acc [0.99213867 0.99125977 0.99204102 0.97817383]\n",
      "2019-03-03T12:19:37.364158: step 15000, loss 0.0257184, acc [0.99248047 0.99077148 0.99174805 0.97797852]\n",
      "\n",
      "Evaluation:\n",
      "2019-03-03T12:19:44.605581: step 15000, loss 0.353118, acc [0.98094885 0.97158846 0.9617075  0.95019472] \n",
      "\n",
      "2019-03-03T12:19:48.228329: step 15040, loss 0.0254793, acc [0.99208984 0.99155273 0.99160156 0.97832031]\n",
      "2019-03-03T12:19:52.397102: step 15080, loss 0.0251824, acc [0.99150391 0.99052734 0.99155273 0.9769043 ]\n",
      "2019-03-03T12:19:56.558800: step 15120, loss 0.0244279, acc [0.99160156 0.99106445 0.99155273 0.97734375]\n",
      "2019-03-03T12:20:00.673118: step 15160, loss 0.02987, acc [0.99243164 0.98945313 0.98984375 0.9753418 ]\n",
      "2019-03-03T12:20:04.783096: step 15200, loss 0.0285801, acc [0.9902832  0.98930664 0.99067383 0.97412109]\n",
      "\n",
      "Evaluation:\n",
      "2019-03-03T12:20:12.021245: step 15200, loss 0.36578, acc [0.98096888 0.97070749 0.96168747 0.9492837 ] \n",
      "\n",
      "2019-03-03T12:20:15.635098: step 15240, loss 0.0297354, acc [0.99111328 0.98969727 0.99155273 0.97573242]\n",
      "2019-03-03T12:20:19.762902: step 15280, loss 0.0268723, acc [0.99169922 0.98896484 0.99150391 0.97519531]\n",
      "2019-03-03T12:20:23.883168: step 15320, loss 0.0259358, acc [0.99077148 0.99067383 0.99082031 0.975     ]\n",
      "2019-03-03T12:20:28.064447: step 15360, loss 0.0348402, acc [0.98984375 0.98891602 0.98862305 0.97158203]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-03T12:20:32.226939: step 15400, loss 0.0334218, acc [0.99008789 0.98823242 0.9890625  0.971875  ]\n",
      "\n",
      "Evaluation:\n",
      "2019-03-03T12:20:39.489072: step 15400, loss 0.360602, acc [0.981079   0.97126811 0.96112685 0.94959405] \n",
      "\n",
      "2019-03-03T12:20:43.103919: step 15440, loss 0.0273426, acc [0.99116211 0.98999023 0.99111328 0.97548828]\n",
      "2019-03-03T12:20:47.279243: step 15480, loss 0.0296015, acc [0.99111328 0.98857422 0.99003906 0.97392578]\n",
      "2019-03-03T12:20:51.391217: step 15520, loss 0.029967, acc [0.99091797 0.98964844 0.98974609 0.97392578]\n",
      "2019-03-03T12:20:55.564957: step 15560, loss 0.0371135, acc [0.99052734 0.98925781 0.98793945 0.97250977]\n",
      "2019-03-03T12:20:59.712643: step 15600, loss 0.0396809, acc [0.9887207  0.98769531 0.98808594 0.9703125 ]\n",
      "\n",
      "Evaluation:\n",
      "2019-03-03T12:21:07.102947: step 15600, loss 0.354453, acc [0.98076865 0.97046722 0.96126701 0.94906346] \n",
      "\n",
      "2019-03-03T12:21:10.865112: step 15640, loss 0.0225059, acc [0.99267578 0.99208984 0.99306641 0.98017578]\n",
      "2019-03-03T12:21:14.997106: step 15680, loss 0.0225586, acc [0.99233398 0.9921875  0.99379883 0.98056641]\n",
      "2019-03-03T12:21:19.186444: step 15720, loss 0.0214221, acc [0.99282227 0.99189453 0.99335938 0.9800293 ]\n",
      "2019-03-03T12:21:23.302984: step 15760, loss 0.0256342, acc [0.99262695 0.99101562 0.99145508 0.97841797]\n",
      "2019-03-03T12:21:27.491212: step 15800, loss 0.0230428, acc [0.99223633 0.99189453 0.99174805 0.97910156]\n",
      "\n",
      "Evaluation:\n",
      "2019-03-03T12:21:34.713245: step 15800, loss 0.365094, acc [0.98120914 0.97154842 0.96147724 0.95041496] \n",
      "\n",
      "2019-03-03T12:21:38.351900: step 15840, loss 0.0241379, acc [0.99331055 0.99140625 0.99238281 0.97958984]\n",
      "2019-03-03T12:21:42.465540: step 15880, loss 0.024611, acc [0.99023438 0.99174805 0.99257812 0.97792969]\n",
      "2019-03-03T12:21:46.657603: step 15920, loss 0.0259619, acc [0.99169922 0.99165039 0.99165039 0.97841797]\n",
      "2019-03-03T12:21:50.832993: step 15960, loss 0.0270194, acc [0.99121094 0.990625   0.99101562 0.97612305]\n",
      "2019-03-03T12:21:55.013659: step 16000, loss 0.0235371, acc [0.99316406 0.99140625 0.99189453 0.97978516]\n",
      "\n",
      "Evaluation:\n",
      "2019-03-03T12:22:02.273336: step 16000, loss 0.370046, acc [0.98079869 0.97173863 0.96171751 0.94997447] \n",
      "\n",
      "2019-03-03T12:22:05.895078: step 16040, loss 0.0282349, acc [0.99140625 0.990625   0.99135742 0.97695312]\n",
      "2019-03-03T12:22:10.021433: step 16080, loss 0.0248958, acc [0.99135742 0.99091797 0.99262695 0.978125  ]\n",
      "2019-03-03T12:22:14.215585: step 16120, loss 0.0262465, acc [0.99023438 0.99057617 0.99189453 0.97646484]\n",
      "2019-03-03T12:22:18.388887: step 16160, loss 0.0317205, acc [0.99194336 0.98950195 0.98935547 0.97509766]\n",
      "2019-03-03T12:22:22.567893: step 16200, loss 0.0287522, acc [0.99165039 0.98969727 0.99111328 0.9769043 ]\n",
      "\n",
      "Evaluation:\n",
      "2019-03-03T12:22:29.840077: step 16200, loss 0.364778, acc [0.98150948 0.97151839 0.96168747 0.95000451] \n",
      "\n",
      "2019-03-03T12:22:33.456861: step 16240, loss 0.0333724, acc [0.99194336 0.98828125 0.98896484 0.97363281]\n",
      "2019-03-03T12:22:37.646695: step 16280, loss 0.0299515, acc [0.99125977 0.98876953 0.99038086 0.97412109]\n",
      "2019-03-03T12:22:41.824798: step 16320, loss 0.0329122, acc [0.99091797 0.98969727 0.98969727 0.97382813]\n",
      "2019-03-03T12:22:45.938592: step 16360, loss 0.0367463, acc [0.99077148 0.98959961 0.9875     0.97255859]\n",
      "2019-03-03T12:22:50.183209: step 16400, loss 0.0275702, acc [0.99165039 0.99072266 0.99111328 0.97768555]\n",
      "\n",
      "Evaluation:\n",
      "2019-03-03T12:22:57.399997: step 16400, loss 0.361645, acc [0.98150948 0.97118802 0.9617075  0.94988437] \n",
      "\n",
      "2019-03-03T12:23:01.021261: step 16440, loss 0.020219, acc [0.99223633 0.9921875  0.99331055 0.98056641]\n",
      "2019-03-03T12:23:05.196702: step 16480, loss 0.0199056, acc [0.99316406 0.99296875 0.99331055 0.98178711]\n",
      "2019-03-03T12:23:09.414544: step 16520, loss 0.0211436, acc [0.9921875  0.99091797 0.99311523 0.97910156]\n",
      "2019-03-03T12:23:13.510635: step 16560, loss 0.0215526, acc [0.99331055 0.99257812 0.99301758 0.98105469]\n",
      "2019-03-03T12:23:17.682688: step 16600, loss 0.0227105, acc [0.99306641 0.99223633 0.99233398 0.98012695]\n",
      "\n",
      "Evaluation:\n",
      "2019-03-03T12:23:24.969811: step 16600, loss 0.370951, acc [0.98200002 0.97130815 0.96218803 0.95122586] \n",
      "\n",
      "2019-03-03T12:23:28.586144: step 16640, loss 0.0237893, acc [0.99272461 0.99106445 0.99228516 0.97875977]\n",
      "2019-03-03T12:23:32.717318: step 16680, loss 0.0252202, acc [0.99296875 0.9909668  0.99165039 0.97836914]\n",
      "2019-03-03T12:23:36.836754: step 16720, loss 0.0247643, acc [0.99291992 0.99082031 0.99140625 0.9784668 ]\n",
      "2019-03-03T12:23:40.955059: step 16760, loss 0.0245485, acc [0.99233398 0.990625   0.99189453 0.97792969]\n",
      "2019-03-03T12:23:45.074817: step 16800, loss 0.0247681, acc [0.9921875  0.99165039 0.99121094 0.97832031]\n",
      "\n",
      "Evaluation:\n",
      "2019-03-03T12:23:52.303050: step 16800, loss 0.377225, acc [0.98172972 0.97131816 0.96201784 0.95094555] \n",
      "\n",
      "2019-03-03T12:23:55.924298: step 16840, loss 0.0260109, acc [0.99238281 0.99111328 0.99145508 0.97749023]\n",
      "2019-03-03T12:24:00.025791: step 16880, loss 0.0304812, acc [0.99101562 0.9894043  0.99003906 0.97460938]\n",
      "2019-03-03T12:24:04.152010: step 16920, loss 0.0262816, acc [0.9918457  0.99174805 0.99121094 0.97719727]\n",
      "2019-03-03T12:24:08.268222: step 16960, loss 0.0268404, acc [0.99125977 0.99018555 0.99072266 0.97607422]\n",
      "2019-03-03T12:24:12.379067: step 17000, loss 0.0307494, acc [0.99047852 0.98925781 0.99042969 0.97446289]\n",
      "\n",
      "Evaluation:\n",
      "2019-03-03T12:24:19.668257: step 17000, loss 0.373878, acc [0.98185986 0.97118802 0.96178758 0.95054511] \n",
      "\n",
      "2019-03-03T12:24:23.280024: step 17040, loss 0.032523, acc [0.99018555 0.98964844 0.9894043  0.97412109]\n",
      "2019-03-03T12:24:27.405529: step 17080, loss 0.0297477, acc [0.99145508 0.98959961 0.99072266 0.97602539]\n",
      "2019-03-03T12:24:31.574682: step 17120, loss 0.0289714, acc [0.99135742 0.99038086 0.99023438 0.97578125]\n",
      "2019-03-03T12:24:35.762662: step 17160, loss 0.031796, acc [0.99155273 0.98950195 0.99003906 0.97548828]\n",
      "2019-03-03T12:24:40.029680: step 17200, loss 0.0240754, acc [0.99394531 0.99316406 0.99194336 0.98066406]\n",
      "\n",
      "Evaluation:\n",
      "2019-03-03T12:24:47.300549: step 17200, loss 0.374811, acc [0.9817097  0.97178869 0.96191773 0.95119583] \n",
      "\n",
      "2019-03-03T12:24:50.912867: step 17240, loss 0.019416, acc [0.99311523 0.99301758 0.99326172 0.98178711]\n",
      "2019-03-03T12:24:55.034088: step 17280, loss 0.0191595, acc [0.99331055 0.9934082  0.9940918  0.98295898]\n",
      "2019-03-03T12:24:59.200626: step 17320, loss 0.0193937, acc [0.99331055 0.99267578 0.99423828 0.98232422]\n",
      "2019-03-03T12:25:03.368107: step 17360, loss 0.0190228, acc [0.99345703 0.99160156 0.99365234 0.98061523]\n",
      "2019-03-03T12:25:07.495063: step 17400, loss 0.0223235, acc [0.99350586 0.99321289 0.99282227 0.98193359]\n",
      "\n",
      "Evaluation:\n",
      "2019-03-03T12:25:14.742810: step 17400, loss 0.379943, acc [0.98113906 0.97166855 0.96159737 0.95016468] \n",
      "\n",
      "2019-03-03T12:25:18.360633: step 17440, loss 0.0236341, acc [0.99326172 0.99165039 0.99233398 0.97998047]\n",
      "2019-03-03T12:25:22.538858: step 17480, loss 0.0242873, acc [0.99306641 0.99257812 0.99208984 0.98081055]\n",
      "2019-03-03T12:25:26.655656: step 17520, loss 0.0279716, acc [0.99189453 0.99135742 0.99086914 0.97749023]\n",
      "2019-03-03T12:25:30.765499: step 17560, loss 0.0267274, acc [0.99199219 0.99179688 0.99057617 0.97773438]\n",
      "2019-03-03T12:25:34.928504: step 17600, loss 0.0262575, acc [0.99204102 0.99155273 0.99160156 0.9784668 ]\n",
      "\n",
      "Evaluation:\n",
      "2019-03-03T12:25:42.238154: step 17600, loss 0.382284, acc [0.98154952 0.97164853 0.96136712 0.95066524] \n",
      "\n",
      "2019-03-03T12:25:45.846558: step 17640, loss 0.0277596, acc [0.99282227 0.99082031 0.99082031 0.97807617]\n",
      "2019-03-03T12:25:50.013840: step 17680, loss 0.0296571, acc [0.99204102 0.99121094 0.9902832  0.97709961]\n",
      "2019-03-03T12:25:54.181234: step 17720, loss 0.024748, acc [0.99204102 0.990625   0.99223633 0.97724609]\n",
      "2019-03-03T12:25:58.348230: step 17760, loss 0.0290633, acc [0.99116211 0.99057617 0.99042969 0.97563477]\n",
      "2019-03-03T12:26:02.546374: step 17800, loss 0.0256284, acc [0.99047852 0.99086914 0.99125977 0.97626953]\n",
      "\n",
      "Evaluation:\n",
      "2019-03-03T12:26:09.843907: step 17800, loss 0.37642, acc [0.98165964 0.97168857 0.96190772 0.95075534] \n",
      "\n",
      "2019-03-03T12:26:13.456310: step 17840, loss 0.0296835, acc [0.99125977 0.9878418  0.99125977 0.97426758]\n",
      "2019-03-03T12:26:17.618236: step 17880, loss 0.0285257, acc [0.99199219 0.9909668  0.98969727 0.97626953]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-03T12:26:21.733610: step 17920, loss 0.0296352, acc [0.99033203 0.99003906 0.99082031 0.975     ]\n",
      "2019-03-03T12:26:26.005163: step 17960, loss 0.025568, acc [0.99223633 0.99130859 0.99194336 0.97861328]\n",
      "2019-03-03T12:26:30.111655: step 18000, loss 0.0194567, acc [0.99448242 0.99350586 0.99384766 0.98330078]\n",
      "\n",
      "Evaluation:\n",
      "2019-03-03T12:26:37.345814: step 18000, loss 0.38061, acc [0.98136932 0.9717086  0.96234821 0.95089549] \n",
      "\n",
      "2019-03-03T12:26:40.957686: step 18040, loss 0.0176533, acc [0.99414062 0.99287109 0.99418945 0.98295898]\n",
      "2019-03-03T12:26:45.123238: step 18080, loss 0.0203421, acc [0.99326172 0.99311523 0.99350586 0.98193359]\n",
      "2019-03-03T12:26:49.250665: step 18120, loss 0.0226666, acc [0.99238281 0.99267578 0.99257812 0.98037109]\n",
      "2019-03-03T12:26:53.370446: step 18160, loss 0.0230562, acc [0.99326172 0.99326172 0.99223633 0.98149414]\n",
      "2019-03-03T12:26:57.541699: step 18200, loss 0.0216556, acc [0.9925293  0.99267578 0.99384766 0.98164063]\n",
      "\n",
      "Evaluation:\n",
      "2019-03-03T12:27:04.769491: step 18200, loss 0.386665, acc [0.9815295  0.97185876 0.96239826 0.95138604] \n",
      "\n",
      "2019-03-03T12:27:08.384291: step 18240, loss 0.0245565, acc [0.99287109 0.99223633 0.99213867 0.9796875 ]\n",
      "2019-03-03T12:27:12.550994: step 18280, loss 0.0209241, acc [0.99272461 0.99228516 0.99331055 0.98071289]\n",
      "2019-03-03T12:27:16.663374: step 18320, loss 0.0236264, acc [0.99287109 0.99228516 0.99277344 0.98037109]\n",
      "2019-03-03T12:27:20.836470: step 18360, loss 0.0232039, acc [0.99350586 0.9918457  0.99189453 0.97983398]\n",
      "2019-03-03T12:27:24.938612: step 18400, loss 0.0236853, acc [0.99335938 0.99169922 0.99296875 0.98081055]\n",
      "\n",
      "Evaluation:\n",
      "2019-03-03T12:27:32.152931: step 18400, loss 0.379197, acc [0.98174974 0.97175865 0.96238825 0.95210684] \n",
      "\n",
      "2019-03-03T12:27:35.767776: step 18440, loss 0.0216311, acc [0.99355469 0.99262695 0.99296875 0.9815918 ]\n",
      "2019-03-03T12:27:39.893491: step 18480, loss 0.027026, acc [0.99169922 0.99077148 0.99067383 0.97646484]\n",
      "2019-03-03T12:27:44.009718: step 18520, loss 0.0246986, acc [0.99301758 0.99130859 0.99223633 0.97939453]\n",
      "2019-03-03T12:27:48.198279: step 18560, loss 0.0256232, acc [0.99111328 0.9909668  0.99194336 0.97749023]\n",
      "2019-03-03T12:27:52.303903: step 18600, loss 0.0268904, acc [0.99199219 0.99174805 0.99125977 0.97841797]\n",
      "\n",
      "Evaluation:\n",
      "2019-03-03T12:27:59.510344: step 18600, loss 0.377689, acc [0.98176976 0.97120804 0.96226812 0.95117581] \n",
      "\n",
      "2019-03-03T12:28:03.120185: step 18640, loss 0.0280111, acc [0.99155273 0.990625   0.99042969 0.9765625 ]\n",
      "2019-03-03T12:28:07.243688: step 18680, loss 0.0275802, acc [0.99194336 0.99140625 0.99023438 0.97680664]\n",
      "2019-03-03T12:28:11.426936: step 18720, loss 0.0288016, acc [0.9909668  0.99038086 0.99038086 0.9753418 ]\n",
      "2019-03-03T12:28:15.730769: step 18760, loss 0.0181313, acc [0.99472656 0.99399414 0.9940918  0.98461914]\n",
      "2019-03-03T12:28:19.894658: step 18800, loss 0.0173066, acc [0.99443359 0.9940918  0.99458008 0.98486328]\n",
      "\n",
      "Evaluation:\n",
      "2019-03-03T12:28:27.173069: step 18800, loss 0.391581, acc [0.98201003 0.97195887 0.96255844 0.95210684] \n",
      "\n",
      "2019-03-03T12:28:30.788907: step 18840, loss 0.0196342, acc [0.99375    0.99389648 0.99438477 0.98408203]\n",
      "2019-03-03T12:28:34.899165: step 18880, loss 0.0195883, acc [0.99345703 0.99272461 0.9934082  0.98208008]\n",
      "2019-03-03T12:28:39.067683: step 18920, loss 0.0183389, acc [0.99423828 0.99301758 0.99423828 0.98378906]\n",
      "2019-03-03T12:28:43.191831: step 18960, loss 0.0203144, acc [0.99438477 0.99267578 0.99399414 0.9831543 ]\n",
      "2019-03-03T12:28:47.392376: step 19000, loss 0.0203847, acc [0.99301758 0.99311523 0.99360352 0.98168945]\n",
      "\n",
      "Evaluation:\n",
      "2019-03-03T12:28:54.619873: step 19000, loss 0.390626, acc [0.98182983 0.97198891 0.962158   0.95198671] \n",
      "\n",
      "2019-03-03T12:28:58.238644: step 19040, loss 0.0197101, acc [0.99355469 0.99296875 0.99394531 0.98276367]\n",
      "2019-03-03T12:29:02.351171: step 19080, loss 0.0207838, acc [0.9925293  0.99223633 0.99326172 0.98061523]\n",
      "2019-03-03T12:29:06.514575: step 19120, loss 0.0212435, acc [0.99428711 0.99306641 0.99272461 0.98217773]\n",
      "2019-03-03T12:29:10.629959: step 19160, loss 0.0233001, acc [0.99238281 0.9921875  0.99277344 0.98046875]\n",
      "2019-03-03T12:29:14.738619: step 19200, loss 0.0205228, acc [0.99277344 0.9918457  0.99375    0.98037109]\n",
      "\n",
      "Evaluation:\n",
      "2019-03-03T12:29:21.997244: step 19200, loss 0.388348, acc [0.98195998 0.97172862 0.96226812 0.95164633] \n",
      "\n",
      "2019-03-03T12:29:25.608620: step 19240, loss 0.0228213, acc [0.99257812 0.99243164 0.99291992 0.98061523]\n",
      "2019-03-03T12:29:29.762065: step 19280, loss 0.0230521, acc [0.99355469 0.99194336 0.99204102 0.98027344]\n",
      "2019-03-03T12:29:33.865078: step 19320, loss 0.0260847, acc [0.99296875 0.99160156 0.99140625 0.97895508]\n",
      "2019-03-03T12:29:37.994226: step 19360, loss 0.027374, acc [0.99375    0.99208984 0.99121094 0.97963867]\n",
      "2019-03-03T12:29:42.166680: step 19400, loss 0.0260021, acc [0.99194336 0.99067383 0.9909668  0.97670898]\n",
      "\n",
      "Evaluation:\n",
      "2019-03-03T12:29:49.399515: step 19400, loss 0.388685, acc [0.98214018 0.9715284  0.96256845 0.95179649] \n",
      "\n",
      "2019-03-03T12:29:53.012872: step 19440, loss 0.0243242, acc [0.9918457  0.99140625 0.99223633 0.97827148]\n",
      "2019-03-03T12:29:57.174572: step 19480, loss 0.028025, acc [0.99145508 0.99135742 0.99038086 0.97695312]\n",
      "2019-03-03T12:30:01.416839: step 19520, loss 0.024559, acc [0.99316406 0.99179688 0.99174805 0.97954102]\n",
      "2019-03-03T12:30:05.522364: step 19560, loss 0.0161405, acc [0.99467773 0.99482422 0.99477539 0.9855957 ]\n",
      "2019-03-03T12:30:09.634610: step 19600, loss 0.0170321, acc [0.99404297 0.99487305 0.99414062 0.98457031]\n",
      "\n",
      "Evaluation:\n",
      "2019-03-03T12:30:16.894801: step 19600, loss 0.39498, acc [0.98227032 0.97223919 0.96282874 0.95276757] \n",
      "\n",
      "2019-03-03T12:30:20.512578: step 19640, loss 0.0177926, acc [0.99423828 0.99384766 0.99472656 0.98505859]\n",
      "2019-03-03T12:30:24.626749: step 19680, loss 0.019357, acc [0.99350586 0.99296875 0.99379883 0.98232422]\n",
      "2019-03-03T12:30:28.739978: step 19720, loss 0.0182173, acc [0.99350586 0.99277344 0.99389648 0.98232422]\n",
      "2019-03-03T12:30:32.859245: step 19760, loss 0.0172189, acc [0.99394531 0.99389648 0.99428711 0.98432617]\n",
      "2019-03-03T12:30:36.980328: step 19800, loss 0.0203155, acc [0.99389648 0.99365234 0.99326172 0.98378906]\n",
      "\n",
      "Evaluation:\n",
      "2019-03-03T12:30:44.253786: step 19800, loss 0.400466, acc [0.98201003 0.97203896 0.96212796 0.95220695] \n",
      "\n",
      "2019-03-03T12:30:47.866152: step 19840, loss 0.0224882, acc [0.99282227 0.99311523 0.99311523 0.98164063]\n",
      "2019-03-03T12:30:52.002304: step 19880, loss 0.0191466, acc [0.99331055 0.99296875 0.99306641 0.98178711]\n",
      "2019-03-03T12:30:56.174394: step 19920, loss 0.0188512, acc [0.9934082  0.99301758 0.99428711 0.9824707 ]\n",
      "2019-03-03T12:31:00.277986: step 19960, loss 0.0225076, acc [0.99291992 0.99057617 0.99345703 0.98041992]\n",
      "2019-03-03T12:31:04.429523: step 20000, loss 0.0216952, acc [0.99277344 0.99130859 0.99326172 0.98027344]\n",
      "\n",
      "Evaluation:\n",
      "2019-03-03T12:31:11.705464: step 20000, loss 0.399317, acc [0.98178979 0.97182873 0.96231817 0.95192664] \n",
      "\n",
      "2019-03-03T12:31:15.316694: step 20040, loss 0.0232729, acc [0.99165039 0.99379883 0.99233398 0.98027344]\n",
      "2019-03-03T12:31:19.442011: step 20080, loss 0.0209917, acc [0.99321289 0.9925293  0.99291992 0.98105469]\n",
      "2019-03-03T12:31:23.638431: step 20120, loss 0.0243063, acc [0.99213867 0.99101562 0.99199219 0.9784668 ]\n",
      "2019-03-03T12:31:27.795317: step 20160, loss 0.0261891, acc [0.99262695 0.99116211 0.99125977 0.97817383]\n",
      "2019-03-03T12:31:31.954629: step 20200, loss 0.0249554, acc [0.99321289 0.99223633 0.99116211 0.97958984]\n",
      "\n",
      "Evaluation:\n",
      "2019-03-03T12:31:39.226947: step 20200, loss 0.397579, acc [0.98211014 0.97231927 0.96243831 0.95231707] \n",
      "\n",
      "2019-03-03T12:31:42.838819: step 20240, loss 0.022823, acc [0.99228516 0.99199219 0.99165039 0.97905273]\n",
      "2019-03-03T12:31:47.023300: step 20280, loss 0.028834, acc [0.99243164 0.98994141 0.99047852 0.97700195]\n",
      "2019-03-03T12:31:51.348270: step 20320, loss 0.0156754, acc [0.99472656 0.99501953 0.99472656 0.98603516]\n",
      "2019-03-03T12:31:55.517691: step 20360, loss 0.0171826, acc [0.99492187 0.99418945 0.99453125 0.98544922]\n",
      "2019-03-03T12:31:59.621782: step 20400, loss 0.0181706, acc [0.99384766 0.99350586 0.99414062 0.98339844]\n",
      "\n",
      "Evaluation:\n",
      "2019-03-03T12:32:06.883150: step 20400, loss 0.407557, acc [0.98220024 0.97208902 0.96198781 0.95214688] \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-03T12:32:10.502994: step 20440, loss 0.0188699, acc [0.99350586 0.99389648 0.99331055 0.9831543 ]\n",
      "2019-03-03T12:32:14.659906: step 20480, loss 0.0150694, acc [0.99360352 0.99379883 0.99570313 0.9847168 ]\n",
      "2019-03-03T12:32:18.821410: step 20520, loss 0.0177899, acc [0.99360352 0.9934082  0.99458008 0.98330078]\n",
      "2019-03-03T12:32:22.983894: step 20560, loss 0.0190696, acc [0.99487305 0.99399414 0.99326172 0.98427734]\n",
      "2019-03-03T12:32:27.163960: step 20600, loss 0.0191762, acc [0.99370117 0.99379883 0.99345703 0.98310547]\n",
      "\n",
      "Evaluation:\n",
      "2019-03-03T12:32:34.371944: step 20600, loss 0.406823, acc [0.98230035 0.97192884 0.96310905 0.95295778] \n",
      "\n",
      "2019-03-03T12:32:37.985304: step 20640, loss 0.0179189, acc [0.99467773 0.99287109 0.99399414 0.98310547]\n",
      "2019-03-03T12:32:42.134264: step 20680, loss 0.0180884, acc [0.9934082  0.99282227 0.99423828 0.98310547]\n",
      "2019-03-03T12:32:46.309057: step 20720, loss 0.0205565, acc [0.99384766 0.9921875  0.99287109 0.98120117]\n",
      "2019-03-03T12:32:50.411667: step 20760, loss 0.0199683, acc [0.99477539 0.99277344 0.99379883 0.98349609]\n",
      "2019-03-03T12:32:54.514729: step 20800, loss 0.0254631, acc [0.9918457  0.99106445 0.99199219 0.97875977]\n",
      "\n",
      "Evaluation:\n",
      "2019-03-03T12:33:01.765395: step 20800, loss 0.411366, acc [0.98236042 0.97181872 0.96208792 0.95202675] \n",
      "\n",
      "2019-03-03T12:33:05.373961: step 20840, loss 0.0235767, acc [0.99296875 0.99296875 0.99213867 0.98032227]\n",
      "2019-03-03T12:33:09.537571: step 20880, loss 0.0228588, acc [0.99291992 0.99345703 0.99238281 0.98120117]\n",
      "2019-03-03T12:33:13.653542: step 20920, loss 0.0257064, acc [0.99301758 0.99067383 0.99145508 0.97832031]\n",
      "2019-03-03T12:33:17.804296: step 20960, loss 0.0256254, acc [0.99291992 0.99135742 0.99228516 0.97949219]\n",
      "2019-03-03T12:33:21.971563: step 21000, loss 0.0263853, acc [0.9921875  0.99086914 0.9918457  0.97807617]\n",
      "\n",
      "Evaluation:\n",
      "2019-03-03T12:33:29.242920: step 21000, loss 0.403666, acc [0.98213016 0.97162851 0.96239826 0.95189661] \n",
      "\n",
      "2019-03-03T12:33:32.854792: step 21040, loss 0.0243675, acc [0.99282227 0.99072266 0.99223633 0.97817383]\n",
      "2019-03-03T12:33:37.118649: step 21080, loss 0.0213183, acc [0.99477539 0.9921875  0.99287109 0.98222656]\n",
      "2019-03-03T12:33:41.232615: step 21120, loss 0.0140669, acc [0.99443359 0.99501953 0.99550781 0.98652344]\n",
      "2019-03-03T12:33:45.386186: step 21160, loss 0.0177831, acc [0.99526367 0.99335938 0.99443359 0.98500977]\n",
      "2019-03-03T12:33:49.534532: step 21200, loss 0.0168367, acc [0.99428711 0.99370117 0.99472656 0.98442383]\n",
      "\n",
      "Evaluation:\n",
      "2019-03-03T12:33:56.826216: step 21200, loss 0.407927, acc [0.98233039 0.9724294  0.96277868 0.95293776] \n",
      "\n",
      "2019-03-03T12:34:00.440028: step 21240, loss 0.0186258, acc [0.99521484 0.99453125 0.99399414 0.98554688]\n",
      "2019-03-03T12:34:04.604235: step 21280, loss 0.0173878, acc [0.99536133 0.99526367 0.99399414 0.98618164]\n",
      "2019-03-03T12:34:08.721277: step 21320, loss 0.0150575, acc [0.99521484 0.99511719 0.99521484 0.98706055]\n",
      "2019-03-03T12:34:12.907567: step 21360, loss 0.0206039, acc [0.99335938 0.99345703 0.99370117 0.98286133]\n",
      "2019-03-03T12:34:17.015642: step 21400, loss 0.0190762, acc [0.9934082  0.99335938 0.99418945 0.98344727]\n",
      "\n",
      "Evaluation:\n",
      "2019-03-03T12:34:24.234364: step 21400, loss 0.41283, acc [0.98224029 0.97246944 0.96276867 0.95314799] \n",
      "\n",
      "2019-03-03T12:34:27.852683: step 21440, loss 0.0171438, acc [0.99360352 0.99350586 0.99448242 0.98417969]\n",
      "2019-03-03T12:34:32.011550: step 21480, loss 0.0177653, acc [0.99379883 0.9934082  0.99443359 0.98349609]\n",
      "2019-03-03T12:34:36.167535: step 21520, loss 0.0177097, acc [0.99311523 0.99311523 0.99418945 0.98217773]\n",
      "2019-03-03T12:34:40.267756: step 21560, loss 0.0184695, acc [0.99370117 0.9934082  0.9934082  0.98261719]\n",
      "2019-03-03T12:34:44.382520: step 21600, loss 0.0207672, acc [0.99399414 0.9934082  0.99350586 0.98305664]\n",
      "\n",
      "Evaluation:\n",
      "2019-03-03T12:34:51.665799: step 21600, loss 0.4172, acc [0.98250058 0.97245943 0.96259848 0.95283765] \n",
      "\n",
      "2019-03-03T12:34:55.292056: step 21640, loss 0.021534, acc [0.99321289 0.99208984 0.99301758 0.98154297]\n",
      "2019-03-03T12:34:59.464074: step 21680, loss 0.024223, acc [0.99267578 0.99155273 0.99272461 0.98066406]\n",
      "2019-03-03T12:35:03.630762: step 21720, loss 0.022349, acc [0.99277344 0.99233398 0.99189453 0.97998047]\n",
      "2019-03-03T12:35:07.747508: step 21760, loss 0.0248528, acc [0.99272461 0.99174805 0.99291992 0.98076172]\n",
      "2019-03-03T12:35:11.850014: step 21800, loss 0.0211309, acc [0.99335938 0.99145508 0.99301758 0.98081055]\n",
      "\n",
      "Evaluation:\n",
      "2019-03-03T12:35:19.162817: step 21800, loss 0.408759, acc [0.98255063 0.97230926 0.96293886 0.95328815] \n",
      "\n",
      "2019-03-03T12:35:22.790017: step 21840, loss 0.0239252, acc [0.99228516 0.99233398 0.99204102 0.97910156]\n",
      "2019-03-03T12:35:27.086973: step 21880, loss 0.0170602, acc [0.99443359 0.99399414 0.99501953 0.98535156]\n",
      "2019-03-03T12:35:31.257211: step 21920, loss 0.0161654, acc [0.99438477 0.99477539 0.99526367 0.98549805]\n",
      "2019-03-03T12:35:35.424859: step 21960, loss 0.0168568, acc [0.99511719 0.99365234 0.9949707  0.98496094]\n",
      "2019-03-03T12:35:39.591390: step 22000, loss 0.017372, acc [0.99453125 0.99438477 0.99433594 0.98525391]\n",
      "\n",
      "Evaluation:\n",
      "2019-03-03T12:35:46.804829: step 22000, loss 0.410804, acc [0.98248055 0.97261961 0.96307902 0.95342831] \n",
      "\n",
      "2019-03-03T12:35:50.414714: step 22040, loss 0.0167261, acc [0.99472656 0.99326172 0.99526367 0.98530273]\n",
      "2019-03-03T12:35:54.543769: step 22080, loss 0.0148087, acc [0.99453125 0.99458008 0.99477539 0.98535156]\n",
      "2019-03-03T12:35:58.719723: step 22120, loss 0.0170421, acc [0.9940918  0.99467773 0.99458008 0.98520508]\n",
      "2019-03-03T12:36:02.923406: step 22160, loss 0.0189803, acc [0.9934082  0.99350586 0.9940918  0.98295898]\n",
      "2019-03-03T12:36:07.041041: step 22200, loss 0.018373, acc [0.99365234 0.99350586 0.99414062 0.98261719]\n",
      "\n",
      "Evaluation:\n",
      "2019-03-03T12:36:14.334844: step 22200, loss 0.411695, acc [0.98204006 0.97270971 0.96288881 0.95334822] \n",
      "\n",
      "2019-03-03T12:36:17.957123: step 22240, loss 0.0187191, acc [0.99370117 0.99306641 0.99394531 0.9828125 ]\n",
      "2019-03-03T12:36:22.118236: step 22280, loss 0.0177389, acc [0.99433594 0.99306641 0.9940918  0.98364258]\n",
      "2019-03-03T12:36:26.803917: step 22320, loss 0.0214912, acc [0.99394531 0.99213867 0.99301758 0.98198242]\n",
      "2019-03-03T12:36:30.909149: step 22360, loss 0.0181653, acc [0.99345703 0.99321289 0.99399414 0.98330078]\n",
      "2019-03-03T12:36:35.079128: step 22400, loss 0.0191818, acc [0.99365234 0.99311523 0.99394531 0.98320312]\n",
      "\n",
      "Evaluation:\n",
      "2019-03-03T12:36:42.329345: step 22400, loss 0.412839, acc [0.98217021 0.97220915 0.96327924 0.95317803] \n",
      "\n",
      "2019-03-03T12:36:45.953570: step 22440, loss 0.0212751, acc [0.9949707  0.99379883 0.99345703 0.98413086]\n",
      "2019-03-03T12:36:50.101964: step 22480, loss 0.0215167, acc [0.99399414 0.99301758 0.99291992 0.98237305]\n",
      "2019-03-03T12:36:54.264867: step 22520, loss 0.0204708, acc [0.99311523 0.99243164 0.99272461 0.98100586]\n",
      "2019-03-03T12:36:58.433228: step 22560, loss 0.0222558, acc [0.99350586 0.99223633 0.99287109 0.98134766]\n",
      "2019-03-03T12:37:02.543762: step 22600, loss 0.0247515, acc [0.99355469 0.99208984 0.9925293  0.98120117]\n",
      "\n",
      "Evaluation:\n",
      "2019-03-03T12:37:09.811851: step 22600, loss 0.408448, acc [0.98245052 0.9726997  0.96298892 0.95347836] \n",
      "\n",
      "2019-03-03T12:37:13.558135: step 22640, loss 0.0191691, acc [0.99482422 0.99287109 0.99345703 0.98383789]\n",
      "2019-03-03T12:37:17.718405: step 22680, loss 0.0145898, acc [0.99453125 0.99467773 0.99570313 0.98676758]\n",
      "2019-03-03T12:37:21.905490: step 22720, loss 0.0147453, acc [0.99477539 0.99487305 0.99516602 0.98608398]\n",
      "2019-03-03T12:37:26.023904: step 22760, loss 0.0151239, acc [0.9953125  0.99482422 0.99487305 0.98642578]\n",
      "2019-03-03T12:37:30.188811: step 22800, loss 0.0186857, acc [0.99414062 0.99428711 0.99423828 0.98427734]\n",
      "\n",
      "Evaluation:\n",
      "2019-03-03T12:37:37.489241: step 22800, loss 0.417534, acc [0.98223028 0.9726997  0.96307902 0.95361852] \n",
      "\n",
      "2019-03-03T12:37:41.101969: step 22840, loss 0.0166519, acc [0.99384766 0.99423828 0.9940918  0.98393555]\n",
      "2019-03-03T12:37:45.262674: step 22880, loss 0.0162235, acc [0.99492187 0.99433594 0.99477539 0.98520508]\n",
      "2019-03-03T12:37:49.384338: step 22920, loss 0.0147634, acc [0.99526367 0.9940918  0.99545898 0.98637695]\n",
      "2019-03-03T12:37:53.500577: step 22960, loss 0.0173719, acc [0.99453125 0.99404297 0.99458008 0.9847168 ]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-03T12:37:57.613583: step 23000, loss 0.0174502, acc [0.99423828 0.99375    0.99375    0.98408203]\n",
      "\n",
      "Evaluation:\n",
      "2019-03-03T12:38:04.821319: step 23000, loss 0.422167, acc [0.98245052 0.97252951 0.9635095  0.95398893] \n",
      "\n",
      "2019-03-03T12:38:08.439639: step 23040, loss 0.0193595, acc [0.99472656 0.99321289 0.9934082  0.98354492]\n",
      "2019-03-03T12:38:12.559958: step 23080, loss 0.0177142, acc [0.99370117 0.99291992 0.99506836 0.98388672]\n",
      "2019-03-03T12:38:16.757510: step 23120, loss 0.0190809, acc [0.99345703 0.99321289 0.99360352 0.9824707 ]\n",
      "2019-03-03T12:38:20.860896: step 23160, loss 0.0186725, acc [0.99438477 0.99335938 0.99394531 0.98369141]\n",
      "2019-03-03T12:38:25.003233: step 23200, loss 0.0193896, acc [0.99355469 0.9934082  0.99370117 0.9824707 ]\n",
      "\n",
      "Evaluation:\n",
      "2019-03-03T12:38:32.236227: step 23200, loss 0.416177, acc [0.98267076 0.97294997 0.96308903 0.95388882] \n",
      "\n",
      "2019-03-03T12:38:35.848595: step 23240, loss 0.0220481, acc [0.99428711 0.99296875 0.99306641 0.98251953]\n",
      "2019-03-03T12:38:39.969319: step 23280, loss 0.0199397, acc [0.99365234 0.99379883 0.99331055 0.98300781]\n",
      "2019-03-03T12:38:44.131348: step 23320, loss 0.0195695, acc [0.99404297 0.99355469 0.99345703 0.98359375]\n",
      "2019-03-03T12:38:48.277875: step 23360, loss 0.0216588, acc [0.99306641 0.99257812 0.9925293  0.98051758]\n",
      "2019-03-03T12:38:52.379175: step 23400, loss 0.0200822, acc [0.99311523 0.99228516 0.99291992 0.9809082 ]\n",
      "\n",
      "Evaluation:\n",
      "2019-03-03T12:38:59.585304: step 23400, loss 0.418362, acc [0.98255063 0.97275976 0.96310905 0.95380873] \n",
      "\n",
      "2019-03-03T12:39:03.327169: step 23440, loss 0.0156883, acc [0.99545898 0.99477539 0.99541016 0.98740234]\n",
      "2019-03-03T12:39:07.478160: step 23480, loss 0.01513, acc [0.99526367 0.99511719 0.99492187 0.98662109]\n",
      "2019-03-03T12:39:11.585038: step 23520, loss 0.013582, acc [0.99482422 0.99511719 0.99541016 0.98642578]\n",
      "2019-03-03T12:39:15.775809: step 23560, loss 0.0150788, acc [0.99492187 0.99560547 0.99477539 0.98696289]\n",
      "2019-03-03T12:39:19.964962: step 23600, loss 0.0143826, acc [0.99487305 0.9956543  0.99458008 0.98662109]\n",
      "\n",
      "Evaluation:\n",
      "2019-03-03T12:39:27.244063: step 23600, loss 0.424331, acc [0.9824305  0.97289992 0.96326923 0.95375867] \n",
      "\n",
      "2019-03-03T12:39:30.859940: step 23640, loss 0.0144474, acc [0.99526367 0.99506836 0.99487305 0.98642578]\n",
      "2019-03-03T12:39:35.033059: step 23680, loss 0.0167372, acc [0.99501953 0.99414062 0.99433594 0.98510742]\n",
      "2019-03-03T12:39:39.204854: step 23720, loss 0.0165719, acc [0.99545898 0.99477539 0.99404297 0.9862793 ]\n",
      "2019-03-03T12:39:43.307716: step 23760, loss 0.0160934, acc [0.99487305 0.9940918  0.99467773 0.98496094]\n",
      "2019-03-03T12:39:47.466181: step 23800, loss 0.016657, acc [0.99370117 0.99472656 0.99482422 0.98491211]\n",
      "\n",
      "Evaluation:\n",
      "2019-03-03T12:39:54.739341: step 23800, loss 0.435303, acc [0.98219023 0.97275976 0.96263853 0.95310795] \n",
      "\n",
      "2019-03-03T12:39:58.359100: step 23840, loss 0.0191521, acc [0.9940918  0.99375    0.99418945 0.98427734]\n",
      "2019-03-03T12:40:02.538025: step 23880, loss 0.0168708, acc [0.99501953 0.99360352 0.99438477 0.9847168 ]\n",
      "2019-03-03T12:40:06.695182: step 23920, loss 0.0184968, acc [0.99487305 0.99433594 0.99384766 0.98500977]\n",
      "2019-03-03T12:40:10.879229: step 23960, loss 0.0212438, acc [0.99370117 0.99243164 0.99355469 0.98237305]\n",
      "2019-03-03T12:40:14.990390: step 24000, loss 0.0191964, acc [0.99394531 0.99296875 0.99355469 0.98286133]\n",
      "\n",
      "Evaluation:\n",
      "2019-03-03T12:40:22.217612: step 24000, loss 0.425279, acc [0.98265074 0.9726096  0.96284876 0.95352842] \n",
      "\n",
      "2019-03-03T12:40:25.829978: step 24040, loss 0.0214787, acc [0.99365234 0.99287109 0.99316406 0.98203125]\n",
      "2019-03-03T12:40:29.981539: step 24080, loss 0.0191781, acc [0.99355469 0.99379883 0.99345703 0.9828125 ]\n",
      "2019-03-03T12:40:34.096193: step 24120, loss 0.0194988, acc [0.99394531 0.99321289 0.99326172 0.98271484]\n",
      "2019-03-03T12:40:38.206768: step 24160, loss 0.0230438, acc [0.9940918  0.99287109 0.99272461 0.98188477]\n",
      "2019-03-03T12:40:42.508331: step 24200, loss 0.0175574, acc [0.99418945 0.99326172 0.99423828 0.98334961]\n",
      "\n",
      "Evaluation:\n",
      "2019-03-03T12:40:49.780867: step 24200, loss 0.425301, acc [0.98242049 0.97258957 0.96259848 0.95319805] \n",
      "\n",
      "2019-03-03T12:40:53.398607: step 24240, loss 0.0147644, acc [0.99482422 0.99501953 0.99506836 0.98608398]\n",
      "2019-03-03T12:40:57.568585: step 24280, loss 0.0148979, acc [0.9956543  0.99492187 0.99482422 0.98642578]\n",
      "2019-03-03T12:41:01.696809: step 24320, loss 0.0132254, acc [0.99560547 0.99511719 0.99584961 0.98745117]\n",
      "2019-03-03T12:41:05.922704: step 24360, loss 0.0147652, acc [0.99550781 0.9949707  0.99526367 0.98696289]\n",
      "2019-03-03T12:46:28.347001: step 24400, loss 0.0165285, acc [0.9949707  0.9949707  0.99492187 0.98681641]\n",
      "\n",
      "Evaluation:\n",
      "2019-03-03T12:46:36.137669: step 24400, loss 0.424436, acc [0.98268078 0.97261961 0.96292885 0.95361852] \n",
      "\n",
      "2019-03-03T12:46:40.142958: step 24440, loss 0.0153591, acc [0.99453125 0.99433594 0.99521484 0.98535156]\n",
      "2019-03-03T12:46:44.886587: step 24480, loss 0.0145826, acc [0.99541016 0.99462891 0.99526367 0.98657227]\n",
      "2019-03-03T12:46:49.067865: step 24520, loss 0.0160978, acc [0.99526367 0.99365234 0.99472656 0.98515625]\n",
      "2019-03-03T12:46:53.249843: step 24560, loss 0.0147025, acc [0.99541016 0.99423828 0.99526367 0.98618164]\n",
      "2019-03-03T12:46:57.560435: step 24600, loss 0.0163081, acc [0.99477539 0.99438477 0.99428711 0.98530273]\n",
      "\n",
      "Evaluation:\n",
      "2019-03-03T12:47:04.953852: step 24600, loss 0.431187, acc [0.98248055 0.9725195  0.96289882 0.95372864] \n",
      "\n",
      "2019-03-03T12:47:08.625198: step 24640, loss 0.0173986, acc [0.99394531 0.99453125 0.99418945 0.98461914]\n",
      "2019-03-03T12:47:12.783077: step 24680, loss 0.0200431, acc [0.99384766 0.99389648 0.99355469 0.98354492]\n",
      "2019-03-03T12:47:16.911284: step 24720, loss 0.0197879, acc [0.99448242 0.99384766 0.9940918  0.98422852]\n",
      "2019-03-03T12:47:21.033904: step 24760, loss 0.0180673, acc [0.99355469 0.99389648 0.99423828 0.9840332 ]\n",
      "2019-03-03T12:47:25.151695: step 24800, loss 0.0182089, acc [0.99311523 0.99370117 0.99414062 0.98286133]\n",
      "\n",
      "Evaluation:\n",
      "2019-03-03T12:47:32.409972: step 24800, loss 0.430758, acc [0.98277088 0.9724294  0.96270861 0.95322808] \n",
      "\n",
      "2019-03-03T12:47:36.030777: step 24840, loss 0.0203346, acc [0.99296875 0.99238281 0.99291992 0.98154297]\n",
      "2019-03-03T12:47:40.153474: step 24880, loss 0.0192278, acc [0.99389648 0.99350586 0.99326172 0.98300781]\n",
      "2019-03-03T12:47:44.295570: step 24920, loss 0.0185313, acc [0.99365234 0.99296875 0.99418945 0.98310547]\n",
      "2019-03-03T12:47:48.402982: step 24960, loss 0.0213182, acc [0.99345703 0.99301758 0.99262695 0.98251953]\n",
      "2019-03-03T12:47:52.608471: step 25000, loss 0.0136504, acc [0.99619141 0.99599609 0.99526367 0.98886719]\n",
      "\n",
      "Evaluation:\n",
      "2019-03-03T12:47:59.841636: step 25000, loss 0.427495, acc [0.98280091 0.97285987 0.96347946 0.95440939] \n",
      "\n",
      "2019-03-03T12:48:03.449009: step 25040, loss 0.0132424, acc [0.99545898 0.99589844 0.99584961 0.98857422]\n",
      "2019-03-03T12:48:07.550269: step 25080, loss 0.0144398, acc [0.99506836 0.99501953 0.99536133 0.98696289]\n",
      "2019-03-03T12:48:11.691400: step 25120, loss 0.0151029, acc [0.99580078 0.99472656 0.99536133 0.98710937]\n",
      "2019-03-03T12:48:15.839780: step 25160, loss 0.0163945, acc [0.99458008 0.99438477 0.99438477 0.98476562]\n",
      "2019-03-03T12:48:19.943065: step 25200, loss 0.0137694, acc [0.99521484 0.99433594 0.99545898 0.98666992]\n",
      "\n",
      "Evaluation:\n",
      "2019-03-03T12:48:27.152978: step 25200, loss 0.442527, acc [0.98249056 0.97316021 0.96316912 0.95372864] \n",
      "\n",
      "2019-03-03T12:48:30.760880: step 25240, loss 0.0150951, acc [0.9949707  0.99477539 0.99580078 0.98720703]\n",
      "2019-03-03T12:48:34.855359: step 25280, loss 0.0109082, acc [0.99521484 0.9956543  0.99658203 0.98842773]\n",
      "2019-03-03T12:48:38.958202: step 25320, loss 0.0156696, acc [0.99477539 0.99399414 0.99453125 0.98530273]\n",
      "2019-03-03T12:48:43.062683: step 25360, loss 0.0158354, acc [0.99448242 0.99521484 0.99453125 0.98544922]\n",
      "2019-03-03T12:48:47.162121: step 25400, loss 0.0176704, acc [0.99443359 0.99389648 0.99433594 0.98486328]\n",
      "\n",
      "Evaluation:\n",
      "2019-03-03T12:48:54.362677: step 25400, loss 0.442964, acc [0.98256064 0.97294997 0.96335933 0.95422919] \n",
      "\n",
      "2019-03-03T12:48:57.961605: step 25440, loss 0.0202338, acc [0.99399414 0.99272461 0.99321289 0.98251953]\n",
      "2019-03-03T12:49:02.073808: step 25480, loss 0.0161825, acc [0.99448242 0.99360352 0.99511719 0.98510742]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-03T12:49:06.182363: step 25520, loss 0.0164282, acc [0.99560547 0.99433594 0.99521484 0.98671875]\n",
      "2019-03-03T12:49:10.292707: step 25560, loss 0.019824, acc [0.99433594 0.99306641 0.99394531 0.98408203]\n",
      "2019-03-03T12:49:14.407110: step 25600, loss 0.0190785, acc [0.99477539 0.99272461 0.99389648 0.98413086]\n",
      "\n",
      "Evaluation:\n",
      "2019-03-03T12:49:21.621852: step 25600, loss 0.434744, acc [0.98214018 0.97217912 0.9629689  0.95325812] \n",
      "\n",
      "2019-03-03T12:49:25.229215: step 25640, loss 0.0156008, acc [0.99487305 0.99462891 0.9949707  0.98613281]\n",
      "2019-03-03T12:49:29.340520: step 25680, loss 0.0168052, acc [0.99355469 0.99389648 0.99477539 0.98374023]\n",
      "2019-03-03T12:49:33.535790: step 25720, loss 0.0190841, acc [0.99521484 0.99248047 0.99375    0.98388672]\n",
      "2019-03-03T12:49:37.774283: step 25760, loss 0.0160987, acc [0.99448242 0.99370117 0.99541016 0.98500977]\n",
      "2019-03-03T12:49:41.905473: step 25800, loss 0.0116461, acc [0.99536133 0.99594727 0.99667969 0.9890625 ]\n",
      "\n",
      "Evaluation:\n",
      "2019-03-03T12:49:49.168249: step 25800, loss 0.435894, acc [0.98295108 0.97250949 0.96318914 0.95395889] \n",
      "\n",
      "2019-03-03T12:49:52.788552: step 25840, loss 0.0128825, acc [0.99521484 0.99541016 0.99584961 0.98793945]\n",
      "2019-03-03T12:49:56.909891: step 25880, loss 0.0146869, acc [0.99545898 0.99516602 0.9956543  0.9875    ]\n",
      "2019-03-03T12:50:01.029461: step 25920, loss 0.0117118, acc [0.99545898 0.99575195 0.99619141 0.98852539]\n",
      "2019-03-03T12:50:05.155662: step 25960, loss 0.0142925, acc [0.9949707  0.9956543  0.99501953 0.98652344]\n",
      "2019-03-03T12:50:09.267433: step 26000, loss 0.0144452, acc [0.99506836 0.99541016 0.99521484 0.98696289]\n",
      "\n",
      "Evaluation:\n",
      "2019-03-03T12:50:16.510589: step 26000, loss 0.448547, acc [0.9827008  0.97327033 0.9629689  0.95417914] \n",
      "\n",
      "2019-03-03T12:50:20.125432: step 26040, loss 0.0143028, acc [0.99482422 0.99477539 0.99550781 0.98662109]\n",
      "2019-03-03T12:50:24.258400: step 26080, loss 0.013889, acc [0.99482422 0.99570313 0.99550781 0.98745117]\n",
      "2019-03-03T12:50:28.368885: step 26120, loss 0.0141681, acc [0.99501953 0.99511719 0.99506836 0.98701172]\n",
      "2019-03-03T12:50:32.475267: step 26160, loss 0.0175007, acc [0.99379883 0.99458008 0.99404297 0.98398438]\n",
      "2019-03-03T12:50:36.617432: step 26200, loss 0.0166057, acc [0.99506836 0.99472656 0.99438477 0.98598633]\n",
      "\n",
      "Evaluation:\n",
      "2019-03-03T12:50:43.890284: step 26200, loss 0.441857, acc [0.98299112 0.97331037 0.96334932 0.95473976] \n",
      "\n",
      "2019-03-03T12:50:47.515051: step 26240, loss 0.0166422, acc [0.99467773 0.99350586 0.99467773 0.98486328]\n",
      "2019-03-03T12:50:51.634815: step 26280, loss 0.0184222, acc [0.9949707  0.99370117 0.99379883 0.98452148]\n",
      "2019-03-03T12:50:55.743243: step 26320, loss 0.0185662, acc [0.99516602 0.99370117 0.99418945 0.98486328]\n",
      "2019-03-03T12:50:59.859369: step 26360, loss 0.0178658, acc [0.99418945 0.99399414 0.99472656 0.98500977]\n",
      "2019-03-03T12:51:04.011393: step 26400, loss 0.017944, acc [0.99428711 0.99365234 0.99375    0.98364258]\n",
      "\n",
      "Evaluation:\n",
      "2019-03-03T12:51:11.253487: step 26400, loss 0.432605, acc [0.98260069 0.9727898  0.96315911 0.95405901] \n",
      "\n",
      "2019-03-03T12:51:14.871310: step 26440, loss 0.0215054, acc [0.99414062 0.99350586 0.99282227 0.98286133]\n",
      "2019-03-03T12:51:19.006140: step 26480, loss 0.0175359, acc [0.99326172 0.99335938 0.99404297 0.98300781]\n",
      "2019-03-03T12:51:23.137057: step 26520, loss 0.0213488, acc [0.9940918  0.99335938 0.99257812 0.98232422]\n",
      "2019-03-03T12:51:27.419597: step 26560, loss 0.0113562, acc [0.99555664 0.99584961 0.99599609 0.98842773]\n",
      "2019-03-03T12:51:31.547169: step 26600, loss 0.0122316, acc [0.99477539 0.99526367 0.99643555 0.98813477]\n",
      "\n",
      "Evaluation:\n",
      "2019-03-03T12:51:38.862318: step 26600, loss 0.435604, acc [0.98287099 0.9735106  0.96390994 0.95536045] \n",
      "\n",
      "2019-03-03T12:51:42.486048: step 26640, loss 0.0109854, acc [0.99575195 0.99575195 0.99643555 0.98886719]\n",
      "2019-03-03T12:51:46.591414: step 26680, loss 0.0126478, acc [0.99580078 0.99589844 0.99594727 0.98857422]\n",
      "2019-03-03T12:51:50.708727: step 26720, loss 0.0132587, acc [0.99555664 0.99511719 0.99584961 0.98754883]\n",
      "2019-03-03T12:51:54.857886: step 26760, loss 0.0141894, acc [0.99633789 0.99536133 0.9949707  0.98793945]\n",
      "2019-03-03T12:51:59.000912: step 26800, loss 0.0125798, acc [0.9949707  0.99511719 0.99594727 0.98701172]\n",
      "\n",
      "Evaluation:\n",
      "2019-03-03T12:52:06.263533: step 26800, loss 0.444393, acc [0.98298111 0.97319024 0.96351951 0.95439938] \n",
      "\n",
      "2019-03-03T12:52:09.872383: step 26840, loss 0.0141317, acc [0.99477539 0.99526367 0.9953125  0.98725586]\n",
      "2019-03-03T12:52:13.988825: step 26880, loss 0.0132316, acc [0.99477539 0.99428711 0.99580078 0.98613281]\n",
      "2019-03-03T12:52:18.127380: step 26920, loss 0.0148077, acc [0.99433594 0.99545898 0.99448242 0.98574219]\n",
      "2019-03-03T12:52:22.231236: step 26960, loss 0.0125733, acc [0.99443359 0.99560547 0.99580078 0.98666992]\n",
      "2019-03-03T12:52:26.333852: step 27000, loss 0.0148308, acc [0.99545898 0.99438477 0.99453125 0.98598633]\n",
      "\n",
      "Evaluation:\n",
      "2019-03-03T12:52:33.598745: step 27000, loss 0.447335, acc [0.98289101 0.97292995 0.96346945 0.95433932] \n",
      "\n",
      "2019-03-03T12:52:37.210617: step 27040, loss 0.0153162, acc [0.99492187 0.99506836 0.99448242 0.98608398]\n",
      "2019-03-03T12:52:41.326920: step 27080, loss 0.0165788, acc [0.99501953 0.99311523 0.99492187 0.98500977]\n",
      "2019-03-03T12:52:45.485053: step 27120, loss 0.0167757, acc [0.99555664 0.99428711 0.99370117 0.98530273]\n",
      "2019-03-03T12:52:49.608419: step 27160, loss 0.015923, acc [0.99526367 0.99477539 0.99443359 0.98623047]\n",
      "2019-03-03T12:52:53.754575: step 27200, loss 0.0176006, acc [0.99355469 0.99375    0.99482422 0.98383789]\n",
      "\n",
      "Evaluation:\n",
      "2019-03-03T12:53:01.013495: step 27200, loss 0.449894, acc [0.98286098 0.97268969 0.96313908 0.95396891] \n",
      "\n",
      "2019-03-03T12:53:04.632311: step 27240, loss 0.0175738, acc [0.99448242 0.99379883 0.99375    0.98408203]\n",
      "2019-03-03T12:53:08.747788: step 27280, loss 0.017382, acc [0.99438477 0.99433594 0.99438477 0.98505859]\n",
      "2019-03-03T12:53:13.041658: step 27320, loss 0.0167018, acc [0.99492187 0.99370117 0.99458008 0.98510742]\n",
      "2019-03-03T12:53:17.151995: step 27360, loss 0.0135571, acc [0.99541016 0.99521484 0.99614258 0.98789063]\n",
      "2019-03-03T12:53:21.296177: step 27400, loss 0.0130896, acc [0.99619141 0.99624023 0.99550781 0.98920898]\n",
      "\n",
      "Evaluation:\n",
      "2019-03-03T12:53:28.560751: step 27400, loss 0.45433, acc [0.98291103 0.97298001 0.96331928 0.95435934] \n",
      "\n",
      "2019-03-03T12:53:32.174109: step 27440, loss 0.0127796, acc [0.9956543  0.99560547 0.99604492 0.98833008]\n",
      "2019-03-03T12:53:36.287722: step 27480, loss 0.0124233, acc [0.99599609 0.99575195 0.99555664 0.98837891]\n",
      "2019-03-03T12:53:40.398810: step 27520, loss 0.0134383, acc [0.99536133 0.99453125 0.99560547 0.98686523]\n",
      "2019-03-03T12:53:44.557062: step 27560, loss 0.0150654, acc [0.99526367 0.99462891 0.99467773 0.98588867]\n",
      "2019-03-03T12:53:48.704826: step 27600, loss 0.0136613, acc [0.99511719 0.9949707  0.99560547 0.98725586]\n",
      "\n",
      "Evaluation:\n",
      "2019-03-03T12:53:55.969504: step 27600, loss 0.452389, acc [0.98268078 0.97302005 0.96363964 0.95469972] \n",
      "\n",
      "2019-03-03T12:53:59.581329: step 27640, loss 0.0151769, acc [0.99453125 0.99521484 0.99492187 0.98642578]\n",
      "2019-03-03T12:54:03.738841: step 27680, loss 0.0144929, acc [0.99584961 0.99487305 0.99555664 0.98779297]\n",
      "2019-03-03T12:54:07.913658: step 27720, loss 0.0146956, acc [0.99648437 0.99448242 0.99541016 0.98793945]\n",
      "2019-03-03T12:54:12.013627: step 27760, loss 0.0152301, acc [0.99438477 0.99487305 0.99472656 0.98574219]\n",
      "2019-03-03T12:54:16.193695: step 27800, loss 0.0138905, acc [0.99550781 0.99501953 0.99501953 0.98710937]\n",
      "\n",
      "Evaluation:\n",
      "2019-03-03T12:54:23.408404: step 27800, loss 0.454415, acc [0.98275085 0.97316021 0.96364965 0.95451952] \n",
      "\n",
      "2019-03-03T12:54:27.024196: step 27840, loss 0.0147259, acc [0.99477539 0.99472656 0.99477539 0.9862793 ]\n",
      "2019-03-03T12:54:31.145768: step 27880, loss 0.0156804, acc [0.99487305 0.99443359 0.99545898 0.98642578]\n",
      "2019-03-03T12:54:35.290936: step 27920, loss 0.0171362, acc [0.99438477 0.99448242 0.99428711 0.98510742]\n",
      "2019-03-03T12:54:39.418775: step 27960, loss 0.0179912, acc [0.99526367 0.99526367 0.99428711 0.98642578]\n",
      "2019-03-03T12:54:43.573336: step 28000, loss 0.0166064, acc [0.99472656 0.99477539 0.99433594 0.98554688]\n",
      "\n",
      "Evaluation:\n",
      "2019-03-03T12:54:50.837492: step 28000, loss 0.456248, acc [0.98298111 0.97317022 0.96328925 0.95458959] \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-03T12:54:54.451843: step 28040, loss 0.0200046, acc [0.9949707  0.99282227 0.9925293  0.98266602]\n",
      "2019-03-03T12:54:58.614093: step 28080, loss 0.0174917, acc [0.99453125 0.99379883 0.99477539 0.98452148]\n",
      "2019-03-03T12:55:02.865133: step 28120, loss 0.0132544, acc [0.99633789 0.99462891 0.99545898 0.98774414]\n",
      "2019-03-03T12:55:07.013251: step 28160, loss 0.0136516, acc [0.99545898 0.99545898 0.99560547 0.98769531]\n",
      "2019-03-03T12:55:11.117477: step 28200, loss 0.0107148, acc [0.99619141 0.99633789 0.99599609 0.9894043 ]\n",
      "\n",
      "Evaluation:\n",
      "2019-03-03T12:55:18.425888: step 28200, loss 0.456316, acc [0.98287099 0.97308012 0.9637798  0.95456957] \n",
      "\n",
      "2019-03-03T12:55:22.046625: step 28240, loss 0.0119336, acc [0.9956543  0.99536133 0.99638672 0.98867187]\n",
      "2019-03-03T12:55:26.146204: step 28280, loss 0.0113722, acc [0.99560547 0.99594727 0.99667969 0.98916016]\n",
      "2019-03-03T12:55:30.250602: step 28320, loss 0.0147625, acc [0.99487305 0.99550781 0.99526367 0.98710937]\n",
      "2019-03-03T12:55:34.363012: step 28360, loss 0.0114409, acc [0.99545898 0.99580078 0.99580078 0.98803711]\n",
      "2019-03-03T12:55:38.488739: step 28400, loss 0.0123348, acc [0.99594727 0.99536133 0.99614258 0.98862305]\n",
      "\n",
      "Evaluation:\n",
      "2019-03-03T12:55:45.754643: step 28400, loss 0.465544, acc [0.98266075 0.97291994 0.96317913 0.95423921] \n",
      "\n",
      "2019-03-03T12:55:49.366015: step 28440, loss 0.0161439, acc [0.9949707  0.99492187 0.99453125 0.9862793 ]\n",
      "2019-03-03T12:55:53.524909: step 28480, loss 0.0138239, acc [0.99521484 0.99482422 0.9953125  0.98691406]\n",
      "2019-03-03T12:55:57.626153: step 28520, loss 0.0141158, acc [0.99521484 0.99550781 0.9953125  0.98710937]\n",
      "2019-03-03T12:56:01.742189: step 28560, loss 0.015011, acc [0.99570313 0.99555664 0.99492187 0.98759766]\n",
      "2019-03-03T12:56:05.913043: step 28600, loss 0.0131363, acc [0.99501953 0.99462891 0.9956543  0.98701172]\n",
      "\n",
      "Evaluation:\n",
      "2019-03-03T12:56:13.255205: step 28600, loss 0.460387, acc [0.98281092 0.97309013 0.96346945 0.95462964] \n",
      "\n",
      "2019-03-03T12:56:16.885418: step 28640, loss 0.0127751, acc [0.99482422 0.99536133 0.99545898 0.98642578]\n",
      "2019-03-03T12:56:21.066472: step 28680, loss 0.0159791, acc [0.99545898 0.99433594 0.99438477 0.98574219]\n",
      "2019-03-03T12:56:25.250997: step 28720, loss 0.0147621, acc [0.99526367 0.99472656 0.99526367 0.98632812]\n",
      "2019-03-03T12:56:29.423690: step 28760, loss 0.0179093, acc [0.99453125 0.99462891 0.99418945 0.98500977]\n",
      "2019-03-03T12:56:33.608536: step 28800, loss 0.0194844, acc [0.99560547 0.99418945 0.99394531 0.9859375 ]\n",
      "\n",
      "Evaluation:\n",
      "2019-03-03T12:56:40.894582: step 28800, loss 0.451876, acc [0.98314129 0.97281983 0.96372974 0.95472975] \n",
      "\n",
      "2019-03-03T12:56:44.539145: step 28840, loss 0.016532, acc [0.99536133 0.99321289 0.99482422 0.98481445]\n",
      "2019-03-03T12:56:48.843869: step 28880, loss 0.0128973, acc [0.99604492 0.99433594 0.99604492 0.98813477]\n",
      "2019-03-03T12:56:53.009647: step 28920, loss 0.0115432, acc [0.9965332  0.99594727 0.99619141 0.98969727]\n",
      "2019-03-03T12:56:57.125432: step 28960, loss 0.0121648, acc [0.99638672 0.99589844 0.99609375 0.98916016]\n",
      "2019-03-03T12:57:01.260793: step 29000, loss 0.013027, acc [0.99648437 0.99604492 0.99550781 0.9890625 ]\n",
      "\n",
      "Evaluation:\n",
      "2019-03-03T12:57:08.583356: step 29000, loss 0.460826, acc [0.98330146 0.97354063 0.96389993 0.95526034] \n",
      "\n",
      "2019-03-03T12:57:12.227468: step 29040, loss 0.0112841, acc [0.99570313 0.99614258 0.99619141 0.98881836]\n",
      "2019-03-03T12:57:16.421609: step 29080, loss 0.0113296, acc [0.99638672 0.99526367 0.99667969 0.98955078]\n",
      "2019-03-03T12:57:20.528651: step 29120, loss 0.012962, acc [0.9956543  0.9956543  0.99609375 0.98867187]\n",
      "2019-03-03T12:57:24.712950: step 29160, loss 0.0133889, acc [0.99438477 0.99482422 0.99589844 0.98647461]\n",
      "2019-03-03T12:57:28.872961: step 29200, loss 0.0134231, acc [0.99472656 0.9953125  0.99575195 0.98725586]\n",
      "\n",
      "Evaluation:\n",
      "2019-03-03T12:57:36.104187: step 29200, loss 0.469586, acc [0.98303116 0.97289992 0.96320916 0.95438937] \n",
      "\n",
      "2019-03-03T12:57:39.733424: step 29240, loss 0.0145625, acc [0.99472656 0.99501953 0.99594727 0.98676758]\n",
      "2019-03-03T12:57:43.854685: step 29280, loss 0.0141477, acc [0.99550781 0.99477539 0.99541016 0.98691406]\n",
      "2019-03-03T12:57:48.041499: step 29320, loss 0.016409, acc [0.9956543  0.99453125 0.99526367 0.98745117]\n",
      "2019-03-03T12:57:52.149866: step 29360, loss 0.0142795, acc [0.9949707  0.99477539 0.9953125  0.98657227]\n",
      "2019-03-03T12:57:56.266562: step 29400, loss 0.0151806, acc [0.99599609 0.99467773 0.99487305 0.9871582 ]\n",
      "\n",
      "Evaluation:\n",
      "2019-03-03T12:58:03.555777: step 29400, loss 0.454875, acc [0.98305119 0.9731502  0.96366967 0.95479983] \n",
      "\n",
      "2019-03-03T12:58:07.179553: step 29440, loss 0.0135387, acc [0.99584961 0.99414062 0.99628906 0.98754883]\n",
      "2019-03-03T12:58:11.309247: step 29480, loss 0.0160889, acc [0.99399414 0.99418945 0.99526367 0.98588867]\n",
      "2019-03-03T12:58:15.439700: step 29520, loss 0.0154704, acc [0.99506836 0.99521484 0.99477539 0.98686523]\n",
      "2019-03-03T12:58:19.559969: step 29560, loss 0.0141079, acc [0.99472656 0.99418945 0.99594727 0.98598633]\n",
      "2019-03-03T12:58:23.689069: step 29600, loss 0.0168123, acc [0.99477539 0.99438477 0.99418945 0.98554688]\n",
      "\n",
      "Evaluation:\n",
      "2019-03-03T12:58:30.928782: step 29600, loss 0.454007, acc [0.98331148 0.9735106  0.96391995 0.95529037] \n",
      "\n",
      "2019-03-03T12:58:34.547548: step 29640, loss 0.0143639, acc [0.9949707  0.99545898 0.99506836 0.9875    ]\n",
      "2019-03-03T12:58:38.789290: step 29680, loss 0.0121899, acc [0.99624023 0.99560547 0.99633789 0.98901367]\n",
      "2019-03-03T12:58:42.904981: step 29720, loss 0.0113643, acc [0.99599609 0.99599609 0.99624023 0.98891602]\n",
      "2019-03-03T12:58:47.009393: step 29760, loss 0.0105863, acc [0.99580078 0.99609375 0.99711914 0.99003906]\n",
      "2019-03-03T12:58:51.117761: step 29800, loss 0.0115001, acc [0.99663086 0.99570313 0.99604492 0.9894043 ]\n",
      "\n",
      "Evaluation:\n",
      "2019-03-03T12:58:58.369386: step 29800, loss 0.469121, acc [0.98309123 0.97343051 0.9637798  0.95481985] \n",
      "\n",
      "2019-03-03T12:59:01.975307: step 29840, loss 0.0116787, acc [0.99506836 0.9956543  0.99633789 0.98842773]\n",
      "2019-03-03T12:59:06.147591: step 29880, loss 0.0124721, acc [0.99536133 0.99555664 0.99633789 0.98833008]\n",
      "2019-03-03T12:59:10.272261: step 29920, loss 0.011966, acc [0.99545898 0.99541016 0.99609375 0.98808594]\n",
      "2019-03-03T12:59:14.394390: step 29960, loss 0.0124171, acc [0.99638672 0.99492187 0.99604492 0.98808594]\n",
      "2019-03-03T12:59:18.501303: step 30000, loss 0.0129611, acc [0.9953125  0.99594727 0.9956543  0.98793945]\n",
      "\n",
      "Evaluation:\n",
      "2019-03-03T12:59:25.714108: step 30000, loss 0.463848, acc [0.9834216  0.97372083 0.96389993 0.95548058] \n",
      "\n",
      "2019-03-03T12:59:29.326970: step 30040, loss 0.0133672, acc [0.99536133 0.9949707  0.99599609 0.98779297]\n",
      "2019-03-03T12:59:33.440827: step 30080, loss 0.0156811, acc [0.9949707  0.99521484 0.99492187 0.98613281]\n",
      "2019-03-03T12:59:37.543802: step 30120, loss 0.0133542, acc [0.99624023 0.9949707  0.99477539 0.98710937]\n",
      "2019-03-03T12:59:41.723884: step 30160, loss 0.0151998, acc [0.99506836 0.99521484 0.9953125  0.98710937]\n",
      "2019-03-03T12:59:45.948772: step 30200, loss 0.0139113, acc [0.99594727 0.99545898 0.99536133 0.98774414]\n",
      "\n",
      "Evaluation:\n",
      "2019-03-03T12:59:53.206244: step 30200, loss 0.471548, acc [0.98322138 0.97319024 0.96374976 0.95524032] \n",
      "\n",
      "2019-03-03T12:59:56.828036: step 30240, loss 0.0168231, acc [0.99414062 0.99458008 0.99458008 0.98486328]\n",
      "2019-03-03T13:00:00.935843: step 30280, loss 0.015838, acc [0.99433594 0.99511719 0.99472656 0.9855957 ]\n",
      "2019-03-03T13:00:05.116448: step 30320, loss 0.0144276, acc [0.99604492 0.9953125  0.99506836 0.98793945]\n",
      "2019-03-03T13:00:09.220210: step 30360, loss 0.0161915, acc [0.99477539 0.99462891 0.99487305 0.98637695]\n",
      "2019-03-03T13:00:13.351433: step 30400, loss 0.0147971, acc [0.99506836 0.99443359 0.99560547 0.98662109]\n",
      "\n",
      "Evaluation:\n",
      "2019-03-03T13:00:20.632634: step 30400, loss 0.462272, acc [0.98292104 0.97321026 0.96413018 0.95492997] \n",
      "\n",
      "2019-03-03T13:00:24.374953: step 30440, loss 0.0126498, acc [0.99619141 0.99584961 0.99575195 0.98920898]\n",
      "2019-03-03T13:00:28.544483: step 30480, loss 0.00971266, acc [0.9965332  0.99604492 0.99677734 0.99033203]\n",
      "2019-03-03T13:00:32.644157: step 30520, loss 0.00994008, acc [0.996875   0.99599609 0.996875   0.99033203]\n",
      "2019-03-03T13:00:36.805218: step 30560, loss 0.01155, acc [0.99624023 0.99516602 0.9965332  0.98901367]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-03T13:00:40.976955: step 30600, loss 0.0106021, acc [0.99521484 0.99584961 0.99677734 0.98886719]\n",
      "\n",
      "Evaluation:\n",
      "2019-03-03T13:00:48.218651: step 30600, loss 0.470044, acc [0.98318133 0.97319024 0.96354954 0.95476979] \n",
      "\n",
      "2019-03-03T13:00:51.838437: step 30640, loss 0.0117543, acc [0.99589844 0.99604492 0.99638672 0.98935547]\n",
      "2019-03-03T13:00:55.947969: step 30680, loss 0.0128178, acc [0.99609375 0.99580078 0.99570313 0.98881836]\n",
      "2019-03-03T13:01:00.132969: step 30720, loss 0.0131512, acc [0.99550781 0.99506836 0.99570313 0.9875    ]\n",
      "2019-03-03T13:01:04.262269: step 30760, loss 0.0124798, acc [0.99609375 0.9956543  0.9956543  0.98876953]\n",
      "2019-03-03T13:01:08.372421: step 30800, loss 0.0126981, acc [0.99521484 0.9956543  0.99648437 0.98833008]\n",
      "\n",
      "Evaluation:\n",
      "2019-03-03T13:01:15.685623: step 30800, loss 0.472093, acc [0.9832414  0.97316021 0.96394998 0.95519026] \n",
      "\n",
      "2019-03-03T13:01:19.304495: step 30840, loss 0.0141745, acc [0.99633789 0.99575195 0.99541016 0.98876953]\n",
      "2019-03-03T13:01:23.418419: step 30880, loss 0.0125977, acc [0.99521484 0.9949707  0.99594727 0.98808594]\n",
      "2019-03-03T13:01:27.558187: step 30920, loss 0.0139102, acc [0.99472656 0.99541016 0.99526367 0.98681641]\n",
      "2019-03-03T13:01:31.677577: step 30960, loss 0.0137926, acc [0.9956543  0.99492187 0.99609375 0.98769531]\n",
      "2019-03-03T13:01:35.793027: step 31000, loss 0.0151556, acc [0.99555664 0.99418945 0.99521484 0.98647461]\n",
      "\n",
      "Evaluation:\n",
      "2019-03-03T13:01:43.022882: step 31000, loss 0.46575, acc [0.98313128 0.9732403  0.9646808  0.95578092] \n",
      "\n",
      "2019-03-03T13:01:46.637233: step 31040, loss 0.0131606, acc [0.99575195 0.99555664 0.99580078 0.98852539]\n",
      "2019-03-03T13:01:50.758212: step 31080, loss 0.0140356, acc [0.99541016 0.99453125 0.99638672 0.98779297]\n",
      "2019-03-03T13:01:54.924098: step 31120, loss 0.0133425, acc [0.99594727 0.9949707  0.99599609 0.98808594]\n",
      "2019-03-03T13:01:59.024989: step 31160, loss 0.0140714, acc [0.99570313 0.99536133 0.99516602 0.9875    ]\n",
      "2019-03-03T13:02:03.186925: step 31200, loss 0.0152856, acc [0.99526367 0.99443359 0.99487305 0.98657227]\n",
      "\n",
      "Evaluation:\n",
      "2019-03-03T13:02:10.388343: step 31200, loss 0.469987, acc [0.98310124 0.97331037 0.96426033 0.95537046] \n",
      "\n",
      "2019-03-03T13:02:14.154468: step 31240, loss 0.0102445, acc [0.99643555 0.99638672 0.99663086 0.99013672]\n",
      "2019-03-03T13:02:18.320372: step 31280, loss 0.0122867, acc [0.99550781 0.99619141 0.99628906 0.98911133]\n",
      "2019-03-03T13:02:22.431687: step 31320, loss 0.010484, acc [0.99545898 0.99614258 0.99702148 0.98955078]\n",
      "2019-03-03T13:02:26.553964: step 31360, loss 0.00958334, acc [0.99648437 0.99550781 0.99672852 0.98959961]\n",
      "2019-03-03T13:02:30.714356: step 31400, loss 0.013653, acc [0.99570313 0.9953125  0.99604492 0.98837891]\n",
      "\n",
      "Evaluation:\n",
      "2019-03-03T13:02:37.987312: step 31400, loss 0.473353, acc [0.98339156 0.97345053 0.96429036 0.95560072] \n",
      "\n",
      "2019-03-03T13:02:41.606622: step 31440, loss 0.0095406, acc [0.99672852 0.99594727 0.99692383 0.99052734]\n",
      "2019-03-03T13:02:45.803819: step 31480, loss 0.0122785, acc [0.99658203 0.99536133 0.99526367 0.98818359]\n",
      "2019-03-03T13:02:49.957128: step 31520, loss 0.0114337, acc [0.99599609 0.99614258 0.99648437 0.98984375]\n",
      "2019-03-03T13:02:54.132561: step 31560, loss 0.0121993, acc [0.99584961 0.99555664 0.99545898 0.98803711]\n",
      "2019-03-03T13:02:58.301956: step 31600, loss 0.0131943, acc [0.99584961 0.99506836 0.99614258 0.98842773]\n",
      "\n",
      "Evaluation:\n",
      "2019-03-03T13:03:05.614388: step 31600, loss 0.476852, acc [0.98340158 0.97329035 0.9644105  0.95576089] \n",
      "\n",
      "2019-03-03T13:03:09.246098: step 31640, loss 0.01553, acc [0.99506836 0.99555664 0.99492187 0.98681641]\n",
      "2019-03-03T13:03:13.458027: step 31680, loss 0.0127807, acc [0.99589844 0.99619141 0.99594727 0.98876953]\n",
      "2019-03-03T13:03:17.621750: step 31720, loss 0.0142695, acc [0.99526367 0.9956543  0.99516602 0.98735352]\n",
      "2019-03-03T13:03:21.789735: step 31760, loss 0.0150969, acc [0.99501953 0.99545898 0.99458008 0.98662109]\n",
      "2019-03-03T13:03:25.960101: step 31800, loss 0.0150124, acc [0.99555664 0.99555664 0.99506836 0.98774414]\n",
      "\n",
      "Evaluation:\n",
      "2019-03-03T13:03:33.307663: step 31800, loss 0.47112, acc [0.98287099 0.97340047 0.96396    0.95510016] \n",
      "\n",
      "2019-03-03T13:03:36.945326: step 31840, loss 0.0147311, acc [0.99545898 0.99492187 0.99521484 0.98710937]\n",
      "2019-03-03T13:03:41.133549: step 31880, loss 0.0150696, acc [0.99599609 0.99541016 0.99550781 0.98823242]\n",
      "2019-03-03T13:03:45.317149: step 31920, loss 0.0151559, acc [0.99511719 0.99521484 0.99492187 0.98710937]\n",
      "2019-03-03T13:03:49.514794: step 31960, loss 0.0130328, acc [0.99516602 0.99501953 0.99594727 0.98745117]\n",
      "2019-03-03T13:03:53.835449: step 32000, loss 0.0117727, acc [0.99604492 0.9949707  0.99604492 0.98833008]\n",
      "\n",
      "Evaluation:\n",
      "2019-03-03T13:04:01.228826: step 32000, loss 0.470561, acc [0.98303116 0.97303006 0.96385988 0.95490995] \n",
      "\n",
      "2019-03-03T13:04:04.868015: step 32040, loss 0.0093339, acc [0.99604492 0.99697266 0.996875   0.99111328]\n",
      "2019-03-03T13:04:09.009312: step 32080, loss 0.0106633, acc [0.99619141 0.99624023 0.99575195 0.9887207 ]\n",
      "2019-03-03T13:04:13.188605: step 32120, loss 0.0106169, acc [0.99619141 0.99628906 0.99638672 0.99008789]\n",
      "2019-03-03T13:04:17.388052: step 32160, loss 0.0123746, acc [0.99594727 0.99580078 0.99667969 0.98945313]\n",
      "2019-03-03T13:04:21.559480: step 32200, loss 0.0100026, acc [0.99536133 0.99667969 0.996875   0.98979492]\n",
      "\n",
      "Evaluation:\n",
      "2019-03-03T13:04:28.949750: step 32200, loss 0.474492, acc [0.9827909  0.9734205  0.96427034 0.95551062] \n",
      "\n",
      "2019-03-03T13:04:32.589397: step 32240, loss 0.0110431, acc [0.99584961 0.99633789 0.99638672 0.98925781]\n",
      "2019-03-03T13:04:36.745453: step 32280, loss 0.00958539, acc [0.99648437 0.99599609 0.99711914 0.99052734]\n",
      "2019-03-03T13:04:40.899424: step 32320, loss 0.00916929, acc [0.99570313 0.99633789 0.99726563 0.99008789]\n",
      "2019-03-03T13:04:45.081570: step 32360, loss 0.012096, acc [0.99624023 0.99643555 0.99619141 0.98994141]\n",
      "2019-03-03T13:04:49.248214: step 32400, loss 0.0122967, acc [0.99575195 0.99633789 0.99638672 0.98950195]\n",
      "\n",
      "Evaluation:\n",
      "2019-03-03T13:04:56.443994: step 32400, loss 0.480895, acc [0.98314129 0.97349057 0.96431038 0.95587102] \n",
      "\n",
      "2019-03-03T13:05:00.057354: step 32440, loss 0.0127906, acc [0.99609375 0.9953125  0.99555664 0.98842773]\n",
      "2019-03-03T13:05:04.167704: step 32480, loss 0.0116448, acc [0.99580078 0.99633789 0.99609375 0.98911133]\n",
      "2019-03-03T13:05:08.266649: step 32520, loss 0.014401, acc [0.99560547 0.99555664 0.99550781 0.98808594]\n",
      "2019-03-03T13:05:12.454361: step 32560, loss 0.0126328, acc [0.99570313 0.99584961 0.99594727 0.98852539]\n",
      "2019-03-03T13:05:16.563216: step 32600, loss 0.0126911, acc [0.99604492 0.99501953 0.9953125  0.98735352]\n",
      "\n",
      "Evaluation:\n",
      "2019-03-03T13:05:23.794397: step 32600, loss 0.475693, acc [0.98318133 0.97337044 0.9640501  0.95592107] \n",
      "\n",
      "2019-03-03T13:05:27.416189: step 32640, loss 0.0122441, acc [0.99580078 0.99560547 0.99609375 0.98847656]\n",
      "2019-03-03T13:05:31.526101: step 32680, loss 0.0120156, acc [0.99555664 0.9953125  0.99682617 0.98886719]\n",
      "2019-03-03T13:05:35.626384: step 32720, loss 0.0130835, acc [0.9956543  0.99516602 0.99536133 0.98769531]\n",
      "2019-03-03T13:05:39.738988: step 32760, loss 0.0125003, acc [0.99541016 0.99584961 0.99633789 0.98920898]\n",
      "2019-03-03T13:05:44.056243: step 32800, loss 0.00965109, acc [0.99638672 0.996875   0.99760742 0.99145508]\n",
      "\n",
      "Evaluation:\n",
      "2019-03-03T13:05:51.308752: step 32800, loss 0.473958, acc [0.98294106 0.97344052 0.96445054 0.95581095] \n",
      "\n",
      "2019-03-03T13:05:54.921616: step 32840, loss 0.00853789, acc [0.99648437 0.99663086 0.99716797 0.99091797]\n",
      "2019-03-03T13:05:59.037264: step 32880, loss 0.0104533, acc [0.99628906 0.99560547 0.99707031 0.98969727]\n",
      "2019-03-03T13:06:03.155532: step 32920, loss 0.00887269, acc [0.99663086 0.99638672 0.99750977 0.99145508]\n",
      "2019-03-03T13:06:07.278219: step 32960, loss 0.0115946, acc [0.99633789 0.99555664 0.99682617 0.98935547]\n",
      "2019-03-03T13:06:11.388614: step 33000, loss 0.0108671, acc [0.99584961 0.9956543  0.99677734 0.98916016]\n",
      "\n",
      "Evaluation:\n",
      "2019-03-03T13:06:18.666525: step 33000, loss 0.482709, acc [0.98347165 0.97358067 0.9643204  0.95591106] \n",
      "\n",
      "2019-03-03T13:06:22.278891: step 33040, loss 0.0103382, acc [0.9956543  0.99619141 0.99624023 0.9890625 ]\n",
      "2019-03-03T13:06:26.399781: step 33080, loss 0.0112238, acc [0.99614258 0.99501953 0.99658203 0.98881836]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-03T13:06:30.531584: step 33120, loss 0.0119705, acc [0.99599609 0.9949707  0.99648437 0.98862305]\n",
      "2019-03-03T13:06:34.627225: step 33160, loss 0.0106881, acc [0.99628906 0.9956543  0.99614258 0.98916016]\n",
      "2019-03-03T13:06:38.741424: step 33200, loss 0.0115274, acc [0.99594727 0.99589844 0.99589844 0.98867187]\n",
      "\n",
      "Evaluation:\n",
      "2019-03-03T13:06:46.016175: step 33200, loss 0.484526, acc [0.98343161 0.9732403  0.96407012 0.95556067] \n",
      "\n",
      "2019-03-03T13:06:49.631021: step 33240, loss 0.0117261, acc [0.99609375 0.99604492 0.99599609 0.98916016]\n",
      "2019-03-03T13:06:53.750581: step 33280, loss 0.0120689, acc [0.99584961 0.99589844 0.99555664 0.98833008]\n",
      "2019-03-03T13:06:57.970052: step 33320, loss 0.0129943, acc [0.99536133 0.99526367 0.99521484 0.98706055]\n",
      "2019-03-03T13:07:02.149565: step 33360, loss 0.0116537, acc [0.99628906 0.99482422 0.99624023 0.98828125]\n",
      "2019-03-03T13:07:06.324669: step 33400, loss 0.0133282, acc [0.99555664 0.99628906 0.99555664 0.98862305]\n",
      "\n",
      "Evaluation:\n",
      "2019-03-03T13:07:13.601530: step 33400, loss 0.484918, acc [0.98290102 0.97326032 0.96443052 0.95577091] \n",
      "\n",
      "2019-03-03T13:07:17.228732: step 33440, loss 0.0132286, acc [0.9953125  0.99638672 0.99521484 0.98823242]\n",
      "2019-03-03T13:07:21.438777: step 33480, loss 0.0141099, acc [0.99594727 0.99560547 0.99511719 0.98828125]\n",
      "2019-03-03T13:07:25.555081: step 33520, loss 0.0153434, acc [0.99506836 0.99560547 0.99521484 0.9878418 ]\n",
      "2019-03-03T13:07:29.786209: step 33560, loss 0.0103685, acc [0.99628906 0.99570313 0.99697266 0.99013672]\n",
      "2019-03-03T13:07:33.947150: step 33600, loss 0.00827443, acc [0.99702148 0.99633789 0.99760742 0.99169922]\n",
      "\n",
      "Evaluation:\n",
      "2019-03-03T13:07:41.228565: step 33600, loss 0.47867, acc [0.98282093 0.97316021 0.96429036 0.95563075] \n",
      "\n",
      "2019-03-03T13:07:44.846386: step 33640, loss 0.0107441, acc [0.99619141 0.99677734 0.99677734 0.990625  ]\n",
      "2019-03-03T13:07:49.011916: step 33680, loss 0.00778738, acc [0.99697266 0.99633789 0.99741211 0.99155273]\n",
      "2019-03-03T13:07:53.120998: step 33720, loss 0.0118883, acc [0.99609375 0.996875   0.99584961 0.98984375]\n",
      "2019-03-03T13:07:57.242171: step 33760, loss 0.0105657, acc [0.99594727 0.99619141 0.99638672 0.98925781]\n",
      "2019-03-03T13:08:01.408295: step 33800, loss 0.0108434, acc [0.99633789 0.99589844 0.99619141 0.98925781]\n",
      "\n",
      "Evaluation:\n",
      "2019-03-03T13:08:08.688575: step 33800, loss 0.489238, acc [0.98301114 0.97316021 0.96412017 0.95546056] \n",
      "\n",
      "2019-03-03T13:08:12.312351: step 33840, loss 0.0119911, acc [0.99672852 0.99584961 0.99594727 0.98955078]\n",
      "2019-03-03T13:08:16.458909: step 33880, loss 0.010888, acc [0.99545898 0.99619141 0.99624023 0.98876953]\n",
      "2019-03-03T13:08:20.642435: step 33920, loss 0.0131158, acc [0.99599609 0.99580078 0.99594727 0.98964844]\n",
      "2019-03-03T13:08:24.800027: step 33960, loss 0.0104504, acc [0.99628906 0.99599609 0.99643555 0.98959961]\n",
      "2019-03-03T13:08:28.977353: step 34000, loss 0.0123947, acc [0.99614258 0.99589844 0.99555664 0.98901367]\n",
      "\n",
      "Evaluation:\n",
      "2019-03-03T13:08:36.170900: step 34000, loss 0.485388, acc [0.98298111 0.97323029 0.96397001 0.95523031] \n",
      "\n",
      "2019-03-03T13:08:39.787685: step 34040, loss 0.0121784, acc [0.99628906 0.99609375 0.99609375 0.98950195]\n",
      "2019-03-03T13:08:43.937389: step 34080, loss 0.00968734, acc [0.99633789 0.99614258 0.99663086 0.98984375]\n",
      "2019-03-03T13:08:48.091609: step 34120, loss 0.0121252, acc [0.99541016 0.99619141 0.99604492 0.98911133]\n",
      "2019-03-03T13:09:01.194598: step 34160, loss 0.0130154, acc [0.99589844 0.99506836 0.99614258 0.98857422]\n",
      "2019-03-03T13:09:05.357154: step 34200, loss 0.0127148, acc [0.99521484 0.99594727 0.99575195 0.9878418 ]\n",
      "\n",
      "Evaluation:\n",
      "2019-03-03T13:09:12.611204: step 34200, loss 0.490031, acc [0.9832414  0.97340047 0.96451061 0.95573086] \n",
      "\n",
      "2019-03-03T13:09:16.221535: step 34240, loss 0.0145516, acc [0.99570313 0.99511719 0.99506836 0.9875    ]\n",
      "2019-03-03T13:09:20.379576: step 34280, loss 0.0149503, acc [0.99580078 0.99570313 0.99487305 0.98769531]\n",
      "2019-03-03T13:09:24.508334: step 34320, loss 0.0122743, acc [0.99570313 0.99584961 0.99619141 0.98886719]\n",
      "2019-03-03T13:09:28.754235: step 34360, loss 0.00997175, acc [0.99614258 0.99692383 0.99702148 0.99072266]\n",
      "2019-03-03T13:09:32.867068: step 34400, loss 0.00885894, acc [0.99716797 0.99648437 0.99697266 0.99130859]\n",
      "\n",
      "Evaluation:\n",
      "2019-03-03T13:09:40.145222: step 34400, loss 0.490179, acc [0.98345163 0.97340047 0.96415021 0.95598114] \n",
      "\n",
      "2019-03-03T13:09:43.774453: step 34440, loss 0.00847983, acc [0.99672852 0.99663086 0.99726563 0.99145508]\n",
      "2019-03-03T13:09:47.896448: step 34480, loss 0.0104936, acc [0.99570313 0.99672852 0.996875   0.99033203]\n",
      "2019-03-03T13:09:52.060035: step 34520, loss 0.00937802, acc [0.99682617 0.99726563 0.996875   0.99160156]\n",
      "2019-03-03T13:09:56.181357: step 34560, loss 0.0109504, acc [0.99672852 0.99619141 0.99658203 0.99042969]\n",
      "2019-03-03T13:10:00.292794: step 34600, loss 0.0100783, acc [0.99628906 0.99633789 0.99648437 0.99023438]\n",
      "\n",
      "Evaluation:\n",
      "2019-03-03T13:10:07.501012: step 34600, loss 0.481885, acc [0.98323139 0.97368079 0.96434042 0.95602118] \n",
      "\n",
      "2019-03-03T13:10:11.115812: step 34640, loss 0.00946006, acc [0.99624023 0.99599609 0.996875   0.98994141]\n",
      "2019-03-03T13:10:15.305219: step 34680, loss 0.0102422, acc [0.99614258 0.9956543  0.99677734 0.99003906]\n",
      "2019-03-03T13:10:19.479354: step 34720, loss 0.0126601, acc [0.99658203 0.99643555 0.99604492 0.99023438]\n",
      "2019-03-03T13:10:23.591688: step 34760, loss 0.0114953, acc [0.99599609 0.99609375 0.99609375 0.98896484]\n",
      "2019-03-03T13:10:27.756803: step 34800, loss 0.0107019, acc [0.99614258 0.99599609 0.99624023 0.98974609]\n",
      "\n",
      "Evaluation:\n",
      "2019-03-03T13:10:34.962305: step 34800, loss 0.485639, acc [0.98314129 0.97359069 0.96392996 0.95574087] \n",
      "\n",
      "2019-03-03T13:10:38.580590: step 34840, loss 0.012286, acc [0.99555664 0.99604492 0.99575195 0.98862305]\n",
      "2019-03-03T13:10:42.696891: step 34880, loss 0.0138841, acc [0.99609375 0.99506836 0.9956543  0.98789063]\n",
      "2019-03-03T13:10:46.812807: step 34920, loss 0.00997224, acc [0.99702148 0.99541016 0.99707031 0.99047852]\n",
      "2019-03-03T13:10:50.931403: step 34960, loss 0.0152567, acc [0.99609375 0.99633789 0.99492187 0.9887207 ]\n",
      "2019-03-03T13:10:55.101142: step 35000, loss 0.0124165, acc [0.99648437 0.99609375 0.99580078 0.98950195]\n",
      "\n",
      "Evaluation:\n",
      "2019-03-03T13:11:02.384291: step 35000, loss 0.48544, acc [0.98322138 0.97344052 0.96384987 0.95570083] \n",
      "\n",
      "2019-03-03T13:11:06.005541: step 35040, loss 0.0130734, acc [0.99628906 0.99526367 0.9949707  0.98764648]\n",
      "2019-03-03T13:11:10.192278: step 35080, loss 0.0127673, acc [0.99663086 0.99521484 0.99570313 0.98920898]\n",
      "2019-03-03T13:11:14.469033: step 35120, loss 0.0115558, acc [0.99599609 0.99624023 0.99609375 0.98955078]\n",
      "2019-03-03T13:11:18.572149: step 35160, loss 0.00982005, acc [0.99667969 0.996875   0.99682617 0.99121094]\n",
      "2019-03-03T13:11:22.697096: step 35200, loss 0.00991393, acc [0.99599609 0.99624023 0.99682617 0.98984375]\n",
      "\n",
      "Evaluation:\n",
      "2019-03-03T13:11:29.949110: step 35200, loss 0.48775, acc [0.9829711  0.97326032 0.9642303  0.95568081] \n",
      "\n",
      "2019-03-03T13:11:33.571893: step 35240, loss 0.00975853, acc [0.99648437 0.9956543  0.99663086 0.98950195]\n",
      "2019-03-03T13:11:37.679523: step 35280, loss 0.0109751, acc [0.99624023 0.99648437 0.99663086 0.99013672]\n",
      "2019-03-03T13:11:41.786526: step 35320, loss 0.0112132, acc [0.9965332  0.99711914 0.99609375 0.990625  ]\n",
      "2019-03-03T13:11:46.003023: step 35360, loss 0.0104067, acc [0.99584961 0.99628906 0.99697266 0.98989258]\n",
      "2019-03-03T13:11:50.114118: step 35400, loss 0.007117, acc [0.99663086 0.99682617 0.99775391 0.99213867]\n",
      "\n",
      "Evaluation:\n",
      "2019-03-03T13:11:57.368417: step 35400, loss 0.491186, acc [0.98320135 0.97335042 0.96460071 0.95629148] \n",
      "\n",
      "2019-03-03T13:12:00.987232: step 35440, loss 0.0096631, acc [0.99667969 0.99614258 0.99692383 0.99072266]\n",
      "2019-03-03T13:12:05.097016: step 35480, loss 0.00977673, acc [0.99643555 0.9972168  0.99707031 0.99125977]\n",
      "2019-03-03T13:12:09.207367: step 35520, loss 0.0113007, acc [0.99638672 0.99609375 0.99648437 0.98979492]\n",
      "2019-03-03T13:12:13.389674: step 35560, loss 0.011221, acc [0.996875   0.9965332  0.99658203 0.99057617]\n",
      "2019-03-03T13:12:17.503150: step 35600, loss 0.010479, acc [0.99604492 0.99545898 0.99697266 0.98969727]\n",
      "\n",
      "Evaluation:\n",
      "2019-03-03T13:12:24.719185: step 35600, loss 0.495168, acc [0.98328144 0.97340047 0.96446055 0.95612129] \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-03T13:12:28.338944: step 35640, loss 0.0118862, acc [0.99624023 0.99589844 0.99658203 0.98955078]\n",
      "2019-03-03T13:12:32.445445: step 35680, loss 0.0120627, acc [0.99609375 0.99506836 0.99575195 0.98842773]\n",
      "2019-03-03T13:12:36.568254: step 35720, loss 0.0118555, acc [0.99575195 0.99575195 0.99643555 0.98925781]\n",
      "2019-03-03T13:12:40.735209: step 35760, loss 0.0120441, acc [0.99541016 0.99580078 0.99609375 0.98857422]\n",
      "2019-03-03T13:12:44.914366: step 35800, loss 0.0128176, acc [0.99550781 0.99594727 0.99609375 0.98857422]\n",
      "\n",
      "Evaluation:\n",
      "2019-03-03T13:12:52.184515: step 35800, loss 0.493872, acc [0.98316131 0.97353062 0.96408013 0.95603119] \n",
      "\n",
      "2019-03-03T13:12:55.805762: step 35840, loss 0.0115536, acc [0.99575195 0.99584961 0.99658203 0.98930664]\n",
      "2019-03-03T13:12:59.986646: step 35880, loss 0.0115576, acc [0.99609375 0.99472656 0.99580078 0.98833008]\n",
      "2019-03-03T13:13:04.240300: step 35920, loss 0.010147, acc [0.99692383 0.99609375 0.99609375 0.99008789]\n",
      "2019-03-03T13:13:08.342217: step 35960, loss 0.00746459, acc [0.99716797 0.99663086 0.99780273 0.99194336]\n",
      "2019-03-03T13:13:12.454235: step 36000, loss 0.00859874, acc [0.9965332  0.99780273 0.99692383 0.99179688]\n",
      "\n",
      "Evaluation:\n",
      "2019-03-03T13:13:19.672297: step 36000, loss 0.506852, acc [0.98322138 0.97343051 0.96385988 0.95573086] \n",
      "\n",
      "2019-03-03T13:13:23.289126: step 36040, loss 0.00892589, acc [0.99682617 0.99682617 0.99711914 0.99135742]\n",
      "2019-03-03T13:13:27.445859: step 36080, loss 0.00834086, acc [0.99692383 0.99711914 0.99702148 0.99179688]\n",
      "2019-03-03T13:13:31.635590: step 36120, loss 0.00949549, acc [0.99716797 0.99570313 0.99731445 0.99130859]\n",
      "2019-03-03T13:13:35.801475: step 36160, loss 0.0105187, acc [0.99594727 0.99624023 0.99692383 0.99008789]\n",
      "2019-03-03T13:13:39.978945: step 36200, loss 0.0101365, acc [0.99619141 0.99609375 0.99658203 0.99023438]\n",
      "\n",
      "Evaluation:\n",
      "2019-03-03T13:13:47.285697: step 36200, loss 0.497729, acc [0.98348166 0.97364074 0.9638699  0.95607124] \n",
      "\n",
      "2019-03-03T13:13:50.902527: step 36240, loss 0.0112223, acc [0.99624023 0.99663086 0.99672852 0.9909668 ]\n",
      "2019-03-03T13:13:55.079813: step 36280, loss 0.00963889, acc [0.99667969 0.99614258 0.99672852 0.99033203]\n",
      "2019-03-03T13:13:59.262436: step 36320, loss 0.0103551, acc [0.99638672 0.996875   0.99658203 0.990625  ]\n",
      "2019-03-03T13:14:03.432731: step 36360, loss 0.0107431, acc [0.99628906 0.99638672 0.99658203 0.990625  ]\n",
      "2019-03-03T13:14:07.584733: step 36400, loss 0.0109167, acc [0.99619141 0.99633789 0.99624023 0.98984375]\n",
      "\n",
      "Evaluation:\n",
      "2019-03-03T13:14:14.935554: step 36400, loss 0.506478, acc [0.98347165 0.97367077 0.96389993 0.95592107] \n",
      "\n",
      "2019-03-03T13:14:18.546389: step 36440, loss 0.0131667, acc [0.9965332  0.99560547 0.99560547 0.98911133]\n",
      "2019-03-03T13:14:22.664993: step 36480, loss 0.0106256, acc [0.99682617 0.99614258 0.99707031 0.99067383]\n",
      "2019-03-03T13:14:26.784130: step 36520, loss 0.0111394, acc [0.99633789 0.99584961 0.9965332  0.98969727]\n",
      "2019-03-03T13:14:30.886397: step 36560, loss 0.0135944, acc [0.99677734 0.99550781 0.99604492 0.98935547]\n",
      "2019-03-03T13:14:35.045683: step 36600, loss 0.0117118, acc [0.99614258 0.99633789 0.99614258 0.9894043 ]\n",
      "\n",
      "Evaluation:\n",
      "2019-03-03T13:14:42.477676: step 36600, loss 0.496155, acc [0.98363183 0.97352061 0.96406011 0.95607124] \n",
      "\n",
      "2019-03-03T13:14:46.194204: step 36640, loss 0.0145307, acc [0.99614258 0.99423828 0.99545898 0.98740234]\n",
      "2019-03-03T13:14:50.727371: step 36680, loss 0.0123616, acc [0.99570313 0.99560547 0.99609375 0.98886719]\n",
      "2019-03-03T13:14:54.977241: step 36720, loss 0.00990252, acc [0.99667969 0.99697266 0.996875   0.99130859]\n",
      "2019-03-03T13:14:59.297396: step 36760, loss 0.0100123, acc [0.99667969 0.99663086 0.99682617 0.99086914]\n",
      "2019-03-03T13:15:03.565086: step 36800, loss 0.00741173, acc [0.99794922 0.99741211 0.99755859 0.99311523]\n",
      "\n",
      "Evaluation:\n",
      "2019-03-03T13:15:11.860319: step 36800, loss 0.502079, acc [0.98329145 0.97350059 0.96425032 0.95583097] \n",
      "\n",
      "2019-03-03T13:15:15.519806: step 36840, loss 0.00864832, acc [0.99711914 0.99736328 0.99760742 0.99262695]\n",
      "2019-03-03T13:15:19.740972: step 36880, loss 0.00919224, acc [0.99702148 0.99672852 0.996875   0.99125977]\n",
      "2019-03-03T13:15:23.908988: step 36920, loss 0.0101863, acc [0.99702148 0.99604492 0.99614258 0.99013672]\n",
      "2019-03-03T13:15:28.151274: step 36960, loss 0.0103534, acc [0.99619141 0.99633789 0.99692383 0.9902832 ]\n",
      "2019-03-03T13:15:32.316858: step 37000, loss 0.00888803, acc [0.99604492 0.99658203 0.99760742 0.99111328]\n",
      "\n",
      "Evaluation:\n",
      "2019-03-03T13:15:39.730832: step 37000, loss 0.503212, acc [0.9833315  0.9733304  0.9640501  0.95553064] \n",
      "\n",
      "2019-03-03T13:15:43.355053: step 37040, loss 0.0107899, acc [0.9956543  0.99677734 0.99648437 0.98994141]\n",
      "2019-03-03T13:15:47.491946: step 37080, loss 0.00955974, acc [0.99672852 0.99555664 0.99716797 0.99038086]\n",
      "2019-03-03T13:15:51.604913: step 37120, loss 0.0107298, acc [0.99658203 0.99589844 0.996875   0.99047852]\n",
      "2019-03-03T13:15:55.795616: step 37160, loss 0.0101183, acc [0.99677734 0.99658203 0.99677734 0.99121094]\n",
      "2019-03-03T13:15:59.991027: step 37200, loss 0.0103024, acc [0.99702148 0.99667969 0.9965332  0.99130859]\n",
      "\n",
      "Evaluation:\n",
      "2019-03-03T13:16:07.284664: step 37200, loss 0.501374, acc [0.98367188 0.97403117 0.9642303  0.95654176] \n",
      "\n",
      "2019-03-03T13:16:10.907447: step 37240, loss 0.00995945, acc [0.99628906 0.99658203 0.99726563 0.9909668 ]\n",
      "2019-03-03T13:16:15.095531: step 37280, loss 0.0132307, acc [0.99584961 0.99609375 0.99545898 0.98867187]\n",
      "2019-03-03T13:16:19.223439: step 37320, loss 0.00996439, acc [0.99570313 0.99643555 0.99692383 0.9902832 ]\n",
      "2019-03-03T13:16:23.397471: step 37360, loss 0.0116833, acc [0.99648437 0.99580078 0.99619141 0.98959961]\n",
      "2019-03-03T13:16:27.589815: step 37400, loss 0.0115793, acc [0.99560547 0.99555664 0.99609375 0.9887207 ]\n",
      "\n",
      "Evaluation:\n",
      "2019-03-03T13:16:34.870310: step 37400, loss 0.506057, acc [0.98339156 0.97376087 0.96390994 0.95606123] \n",
      "\n",
      "2019-03-03T13:16:38.488630: step 37440, loss 0.0109828, acc [0.9965332  0.99633789 0.99643555 0.99018555]\n",
      "2019-03-03T13:16:42.729923: step 37480, loss 0.00977125, acc [0.99741211 0.99658203 0.99682617 0.99145508]\n",
      "2019-03-03T13:16:46.831845: step 37520, loss 0.00875849, acc [0.99663086 0.99682617 0.99707031 0.99106445]\n",
      "2019-03-03T13:16:50.934640: step 37560, loss 0.00875402, acc [0.99663086 0.9965332  0.99697266 0.99130859]\n",
      "2019-03-03T13:16:55.060706: step 37600, loss 0.00796093, acc [0.99765625 0.9965332  0.99741211 0.99213867]\n",
      "\n",
      "Evaluation:\n",
      "2019-03-03T13:17:02.407445: step 37600, loss 0.516712, acc [0.98327143 0.97365075 0.96390994 0.95579093] \n",
      "\n",
      "2019-03-03T13:17:06.023780: step 37640, loss 0.00812205, acc [0.99677734 0.99663086 0.99750977 0.99169922]\n",
      "2019-03-03T13:17:10.224941: step 37680, loss 0.00814021, acc [0.99682617 0.99667969 0.99741211 0.99160156]\n",
      "2019-03-03T13:17:14.347276: step 37720, loss 0.00904561, acc [0.99707031 0.99707031 0.99770508 0.99267578]\n",
      "2019-03-03T13:17:18.469078: step 37760, loss 0.0121689, acc [0.99667969 0.99638672 0.99614258 0.98989258]\n",
      "2019-03-03T13:17:22.628068: step 37800, loss 0.00988416, acc [0.99633789 0.99672852 0.99677734 0.99091797]\n",
      "\n",
      "Evaluation:\n",
      "2019-03-03T13:17:29.890694: step 37800, loss 0.519284, acc [0.98325141 0.97354063 0.9636897  0.95538047] \n",
      "\n",
      "2019-03-03T13:17:33.507029: step 37840, loss 0.0101028, acc [0.99589844 0.99619141 0.99692383 0.98989258]\n",
      "2019-03-03T13:17:37.687811: step 37880, loss 0.0105205, acc [0.99589844 0.99624023 0.99677734 0.99008789]\n",
      "2019-03-03T13:17:41.796065: step 37920, loss 0.0113004, acc [0.9965332  0.99550781 0.99628906 0.9894043 ]\n",
      "2019-03-03T13:17:45.968044: step 37960, loss 0.0117954, acc [0.99648437 0.99628906 0.99580078 0.98955078]\n",
      "2019-03-03T13:17:50.117099: step 38000, loss 0.0107861, acc [0.99638672 0.99643555 0.99628906 0.98989258]\n",
      "\n",
      "Evaluation:\n",
      "2019-03-03T13:17:57.497081: step 38000, loss 0.50864, acc [0.98316131 0.97357066 0.96375977 0.95553064] \n",
      "\n",
      "2019-03-03T13:18:01.159543: step 38040, loss 0.0113058, acc [0.99619141 0.99643555 0.99619141 0.98969727]\n",
      "2019-03-03T13:18:05.347981: step 38080, loss 0.0120176, acc [0.99624023 0.99672852 0.99594727 0.98989258]\n",
      "2019-03-03T13:18:09.536428: step 38120, loss 0.0125582, acc [0.99648437 0.99560547 0.99599609 0.98935547]\n",
      "2019-03-03T13:18:13.738875: step 38160, loss 0.0138988, acc [0.99604492 0.99482422 0.99570313 0.98798828]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-03T13:18:18.037515: step 38200, loss 0.011855, acc [0.99599609 0.99555664 0.99697266 0.98945313]\n",
      "\n",
      "Evaluation:\n",
      "2019-03-03T13:18:25.511987: step 38200, loss 0.506581, acc [0.98325141 0.97343051 0.96383986 0.95543053] \n",
      "\n",
      "2019-03-03T13:18:29.301427: step 38240, loss 0.0115476, acc [0.996875   0.99594727 0.99619141 0.99038086]\n",
      "2019-03-03T13:18:33.485294: step 38280, loss 0.00903957, acc [0.99702148 0.99672852 0.99677734 0.99169922]\n",
      "2019-03-03T13:18:37.667572: step 38320, loss 0.00952305, acc [0.99702148 0.99726563 0.99619141 0.99111328]\n",
      "2019-03-03T13:18:41.766093: step 38360, loss 0.00892159, acc [0.99697266 0.9972168  0.99707031 0.99179688]\n",
      "2019-03-03T13:18:45.952169: step 38400, loss 0.00961889, acc [0.996875   0.99677734 0.99658203 0.99082031]\n",
      "\n",
      "Evaluation:\n",
      "2019-03-03T13:18:53.164053: step 38400, loss 0.51202, acc [0.98354173 0.97377089 0.96371973 0.95571084] \n",
      "\n",
      "2019-03-03T13:18:56.780838: step 38440, loss 0.0118786, acc [0.99736328 0.99614258 0.99589844 0.99082031]\n"
     ]
    }
   ],
   "source": [
    "y_train = [train_y_label1_one_hot,train_y_label2_one_hot,train_y_label3_one_hot]\n",
    "y_test = [test_y_label1_one_hot,test_y_label2_one_hot,test_y_label3_one_hot]\n",
    "train(x_train, y_train,  x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_y_label3_one_hot.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y_label3_one_hot.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
