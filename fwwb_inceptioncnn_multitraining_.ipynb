{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.utils import shuffle\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "特征如下：['ITEM_NAME' 'TYPE'] \n",
      "\n",
      "数据数目:  499447\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('data/train/trainx.txt', sep='\\t')\n",
    "features = data.columns.values\n",
    "print(\"特征如下：{} \\n\".format(features))\n",
    "print(\"数据数目: \", len(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据清洗\n",
    "* 替换一些字符 类似于 （ ） ‘ ’ _"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ITEM_NAME</th>\n",
       "      <th>TYPE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>腾讯QQ黄钻三个月QQ黄钻3个月季卡官方自动充值可查时间可续费</td>\n",
       "      <td>本地生活--游戏充值--QQ充值</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>腾讯q币充值30元qq币30qb30q币自动充值</td>\n",
       "      <td>本地生活--游戏充值--QQ充值</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>腾讯QQ红钻包月卡 QQ红钻一个月QQ红钻1个月</td>\n",
       "      <td>本地生活--游戏充值--QQ充值</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>腾讯QQ蓝钻贵族31个月直充</td>\n",
       "      <td>本地生活--游戏充值--QQ充值</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>腾讯QQ币148元148QQ币148个直充148Q币148个Q币148个QQB★自动充值</td>\n",
       "      <td>本地生活--游戏充值--QQ充值</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      ITEM_NAME              TYPE\n",
       "0               腾讯QQ黄钻三个月QQ黄钻3个月季卡官方自动充值可查时间可续费  本地生活--游戏充值--QQ充值\n",
       "1                      腾讯q币充值30元qq币30qb30q币自动充值  本地生活--游戏充值--QQ充值\n",
       "2                      腾讯QQ红钻包月卡 QQ红钻一个月QQ红钻1个月  本地生活--游戏充值--QQ充值\n",
       "3                                腾讯QQ蓝钻贵族31个月直充  本地生活--游戏充值--QQ充值\n",
       "4  腾讯QQ币148元148QQ币148个直充148Q币148个Q币148个QQB★自动充值  本地生活--游戏充值--QQ充值"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "from zhon.hanzi import punctuation\n",
    "from string import punctuation as english_punc\n",
    "def  rm_punc(strs):\n",
    "    return re.sub(r\"[{}]+\".format(punctuation + english_punc),\" \",strs)\n",
    "data[features[0]]  = data[features[0]].apply(rm_punc)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练数据数目:   399558\n",
      "测试数据数目:   99889\n",
      "(399558,) (99889,)\n"
     ]
    }
   ],
   "source": [
    "# 因为有的label里本身就存在空格，所以先要把空格去掉，再去掉- - ， 再以空格划分\n",
    "data = shuffle(data)\n",
    "data_y = data[features[1]]\n",
    "\n",
    "# 取训练数据和测试数据\n",
    "train = data.sample(frac= 0.8).reset_index()\n",
    "test = data.sample(frac= 0.2).reset_index()\n",
    "\n",
    "\n",
    "print(\"训练数据数目:  \", len(train))\n",
    "print(\"测试数据数目:  \", len(test))\n",
    "\n",
    "train_x  = train[features[0]]\n",
    "train_y  = train[features[1]]\n",
    "\n",
    "test_x  = test[features[0]]\n",
    "test_y  = test[features[1]]\n",
    "\n",
    "print(train_x.shape, test_x.shape)\n",
    "\n",
    "# 分出数据集 训练集 测试集 数据集用来训练词向量\n",
    "data_y = data_y.apply(lambda x: x.replace(\" \", \"\").replace(\"--\", \" \").replace(\"/\", \"\")).tolist()\n",
    "data_y= [each.split() for each in data_y]\n",
    "\n",
    "train_y = train_y.apply(lambda x: x.replace(\" \", \"\").replace(\"--\", \" \").replace(\"/\", \"\")).tolist()\n",
    "train_y = [each.split() for each in train_y]\n",
    "\n",
    "test_y = test_y.apply(lambda x: x.replace(\" \", \"\").replace(\"--\", \" \").replace(\"/\", \"\")).tolist()\n",
    "test_y = [each.split() for each in test_y]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 分词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba\n",
    "from random import shuffle\n",
    "def participle(data, doshuffle = False):\n",
    "    words = []\n",
    "    for i in range(len(data)):\n",
    "        result=[]\n",
    "        seg_list = jieba.cut(data.iloc[i])\n",
    "        for w in seg_list :#读取每一行分词\n",
    "            if w != \" \":\n",
    "                result.append(w)\n",
    "        # 打乱每条数据的分词\n",
    "        if doshuffle:\n",
    "            shuffle(result)\n",
    "        words.append(result)#将该行分词写入列表形式的总分词列表\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\Richado\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.729 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "# 每条数据的分词结果是个长度不等的词列表\n",
    "word_data = participle(data[features[0]], doshuffle = False)\n",
    "word_data_train = participle(train_x,doshuffle = False)\n",
    "word_data_test = participle(test_x, doshuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'腾讯QQ黄钻三个月QQ黄钻3个月季卡官方自动充值可查时间可续费'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[features[0]][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['WRC',\n",
       " '汽车',\n",
       " '专用',\n",
       " '四季',\n",
       " '通用',\n",
       " '丝圈',\n",
       " '脚垫',\n",
       " '红',\n",
       " '黑色',\n",
       " '雪铁龙',\n",
       " 'C3',\n",
       " 'XR',\n",
       " '世嘉三厢',\n",
       " 'C5',\n",
       " 'C4L']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from random import shuffle\n",
    "word_data[8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 标签分离(一个标签分成三个标签)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y = pd.DataFrame(train_y, columns=['label1', 'label2', 'label3'])\n",
    "test_y = pd.DataFrame(test_y, columns=['label1', 'label2', 'label3'])\n",
    "data_y = pd.DataFrame(data_y, columns=['label1', 'label2', 'label3'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_y[\"label1\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "191"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_y[\"label2\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1192"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_y[\"label3\"].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 标签onn_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import to_categorical\n",
    "# 先生成唯一数组\n",
    "y_label1 = data_y['label1'].unique().tolist()\n",
    "y_label2 = data_y['label2'].unique().tolist()\n",
    "y_label3 = data_y['label3'].unique().tolist()\n",
    "\n",
    "# 获取在唯一数组中的索引(训练集和测试集各有3个标签需要处理)\n",
    "train_y_label1_map = train_y['label1'].apply(lambda x: y_label1.index(x))\n",
    "train_y_label2_map = train_y['label2'].apply(lambda x: y_label2.index(x))\n",
    "train_y_label3_map = train_y['label3'].apply(lambda x: y_label3.index(x))\n",
    "test_y_label1_map = test_y['label1'].apply(lambda x: y_label1.index(x))\n",
    "test_y_label2_map = test_y['label2'].apply(lambda x: y_label2.index(x))\n",
    "test_y_label3_map = test_y['label3'].apply(lambda x: y_label3.index(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生成对应one-hot(用做训练模型的标签)\n",
    "train_y_label1_one_hot = to_categorical(train_y_label1_map, 22)\n",
    "train_y_label2_one_hot = to_categorical(train_y_label2_map, 191)\n",
    "train_y_label3_one_hot = to_categorical(train_y_label3_map, 1192)\n",
    "test_y_label1_one_hot = to_categorical(test_y_label1_map, 22)\n",
    "test_y_label2_one_hot = to_categorical(test_y_label2_map, 191)\n",
    "test_y_label3_one_hot = to_categorical(test_y_label3_map, 1192)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import *\n",
    "from keras.preprocessing import sequence\n",
    "tok = Tokenizer()\n",
    "tok.fit_on_texts(word_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'新款': 1,\n",
       " '1': 2,\n",
       " '汽车': 3,\n",
       " '时尚': 4,\n",
       " '黑色': 5,\n",
       " '鞋': 6,\n",
       " '2016': 7,\n",
       " '休闲': 8,\n",
       " '款': 9,\n",
       " '专用': 10,\n",
       " '2': 11,\n",
       " '5': 12,\n",
       " '男士': 13,\n",
       " '跟': 14,\n",
       " '手机': 15,\n",
       " '适用': 16,\n",
       " '女': 17,\n",
       " '套装': 18,\n",
       " '男': 19,\n",
       " '韩版': 20,\n",
       " '夏季': 21,\n",
       " '儿童': 22,\n",
       " '3': 23,\n",
       " '包': 24,\n",
       " '于': 25,\n",
       " '透气': 26,\n",
       " '壳': 27,\n",
       " '套': 28,\n",
       " '休闲鞋': 29,\n",
       " '米': 30,\n",
       " '户外': 31,\n",
       " '男鞋': 32,\n",
       " '真皮': 33,\n",
       " '垫': 34,\n",
       " '红色': 35,\n",
       " '6': 36,\n",
       " '4': 37,\n",
       " '脚垫': 38,\n",
       " '女鞋': 39,\n",
       " '男女': 40,\n",
       " '白色': 41,\n",
       " '全': 42,\n",
       " '蓝色': 43,\n",
       " '创意': 44,\n",
       " '宝宝': 45,\n",
       " '防水': 46,\n",
       " '简约': 47,\n",
       " '四季': 48,\n",
       " '女士': 49,\n",
       " '8': 50,\n",
       " '保护套': 51,\n",
       " '凉鞋': 52,\n",
       " '牛皮': 53,\n",
       " '四件套': 54,\n",
       " '皮鞋': 55,\n",
       " '系列': 56,\n",
       " '专车': 57,\n",
       " '情侣': 58,\n",
       " '包围': 59,\n",
       " '灯': 60,\n",
       " '通用': 61,\n",
       " '纯棉': 62,\n",
       " '加厚': 63,\n",
       " '小': 64,\n",
       " '运动': 65,\n",
       " '单鞋': 66,\n",
       " '商务': 67,\n",
       " '学生': 68,\n",
       " '大': 69,\n",
       " '2015': 70,\n",
       " '送': 71,\n",
       " '潮流': 72,\n",
       " '客厅': 73,\n",
       " '婴儿': 74,\n",
       " '带': 75,\n",
       " '新': 76,\n",
       " '卡通': 77,\n",
       " '号': 78,\n",
       " '全棉': 79,\n",
       " '英伦': 80,\n",
       " '7': 81,\n",
       " '0': 82,\n",
       " '寸': 83,\n",
       " '可': 84,\n",
       " '现代': 85,\n",
       " '鞋子': 86,\n",
       " 'led': 87,\n",
       " '单肩': 88,\n",
       " '玩具': 89,\n",
       " '黑': 90,\n",
       " '被': 91,\n",
       " '座垫': 92,\n",
       " '39': 93,\n",
       " '礼物': 94,\n",
       " '苹果': 95,\n",
       " '40': 96,\n",
       " '版': 97,\n",
       " '的': 98,\n",
       " '摆件': 99,\n",
       " '保暖': 100,\n",
       " '礼品': 101,\n",
       " '码': 102,\n",
       " '38': 103,\n",
       " '保护': 104,\n",
       " '改装': 105,\n",
       " '灰色': 106,\n",
       " '春季': 107,\n",
       " '床上用品': 108,\n",
       " '舒适': 109,\n",
       " '卧室': 110,\n",
       " '可爱': 111,\n",
       " '印花': 112,\n",
       " '拖鞋': 113,\n",
       " '装饰': 114,\n",
       " '多功能': 115,\n",
       " '高': 116,\n",
       " '坐垫': 117,\n",
       " '棕色': 118,\n",
       " '被套': 119,\n",
       " '茶具': 120,\n",
       " '进口': 121,\n",
       " '系带': 122,\n",
       " '米床': 123,\n",
       " '10': 124,\n",
       " '防': 125,\n",
       " '背包': 126,\n",
       " '金属': 127,\n",
       " '防滑': 128,\n",
       " '杯': 129,\n",
       " '复古': 130,\n",
       " '座套': 131,\n",
       " '经典': 132,\n",
       " '正版': 133,\n",
       " '秋冬': 134,\n",
       " '系': 135,\n",
       " '贴': 136,\n",
       " '春夏': 137,\n",
       " '12': 138,\n",
       " '汽车坐垫': 139,\n",
       " '奥迪': 140,\n",
       " '不锈钢': 141,\n",
       " '陶瓷': 142,\n",
       " '粉色': 143,\n",
       " '36': 144,\n",
       " '床单': 145,\n",
       " '欧美': 146,\n",
       " '床': 147,\n",
       " '定制': 148,\n",
       " '生日礼物': 149,\n",
       " '双人': 150,\n",
       " '手表': 151,\n",
       " '35': 152,\n",
       " '孕妇': 153,\n",
       " '板鞋': 154,\n",
       " '宝马': 155,\n",
       " '女款': 156,\n",
       " '车载': 157,\n",
       " '大众': 158,\n",
       " '套件': 159,\n",
       " '实木': 160,\n",
       " '女包': 161,\n",
       " '年': 162,\n",
       " '夹': 163,\n",
       " '枕': 164,\n",
       " '空调': 165,\n",
       " '37': 166,\n",
       " '办公': 167,\n",
       " '膜': 168,\n",
       " 'l': 169,\n",
       " '红': 170,\n",
       " '风': 171,\n",
       " '手机套': 172,\n",
       " '欧式': 173,\n",
       " '15': 174,\n",
       " '平底': 175,\n",
       " '车': 176,\n",
       " '尖头': 177,\n",
       " '男款': 178,\n",
       " '装': 179,\n",
       " '个性': 180,\n",
       " '手链': 181,\n",
       " '42': 182,\n",
       " '运动鞋': 183,\n",
       " '41': 184,\n",
       " '靴': 185,\n",
       " 'm': 186,\n",
       " '英寸': 187,\n",
       " '钻': 188,\n",
       " '皮革': 189,\n",
       " '色': 190,\n",
       " '防晒': 191,\n",
       " '新品': 192,\n",
       " '金': 193,\n",
       " '家用': 194,\n",
       " '水晶': 195,\n",
       " '懒人': 196,\n",
       " '吊坠': 197,\n",
       " '厚底': 198,\n",
       " '中': 199,\n",
       " '家居': 200,\n",
       " '配件': 201,\n",
       " '43': 202,\n",
       " '黄色': 203,\n",
       " '组合': 204,\n",
       " '粗': 205,\n",
       " '全包': 206,\n",
       " '书包': 207,\n",
       " '200': 208,\n",
       " '架': 209,\n",
       " '高跟鞋': 210,\n",
       " '饰品': 211,\n",
       " '三星': 212,\n",
       " '米色': 213,\n",
       " '公仔': 214,\n",
       " '硅胶': 215,\n",
       " '绿色': 216,\n",
       " '内': 217,\n",
       " '百搭': 218,\n",
       " '丝圈': 219,\n",
       " '后备箱': 220,\n",
       " '吸顶灯': 221,\n",
       " '皮套': 222,\n",
       " '透明': 223,\n",
       " '电脑': 224,\n",
       " '钱包': 225,\n",
       " 's': 226,\n",
       " '三件套': 227,\n",
       " '16': 228,\n",
       " '手工': 229,\n",
       " '蓝': 230,\n",
       " '家具': 231,\n",
       " '厘米': 232,\n",
       " '中国': 233,\n",
       " '个': 234,\n",
       " '高清': 235,\n",
       " '正品': 236,\n",
       " '型': 237,\n",
       " '毛绒玩具': 238,\n",
       " '9': 239,\n",
       " '春秋': 240,\n",
       " '台': 241,\n",
       " '冬季': 242,\n",
       " '挂件': 243,\n",
       " '奔驰': 244,\n",
       " '金色': 245,\n",
       " '跑步': 246,\n",
       " '银': 247,\n",
       " '包邮': 248,\n",
       " '智能': 249,\n",
       " '抱': 250,\n",
       " '3d': 251,\n",
       " '斜挎包': 252,\n",
       " 'a': 253,\n",
       " '双肩包': 254,\n",
       " 'xl': 255,\n",
       " '福克斯': 256,\n",
       " '纯色': 257,\n",
       " '岁': 258,\n",
       " '婚庆': 259,\n",
       " '天然': 260,\n",
       " '级': 261,\n",
       " '潮': 262,\n",
       " '礼盒': 263,\n",
       " '增高': 264,\n",
       " '白': 265,\n",
       " '20': 266,\n",
       " '灯具': 267,\n",
       " '一': 268,\n",
       " '帆布鞋': 269,\n",
       " '家纺': 270,\n",
       " '华为': 271,\n",
       " '银色': 272,\n",
       " '紫色': 273,\n",
       " '迷你': 274,\n",
       " '双层': 275,\n",
       " '丰田': 276,\n",
       " '斜': 277,\n",
       " '单': 278,\n",
       " '本田': 279,\n",
       " '单人': 280,\n",
       " '卡': 281,\n",
       " '支架': 282,\n",
       " '短袖': 283,\n",
       " '手提包': 284,\n",
       " '戒指': 285,\n",
       " '皮': 286,\n",
       " '用品': 287,\n",
       " '件套': 288,\n",
       " '钻石': 289,\n",
       " '圆头': 290,\n",
       " '片': 291,\n",
       " '14': 292,\n",
       " '双': 293,\n",
       " '项链': 294,\n",
       " '耐磨': 295,\n",
       " '车型': 296,\n",
       " '立体': 297,\n",
       " '月': 298,\n",
       " '玻璃': 299,\n",
       " '水钻': 300,\n",
       " '性感': 301,\n",
       " 't恤': 302,\n",
       " '餐厅': 303,\n",
       " '方向盘': 304,\n",
       " '边框': 305,\n",
       " '高尔夫': 306,\n",
       " '贴纸': 307,\n",
       " '贴膜': 308,\n",
       " '头层': 309,\n",
       " '外壳': 310,\n",
       " '式': 311,\n",
       " '镂空': 312,\n",
       " '原装': 313,\n",
       " '盒': 314,\n",
       " '冰丝': 315,\n",
       " '豆豆': 316,\n",
       " '100': 317,\n",
       " '益智': 318,\n",
       " '椅': 319,\n",
       " '扣': 320,\n",
       " '头': 321,\n",
       " '磨砂': 322,\n",
       " '包包': 323,\n",
       " '彩色': 324,\n",
       " '44': 325,\n",
       " '床品': 326,\n",
       " '一对': 327,\n",
       " '别克': 328,\n",
       " '摔': 329,\n",
       " '玫瑰': 330,\n",
       " '棉': 331,\n",
       " '册': 332,\n",
       " '帕萨特': 333,\n",
       " '把': 334,\n",
       " '充电': 335,\n",
       " '收纳': 336,\n",
       " '十字绣': 337,\n",
       " '旅行': 338,\n",
       " '折叠': 339,\n",
       " '公主': 340,\n",
       " '浅口': 341,\n",
       " '功夫': 342,\n",
       " '环保': 343,\n",
       " '韩国': 344,\n",
       " '人': 345,\n",
       " '小米': 346,\n",
       " '车用': 347,\n",
       " '多': 348,\n",
       " '不': 349,\n",
       " '粉': 350,\n",
       " '衣': 351,\n",
       " '吊灯': 352,\n",
       " '彩绘': 353,\n",
       " '手': 354,\n",
       " '钥匙包': 355,\n",
       " '条': 356,\n",
       " '夏': 357,\n",
       " '春夏季': 358,\n",
       " '大容量': 359,\n",
       " '笔': 360,\n",
       " '凉': 361,\n",
       " '布鞋': 362,\n",
       " '24': 363,\n",
       " '厨房': 364,\n",
       " '斜纹': 365,\n",
       " '茶杯': 366,\n",
       " '水杯': 367,\n",
       " '与': 368,\n",
       " '结婚': 369,\n",
       " '途观': 370,\n",
       " '专用汽车': 371,\n",
       " '被子': 372,\n",
       " '衣服': 373,\n",
       " '本': 374,\n",
       " '925': 375,\n",
       " '猫': 376,\n",
       " '灯饰': 377,\n",
       " '和': 378,\n",
       " '座椅': 379,\n",
       " '套餐': 380,\n",
       " '子': 381,\n",
       " '拉': 382,\n",
       " '无线': 383,\n",
       " '遥控': 384,\n",
       " '小号': 385,\n",
       " '细': 386,\n",
       " '卡罗': 387,\n",
       " '沙发': 388,\n",
       " '低帮': 389,\n",
       " '便携': 390,\n",
       " '办公室': 391,\n",
       " '花': 392,\n",
       " '无': 393,\n",
       " '手提': 394,\n",
       " '蝴蝶结': 395,\n",
       " '潮鞋': 396,\n",
       " '低': 397,\n",
       " '13': 398,\n",
       " '标准': 399,\n",
       " '纹': 400,\n",
       " '福特': 401,\n",
       " '笔记本': 402,\n",
       " '桌': 403,\n",
       " '钢化': 404,\n",
       " '全新': 405,\n",
       " '袋': 406,\n",
       " '坡': 407,\n",
       " 'diy': 408,\n",
       " 'plus': 409,\n",
       " '30': 410,\n",
       " 'iphone6': 411,\n",
       " '双肩': 412,\n",
       " '女单': 413,\n",
       " '零食': 414,\n",
       " '6s': 415,\n",
       " '北京': 416,\n",
       " '粉红色': 417,\n",
       " '短靴': 418,\n",
       " '咖啡色': 419,\n",
       " '凯美瑞': 420,\n",
       " '芯': 421,\n",
       " '套脚': 422,\n",
       " '驾车': 423,\n",
       " '起亚': 424,\n",
       " '工具': 425,\n",
       " 'usb': 426,\n",
       " '秋季': 427,\n",
       " '书籍': 428,\n",
       " '罩': 429,\n",
       " '60': 430,\n",
       " '备注': 431,\n",
       " '钥匙': 432,\n",
       " '50': 433,\n",
       " '均码': 434,\n",
       " '坠': 435,\n",
       " '后': 436,\n",
       " '老': 437,\n",
       " '器': 438,\n",
       " '颜色': 439,\n",
       " '女童': 440,\n",
       " '工艺品': 441,\n",
       " '塑料': 442,\n",
       " '模型': 443,\n",
       " '230cm': 444,\n",
       " '情人节': 445,\n",
       " '防风': 446,\n",
       " '大码': 447,\n",
       " '11': 448,\n",
       " '爱': 449,\n",
       " '线': 450,\n",
       " '徒步': 451,\n",
       " '鲜花': 452,\n",
       " '克': 453,\n",
       " '雅阁': 454,\n",
       " '酒': 455,\n",
       " '柜': 456,\n",
       " '34': 457,\n",
       " '尾箱': 458,\n",
       " '活性': 459,\n",
       " '18': 460,\n",
       " '外套': 461,\n",
       " '积木': 462,\n",
       " '标配': 463,\n",
       " '手串': 464,\n",
       " '新生儿': 465,\n",
       " '拉链': 466,\n",
       " '紫': 467,\n",
       " '花花公子': 468,\n",
       " '教材': 469,\n",
       " 'b': 470,\n",
       " '长': 471,\n",
       " '迈腾': 472,\n",
       " '中式': 473,\n",
       " '绿': 474,\n",
       " '滤清器': 475,\n",
       " 't': 476,\n",
       " '宽': 477,\n",
       " '150': 478,\n",
       " '田园': 479,\n",
       " '灰': 480,\n",
       " '孕妇装': 481,\n",
       " '甜美': 482,\n",
       " '玩偶': 483,\n",
       " '女式': 484,\n",
       " '壶': 485,\n",
       " 'c': 486,\n",
       " '现货': 487,\n",
       " '马丁': 488,\n",
       " '安全': 489,\n",
       " '亚麻': 490,\n",
       " '遮阳': 491,\n",
       " '脚': 492,\n",
       " '自动': 493,\n",
       " '圆形': 494,\n",
       " '沙滩鞋': 495,\n",
       " '长安': 496,\n",
       " '裤': 497,\n",
       " '座': 498,\n",
       " '女友': 499,\n",
       " '豪华版': 500,\n",
       " '虎': 501,\n",
       " '温馨': 502,\n",
       " '咖啡': 503,\n",
       " '女孩': 504,\n",
       " '美': 505,\n",
       " '长袖': 506,\n",
       " '帆布': 507,\n",
       " '软壳': 508,\n",
       " '女生': 509,\n",
       " '松糕': 510,\n",
       " '条纹': 511,\n",
       " '茶盘': 512,\n",
       " '长款': 513,\n",
       " '迪士尼': 514,\n",
       " '美式': 515,\n",
       " '熊': 516,\n",
       " '两用': 517,\n",
       " '用': 518,\n",
       " '马自达': 519,\n",
       " '标致': 520,\n",
       " '游戏': 521,\n",
       " '日产': 522,\n",
       " '含': 523,\n",
       " '香水': 524,\n",
       " '滤': 525,\n",
       " '美国': 526,\n",
       " '板': 527,\n",
       " '专业': 528,\n",
       " '冲锋衣': 529,\n",
       " '速腾': 530,\n",
       " '布娃娃': 531,\n",
       " '挡': 532,\n",
       " '空气': 533,\n",
       " '韩版潮': 534,\n",
       " '蓝牙': 535,\n",
       " '装饰品': 536,\n",
       " '一字': 537,\n",
       " '白光': 538,\n",
       " '科鲁兹': 539,\n",
       " 'crv': 540,\n",
       " '音乐': 541,\n",
       " '服': 542,\n",
       " '成人': 543,\n",
       " '车衣': 544,\n",
       " '双人床': 545,\n",
       " '学院': 546,\n",
       " '平板': 547,\n",
       " '斤': 548,\n",
       " '隔热': 549,\n",
       " '网面': 550,\n",
       " '凉席': 551,\n",
       " '靴子': 552,\n",
       " '优雅': 553,\n",
       " '垫子': 554,\n",
       " '键盘': 555,\n",
       " '度': 556,\n",
       " '件': 557,\n",
       " '棕': 558,\n",
       " '日本': 559,\n",
       " '盖': 560,\n",
       " '皮肤': 561,\n",
       " '配': 562,\n",
       " '黄': 563,\n",
       " '蕾丝': 564,\n",
       " '德国': 565,\n",
       " '证书': 566,\n",
       " '联想': 567,\n",
       " '翡翠': 568,\n",
       " '茶': 569,\n",
       " '轻便': 570,\n",
       " '速干': 571,\n",
       " '拖': 572,\n",
       " '三': 573,\n",
       " '捷达': 574,\n",
       " '全套': 575,\n",
       " '深蓝色': 576,\n",
       " '碗': 577,\n",
       " '糖果': 578,\n",
       " '茶壶': 579,\n",
       " '仿真': 580,\n",
       " '小包': 581,\n",
       " '英朗': 582,\n",
       " 'polo': 583,\n",
       " '内衣': 584,\n",
       " '180': 585,\n",
       " '达': 586,\n",
       " '书房': 587,\n",
       " '请': 588,\n",
       " '上衣': 589,\n",
       " '办公桌': 590,\n",
       " '橙色': 591,\n",
       " '挎': 592,\n",
       " '合金': 593,\n",
       " '小学生': 594,\n",
       " '钥匙扣': 595,\n",
       " '宝': 596,\n",
       " '逸': 597,\n",
       " '监控': 598,\n",
       " '儿童玩具': 599,\n",
       " '钢化玻璃': 600,\n",
       " '格': 601,\n",
       " '蒙迪欧': 602,\n",
       " '水壶': 603,\n",
       " '浅': 604,\n",
       " '汽车用品': 605,\n",
       " '特产': 606,\n",
       " '妈妈': 607,\n",
       " '婴幼儿': 608,\n",
       " '枕套': 609,\n",
       " '耳机': 610,\n",
       " '玫': 611,\n",
       " '箱': 612,\n",
       " '天籁': 613,\n",
       " '狗': 614,\n",
       " '鱼': 615,\n",
       " '浮雕': 616,\n",
       " '2014': 617,\n",
       " '比亚迪': 618,\n",
       " '钱': 619,\n",
       " '椅子': 620,\n",
       " '清新': 621,\n",
       " '200cm': 622,\n",
       " '适合': 623,\n",
       " '直径': 624,\n",
       " '插': 625,\n",
       " '坐套': 626,\n",
       " '登山': 627,\n",
       " '下': 628,\n",
       " '紫砂': 629,\n",
       " 'ipad': 630,\n",
       " '80': 631,\n",
       " '被罩': 632,\n",
       " '修复': 633,\n",
       " '滤芯': 634,\n",
       " '电池': 635,\n",
       " '明锐': 636,\n",
       " '娃娃': 637,\n",
       " '迷彩': 638,\n",
       " '卡其色': 639,\n",
       " '盘': 640,\n",
       " '置物架': 641,\n",
       " '绣': 642,\n",
       " '18k': 643,\n",
       " '货到付款': 644,\n",
       " '味': 645,\n",
       " '移动': 646,\n",
       " '随机': 647,\n",
       " '脚蹬': 648,\n",
       " '茶几': 649,\n",
       " '男包': 650,\n",
       " '瓶': 651,\n",
       " 'rav4': 652,\n",
       " '玉石': 653,\n",
       " 'xxl': 654,\n",
       " '开关': 655,\n",
       " '珠宝': 656,\n",
       " '博世': 657,\n",
       " '机油': 658,\n",
       " '浪漫': 659,\n",
       " '高档': 660,\n",
       " '雨刮器': 661,\n",
       " '书': 662,\n",
       " '双面': 663,\n",
       " '佛珠': 664,\n",
       " '睡袋': 665,\n",
       " '翼': 666,\n",
       " '加绒': 667,\n",
       " '拼色': 668,\n",
       " '电动': 669,\n",
       " '夏装': 670,\n",
       " '超薄': 671,\n",
       " '链': 672,\n",
       " 'u': 673,\n",
       " '景德镇': 674,\n",
       " '短款': 675,\n",
       " '餐具': 676,\n",
       " '软底': 677,\n",
       " '沙滩': 678,\n",
       " '男式': 679,\n",
       " '秋冬季': 680,\n",
       " '考试': 681,\n",
       " '长裤': 682,\n",
       " '喷漆': 683,\n",
       " '镶': 684,\n",
       " '提花': 685,\n",
       " '和田玉': 686,\n",
       " '室内': 687,\n",
       " '加大': 688,\n",
       " '花瓶': 689,\n",
       " '全国': 690,\n",
       " '探路者': 691,\n",
       " '吉普': 692,\n",
       " '精品': 693,\n",
       " '荣耀': 694,\n",
       " '阳光': 695,\n",
       " '日': 696,\n",
       " '高帮': 697,\n",
       " '雪佛兰': 698,\n",
       " '保温杯': 699,\n",
       " '手套': 700,\n",
       " 'cd': 701,\n",
       " '护板': 702,\n",
       " '电源': 703,\n",
       " '哈弗': 704,\n",
       " '飞度': 705,\n",
       " '有': 706,\n",
       " '户外运动': 707,\n",
       " '4g': 708,\n",
       " '帮': 709,\n",
       " '一体机': 710,\n",
       " '夏凉': 711,\n",
       " '画': 712,\n",
       " '留言': 713,\n",
       " '后盖': 714,\n",
       " '帐篷': 715,\n",
       " '45': 716,\n",
       " '汉兰达': 717,\n",
       " '机': 718,\n",
       " '电子': 719,\n",
       " '皇冠': 720,\n",
       " '衣柜': 721,\n",
       " '网': 722,\n",
       " 'a6l': 723,\n",
       " '生日': 724,\n",
       " 'pu': 725,\n",
       " '上': 726,\n",
       " 'q5': 727,\n",
       " '世界': 728,\n",
       " '220': 729,\n",
       " '平': 730,\n",
       " '旅游': 731,\n",
       " '拼接': 732,\n",
       " '内饰': 733,\n",
       " '手镯': 734,\n",
       " '气质': 735,\n",
       " '摆设': 736,\n",
       " '面板': 737,\n",
       " 'dvd': 738,\n",
       " '帽': 739,\n",
       " '跨': 740,\n",
       " '灯泡': 741,\n",
       " '特价': 742,\n",
       " '鳄鱼': 743,\n",
       " '餐桌': 744,\n",
       " '磨毛': 745,\n",
       " '拍': 746,\n",
       " '木': 747,\n",
       " '长城': 748,\n",
       " '雪铁龙': 749,\n",
       " '杯子': 750,\n",
       " '元': 751,\n",
       " '支': 752,\n",
       " '动物': 753,\n",
       " '春': 754,\n",
       " '靠垫': 755,\n",
       " '罗马': 756,\n",
       " '玫红': 757,\n",
       " '夏天': 758,\n",
       " '防雨': 759,\n",
       " '早教': 760,\n",
       " '拉杆箱': 761,\n",
       " '其他': 762,\n",
       " '食品': 763,\n",
       " '干': 764,\n",
       " '男孩': 765,\n",
       " '鱼嘴': 766,\n",
       " '充电器': 767,\n",
       " '日常': 768,\n",
       " '头盔': 769,\n",
       " '毯': 770,\n",
       " '升级版': 771,\n",
       " '纯': 772,\n",
       " '罐': 773,\n",
       " '豪华': 774,\n",
       " '旅行包': 775,\n",
       " '泳衣': 776,\n",
       " 'x5': 777,\n",
       " '思域': 778,\n",
       " '嘴': 779,\n",
       " '实用': 780,\n",
       " '光盘': 781,\n",
       " '木质': 782,\n",
       " '升级': 783,\n",
       " '行李箱': 784,\n",
       " '趾': 785,\n",
       " '两件套': 786,\n",
       " 'e': 787,\n",
       " '单个': 788,\n",
       " '钢笔': 789,\n",
       " '凯越': 790,\n",
       " '马': 791,\n",
       " '支装': 792,\n",
       " '米奇': 793,\n",
       " '保温': 794,\n",
       " '抓': 795,\n",
       " '摩托车': 796,\n",
       " '款式': 797,\n",
       " 'ix35': 798,\n",
       " '男表': 799,\n",
       " '正装': 800,\n",
       " '乐福鞋': 801,\n",
       " '第': 802,\n",
       " '黄金': 803,\n",
       " '记录仪': 804,\n",
       " '网鞋': 805,\n",
       " '纸': 806,\n",
       " '连衣裙': 807,\n",
       " '片装': 808,\n",
       " '毛毯': 809,\n",
       " '划痕': 810,\n",
       " '宝来': 811,\n",
       " '居家': 812,\n",
       " '简易': 813,\n",
       " '男童': 814,\n",
       " '连体': 815,\n",
       " '230': 816,\n",
       " '内胆': 817,\n",
       " '大号': 818,\n",
       " '拼': 819,\n",
       " '皮带': 820,\n",
       " '拼装': 821,\n",
       " '之': 822,\n",
       " '三合一': 823,\n",
       " '松糕鞋': 824,\n",
       " '层': 825,\n",
       " '儿童节': 826,\n",
       " '调光': 827,\n",
       " '修身': 828,\n",
       " '附': 829,\n",
       " 'oppo': 830,\n",
       " '19': 831,\n",
       " '设计': 832,\n",
       " '包头': 833,\n",
       " '我': 834,\n",
       " '单件': 835,\n",
       " '编织': 836,\n",
       " '阳台': 837,\n",
       " '音箱': 838,\n",
       " '登山鞋': 839,\n",
       " '咖色': 840,\n",
       " '贡缎': 841,\n",
       " '镜': 842,\n",
       " '上海': 843,\n",
       " '棉鞋': 844,\n",
       " '安装': 845,\n",
       " '钓鱼': 846,\n",
       " '锐志': 847,\n",
       " '训练': 848,\n",
       " '漆皮': 849,\n",
       " '桌椅': 850,\n",
       " '宠物': 851,\n",
       " '游泳': 852,\n",
       " '仕': 853,\n",
       " '硬盘': 854,\n",
       " '骑行': 855,\n",
       " '逍客': 856,\n",
       " '索纳塔': 857,\n",
       " '英语': 858,\n",
       " '漆': 859,\n",
       " '故事': 860,\n",
       " '路': 861,\n",
       " '帽子': 862,\n",
       " '选': 863,\n",
       " '兔': 864,\n",
       " '方形': 865,\n",
       " '前后': 866,\n",
       " '土豪': 867,\n",
       " '开': 868,\n",
       " '约': 869,\n",
       " '摄像头': 870,\n",
       " 'a3': 871,\n",
       " '25': 872,\n",
       " '夜光': 873,\n",
       " 'a4l': 874,\n",
       " '导航': 875,\n",
       " '120': 876,\n",
       " '铆钉': 877,\n",
       " '绣花': 878,\n",
       " '睿': 879,\n",
       " '风格': 880,\n",
       " 'cc': 881,\n",
       " '学习': 882,\n",
       " '旅行箱': 883,\n",
       " '打火机': 884,\n",
       " '自': 885,\n",
       " '8g': 886,\n",
       " '桑塔纳': 887,\n",
       " '东风': 888,\n",
       " '整套': 889,\n",
       " 'h6': 890,\n",
       " '铝合金': 891,\n",
       " '长方形': 892,\n",
       " '软': 893,\n",
       " '扳手': 894,\n",
       " '仿古': 895,\n",
       " '貔貅': 896,\n",
       " '珍珠': 897,\n",
       " '斯柯达': 898,\n",
       " '行车': 899,\n",
       " 'd': 900,\n",
       " 's6': 901,\n",
       " '张': 902,\n",
       " '底': 903,\n",
       " '水洗': 904,\n",
       " '球': 905,\n",
       " '卷': 906,\n",
       " '点': 907,\n",
       " '速递': 908,\n",
       " '鼠标': 909,\n",
       " '咖': 910,\n",
       " '配饰': 911,\n",
       " '出版社': 912,\n",
       " '免': 913,\n",
       " '黑白': 914,\n",
       " '5mm': 915,\n",
       " '裙': 916,\n",
       " '图书': 917,\n",
       " '22': 918,\n",
       " '大红': 919,\n",
       " '车垫': 920,\n",
       " '宝骏': 921,\n",
       " '雕花': 922,\n",
       " '定做': 923,\n",
       " '电镀': 924,\n",
       " '盒装': 925,\n",
       " '好': 926,\n",
       " '超高': 927,\n",
       " '翻盖': 928,\n",
       " '音响': 929,\n",
       " '手绘': 930,\n",
       " '台灯': 931,\n",
       " '骆驼': 932,\n",
       " '圆领': 933,\n",
       " 'v': 934,\n",
       " '拉手': 935,\n",
       " '古典': 936,\n",
       " '浅蓝色': 937,\n",
       " '年级': 938,\n",
       " 'pro': 939,\n",
       " '天': 940,\n",
       " '台湾': 941,\n",
       " '酒杯': 942,\n",
       " '吊': 943,\n",
       " '品': 944,\n",
       " '笔记本电脑': 945,\n",
       " '镜头': 946,\n",
       " '绒': 947,\n",
       " '精装': 948,\n",
       " '世家': 949,\n",
       " '指环': 950,\n",
       " '后视镜': 951,\n",
       " '铜': 952,\n",
       " '生肖': 953,\n",
       " '幼儿园': 954,\n",
       " '烤': 955,\n",
       " '网布': 956,\n",
       " '可折叠': 957,\n",
       " '标签': 958,\n",
       " '必备': 959,\n",
       " '朗逸': 960,\n",
       " '边': 961,\n",
       " '共': 962,\n",
       " '魅族': 963,\n",
       " '电脑包': 964,\n",
       " '补漆笔': 965,\n",
       " '小熊': 966,\n",
       " '轻薄': 967,\n",
       " '静音': 968,\n",
       " '网络': 969,\n",
       " '泰迪熊': 970,\n",
       " '艺术': 971,\n",
       " '标准版': 972,\n",
       " '卡宴': 973,\n",
       " '腰包': 974,\n",
       " '文具': 975,\n",
       " '500ml': 976,\n",
       " '电视': 977,\n",
       " '头枕': 978,\n",
       " '风衣': 979,\n",
       " '年份': 980,\n",
       " '男装': 981,\n",
       " '插座': 982,\n",
       " '90': 983,\n",
       " '电视柜': 984,\n",
       " '刹车片': 985,\n",
       " '超': 986,\n",
       " '奶瓶': 987,\n",
       " '冬': 988,\n",
       " '昂科威': 989,\n",
       " '主机': 990,\n",
       " '富贵': 991,\n",
       " '自由': 992,\n",
       " '开业': 993,\n",
       " '仪表': 994,\n",
       " '工装': 995,\n",
       " 'vivo': 996,\n",
       " '耳钉': 997,\n",
       " '书桌': 998,\n",
       " '羊皮': 999,\n",
       " '机械': 1000,\n",
       " ...}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "274420"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tok.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = tok.texts_to_sequences(word_data_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = tok.texts_to_sequences(word_data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = sequence.pad_sequences(x_train, maxlen=20)\n",
    "x_test  = sequence.pad_sequences(x_test, maxlen=20) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[    0,     0,     0, ...,  7750, 27959, 72790],\n",
       "       [    0,     0, 32342, ...,     2,    12,   123],\n",
       "       [    0,     0, 18533, ...,    47,     4,  3167],\n",
       "       ...,\n",
       "       [    0,     0,     0, ...,  5760,  1549,   179],\n",
       "       [    1,  1025,   497, ...,   611,    35,   169],\n",
       "       [    0,     0,  8268, ...,   376,   143,   305]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## GRU Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.contrib import rnn\n",
    "import tensorflow.contrib.layers as layers\n",
    "class GruModel(object):\n",
    "    \"\"\"\n",
    "    A CNN for text classification.\n",
    "    Uses an embedding layer, followed by a convolutional, max-pooling and softmax layer.\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 sequence_length, \n",
    "                 num_classes, \n",
    "                 embedding_size,  \n",
    "                 num_filters, \n",
    "                 l2_reg_lambda=0.0, \n",
    "                 n_layer = 1, \n",
    "                 hidden_size = 32, \n",
    "                 batch_size = 256, \n",
    "                 vac_size = 27440):\n",
    "        \"\"\"\n",
    "        sequence_length : 一个句子的长度（词的个数）\n",
    "        embedding_size : 词向量的长度\n",
    "        num_classes : 三个标签的类别数\n",
    "        vac_size : 词的个数\n",
    "        \"\"\"\n",
    "        # Placeholders for input, output and dropout\n",
    "        self.input_x = tf.placeholder(tf.int32, [None, sequence_length], name=\"input_x\")\n",
    "        self.input_y1 = tf.placeholder(tf.float32, [None, num_classes[0]], name=\"input_y1\")\n",
    "        self.input_y2 = tf.placeholder(tf.float32, [None, num_classes[1]], name=\"input_y2\")\n",
    "        self.input_y3 = tf.placeholder(tf.float32, [None, num_classes[2]], name=\"input_y3\")\n",
    "        self.keep_prob = tf.placeholder(tf.float32, name=\"keep_prob\")\n",
    "        self.n_layer = n_layer\n",
    "        self.batch_size = batch_size\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        \n",
    "        with tf.name_scope(\"embedding\"):\n",
    "            W = tf.Variable(\n",
    "                tf.random_uniform([274420, embedding_size], -1.0, 1.0),\n",
    "                name=\"W\")\n",
    "            self.embedded_chars = tf.nn.embedding_lookup(W, self.input_x)\n",
    "\n",
    "        l2_loss = tf.constant(0.0)\n",
    "        \n",
    "        self.embedded_input = tf.layers.batch_normalization(self.embedded_chars, training=is_training)\n",
    "        self.embedded_input = tf.expand_dims(self.embedded_input, -1)\n",
    "        \n",
    "        with tf.variable_scope(\"cnn1\"):\n",
    "            filter_shape = [3, 3, 1, num_filters]\n",
    "            W = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.1), name=\"W\")\n",
    "            b = tf.Variable(tf.constant(0.1, shape=[num_filters]), name=\"b\")\n",
    "            self.conv_output = tf.nn.conv2d(\n",
    "                    self.embedded_input,\n",
    "                    W,\n",
    "                    strides=[1, 1, 1, 1],\n",
    "                    padding=\"SAME\",\n",
    "                    name=\"conv\")\n",
    "            self.conv_output = tf.nn.dropout(self.conv_output, self.keep_prob)\n",
    "            \n",
    "        #gru模型\n",
    "        with tf.variable_scope('inception_text'):\n",
    "            self.output = self.text_inception(self.conv_output)\n",
    "        \n",
    "        with tf.variable_scope(\"cnn2\"):\n",
    "            filter_shape = [5, 5, 16, num_filters]\n",
    "            W = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.1), name=\"W\")\n",
    "            b = tf.Variable(tf.constant(0.1, shape=[num_filters]), name=\"b\")\n",
    "            self.conv_output = tf.nn.conv2d(\n",
    "                    self.output,\n",
    "                    W,\n",
    "                    strides=[1, 1, 1, 1],\n",
    "                    padding=\"VALID\",\n",
    "                    name=\"conv\")\n",
    "            self.conv_output = tf.nn.dropout(self.conv_output, self.keep_prob)\n",
    "        print(self.conv_output)\n",
    "        self.h_drop = tf.reshape(self.conv_output,(-1, 16 * 20 * 4))\n",
    "        print(self.h_drop.shape)\n",
    "        num_filters_total = self.h_drop.shape[1]\n",
    "        \n",
    "        \n",
    "        with tf.name_scope(\"output1\"):\n",
    "            W1 = tf.get_variable(\n",
    "                \"W11\",\n",
    "                shape=[num_filters_total, 64],\n",
    "                initializer=tf.contrib.layers.xavier_initializer())\n",
    "            b1 = tf.Variable(tf.constant(0.1, shape=[64]), name=\"b11\")\n",
    "            self.output = tf.nn.xw_plus_b(self.h_drop, W1, b1, name=\"output1\")\n",
    "            self.output = tf.layers.batch_normalization(self.output, training=is_training)\n",
    "            self.output = tf.nn.relu(self.output)\n",
    "            self.output = tf.nn.dropout(self.output, self.keep_prob)\n",
    "            W2 = tf.get_variable(\n",
    "                \"W12\",\n",
    "                shape=[64, num_classes[0]],\n",
    "                initializer=tf.contrib.layers.xavier_initializer())\n",
    "            b2 = tf.Variable(tf.constant(0.1, shape=[num_classes[0]]), name=\"b12\")\n",
    "            l2_loss += tf.nn.l2_loss(W1)\n",
    "            l2_loss += tf.nn.l2_loss(b1)\n",
    "            self.scores1 = tf.nn.xw_plus_b(self.output, W2, b2, name=\"scores\")\n",
    "            self.predictions1 = tf.argmax(self.scores1, 1, name=\"predictions\")\n",
    "            \n",
    "        with tf.name_scope(\"output2\"):\n",
    "            W1 = tf.get_variable(\n",
    "                \"W21\",\n",
    "                shape=[num_filters_total, 128],\n",
    "                initializer=tf.contrib.layers.xavier_initializer())\n",
    "            b1 = tf.Variable(tf.constant(0.1, shape=[128]), name=\"b21\")\n",
    "            self.output = tf.nn.xw_plus_b(self.h_drop, W1, b1, name=\"output2\")\n",
    "            self.output = tf.layers.batch_normalization(self.output, training=is_training)\n",
    "            self.output = tf.nn.relu(self.output)\n",
    "            self.output = tf.nn.dropout(self.output, self.keep_prob)\n",
    "            W2 = tf.get_variable(\n",
    "                \"W22\",\n",
    "                shape=[128, num_classes[1]],\n",
    "                initializer=tf.contrib.layers.xavier_initializer())\n",
    "            b2 = tf.Variable(tf.constant(0.1, shape=[num_classes[1]]), name=\"b22\")\n",
    "            l2_loss += tf.nn.l2_loss(W1)\n",
    "            l2_loss += tf.nn.l2_loss(b1)\n",
    "            self.scores2 = tf.nn.xw_plus_b(self.output, W2, b2, name=\"scores\")\n",
    "            self.predictions2 = tf.argmax(self.scores2, 1, name=\"predictions\")\n",
    "            \n",
    "        with tf.name_scope(\"output3\"):\n",
    "            W1 = tf.get_variable(\n",
    "                \"W31\",\n",
    "                shape=[num_filters_total, 256],\n",
    "                initializer=tf.contrib.layers.xavier_initializer())\n",
    "            b1 = tf.Variable(tf.constant(0.1, shape=[256]), name=\"b31\")\n",
    "            self.output = tf.nn.xw_plus_b(self.h_drop, W1, b1, name=\"output3\")\n",
    "            self.output = tf.layers.batch_normalization(self.output, training=is_training)\n",
    "            self.output = tf.nn.relu(self.output)\n",
    "            self.output = tf.nn.dropout(self.output, self.keep_prob)\n",
    "            W2 = tf.get_variable(\n",
    "                \"W32\",\n",
    "                shape=[256, num_classes[2]],\n",
    "                initializer=tf.contrib.layers.xavier_initializer())\n",
    "            b2 = tf.Variable(tf.constant(0.1, shape=[num_classes[2]]), name=\"b32\")\n",
    "            l2_loss += tf.nn.l2_loss(W1)\n",
    "            l2_loss += tf.nn.l2_loss(b1)\n",
    "            self.scores3 = tf.nn.xw_plus_b(self.output, W2, b2, name=\"scores\")\n",
    "            self.predictions3 = tf.argmax(self.scores3, 1, name=\"predictions\")\n",
    "        # Calculate mean cross-entropy loss\n",
    "        \n",
    "        with tf.name_scope(\"loss\"):\n",
    "            losses1 = tf.nn.softmax_cross_entropy_with_logits(logits=self.scores1, labels=self.input_y1)\n",
    "            losses2 = tf.nn.softmax_cross_entropy_with_logits(logits=self.scores2, labels=self.input_y2)\n",
    "            losses3 = tf.nn.softmax_cross_entropy_with_logits(logits=self.scores3, labels=self.input_y3)\n",
    "            self.loss = 0.1 * tf.reduce_mean(losses1) + 0.2 * tf.reduce_mean(losses2) + 0.7*tf.reduce_mean(losses3) + l2_reg_lambda * l2_loss\n",
    "            tf.summary.scalar('loss1',tf.reduce_mean(losses1))\n",
    "            tf.summary.scalar('loss2',tf.reduce_mean(losses2))\n",
    "            tf.summary.scalar('loss3',tf.reduce_mean(losses3))\n",
    "            tf.summary.scalar('loss',self.loss)\n",
    "        # Accuracy\n",
    "        with tf.name_scope(\"accuracy\"):\n",
    "            correct_predictions1 = tf.equal(self.predictions1, tf.argmax(self.input_y1, 1))\n",
    "            correct_predictions2 = tf.equal(self.predictions2, tf.argmax(self.input_y2, 1))\n",
    "            correct_predictions3 = tf.equal(self.predictions3, tf.argmax(self.input_y3, 1))\n",
    "#           单独的准确率\n",
    "            self.accuracy1 = tf.reduce_mean(tf.cast(correct_predictions1, \"float\"), name=\"accuracy1\")\n",
    "            self.accuracy2 = tf.reduce_mean(tf.cast(correct_predictions2, \"float\"), name=\"accuracy2\")\n",
    "            self.accuracy3 = tf.reduce_mean(tf.cast(correct_predictions3, \"float\"), name=\"accuracy3\")\n",
    "#           一起的准确率\n",
    "            self.acc = tf.reduce_mean(tf.cast(correct_predictions1, \"float\")*tf.cast(correct_predictions2, \"float\")*tf.cast(correct_predictions3, \"float\"))\n",
    "            tf.summary.scalar('acc1',self.accuracy1)\n",
    "            tf.summary.scalar('acc2',self.accuracy2)\n",
    "            tf.summary.scalar('acc3',self.accuracy3)\n",
    "            tf.summary.scalar('acc',self.acc)\n",
    "            \n",
    "    def text_inception(self, inputs):\n",
    "        output = []\n",
    "        with tf.name_scope(\"conv-1\"):\n",
    "            filter_shape = [1, 1, num_filters, num_filters]\n",
    "            W = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.1), name=\"W\")\n",
    "            b = tf.Variable(tf.constant(0.1, shape=[num_filters]), name=\"b\")\n",
    "            conv = tf.nn.conv2d(\n",
    "                    inputs,\n",
    "                    W,\n",
    "                    strides=[1, 1, 1, 1],\n",
    "                    padding=\"VALID\",\n",
    "                    name=\"conv\")\n",
    "            output.append(conv)\n",
    "        with tf.name_scope(\"conv-2\"):\n",
    "            filter_shape1 = [1, 1, num_filters, num_filters]\n",
    "            W = tf.Variable(tf.truncated_normal(filter_shape1, stddev=0.1), name=\"W\")\n",
    "            b = tf.Variable(tf.constant(0.1, shape=[num_filters]), name=\"b\")\n",
    "            conv1 = tf.nn.conv2d(\n",
    "                    inputs,\n",
    "                    W,\n",
    "                    strides=[1, 1, 1, 1],\n",
    "                    padding=\"VALID\",\n",
    "                    name=\"conv\")\n",
    "            filter_shape2 = [3, 3, num_filters, 2 * num_filters]\n",
    "            W1 = tf.Variable(tf.truncated_normal(filter_shape2, stddev=0.1), name=\"W\")\n",
    "            b1 = tf.Variable(tf.constant(0.1, shape=[num_filters]), name=\"b\")\n",
    "            conv2 = tf.nn.conv2d(\n",
    "                    conv1,\n",
    "                    W1,\n",
    "                    strides=[1, 1, 1, 1],\n",
    "                    padding=\"SAME\",\n",
    "                    name=\"conv\")   \n",
    "            output.append(conv2)\n",
    "        with tf.name_scope(\"conv-4\"):\n",
    "            filter_shape = [3, 3, num_filters, num_filters]\n",
    "            W = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.1), name=\"W\")\n",
    "            b = tf.Variable(tf.constant(0.1, shape=[num_filters]), name=\"b\")\n",
    "            conv = tf.nn.conv2d(\n",
    "                    inputs,\n",
    "                    W,\n",
    "                    strides=[1, 1, 1, 1],\n",
    "                    padding=\"SAME\",\n",
    "                    name=\"conv\")\n",
    "            output.append(conv)\n",
    "        print(tf.concat(output, 3).shape)\n",
    "        return tf.concat(output, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# batch 生成函数\n",
    "import math\n",
    "def batch_iter(data, batch_size, num_epochs, shuffle=True):\n",
    "    \"\"\"\n",
    "    Generates a batch iterator for a dataset.\n",
    "    \"\"\"\n",
    "    data = np.array(data)\n",
    "    data_size = len(data)\n",
    "    num_batches_per_epoch = int((len(data)-1)/batch_size) + 1\n",
    "    for epoch in range(num_epochs):\n",
    "        # Shuffle the data at each epoch\n",
    "        if shuffle:\n",
    "            shuffle_indices = np.random.permutation(np.arange(data_size))\n",
    "            shuffled_data = data[shuffle_indices]\n",
    "        else:\n",
    "            shuffled_data = data\n",
    "  \n",
    "        for batch_num in range(num_batches_per_epoch):\n",
    "            start_index = batch_num * batch_size\n",
    "            end_index = np.min(((batch_num + 1) * batch_size, data_size))\n",
    "            yield shuffled_data[start_index:end_index]\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 特征数这里是第三级标签的特征数\n",
    "max_features = 1192\n",
    "\n",
    "# 句子填充的长度\n",
    "sequence_len = 20\n",
    "\n",
    "# batch 大小\n",
    "batch_size = 256 \n",
    "\n",
    "# 迭代次数\n",
    "epochs = 100\n",
    "\n",
    "is_training = True\n",
    "\n",
    "# 词向量长度\n",
    "embedding_dims = 24\n",
    "\n",
    "# gru  的filters\n",
    "num_filters = 4\n",
    "\n",
    "# filter 的大小\n",
    "filter_size = [1, 2, 3, 4, 5]\n",
    "\n",
    "# 三个标签的类数\n",
    "num_classes = [22, 191, 1192]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "from tensorflow.contrib import learn\n",
    "\n",
    "\n",
    "\n",
    "FLAGS = tf.flags.FLAGS\n",
    "\n",
    "def train(x_train, y_train, x_dev, y_dev):\n",
    "    # Training\n",
    "    # ==================================================\n",
    "\n",
    "    with tf.Graph().as_default():\n",
    "        session_conf = tf.ConfigProto()\n",
    "        sess = tf.Session(config=session_conf)\n",
    "        with sess.as_default():\n",
    "            cnn = GruModel(sequence_length = sequence_len, \n",
    "                          num_classes = num_classes, \n",
    "                          embedding_size = embedding_dims, \n",
    "                          num_filters = num_filters)\n",
    "\n",
    "            # Define Training procedure\n",
    "\n",
    "            global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "#           学习率衰减\n",
    "            learning_rate = tf.train.exponential_decay(\n",
    "                0.01,\n",
    "                global_step,\n",
    "                x_train.shape[0] / batch_size,\n",
    "                0.99,\n",
    "                staircase=True,)\n",
    "            optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "            grads_and_vars = optimizer.compute_gradients(cnn.loss)\n",
    "            train_op = optimizer.apply_gradients(grads_and_vars, global_step=global_step)\n",
    "\n",
    "            # Initialize all variables\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            \n",
    "            merged = tf.summary.merge_all()\n",
    "            \n",
    "            writer = tf.summary.FileWriter(\"fwwb\",sess.graph)\n",
    "            acc_sum = np.array([0.0,0.0,0.0,0.0])\n",
    "            loss_sum = 0\n",
    "            \n",
    "            def train_step(x_batch, y_batch1, y_batch2, y_batch3, loss_sum, acc_sum, interal = 10):\n",
    "  \n",
    "                \"\"\"\n",
    "                A single training step\n",
    "                \"\"\"\n",
    "                feed_dict = {\n",
    "                  cnn.input_x: x_batch,\n",
    "                  cnn.input_y1: y_batch1,\n",
    "                  cnn.input_y2: y_batch2,\n",
    "                  cnn.input_y3: y_batch3,\n",
    "                  cnn.keep_prob: 0.9\n",
    "                }\n",
    "\n",
    "#         三个标签单独的准确率， 一起的准确率共四个准确率\n",
    "                rs, _, step,  loss, accuracy1, accuracy2, accuracy3, acc = sess.run(\n",
    "                    [merged, train_op, global_step, cnn.loss, cnn.accuracy1, cnn.accuracy2, cnn.accuracy3, cnn.acc],\n",
    "                    feed_dict)\n",
    "                time_str = datetime.datetime.now().isoformat()\n",
    "                acc_sum += [accuracy1, accuracy2, accuracy3, acc]\n",
    "                loss_sum += loss\n",
    "                if step % interal == 0:\n",
    "                    print(\"{}: step {}, loss {:g}, acc {}\".format(time_str, step, loss_sum/interal, acc_sum/interal))\n",
    "                    loss_sum = 0\n",
    "                    acc_sum = np.array([0.0,0.0,0.0,0.0])\n",
    "                    writer.add_summary(rs, step)\n",
    "                return loss_sum, acc_sum\n",
    "            # 评估步骤\n",
    "            def dev_step(x_batch, y_batch):\n",
    "                \"\"\"\n",
    "                Evaluates model on a dev set\n",
    "                \"\"\"\n",
    "\n",
    "                dev_batches = batch_iter(\n",
    "                    list(zip(x_batch, y_batch[0], y_batch[1], y_batch[2])), batch_size, 1)\n",
    "                step = 0\n",
    "                loss = 0\n",
    "                accuracy = np.array([0.0, 0.0, 0.0,0.0])\n",
    "                for batch in dev_batches:\n",
    "                    x_batch_dev, y_batch1_dev, y_batch2_dev, y_batch3_dev = zip(*batch)\n",
    "                    feed_dict = {\n",
    "                      cnn.input_x: x_batch_dev,\n",
    "                      cnn.input_y1: y_batch1_dev,\n",
    "                      cnn.input_y2: y_batch2_dev,\n",
    "                      cnn.input_y3: y_batch3_dev,\n",
    "                      cnn.keep_prob: 1\n",
    "                    }\n",
    "                    if len(x_batch_dev) < batch_size:\n",
    "                        continue\n",
    "                    step,  temp_loss, temp_accuracy1, temp_accuracy2, temp_accuracy3 , acc= sess.run(\n",
    "                        [global_step,cnn.loss, cnn.accuracy1, cnn.accuracy2, cnn.accuracy3, cnn.acc],\n",
    "                        feed_dict)\n",
    "                    accuracy[0] += temp_accuracy1 * len(x_batch_dev)\n",
    "                    accuracy[1] += temp_accuracy2 * len(x_batch_dev)\n",
    "                    accuracy[2] += temp_accuracy3 * len(x_batch_dev)\n",
    "                    accuracy[3] += acc * len(x_batch_dev)\n",
    "                    loss += temp_loss * len(x_batch_dev)\n",
    "                accuracy /= x_batch.shape[0]\n",
    "                loss /= x_batch.shape[0]\n",
    "                time_str = datetime.datetime.now().isoformat()                    \n",
    "\n",
    "                print(\"{}: step {}, loss {:g}, acc {} \".format(time_str, step, loss, accuracy))\n",
    "\n",
    "\n",
    "            # Generate batches\n",
    "            batches = batch_iter(\n",
    "                list(zip(x_train, y_train[0],y_train[1], y_train[2] )), batch_size, 200)\n",
    "            # Training loop. For each batch...\n",
    "            for batch in batches:\n",
    "                x_batch, y_batch1, y_batch2, y_batch3 = zip(*batch)\n",
    "                if len(x_batch) < batch_size:\n",
    "                    continue\n",
    "                loss_sum, acc_sum = train_step(x_batch, y_batch1, y_batch2, y_batch3,  loss_sum, acc_sum, 40)\n",
    "                current_step = tf.train.global_step(sess, global_step)\n",
    "                if current_step % 200 == 0:\n",
    "                    print(\"\\nEvaluation:\")\n",
    "                    dev_step(x_dev, y_dev)\n",
    "                    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 20, 24, 16)\n",
      "Tensor(\"cnn2/dropout/mul:0\", shape=(?, 16, 20, 4), dtype=float32)\n",
      "(?, 1280)\n",
      "2019-03-02T16:44:26.055944: step 40, loss 4.1152, acc [0.39746094 0.32753906 0.23457031 0.17675781]\n",
      "2019-03-02T16:44:28.508144: step 80, loss 2.93779, acc [0.58476562 0.49755859 0.40205078 0.32089844]\n",
      "2019-03-02T16:44:30.908728: step 120, loss 2.39895, acc [0.67626953 0.58769531 0.50390625 0.41230469]\n",
      "2019-03-02T16:44:33.259769: step 160, loss 2.02526, acc [0.74150391 0.64541016 0.56494141 0.47841797]\n",
      "2019-03-02T16:44:35.593943: step 200, loss 1.84884, acc [0.77402344 0.68017578 0.59638672 0.51464844]\n",
      "\n",
      "Evaluation:\n",
      "2019-03-02T16:44:45.378536: step 200, loss 1.67541, acc [0.80206029 0.71077896 0.62220064 0.55438537] \n",
      "\n",
      "2019-03-02T16:44:48.258806: step 240, loss 1.63392, acc [0.79960937 0.70732422 0.63212891 0.54765625]\n",
      "2019-03-02T16:44:50.937700: step 280, loss 1.57393, acc [0.81386719 0.72080078 0.64296875 0.56386719]\n",
      "2019-03-02T16:44:53.618578: step 320, loss 1.4657, acc [0.82675781 0.74082031 0.6625     0.58994141]\n",
      "2019-03-02T16:44:56.316366: step 360, loss 1.40633, acc [0.83154297 0.74628906 0.67890625 0.60351562]\n",
      "2019-03-02T16:44:58.889074: step 400, loss 1.34495, acc [0.84121094 0.75410156 0.6796875  0.60449219]\n",
      "\n",
      "Evaluation:\n",
      "2019-03-02T16:45:07.113746: step 400, loss 1.18658, acc [0.86327824 0.78257866 0.71917829 0.65627847] \n",
      "\n",
      "2019-03-02T16:45:09.859104: step 440, loss 1.29757, acc [0.84599609 0.76494141 0.68974609 0.61933594]\n",
      "2019-03-02T16:45:12.521630: step 480, loss 1.24998, acc [0.85136719 0.77216797 0.7078125  0.63701172]\n",
      "2019-03-02T16:45:15.073056: step 520, loss 1.2049, acc [0.85800781 0.78613281 0.71357422 0.64873047]\n",
      "2019-03-02T16:45:17.632907: step 560, loss 1.1678, acc [0.86601562 0.7875     0.71992188 0.65830078]\n",
      "2019-03-02T16:45:20.100524: step 600, loss 1.13664, acc [0.86962891 0.79414063 0.721875   0.65810547]\n",
      "\n",
      "Evaluation:\n",
      "2019-03-02T16:45:28.654520: step 600, loss 0.983564, acc [0.88695452 0.81996016 0.75772107 0.70231957] \n",
      "\n",
      "2019-03-02T16:45:31.509990: step 640, loss 1.09262, acc [0.87304688 0.80400391 0.73457031 0.66738281]\n",
      "2019-03-02T16:45:34.158637: step 680, loss 1.07668, acc [0.86835938 0.79746094 0.73701172 0.66533203]\n",
      "2019-03-02T16:45:36.827110: step 720, loss 1.01425, acc [0.87998047 0.81308594 0.74560547 0.68134766]\n",
      "2019-03-02T16:45:39.261976: step 760, loss 1.02925, acc [0.88486328 0.80820313 0.74931641 0.68652344]\n",
      "2019-03-02T16:45:41.674022: step 800, loss 1.02715, acc [0.87646484 0.80927734 0.74492187 0.68193359]\n",
      "\n",
      "Evaluation:\n",
      "2019-03-02T16:45:50.662530: step 800, loss 0.862871, acc [0.89955851 0.83884111 0.77964541 0.72460431] \n",
      "\n",
      "2019-03-02T16:45:53.222882: step 840, loss 1.00062, acc [0.88271484 0.81962891 0.75644531 0.69492188]\n",
      "2019-03-02T16:45:55.619551: step 880, loss 0.983895, acc [0.88681641 0.82050781 0.75498047 0.69492188]\n",
      "2019-03-02T16:45:58.757250: step 920, loss 0.948463, acc [0.88515625 0.82519531 0.76396484 0.70546875]\n",
      "2019-03-02T16:46:01.839391: step 960, loss 0.967052, acc [0.88457031 0.81728516 0.76240234 0.70058594]\n",
      "2019-03-02T16:46:05.361488: step 1000, loss 0.931836, acc [0.89150391 0.83085937 0.77041016 0.71386719]\n",
      "\n",
      "Evaluation:\n",
      "2019-03-02T16:46:14.940239: step 1000, loss 0.758287, acc [0.9109011  0.85560973 0.80336173 0.75390684] \n",
      "\n",
      "2019-03-02T16:46:17.461899: step 1040, loss 0.932281, acc [0.88935547 0.83027344 0.76992187 0.71064453]\n",
      "2019-03-02T16:46:19.818941: step 1080, loss 0.925012, acc [0.89208984 0.83085937 0.76894531 0.71181641]\n",
      "2019-03-02T16:46:22.123306: step 1120, loss 0.884985, acc [0.8921875  0.83427734 0.77929688 0.71835938]\n",
      "2019-03-02T16:46:24.484269: step 1160, loss 0.877421, acc [0.89580078 0.84082031 0.78154297 0.72363281]\n",
      "2019-03-02T16:46:26.794139: step 1200, loss 0.906579, acc [0.89375    0.82714844 0.77138672 0.71269531]\n",
      "\n",
      "Evaluation:\n",
      "2019-03-02T16:46:35.801493: step 1200, loss 0.682352, acc [0.92021143 0.86879436 0.81946961 0.77259758] \n",
      "\n",
      "2019-03-02T16:46:38.368292: step 1240, loss 0.848213, acc [0.89707031 0.84355469 0.78496094 0.72734375]\n",
      "2019-03-02T16:46:40.654356: step 1280, loss 0.830426, acc [0.89697266 0.84257812 0.78710938 0.73105469]\n",
      "2019-03-02T16:46:43.005383: step 1320, loss 0.826583, acc [0.90537109 0.84267578 0.78964844 0.73359375]\n",
      "2019-03-02T16:46:45.309797: step 1360, loss 0.856994, acc [0.89697266 0.84003906 0.78476563 0.72646484]\n",
      "2019-03-02T16:46:47.652409: step 1400, loss 0.824991, acc [0.89814453 0.84316406 0.78828125 0.73320312]\n",
      "\n",
      "Evaluation:\n",
      "2019-03-02T16:46:56.317522: step 1400, loss 0.622448, acc [0.9262181  0.88010692 0.83316481 0.78812482] \n",
      "\n",
      "2019-03-02T16:46:58.730565: step 1440, loss 0.810898, acc [0.903125   0.84873047 0.79238281 0.73779297]\n",
      "2019-03-02T16:47:00.982403: step 1480, loss 0.83226, acc [0.90205078 0.84404297 0.79248047 0.73417969]\n",
      "2019-03-02T16:47:03.249122: step 1520, loss 0.805391, acc [0.89951172 0.84492188 0.79570312 0.73740234]\n",
      "2019-03-02T16:47:05.548085: step 1560, loss 0.812654, acc [0.90234375 0.84462891 0.79384766 0.73789063]\n",
      "2019-03-02T16:47:08.009727: step 1600, loss 0.566216, acc [0.92529297 0.88515625 0.84863281 0.79384766]\n",
      "\n",
      "Evaluation:\n",
      "2019-03-02T16:47:16.777063: step 1600, loss 0.557442, acc [0.93192444 0.88877654 0.8472104  0.80228053] \n",
      "\n",
      "2019-03-02T16:47:19.180684: step 1640, loss 0.529831, acc [0.93007812 0.89052734 0.85107422 0.79726562]\n",
      "2019-03-02T16:47:21.601620: step 1680, loss 0.545692, acc [0.92353516 0.88710937 0.84921875 0.79238281]\n",
      "2019-03-02T16:47:24.081611: step 1720, loss 0.570511, acc [0.92841797 0.88525391 0.84511719 0.79111328]\n",
      "2019-03-02T16:47:26.478060: step 1760, loss 0.530225, acc [0.92607422 0.88994141 0.8515625  0.79658203]\n",
      "2019-03-02T16:47:28.761867: step 1800, loss 0.550893, acc [0.92939453 0.88310547 0.84453125 0.79179687]\n",
      "\n",
      "Evaluation:\n",
      "2019-03-02T16:47:38.367399: step 1800, loss 0.5345, acc [0.93609907 0.89482325 0.85326713 0.80957863] \n",
      "\n",
      "2019-03-02T16:47:40.804247: step 1840, loss 0.567202, acc [0.92734375 0.88232422 0.84726563 0.79287109]\n",
      "2019-03-02T16:47:43.108662: step 1880, loss 0.581611, acc [0.92382812 0.88085938 0.83886719 0.78105469]\n",
      "2019-03-02T16:47:45.465157: step 1920, loss 0.558138, acc [0.93027344 0.88691406 0.84873047 0.79570312]\n",
      "2019-03-02T16:47:47.731880: step 1960, loss 0.551024, acc [0.92939453 0.88652344 0.84833984 0.79521484]\n",
      "2019-03-02T16:47:49.957428: step 2000, loss 0.547636, acc [0.93105469 0.88740234 0.84785156 0.79335937]\n",
      "\n",
      "Evaluation:\n",
      "2019-03-02T16:47:58.862118: step 2000, loss 0.505624, acc [0.938742   0.89929822 0.86247735 0.81990009] \n",
      "\n",
      "2019-03-02T16:48:01.228034: step 2040, loss 0.546341, acc [0.93154297 0.88798828 0.84824219 0.79482422]\n",
      "2019-03-02T16:48:03.430274: step 2080, loss 0.558163, acc [0.92392578 0.88554687 0.85009766 0.79316406]\n",
      "2019-03-02T16:48:05.633010: step 2120, loss 0.535558, acc [0.92958984 0.88935547 0.84521484 0.79501953]\n",
      "2019-03-02T16:48:07.831818: step 2160, loss 0.528143, acc [0.93046875 0.89140625 0.85488281 0.80117187]\n",
      "2019-03-02T16:48:10.115903: step 2200, loss 0.549003, acc [0.92900391 0.88916016 0.85175781 0.79931641]\n",
      "\n",
      "Evaluation:\n",
      "2019-03-02T16:48:18.509205: step 2200, loss 0.478834, acc [0.94127482 0.90275205 0.86836388 0.82634725] \n",
      "\n",
      "2019-03-02T16:48:20.936593: step 2240, loss 0.5796, acc [0.92441406 0.87939453 0.83681641 0.78320312]\n",
      "2019-03-02T16:48:23.286639: step 2280, loss 0.543411, acc [0.93154297 0.88955078 0.85361328 0.8015625 ]\n",
      "2019-03-02T16:48:25.573546: step 2320, loss 0.531491, acc [0.92763672 0.88701172 0.8515625  0.79501953]\n",
      "2019-03-02T16:48:27.778758: step 2360, loss 0.553755, acc [0.92421875 0.8859375  0.84882813 0.79628906]\n",
      "2019-03-02T16:48:29.980505: step 2400, loss 0.528451, acc [0.93134766 0.89042969 0.85390625 0.80175781]\n",
      "\n",
      "Evaluation:\n",
      "2019-03-02T16:48:38.957607: step 2400, loss 0.458536, acc [0.94263633 0.90571534 0.87226822 0.83088228] \n",
      "\n",
      "2019-03-02T16:48:41.368165: step 2440, loss 0.563035, acc [0.93027344 0.88515625 0.84951172 0.79765625]\n",
      "2019-03-02T16:48:43.576897: step 2480, loss 0.56345, acc [0.92822266 0.88984375 0.84658203 0.79511719]\n",
      "2019-03-02T16:48:45.930864: step 2520, loss 0.572111, acc [0.92597656 0.88681641 0.84589844 0.79589844]\n",
      "2019-03-02T16:48:48.256610: step 2560, loss 0.541081, acc [0.92294922 0.88691406 0.84921875 0.79267578]\n",
      "2019-03-02T16:48:50.489142: step 2600, loss 0.557043, acc [0.93007812 0.88876953 0.84990234 0.79873047]\n",
      "\n",
      "Evaluation:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-02T16:48:58.773789: step 2600, loss 0.433343, acc [0.94466858 0.912613   0.87913584 0.83776992] \n",
      "\n",
      "2019-03-02T16:49:01.338604: step 2640, loss 0.552885, acc [0.92939453 0.88857422 0.85068359 0.79902344]\n",
      "2019-03-02T16:49:03.565688: step 2680, loss 0.574283, acc [0.92578125 0.88525391 0.84589844 0.79677734]\n",
      "2019-03-02T16:49:05.892923: step 2720, loss 0.527153, acc [0.92822266 0.89140625 0.85527344 0.80576172]\n",
      "2019-03-02T16:49:08.174475: step 2760, loss 0.516726, acc [0.93027344 0.89892578 0.85976562 0.80751953]\n",
      "2019-03-02T16:49:10.370760: step 2800, loss 0.556211, acc [0.92910156 0.88496094 0.84960938 0.79726562]\n",
      "\n",
      "Evaluation:\n",
      "2019-03-02T16:49:18.985288: step 2800, loss 0.411729, acc [0.94822253 0.91766861 0.88399123 0.84602909] \n",
      "\n",
      "2019-03-02T16:49:21.391880: step 2840, loss 0.545956, acc [0.92910156 0.88720703 0.85380859 0.79902344]\n",
      "2019-03-02T16:49:23.589707: step 2880, loss 0.547007, acc [0.92910156 0.88730469 0.85126953 0.796875  ]\n",
      "2019-03-02T16:49:25.851910: step 2920, loss 0.558284, acc [0.92695313 0.88857422 0.84804687 0.79472656]\n",
      "2019-03-02T16:49:28.197492: step 2960, loss 0.547964, acc [0.93154297 0.890625   0.85166016 0.79912109]\n",
      "2019-03-02T16:49:30.439909: step 3000, loss 0.540227, acc [0.92490234 0.89228516 0.85166016 0.80146484]\n",
      "\n",
      "Evaluation:\n",
      "2019-03-02T16:49:38.911130: step 3000, loss 0.379486, acc [0.95164633 0.92283435 0.89266085 0.85418815] \n",
      "\n",
      "2019-03-02T16:49:41.257667: step 3040, loss 0.542442, acc [0.92900391 0.89130859 0.84609375 0.79833984]\n",
      "2019-03-02T16:49:43.539760: step 3080, loss 0.552626, acc [0.93154297 0.88837891 0.84853516 0.79677734]\n",
      "2019-03-02T16:49:45.887604: step 3120, loss 0.542156, acc [0.92939453 0.88935547 0.84677734 0.79785156]\n",
      "2019-03-02T16:49:48.334633: step 3160, loss 0.309331, acc [0.95126953 0.92919922 0.90859375 0.85878906]\n",
      "2019-03-02T16:49:50.543774: step 3200, loss 0.300676, acc [0.95136719 0.928125   0.90898437 0.86132812]\n",
      "\n",
      "Evaluation:\n",
      "2019-03-02T16:49:59.549146: step 3200, loss 0.357189, acc [0.95311796 0.92641832 0.89914805 0.86117591] \n",
      "\n",
      "2019-03-02T16:50:01.919530: step 3240, loss 0.300753, acc [0.95195312 0.93085938 0.91074219 0.85859375]\n",
      "2019-03-02T16:50:04.122763: step 3280, loss 0.292773, acc [0.95458984 0.93359375 0.91152344 0.865625  ]\n",
      "2019-03-02T16:50:06.357248: step 3320, loss 0.310789, acc [0.94755859 0.92744141 0.90712891 0.85439453]\n",
      "2019-03-02T16:50:08.758380: step 3360, loss 0.343699, acc [0.94853516 0.91806641 0.9        0.84833984]\n",
      "2019-03-02T16:50:11.057836: step 3400, loss 0.312842, acc [0.95244141 0.92617187 0.90957031 0.85791016]\n",
      "\n",
      "Evaluation:\n",
      "2019-03-02T16:50:20.195142: step 3400, loss 0.353424, acc [0.95534043 0.92898117 0.9009901  0.86505021] \n",
      "\n",
      "2019-03-02T16:50:22.592808: step 3440, loss 0.303716, acc [0.95185547 0.92900391 0.91289062 0.86494141]\n",
      "2019-03-02T16:50:24.934421: step 3480, loss 0.333012, acc [0.94824219 0.92158203 0.90068359 0.84726563]\n",
      "2019-03-02T16:50:27.304359: step 3520, loss 0.336807, acc [0.95097656 0.92441406 0.89980469 0.85      ]\n",
      "2019-03-02T16:50:29.663289: step 3560, loss 0.324796, acc [0.94804687 0.92402344 0.90175781 0.85214844]\n",
      "2019-03-02T16:50:31.913137: step 3600, loss 0.337317, acc [0.94726562 0.92158203 0.90126953 0.85019531]\n",
      "\n",
      "Evaluation:\n",
      "2019-03-02T16:50:40.458223: step 3600, loss 0.34884, acc [0.95548058 0.93013245 0.90132047 0.86512028] \n",
      "\n",
      "2019-03-02T16:50:42.824356: step 3640, loss 0.334371, acc [0.9484375  0.92236328 0.89960938 0.85263672]\n",
      "2019-03-02T16:50:45.052174: step 3680, loss 0.334447, acc [0.95009766 0.92412109 0.90029297 0.85039062]\n",
      "2019-03-02T16:50:47.342702: step 3720, loss 0.330806, acc [0.946875   0.92685547 0.90214844 0.85      ]\n",
      "2019-03-02T16:50:49.586112: step 3760, loss 0.353523, acc [0.94824219 0.925      0.896875   0.84833984]\n",
      "2019-03-02T16:50:51.969932: step 3800, loss 0.340008, acc [0.95068359 0.92431641 0.89677734 0.84873047]\n",
      "\n",
      "Evaluation:\n",
      "2019-03-02T16:51:00.890443: step 3800, loss 0.338709, acc [0.95559071 0.93055291 0.90669643 0.86905465] \n",
      "\n",
      "2019-03-02T16:51:03.277693: step 3840, loss 0.353597, acc [0.94697266 0.91992188 0.89394531 0.84091797]\n",
      "2019-03-02T16:51:05.537965: step 3880, loss 0.351323, acc [0.95048828 0.9234375  0.89521484 0.84892578]\n",
      "2019-03-02T16:51:07.744665: step 3920, loss 0.359349, acc [0.94736328 0.91962891 0.89658203 0.84316406]\n",
      "2019-03-02T16:51:09.952855: step 3960, loss 0.34436, acc [0.94726562 0.92207031 0.9        0.84970703]\n",
      "2019-03-02T16:51:12.155592: step 4000, loss 0.353417, acc [0.94960937 0.92167969 0.89980469 0.84873047]\n",
      "\n",
      "Evaluation:\n",
      "2019-03-02T16:51:21.050851: step 4000, loss 0.325713, acc [0.95747279 0.93213467 0.91043058 0.87352962] \n",
      "\n",
      "2019-03-02T16:51:23.406852: step 4040, loss 0.369571, acc [0.94423828 0.91855469 0.89443359 0.84072266]\n",
      "2019-03-02T16:51:25.765331: step 4080, loss 0.359875, acc [0.94267578 0.91777344 0.89550781 0.84277344]\n",
      "2019-03-02T16:51:27.962939: step 4120, loss 0.354886, acc [0.9515625  0.91728516 0.89267578 0.84394531]\n",
      "2019-03-02T16:51:30.168601: step 4160, loss 0.330677, acc [0.94707031 0.92373047 0.89921875 0.84736328]\n",
      "2019-03-02T16:51:32.473017: step 4200, loss 0.347237, acc [0.94609375 0.92294922 0.89785156 0.84716797]\n",
      "\n",
      "Evaluation:\n",
      "2019-03-02T16:51:41.229442: step 4200, loss 0.312024, acc [0.95864409 0.93498784 0.91526595 0.87843506] \n",
      "\n",
      "2019-03-02T16:51:43.637532: step 4240, loss 0.34654, acc [0.94736328 0.92050781 0.89892578 0.84951172]\n",
      "2019-03-02T16:51:46.001466: step 4280, loss 0.373884, acc [0.94462891 0.92060547 0.89423828 0.84199219]\n",
      "2019-03-02T16:51:48.327652: step 4320, loss 0.359843, acc [0.94609375 0.92324219 0.8953125  0.84589844]\n",
      "2019-03-02T16:51:50.642979: step 4360, loss 0.358177, acc [0.94794922 0.92167969 0.89472656 0.84736328]\n",
      "2019-03-02T16:51:52.984103: step 4400, loss 0.364137, acc [0.94628906 0.92197266 0.89570313 0.8453125 ]\n",
      "\n",
      "Evaluation:\n",
      "2019-03-02T16:52:01.894245: step 4400, loss 0.303777, acc [0.95914465 0.93713021 0.91627707 0.88015697] \n",
      "\n",
      "2019-03-02T16:52:04.252228: step 4440, loss 0.349858, acc [0.94628906 0.92050781 0.89433594 0.84394531]\n",
      "2019-03-02T16:52:06.529366: step 4480, loss 0.361735, acc [0.94335938 0.91875    0.89169922 0.84101563]\n",
      "2019-03-02T16:52:08.789631: step 4520, loss 0.369401, acc [0.94697266 0.91425781 0.89003906 0.84003906]\n",
      "2019-03-02T16:52:11.074208: step 4560, loss 0.364326, acc [0.94541016 0.92021484 0.89111328 0.84150391]\n",
      "2019-03-02T16:52:13.273966: step 4600, loss 0.373209, acc [0.94511719 0.91855469 0.88964844 0.84121094]\n",
      "\n",
      "Evaluation:\n",
      "2019-03-02T16:52:21.981492: step 4600, loss 0.287991, acc [0.96175755 0.93921253 0.92095226 0.88589334] \n",
      "\n",
      "2019-03-02T16:52:24.335470: step 4640, loss 0.363408, acc [0.9484375  0.92109375 0.8953125  0.84785156]\n",
      "2019-03-02T16:52:26.656749: step 4680, loss 0.362375, acc [0.94736328 0.91816406 0.89140625 0.84384766]\n",
      "2019-03-02T16:52:29.140719: step 4720, loss 0.188354, acc [0.96757812 0.95292969 0.94267578 0.90263672]\n",
      "2019-03-02T16:52:31.349943: step 4760, loss 0.208961, acc [0.95830078 0.94501953 0.93652344 0.88779297]\n",
      "2019-03-02T16:52:33.656797: step 4800, loss 0.202405, acc [0.96425781 0.94609375 0.93496094 0.88925781]\n",
      "\n",
      "Evaluation:\n",
      "2019-03-02T16:52:43.129898: step 4800, loss 0.283538, acc [0.96370972 0.94232598 0.92532711 0.89177988] \n",
      "\n",
      "2019-03-02T16:52:45.912844: step 4840, loss 0.193647, acc [0.959375   0.94902344 0.93994141 0.89023438]\n",
      "2019-03-02T16:52:48.213284: step 4880, loss 0.194757, acc [0.96220703 0.95185547 0.94052734 0.89707031]\n",
      "2019-03-02T16:52:50.410564: step 4920, loss 0.209522, acc [0.96425781 0.95078125 0.93398437 0.89277344]\n",
      "2019-03-02T16:52:52.712002: step 4960, loss 0.199622, acc [0.96669922 0.94707031 0.9359375  0.89013672]\n",
      "2019-03-02T16:52:54.953427: step 5000, loss 0.223807, acc [0.96015625 0.94316406 0.93085938 0.88417969]\n",
      "\n",
      "Evaluation:\n",
      "2019-03-02T16:53:03.363140: step 5000, loss 0.286899, acc [0.96366967 0.94404789 0.92563746 0.89248065] \n",
      "\n",
      "2019-03-02T16:53:05.725552: step 5040, loss 0.206693, acc [0.96230469 0.94589844 0.93613281 0.89150391]\n",
      "2019-03-02T16:53:07.929776: step 5080, loss 0.230164, acc [0.96259766 0.94335938 0.92851562 0.88271484]\n",
      "2019-03-02T16:53:10.252044: step 5120, loss 0.223999, acc [0.96230469 0.94072266 0.93105469 0.88408203]\n",
      "2019-03-02T16:53:12.474622: step 5160, loss 0.202403, acc [0.9609375  0.94736328 0.93583984 0.88779297]\n",
      "2019-03-02T16:53:14.674876: step 5200, loss 0.216879, acc [0.95761719 0.94707031 0.93183594 0.88398438]\n",
      "\n",
      "Evaluation:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-02T16:53:23.254189: step 5200, loss 0.288251, acc [0.96220805 0.94363744 0.92482656 0.89110913] \n",
      "\n",
      "2019-03-02T16:53:25.605224: step 5240, loss 0.222212, acc [0.95820313 0.94208984 0.92841797 0.87792969]\n",
      "2019-03-02T16:53:27.816939: step 5280, loss 0.228498, acc [0.96044922 0.93945312 0.93183594 0.88164062]\n",
      "2019-03-02T16:53:30.019127: step 5320, loss 0.222194, acc [0.96132812 0.94628906 0.92763672 0.88447266]\n",
      "2019-03-02T16:53:32.278406: step 5360, loss 0.234046, acc [0.96123047 0.94306641 0.92753906 0.88203125]\n",
      "2019-03-02T16:53:34.477177: step 5400, loss 0.223808, acc [0.95986328 0.94648438 0.93037109 0.88408203]\n",
      "\n",
      "Evaluation:\n",
      "2019-03-02T16:53:43.392280: step 5400, loss 0.283937, acc [0.96417023 0.94325702 0.92645837 0.89237053] \n",
      "\n",
      "2019-03-02T16:53:45.765141: step 5440, loss 0.225829, acc [0.96201172 0.94355469 0.93164062 0.88662109]\n",
      "2019-03-02T16:53:48.044144: step 5480, loss 0.236605, acc [0.95927734 0.94130859 0.92451172 0.87529297]\n",
      "2019-03-02T16:53:50.350657: step 5520, loss 0.259032, acc [0.95839844 0.93857422 0.92109375 0.87451172]\n",
      "2019-03-02T16:53:52.659546: step 5560, loss 0.258257, acc [0.95458984 0.93271484 0.92314453 0.87099609]\n",
      "2019-03-02T16:53:54.915344: step 5600, loss 0.250617, acc [0.96044922 0.93935547 0.92529297 0.87890625]\n",
      "\n",
      "Evaluation:\n",
      "2019-03-02T16:54:03.842356: step 5600, loss 0.271997, acc [0.96503118 0.94541942 0.9301825  0.89692559] \n",
      "\n",
      "2019-03-02T16:54:06.241998: step 5640, loss 0.254414, acc [0.95898438 0.93847656 0.92275391 0.87373047]\n",
      "2019-03-02T16:54:08.482431: step 5680, loss 0.223766, acc [0.96054688 0.94550781 0.92919922 0.88212891]\n",
      "2019-03-02T16:54:10.682191: step 5720, loss 0.235673, acc [0.95849609 0.93867188 0.92861328 0.87929687]\n",
      "2019-03-02T16:54:12.945438: step 5760, loss 0.239046, acc [0.96181641 0.94521484 0.92460937 0.88183594]\n",
      "2019-03-02T16:54:15.236954: step 5800, loss 0.242447, acc [0.95761719 0.94101563 0.92128906 0.87451172]\n",
      "\n",
      "Evaluation:\n",
      "2019-03-02T16:54:24.026567: step 5800, loss 0.269461, acc [0.96461072 0.94674088 0.93138384 0.89850734] \n",
      "\n",
      "2019-03-02T16:54:26.426752: step 5840, loss 0.250472, acc [0.95712891 0.93798828 0.92695313 0.87666016]\n",
      "2019-03-02T16:54:28.627466: step 5880, loss 0.246828, acc [0.95751953 0.93740234 0.92558594 0.87646484]\n",
      "2019-03-02T16:54:30.857977: step 5920, loss 0.243699, acc [0.96210938 0.93994141 0.92451172 0.87822266]\n",
      "2019-03-02T16:54:33.053311: step 5960, loss 0.241004, acc [0.96230469 0.94423828 0.92832031 0.8875    ]\n",
      "2019-03-02T16:54:35.267414: step 6000, loss 0.275956, acc [0.95302734 0.93886719 0.91669922 0.86660156]\n",
      "\n",
      "Evaluation:\n",
      "2019-03-02T16:54:43.826925: step 6000, loss 0.258429, acc [0.96704342 0.94768193 0.93336604 0.90155072] \n",
      "\n",
      "2019-03-02T16:54:46.189828: step 6040, loss 0.249364, acc [0.95517578 0.93779297 0.92314453 0.87109375]\n",
      "2019-03-02T16:54:48.394584: step 6080, loss 0.260524, acc [0.95595703 0.93496094 0.921875   0.87216797]\n",
      "2019-03-02T16:54:50.713350: step 6120, loss 0.264539, acc [0.95664063 0.93681641 0.92050781 0.87333984]\n",
      "2019-03-02T16:54:52.966674: step 6160, loss 0.266825, acc [0.95605469 0.94003906 0.91748047 0.87099609]\n",
      "2019-03-02T16:54:55.171891: step 6200, loss 0.270545, acc [0.95605469 0.9375     0.91894531 0.87060547]\n",
      "\n",
      "Evaluation:\n",
      "2019-03-02T16:55:03.979857: step 6200, loss 0.251134, acc [0.96739381 0.94896335 0.93475758 0.90276207] \n",
      "\n",
      "2019-03-02T16:55:06.347757: step 6240, loss 0.252649, acc [0.95859375 0.94042969 0.9203125  0.87333984]\n",
      "2019-03-02T16:55:08.872892: step 6280, loss 0.139836, acc [0.97070312 0.96171875 0.95634766 0.91572266]\n",
      "2019-03-02T16:55:11.195658: step 6320, loss 0.132487, acc [0.97080078 0.96298828 0.95927734 0.92128906]\n",
      "2019-03-02T16:55:13.481722: step 6360, loss 0.138054, acc [0.97050781 0.96015625 0.95634766 0.91855469]\n",
      "2019-03-02T16:55:15.899226: step 6400, loss 0.138324, acc [0.96972656 0.96357422 0.95546875 0.91669922]\n",
      "\n",
      "Evaluation:\n",
      "2019-03-02T16:55:24.793498: step 6400, loss 0.258754, acc [0.96789436 0.95218693 0.93727037 0.90759743] \n",
      "\n",
      "2019-03-02T16:55:27.150982: step 6440, loss 0.138192, acc [0.97041016 0.95947266 0.95634766 0.91201172]\n",
      "2019-03-02T16:55:29.355740: step 6480, loss 0.147055, acc [0.97294922 0.96220703 0.9546875  0.91777344]\n",
      "2019-03-02T16:55:31.577787: step 6520, loss 0.143622, acc [0.97099609 0.96201172 0.9546875  0.91650391]\n",
      "2019-03-02T16:55:33.786468: step 6560, loss 0.143361, acc [0.97060547 0.96035156 0.95429688 0.91337891]\n",
      "2019-03-02T16:55:36.076003: step 6600, loss 0.154851, acc [0.97011719 0.96005859 0.95019531 0.90908203]\n",
      "\n",
      "Evaluation:\n",
      "2019-03-02T16:55:44.886984: step 6600, loss 0.260052, acc [0.96897556 0.95212686 0.93778094 0.9088288 ] \n",
      "\n",
      "2019-03-02T16:55:47.250922: step 6640, loss 0.158788, acc [0.96875    0.95869141 0.94921875 0.91054687]\n",
      "2019-03-02T16:55:49.452623: step 6680, loss 0.166044, acc [0.96816406 0.95673828 0.94511719 0.90166016]\n",
      "2019-03-02T16:55:51.658872: step 6720, loss 0.150844, acc [0.97158203 0.95703125 0.95048828 0.91054687]\n",
      "2019-03-02T16:55:53.969205: step 6760, loss 0.161878, acc [0.96826172 0.95576172 0.94804687 0.90634766]\n",
      "2019-03-02T16:55:56.286013: step 6800, loss 0.151065, acc [0.96845703 0.95947266 0.94970703 0.91083984]\n",
      "\n",
      "Evaluation:\n",
      "2019-03-02T16:56:05.328589: step 6800, loss 0.259734, acc [0.96910571 0.95342831 0.93890218 0.91022034] \n",
      "\n",
      "2019-03-02T16:56:07.704927: step 6840, loss 0.168336, acc [0.96367187 0.95507812 0.94707031 0.90205078]\n",
      "2019-03-02T16:56:09.951310: step 6880, loss 0.174156, acc [0.96601563 0.953125   0.94443359 0.90087891]\n",
      "2019-03-02T16:56:12.194766: step 6920, loss 0.164118, acc [0.96572266 0.95644531 0.94697266 0.90458984]\n",
      "2019-03-02T16:56:14.426258: step 6960, loss 0.169178, acc [0.96787109 0.95537109 0.94482422 0.90253906]\n",
      "2019-03-02T16:56:16.781238: step 7000, loss 0.178396, acc [0.96728516 0.95400391 0.94589844 0.903125  ]\n",
      "\n",
      "Evaluation:\n",
      "2019-03-02T16:56:25.109558: step 7000, loss 0.259585, acc [0.96812462 0.95306791 0.93781097 0.90879877] \n",
      "\n",
      "2019-03-02T16:56:27.547635: step 7040, loss 0.178183, acc [0.96425781 0.95351562 0.94472656 0.90107422]\n",
      "2019-03-02T16:56:29.910342: step 7080, loss 0.168987, acc [0.96542969 0.95449219 0.94482422 0.89902344]\n",
      "2019-03-02T16:56:32.261422: step 7120, loss 0.168393, acc [0.96630859 0.95546875 0.94521484 0.90292969]\n",
      "2019-03-02T16:56:34.552402: step 7160, loss 0.177038, acc [0.96582031 0.95371094 0.94179687 0.89912109]\n",
      "2019-03-02T16:56:36.949577: step 7200, loss 0.187371, acc [0.96816406 0.94863281 0.9390625  0.89648438]\n",
      "\n",
      "Evaluation:\n",
      "2019-03-02T16:56:45.242690: step 7200, loss 0.259448, acc [0.96989659 0.95423921 0.93812131 0.91004014] \n",
      "\n",
      "2019-03-02T16:56:47.780723: step 7240, loss 0.179379, acc [0.9625     0.95400391 0.94482422 0.90009766]\n",
      "2019-03-02T16:56:50.025614: step 7280, loss 0.173868, acc [0.96513672 0.95449219 0.94599609 0.90332031]\n",
      "2019-03-02T16:56:52.286149: step 7320, loss 0.192753, acc [0.96484375 0.95029297 0.94042969 0.89726562]\n",
      "2019-03-02T16:56:54.596749: step 7360, loss 0.183445, acc [0.96650391 0.95283203 0.94394531 0.90029297]\n",
      "2019-03-02T16:56:56.890253: step 7400, loss 0.174887, acc [0.96806641 0.95419922 0.94238281 0.89951172]\n",
      "\n",
      "Evaluation:\n",
      "2019-03-02T16:57:06.140284: step 7400, loss 0.255176, acc [0.97002673 0.95503008 0.93999339 0.9127932 ] \n",
      "\n",
      "2019-03-02T16:57:08.738202: step 7440, loss 0.190459, acc [0.96435547 0.95126953 0.94111328 0.89667969]\n",
      "2019-03-02T16:57:11.065933: step 7480, loss 0.193904, acc [0.96318359 0.94951172 0.93574219 0.89492187]\n",
      "2019-03-02T16:57:13.400600: step 7520, loss 0.198596, acc [0.96152344 0.94755859 0.93847656 0.88818359]\n",
      "2019-03-02T16:57:15.647485: step 7560, loss 0.184752, acc [0.965625   0.94990234 0.94267578 0.89667969]\n",
      "2019-03-02T16:57:17.896345: step 7600, loss 0.199062, acc [0.96445313 0.94775391 0.93681641 0.89150391]\n",
      "\n",
      "Evaluation:\n",
      "2019-03-02T16:57:26.952848: step 7600, loss 0.248605, acc [0.97193885 0.95512018 0.94046391 0.91431489] \n",
      "\n",
      "2019-03-02T16:57:29.410979: step 7640, loss 0.19728, acc [0.96826172 0.95449219 0.93369141 0.89482422]\n",
      "2019-03-02T16:57:31.720904: step 7680, loss 0.189722, acc [0.96591797 0.94638672 0.94189453 0.89521484]\n",
      "2019-03-02T16:57:34.185580: step 7720, loss 0.185847, acc [0.96474609 0.95009766 0.93974609 0.89648438]\n",
      "2019-03-02T16:57:36.592563: step 7760, loss 0.208654, acc [0.96328125 0.94472656 0.93398437 0.88730469]\n",
      "2019-03-02T16:57:39.030947: step 7800, loss 0.194861, acc [0.96337891 0.94873047 0.93828125 0.89179688]\n",
      "\n",
      "Evaluation:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-02T16:57:47.274914: step 7800, loss 0.239984, acc [0.97168857 0.95759293 0.94345724 0.91736828] \n",
      "\n",
      "2019-03-02T16:57:49.925037: step 7840, loss 0.102667, acc [0.97568359 0.96894531 0.96835938 0.93251953]\n",
      "2019-03-02T16:57:52.211105: step 7880, loss 0.0986808, acc [0.97392578 0.96855469 0.96835938 0.93017578]\n",
      "2019-03-02T16:57:54.553708: step 7920, loss 0.10207, acc [0.97636719 0.96699219 0.96669922 0.92998047]\n",
      "2019-03-02T16:57:56.795132: step 7960, loss 0.110479, acc [0.97617188 0.96943359 0.96240234 0.92753906]\n",
      "2019-03-02T16:57:59.114964: step 8000, loss 0.113002, acc [0.9734375  0.96845703 0.96396484 0.92910156]\n",
      "\n",
      "Evaluation:\n",
      "2019-03-02T16:58:09.233865: step 8000, loss 0.257903, acc [0.97199892 0.95780316 0.94496892 0.91922033] \n",
      "\n",
      "2019-03-02T16:58:11.758953: step 8040, loss 0.107196, acc [0.97451172 0.96923828 0.96669922 0.93046875]\n",
      "2019-03-02T16:58:13.981526: step 8080, loss 0.104184, acc [0.97392578 0.96767578 0.96699219 0.92900391]\n",
      "2019-03-02T16:58:16.228018: step 8120, loss 0.106614, acc [0.97763672 0.96982422 0.96552734 0.93017578]\n",
      "2019-03-02T16:58:18.471317: step 8160, loss 0.103773, acc [0.97822266 0.96806641 0.96601563 0.93183594]\n",
      "2019-03-02T16:58:20.742499: step 8200, loss 0.118008, acc [0.97138672 0.96494141 0.95966797 0.92216797]\n",
      "\n",
      "Evaluation:\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-86-87d756c70f63>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0my_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtrain_y_label1_one_hot\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrain_y_label2_one_hot\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrain_y_label3_one_hot\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0my_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtest_y_label1_one_hot\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtest_y_label2_one_hot\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtest_y_label3_one_hot\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-85-e00393d77016>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(x_train, y_train, x_dev, y_dev)\u001b[0m\n\u001b[0;32m    121\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mcurrent_step\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m200\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    122\u001b[0m                     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\\nEvaluation:\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 123\u001b[1;33m                     \u001b[0mdev_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_dev\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_dev\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    124\u001b[0m                     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-85-e00393d77016>\u001b[0m in \u001b[0;36mdev_step\u001b[1;34m(x_batch, y_batch)\u001b[0m\n\u001b[0;32m     96\u001b[0m                     step,  temp_loss, temp_accuracy1, temp_accuracy2, temp_accuracy3 , acc= sess.run(\n\u001b[0;32m     97\u001b[0m                         \u001b[1;33m[\u001b[0m\u001b[0mglobal_step\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maccuracy1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maccuracy2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maccuracy3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0macc\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 98\u001b[1;33m                         feed_dict)\n\u001b[0m\u001b[0;32m     99\u001b[0m                     \u001b[0maccuracy\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mtemp_accuracy1\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_batch_dev\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    100\u001b[0m                     \u001b[0maccuracy\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mtemp_accuracy2\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_batch_dev\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\richado\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    885\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    886\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 887\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    888\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    889\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\richado\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1108\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1109\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1110\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1111\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1112\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\richado\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1284\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1285\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1286\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1287\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1288\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\richado\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1290\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1291\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1292\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1293\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1294\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\richado\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1275\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1276\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m-> 1277\u001b[1;33m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1278\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1279\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\richado\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1365\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[0;32m   1366\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1367\u001b[1;33m         run_metadata)\n\u001b[0m\u001b[0;32m   1368\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1369\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "y_train = [train_y_label1_one_hot,train_y_label2_one_hot,train_y_label3_one_hot]\n",
    "y_test = [test_y_label1_one_hot,test_y_label2_one_hot,test_y_label3_one_hot]\n",
    "train(x_train, y_train,  x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_y_label3_one_hot.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y_label3_one_hot.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
