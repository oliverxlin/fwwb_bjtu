{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.utils import shuffle\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "特征如下：['ITEM_NAME' 'TYPE'] \n",
      "\n",
      "数据数目:  499447\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('data/train/trainx.txt', sep='\\t')\n",
    "features = data.columns.values\n",
    "print(\"特征如下：{} \\n\".format(features))\n",
    "print(\"数据数目: \", len(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据清洗\n",
    "* 替换一些字符 类似于 （ ） ‘ ’ _"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ITEM_NAME</th>\n",
       "      <th>TYPE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>腾讯QQ黄钻三个月QQ黄钻3个月季卡官方自动充值可查时间可续费</td>\n",
       "      <td>本地生活--游戏充值--QQ充值</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>腾讯q币充值30元qq币30qb30q币自动充值</td>\n",
       "      <td>本地生活--游戏充值--QQ充值</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>腾讯QQ红钻包月卡 QQ红钻一个月QQ红钻1个月</td>\n",
       "      <td>本地生活--游戏充值--QQ充值</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>腾讯QQ蓝钻贵族31个月直充</td>\n",
       "      <td>本地生活--游戏充值--QQ充值</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>腾讯QQ币148元148QQ币148个直充148Q币148个Q币148个QQB★自动充值</td>\n",
       "      <td>本地生活--游戏充值--QQ充值</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      ITEM_NAME              TYPE\n",
       "0               腾讯QQ黄钻三个月QQ黄钻3个月季卡官方自动充值可查时间可续费  本地生活--游戏充值--QQ充值\n",
       "1                      腾讯q币充值30元qq币30qb30q币自动充值  本地生活--游戏充值--QQ充值\n",
       "2                      腾讯QQ红钻包月卡 QQ红钻一个月QQ红钻1个月  本地生活--游戏充值--QQ充值\n",
       "3                                腾讯QQ蓝钻贵族31个月直充  本地生活--游戏充值--QQ充值\n",
       "4  腾讯QQ币148元148QQ币148个直充148Q币148个Q币148个QQB★自动充值  本地生活--游戏充值--QQ充值"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "from zhon.hanzi import punctuation\n",
    "from string import punctuation as english_punc\n",
    "def  rm_punc(strs):\n",
    "    return re.sub(r\"[{}]+\".format(punctuation + english_punc),\" \",strs)\n",
    "data[features[0]]  = data[features[0]].apply(rm_punc)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练数据数目:   399558\n",
      "测试数据数目:   99889\n",
      "(399558,) (99889,)\n"
     ]
    }
   ],
   "source": [
    "# 因为有的label里本身就存在空格，所以先要把空格去掉，再去掉- - ， 再以空格划分\n",
    "data = shuffle(data)\n",
    "data_y = data[features[1]]\n",
    "\n",
    "# 取训练数据和测试数据\n",
    "train = data.sample(frac= 0.8).reset_index()\n",
    "test = data.sample(frac= 0.2).reset_index()\n",
    "\n",
    "\n",
    "print(\"训练数据数目:  \", len(train))\n",
    "print(\"测试数据数目:  \", len(test))\n",
    "\n",
    "train_x  = train[features[0]]\n",
    "train_y  = train[features[1]]\n",
    "\n",
    "test_x  = test[features[0]]\n",
    "test_y  = test[features[1]]\n",
    "\n",
    "print(train_x.shape, test_x.shape)\n",
    "\n",
    "# 分出数据集 训练集 测试集 数据集用来训练词向量\n",
    "data_y = data_y.apply(lambda x: x.replace(\" \", \"\").replace(\"--\", \" \").replace(\"/\", \"\")).tolist()\n",
    "data_y= [each.split() for each in data_y]\n",
    "\n",
    "train_y = train_y.apply(lambda x: x.replace(\" \", \"\").replace(\"--\", \" \").replace(\"/\", \"\")).tolist()\n",
    "train_y = [each.split() for each in train_y]\n",
    "\n",
    "test_y = test_y.apply(lambda x: x.replace(\" \", \"\").replace(\"--\", \" \").replace(\"/\", \"\")).tolist()\n",
    "test_y = [each.split() for each in test_y]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 分词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba\n",
    "from random import shuffle\n",
    "def participle(data, doshuffle = False):\n",
    "    words = []\n",
    "    for i in range(len(data)):\n",
    "        result=[]\n",
    "        seg_list = jieba.cut(data.iloc[i])\n",
    "        for w in seg_list :#读取每一行分词\n",
    "            if w != \" \":\n",
    "                result.append(w)\n",
    "        # 打乱每条数据的分词\n",
    "        if doshuffle:\n",
    "            shuffle(result)\n",
    "        words.append(result)#将该行分词写入列表形式的总分词列表\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\Richado\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.734 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "# 每条数据的分词结果是个长度不等的词列表\n",
    "word_data = participle(data[features[0]], doshuffle = False)\n",
    "word_data_train = participle(train_x,doshuffle = False)\n",
    "word_data_test = participle(test_x, doshuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'腾讯QQ黄钻三个月QQ黄钻3个月季卡官方自动充值可查时间可续费'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[features[0]][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['货到付款',\n",
       " '一步',\n",
       " '赢',\n",
       " '2014',\n",
       " '秋季',\n",
       " '松糕',\n",
       " '帆布鞋',\n",
       " '女',\n",
       " '韩版潮',\n",
       " '厚底',\n",
       " '松糕鞋',\n",
       " '学生',\n",
       " '高帮',\n",
       " '休闲',\n",
       " '板鞋',\n",
       " '黑',\n",
       " '白色',\n",
       " '35',\n",
       " '正规',\n",
       " '码']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from random import shuffle\n",
    "word_data[8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 标签分离(一个标签分成三个标签)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y = pd.DataFrame(train_y, columns=['label1', 'label2', 'label3'])\n",
    "test_y = pd.DataFrame(test_y, columns=['label1', 'label2', 'label3'])\n",
    "data_y = pd.DataFrame(data_y, columns=['label1', 'label2', 'label3'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_y[\"label1\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "191"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_y[\"label2\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1192"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_y[\"label3\"].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 标签onn_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import to_categorical\n",
    "# 先生成唯一数组\n",
    "y_label1 = data_y['label1'].unique().tolist()\n",
    "y_label2 = data_y['label2'].unique().tolist()\n",
    "y_label3 = data_y['label3'].unique().tolist()\n",
    "\n",
    "# 获取在唯一数组中的索引(训练集和测试集各有3个标签需要处理)\n",
    "train_y_label1_map = train_y['label1'].apply(lambda x: y_label1.index(x))\n",
    "train_y_label2_map = train_y['label2'].apply(lambda x: y_label2.index(x))\n",
    "train_y_label3_map = train_y['label3'].apply(lambda x: y_label3.index(x))\n",
    "test_y_label1_map = test_y['label1'].apply(lambda x: y_label1.index(x))\n",
    "test_y_label2_map = test_y['label2'].apply(lambda x: y_label2.index(x))\n",
    "test_y_label3_map = test_y['label3'].apply(lambda x: y_label3.index(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生成对应one-hot(用做训练模型的标签)\n",
    "train_y_label1_one_hot = to_categorical(train_y_label1_map, 22)\n",
    "train_y_label2_one_hot = to_categorical(train_y_label2_map, 191)\n",
    "train_y_label3_one_hot = to_categorical(train_y_label3_map, 1192)\n",
    "test_y_label1_one_hot = to_categorical(test_y_label1_map, 22)\n",
    "test_y_label2_one_hot = to_categorical(test_y_label2_map, 191)\n",
    "test_y_label3_one_hot = to_categorical(test_y_label3_map, 1192)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 词向量训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "from gensim.models.word2vec import Word2Vec \n",
    "if os.path.exists(\"w2v_model\"):\n",
    "    model = Word2Vec.load(\"w2v_model\")\n",
    "    print(1)\n",
    "else:\n",
    "    sentences = word_data\n",
    "    model= Word2Vec(size=50, window=10, min_count = 1)\n",
    "    model.build_vocab(sentences)\n",
    "    model.train(sentences,total_examples = model.corpus_count,epochs = model.iter)\n",
    "    model.save(\"w2v_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 填充词向量(构造输入输出)（让每个句子拥有同样的维度）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\richado\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\ipykernel_launcher.py:2: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \n",
      "c:\\users\\richado\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\ipykernel_launcher.py:3: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing import sequence\n",
    "x_train = [model[word] for word in word_data_train]\n",
    "x_test = [model[word] for word in word_data_test]\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=18)\n",
    "x_test  = sequence.pad_sequences(x_test, maxlen=18) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 默认参数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## TextCnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class TextCNN(object):\n",
    "    \"\"\"\n",
    "    A CNN for text classification.\n",
    "    Uses an embedding layer, followed by a convolutional, max-pooling and softmax layer.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "      self, maxlen, sequence_length, num_classes, filter_sizes, embedding_size, num_filters, l2_reg_lambda=0.0):\n",
    "\n",
    "        # Placeholders for input, output and dropout\n",
    "        self.input_x = tf.placeholder(tf.float32, [None, sequence_length, embedding_size], name=\"input_x\")\n",
    "        self.input_y1 = tf.placeholder(tf.float32, [None, num_classes[0]], name=\"input_y1\")\n",
    "        self.input_y2 = tf.placeholder(tf.float32, [None, num_classes[1]], name=\"input_y2\")\n",
    "        self.input_y3 = tf.placeholder(tf.float32, [None, num_classes[2]], name=\"input_y3\")\n",
    "        \n",
    "        # Keeping track of l2 regularization loss (optional)\n",
    "        l2_loss = tf.constant(0.0)\n",
    "        keep_prob = 0.9\n",
    "        pooled_outputs = []\n",
    "        self.input_x_expand = tf.expand_dims(self.input_x, -1)\n",
    "        print(self.input_x_expand.shape)\n",
    "        for i, filter_size in enumerate(filter_sizes):\n",
    "            with tf.name_scope(\"conv-maxpool-%s\" % filter_size):\n",
    "                # Convolution Layer\n",
    "                filter_shape = [filter_size, embedding_size, 1, num_filters]\n",
    "                W = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.1), name=\"W\")\n",
    "                b = tf.Variable(tf.constant(0.1, shape=[num_filters]), name=\"b\")\n",
    "                # 卷积\n",
    "                conv = tf.nn.conv2d(\n",
    "                    self.input_x_expand,\n",
    "                    W,\n",
    "                    strides=[1, 1, 1, 1],\n",
    "                    padding=\"VALID\",\n",
    "                    name=\"conv\")\n",
    "        \n",
    "        \n",
    "                h = tf.nn.relu(tf.nn.bias_add(conv, b), name=\"relu\")\n",
    "            \n",
    "                # 最大池化\n",
    "                pooled = tf.nn.max_pool(\n",
    "                    h,\n",
    "                    ksize=[1, sequence_length - filter_size + 1, 1, 1],\n",
    "                    strides=[1, 1, 1, 1],\n",
    "                    padding='VALID',\n",
    "                    name=\"pool\")\n",
    "                pooled = tf.nn.dropout(pooled, keep_prob)\n",
    "                pooled_outputs.append(pooled)\n",
    "                \n",
    "\n",
    "        # Combine all the pooled features\n",
    "        num_filters_total = num_filters * len(filter_sizes)\n",
    "        self.h_pool = tf.concat(pooled_outputs, 3)\n",
    "        print(self.h_pool.shape)\n",
    "        \n",
    "        self.h_pool_flat = tf.reshape(self.h_pool, [-1, num_filters_total])\n",
    "        self.h_drop = self.h_pool_flat\n",
    "        \n",
    "        # Final (unnormalized) scores and predictions\n",
    "        \n",
    "        with tf.name_scope(\"output1\"):\n",
    "            W1 = tf.get_variable(\n",
    "                \"W11\",\n",
    "                shape=[num_filters_total, 64],\n",
    "                initializer=tf.contrib.layers.xavier_initializer())\n",
    "            b1 = tf.Variable(tf.constant(0.1, shape=[64]), name=\"b11\")\n",
    "            self.output = tf.nn.xw_plus_b(self.h_drop, W1, b1, name=\"output1\")\n",
    "            self.output = tf.layers.batch_normalization(self.output, training=is_training)\n",
    "            self.output = tf.nn.relu(self.output)\n",
    "            self.output = tf.nn.dropout(self.output, keep_prob)\n",
    "            W2 = tf.get_variable(\n",
    "                \"W12\",\n",
    "                shape=[64, num_classes[0]],\n",
    "                initializer=tf.contrib.layers.xavier_initializer())\n",
    "            b2 = tf.Variable(tf.constant(0.1, shape=[num_classes[0]]), name=\"b12\")\n",
    "            l2_loss += tf.nn.l2_loss(W1)\n",
    "            l2_loss += tf.nn.l2_loss(b1)\n",
    "            self.scores1 = tf.nn.xw_plus_b(self.output, W2, b2, name=\"scores\")\n",
    "            self.predictions1 = tf.argmax(self.scores1, 1, name=\"predictions\")\n",
    "            \n",
    "        with tf.name_scope(\"output2\"):\n",
    "            W1 = tf.get_variable(\n",
    "                \"W21\",\n",
    "                shape=[num_filters_total, 128],\n",
    "                initializer=tf.contrib.layers.xavier_initializer())\n",
    "            b1 = tf.Variable(tf.constant(0.1, shape=[128]), name=\"b21\")\n",
    "            self.output = tf.nn.xw_plus_b(self.h_drop, W1, b1, name=\"output2\")\n",
    "            self.output = tf.layers.batch_normalization(self.output, training=is_training)\n",
    "            self.output = tf.nn.relu(self.output)\n",
    "            self.output = tf.nn.dropout(self.output, keep_prob)\n",
    "            W2 = tf.get_variable(\n",
    "                \"W22\",\n",
    "                shape=[128, num_classes[1]],\n",
    "                initializer=tf.contrib.layers.xavier_initializer())\n",
    "            b2 = tf.Variable(tf.constant(0.1, shape=[num_classes[1]]), name=\"b22\")\n",
    "            l2_loss += tf.nn.l2_loss(W1)\n",
    "            l2_loss += tf.nn.l2_loss(b1)\n",
    "            self.scores2 = tf.nn.xw_plus_b(self.output, W2, b2, name=\"scores\")\n",
    "            self.predictions2 = tf.argmax(self.scores2, 1, name=\"predictions\")\n",
    "            \n",
    "        with tf.name_scope(\"output3\"):\n",
    "            W1 = tf.get_variable(\n",
    "                \"W31\",\n",
    "                shape=[num_filters_total, 256],\n",
    "                initializer=tf.contrib.layers.xavier_initializer())\n",
    "            b1 = tf.Variable(tf.constant(0.1, shape=[256]), name=\"b31\")\n",
    "            self.output = tf.nn.xw_plus_b(self.h_drop, W1, b1, name=\"output3\")\n",
    "            self.output = tf.layers.batch_normalization(self.output, training=is_training)\n",
    "            self.output = tf.nn.relu(self.output)\n",
    "            self.output = tf.nn.dropout(self.output, keep_prob)\n",
    "            W2 = tf.get_variable(\n",
    "                \"W32\",\n",
    "                shape=[256, num_classes[2]],\n",
    "                initializer=tf.contrib.layers.xavier_initializer())\n",
    "            b2 = tf.Variable(tf.constant(0.1, shape=[num_classes[2]]), name=\"b32\")\n",
    "            l2_loss += tf.nn.l2_loss(W1)\n",
    "            l2_loss += tf.nn.l2_loss(b1)\n",
    "            self.scores3 = tf.nn.xw_plus_b(self.output, W2, b2, name=\"scores\")\n",
    "            self.predictions3 = tf.argmax(self.scores3, 1, name=\"predictions\")\n",
    "        # Calculate mean cross-entropy loss\n",
    "        \n",
    "        with tf.name_scope(\"loss\"):\n",
    "            losses1 = tf.nn.softmax_cross_entropy_with_logits(logits=self.scores1, labels=self.input_y1)\n",
    "            losses2 = tf.nn.softmax_cross_entropy_with_logits(logits=self.scores2, labels=self.input_y2)\n",
    "            losses3 = tf.nn.softmax_cross_entropy_with_logits(logits=self.scores3, labels=self.input_y3)\n",
    "            self.loss = 0.1 * tf.reduce_mean(losses1) + 0.2 * tf.reduce_mean(losses2) + 0.7*tf.reduce_mean(losses3) + l2_reg_lambda * l2_loss\n",
    "            tf.summary.scalar('loss1',tf.reduce_mean(losses1))\n",
    "            tf.summary.scalar('loss2',tf.reduce_mean(losses2))\n",
    "            tf.summary.scalar('loss3',tf.reduce_mean(losses3))\n",
    "            tf.summary.scalar('loss',self.loss)\n",
    "        # Accuracy\n",
    "        with tf.name_scope(\"accuracy\"):\n",
    "            correct_predictions1 = tf.equal(self.predictions1, tf.argmax(self.input_y1, 1))\n",
    "            correct_predictions2 = tf.equal(self.predictions2, tf.argmax(self.input_y2, 1))\n",
    "            correct_predictions3 = tf.equal(self.predictions3, tf.argmax(self.input_y3, 1))\n",
    "#           单独的准确率\n",
    "            self.accuracy1 = tf.reduce_mean(tf.cast(correct_predictions1, \"float\"), name=\"accuracy1\")\n",
    "            self.accuracy2 = tf.reduce_mean(tf.cast(correct_predictions2, \"float\"), name=\"accuracy2\")\n",
    "            self.accuracy3 = tf.reduce_mean(tf.cast(correct_predictions3, \"float\"), name=\"accuracy3\")\n",
    "#           一起的准确率\n",
    "            self.acc = tf.reduce_mean(tf.cast(correct_predictions1, \"float\")*tf.cast(correct_predictions2, \"float\")*tf.cast(correct_predictions3, \"float\"))\n",
    "            tf.summary.scalar('acc1',self.accuracy1)\n",
    "            tf.summary.scalar('acc2',self.accuracy2)\n",
    "            tf.summary.scalar('acc3',self.accuracy3)\n",
    "            tf.summary.scalar('acc',self.acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def batch_iter(data, batch_size, num_epochs, shuffle=True):\n",
    "    \"\"\"\n",
    "    Generates a batch iterator for a dataset.\n",
    "    \"\"\"\n",
    "    data = np.array(data)\n",
    "    data_size = len(data)\n",
    "    num_batches_per_epoch = int((len(data)-1)/batch_size) + 1\n",
    "    for epoch in range(num_epochs):\n",
    "        # Shuffle the data at each epoch\n",
    "        if shuffle:\n",
    "            shuffle_indices = np.random.permutation(np.arange(data_size))\n",
    "            shuffled_data = data[shuffle_indices]\n",
    "        else:\n",
    "            shuffled_data = data\n",
    "  \n",
    "        for batch_num in range(num_batches_per_epoch):\n",
    "            start_index = batch_num * batch_size\n",
    "            end_index = np.min(((batch_num + 1) * batch_size, data_size))\n",
    "            yield shuffled_data[start_index:end_index]\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "from tensorflow.contrib import learn\n",
    "\n",
    "# 特征数这里是第三级标签的特征数\n",
    "max_features = 1192\n",
    "\n",
    "# 句子填充的长度\n",
    "maxlen = 16\n",
    "\n",
    "batch_size = 256 \n",
    "\n",
    "epochs = 100\n",
    "\n",
    "is_training = True\n",
    "# 词向量长度\n",
    "embedding_dims = 50\n",
    "\n",
    "filters = 64\n",
    "kernel_size = 5\n",
    "hidden_dims = 128\n",
    "epochs = 100\n",
    "\n",
    "# Parameters\n",
    "# ==================================================\n",
    "\n",
    "# Data loading params\n",
    "# tf.flags.DEFINE_float(\"dev_sample_percentage\", .1, \"Percentage of the training data to use for validation\")\n",
    "# tf.flags.DEFINE_string(\"positive_data_file\", \"./data/rt-polaritydata/rt-polarity.pos\", \"Data source for the positive data.\")\n",
    "# tf.flags.DEFINE_string(\"negative_data_file\", \"./data/rt-polaritydata/rt-polarity.neg\", \"Data source for the negative data.\")\n",
    "\n",
    "# Model Hyperparameters\n",
    "# tf.flags.DEFINE_integer(\"embedding_dim\", 128, \"Dimensionality of character embedding (default: 128)\")\n",
    "# tf.flags.DEFINE_string(\"filter_sizes\", \"3,4,5\", \"Comma-separated filter sizes (default: '3,4,5')\")\n",
    "# tf.flags.DEFINE_integer(\"num_filters\", 128, \"Number of filters per filter size (default: 128)\")\n",
    "# tf.flags.DEFINE_float(\"dropout_keep_prob\", 0.5, \"Dropout keep probability (default: 0.5)\")\n",
    "# tf.flags.DEFINE_float(\"l2_reg_lambda\", 0.0, \"L2 regularization lambda (default: 0.0)\")\n",
    "\n",
    "# Training parameters\n",
    "# tf.flags.DEFINE_integer(\"batch_size\", 64, \"Batch Size (default: 64)\")\n",
    "# tf.flags.DEFINE_integer(\"num_epochs\", 200, \"Number of training epochs (default: 200)\")\n",
    "# tf.flags.DEFINE_integer(\"evaluate_every\", 100, \"Evaluate model on dev set after this many steps (default: 100)\")\n",
    "# tf.flags.DEFINE_integer(\"checkpoint_every\", 100, \"Save model after this many steps (default: 100)\")\n",
    "# tf.flags.DEFINE_integer(\"num_checkpoints\", 5, \"Number of checkpoints to store (default: 5)\")\n",
    "# Misc Parameters\n",
    "# tf.flags.DEFINE_boolean(\"allow_soft_placement\", True, \"Allow device soft device placement\")\n",
    "# tf.flags.DEFINE_boolean(\"log_device_placement\", False, \"Log placement of ops on devices\")\n",
    "\n",
    "FLAGS = tf.flags.FLAGS\n",
    "\n",
    "def train(x_train, y_train, x_dev, y_dev):\n",
    "    # Training\n",
    "    # ==================================================\n",
    "\n",
    "    with tf.Graph().as_default():\n",
    "        session_conf = tf.ConfigProto(\n",
    "#           allow_soft_placement=FLAGS.allow_soft_placement,\n",
    "#           log_device_placement=FLAGS.log_device_placement\n",
    "        )\n",
    "        sess = tf.Session(config=session_conf)\n",
    "        with sess.as_default():\n",
    "            cnn = TextCNN(maxlen = 10, sequence_length = 18, num_classes = [22, 191, 1192], filter_sizes = [1,2, 3, 4, 5], embedding_size = 50, num_filters = 64)\n",
    "\n",
    "            # Define Training procedure\n",
    "\n",
    "            global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "#           学习率衰减\n",
    "            learning_rate = tf.train.exponential_decay(\n",
    "                0.01,\n",
    "                global_step,\n",
    "                x_train.shape[0] / batch_size,\n",
    "                0.99,\n",
    "                staircase=True,)\n",
    "            optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "            grads_and_vars = optimizer.compute_gradients(cnn.loss)\n",
    "            train_op = optimizer.apply_gradients(grads_and_vars, global_step=global_step)\n",
    "\n",
    "            # Initialize all variables\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            \n",
    "            merged = tf.summary.merge_all()\n",
    "            \n",
    "            writer = tf.summary.FileWriter(\"fwwb\",sess.graph)\n",
    "            acc_sum = np.array([0.0,0.0,0.0,0.0])\n",
    "            loss_sum = 0\n",
    "            \n",
    "            def train_step(x_batch, y_batch1, y_batch2, y_batch3, loss_sum, acc_sum, interal = 10):\n",
    "  \n",
    "                \"\"\"\n",
    "                A single training step\n",
    "                \"\"\"\n",
    "                feed_dict = {\n",
    "                  cnn.input_x: x_batch,\n",
    "                  cnn.input_y1: y_batch1,\n",
    "                  cnn.input_y2: y_batch2,\n",
    "                  cnn.input_y3: y_batch3\n",
    "                }\n",
    "#         三个标签单独的准确率， 一起的准确率共四个准确率\n",
    "                rs, _, step,  loss, accuracy1, accuracy2, accuracy3, acc = sess.run(\n",
    "                    [merged, train_op, global_step, cnn.loss, cnn.accuracy1, cnn.accuracy2, cnn.accuracy3, cnn.acc],\n",
    "                    feed_dict)\n",
    "                time_str = datetime.datetime.now().isoformat()\n",
    "                acc_sum += [accuracy1, accuracy2, accuracy3, acc]\n",
    "                loss_sum += loss\n",
    "                if step % interal == 0:\n",
    "                    print(\"{}: step {}, loss {:g}, acc {}\".format(time_str, step, loss_sum/interal, acc_sum/interal))\n",
    "                    loss_sum = 0\n",
    "                    acc_sum = np.array([0.0,0.0,0.0,0.0])\n",
    "                    writer.add_summary(rs, step)\n",
    "                return loss_sum, acc_sum\n",
    "            # 评估步骤\n",
    "            def dev_step(x_batch, y_batch):\n",
    "                \"\"\"\n",
    "                Evaluates model on a dev set\n",
    "                \"\"\"\n",
    "\n",
    "                dev_batches = batch_iter(\n",
    "                    list(zip(x_batch, y_batch[0], y_batch[1], y_batch[2])), 640, 1)\n",
    "                step = 0\n",
    "                loss = 0\n",
    "                accuracy = np.array([0.0, 0.0, 0.0,0.0])\n",
    "                for batch in dev_batches:\n",
    "                    x_batch_dev, y_batch1_dev, y_batch2_dev, y_batch3_dev = zip(*batch)\n",
    "                    feed_dict = {\n",
    "                      cnn.input_x: x_batch_dev,\n",
    "                      cnn.input_y1: y_batch1_dev,\n",
    "                      cnn.input_y2: y_batch2_dev,\n",
    "                      cnn.input_y3: y_batch3_dev\n",
    "                    }\n",
    "                    step,  temp_loss, temp_accuracy1, temp_accuracy2, temp_accuracy3 , acc= sess.run(\n",
    "                        [global_step,cnn.loss, cnn.accuracy1, cnn.accuracy2, cnn.accuracy3, cnn.acc],\n",
    "                        feed_dict)\n",
    "                    accuracy[0] += temp_accuracy1 * len(x_batch_dev)\n",
    "                    accuracy[1] += temp_accuracy2 * len(x_batch_dev)\n",
    "                    accuracy[2] += temp_accuracy3 * len(x_batch_dev)\n",
    "                    accuracy[3] += acc * len(x_batch_dev)\n",
    "                    loss += temp_loss * len(x_batch_dev)\n",
    "                accuracy /= x_batch.shape[0]\n",
    "                loss /= x_batch.shape[0]\n",
    "                time_str = datetime.datetime.now().isoformat()                    \n",
    "\n",
    "                print(\"{}: step {}, loss {:g}, acc {} \".format(time_str, step, loss, accuracy))\n",
    "\n",
    "\n",
    "            # Generate batches\n",
    "            batches = batch_iter(\n",
    "                list(zip(x_train, y_train[0],y_train[1], y_train[2] )), batch_size, 200)\n",
    "            # Training loop. For each batch...\n",
    "            for batch in batches:\n",
    "                x_batch, y_batch1, y_batch2, y_batch3 = zip(*batch)\n",
    "                loss_sum, acc_sum = train_step(x_batch, y_batch1, y_batch2, y_batch3,  loss_sum, acc_sum, 40)\n",
    "                current_step = tf.train.global_step(sess, global_step)\n",
    "                if current_step % 200 == 0:\n",
    "                    print(\"\\nEvaluation:\")\n",
    "                    dev_step(x_dev, y_dev)\n",
    "                    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 18, 50, 1)\n",
      "(?, 1, 1, 320)\n",
      "2019-02-22T00:00:48.701899: step 40, loss 2.81225, acc [0.73701172 0.57236328 0.45078125 0.38056641]\n",
      "2019-02-22T00:00:50.010345: step 80, loss 1.75888, acc [0.82949219 0.71572266 0.60439453 0.53583984]\n",
      "2019-02-22T00:00:51.284568: step 120, loss 1.58793, acc [0.83134766 0.72978516 0.61855469 0.55146484]\n",
      "2019-02-22T00:00:52.542918: step 160, loss 1.46455, acc [0.8359375  0.74003906 0.6421875  0.57509766]\n",
      "2019-02-22T00:00:53.807220: step 200, loss 1.38436, acc [0.846875   0.75048828 0.65107422 0.58085937]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:00:58.070829: step 200, loss 1.35978, acc [0.84823154 0.74875112 0.65562775 0.58517955] \n",
      "\n",
      "2019-02-22T00:00:59.430365: step 240, loss 1.33695, acc [0.85214844 0.76015625 0.66240234 0.59619141]\n",
      "2019-02-22T00:01:00.733355: step 280, loss 1.31144, acc [0.85517578 0.75849609 0.66269531 0.59501953]\n",
      "2019-02-22T00:01:02.018489: step 320, loss 1.27594, acc [0.84853516 0.76416016 0.66933594 0.59785156]\n",
      "2019-02-22T00:01:03.300649: step 360, loss 1.25665, acc [0.85244141 0.76044922 0.67626953 0.60664063]\n",
      "2019-02-22T00:01:04.613556: step 400, loss 1.25641, acc [0.85166016 0.76435547 0.67519531 0.60908203]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:01:08.826574: step 400, loss 1.21134, acc [0.85786223 0.76475888 0.68175675 0.60811501] \n",
      "\n",
      "2019-02-22T00:01:10.191068: step 440, loss 1.19432, acc [0.86787109 0.77714844 0.69072266 0.62763672]\n",
      "2019-02-22T00:01:11.463803: step 480, loss 1.25266, acc [0.85605469 0.76298828 0.67197266 0.60761719]\n",
      "2019-02-22T00:01:12.752410: step 520, loss 1.23528, acc [0.86123047 0.77089844 0.67705078 0.61503906]\n",
      "2019-02-22T00:01:14.077732: step 560, loss 1.21237, acc [0.85859375 0.76787109 0.67421875 0.60888672]\n",
      "2019-02-22T00:01:15.397077: step 600, loss 1.1753, acc [0.86240234 0.77265625 0.68662109 0.62167969]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:01:19.417152: step 600, loss 1.13396, acc [0.86221706 0.77983562 0.69718387 0.62870787] \n",
      "\n",
      "2019-02-22T00:01:20.742461: step 640, loss 1.18418, acc [0.86005859 0.775      0.68652344 0.62246094]\n",
      "2019-02-22T00:01:22.022809: step 680, loss 1.13329, acc [0.86181641 0.78330078 0.69785156 0.62900391]\n",
      "2019-02-22T00:01:23.266768: step 720, loss 1.13199, acc [0.86728516 0.78085938 0.68730469 0.62353516]\n",
      "2019-02-22T00:01:24.515695: step 760, loss 1.17619, acc [0.865625   0.77421875 0.69179687 0.62773437]\n",
      "2019-02-22T00:01:25.762760: step 800, loss 1.15409, acc [0.86904297 0.77783203 0.69306641 0.63085938]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:01:29.772317: step 800, loss 1.08284, acc [0.86885443 0.78838511 0.70863659 0.64353432] \n",
      "\n",
      "2019-02-22T00:01:31.092370: step 840, loss 1.14353, acc [0.87138672 0.7875     0.70166016 0.64082031]\n",
      "2019-02-22T00:01:32.319472: step 880, loss 1.15033, acc [0.86181641 0.77998047 0.69658203 0.6328125 ]\n",
      "2019-02-22T00:01:33.550046: step 920, loss 1.1398, acc [0.86474609 0.77666016 0.69365234 0.62666016]\n",
      "2019-02-22T00:01:34.770700: step 960, loss 1.14015, acc [0.86142578 0.78154297 0.69814453 0.63095703]\n",
      "2019-02-22T00:01:36.007720: step 1000, loss 1.10537, acc [0.87050781 0.78964844 0.7046875  0.63759766]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:01:39.968771: step 1000, loss 1.05176, acc [0.87333941 0.79399133 0.71473335 0.65165334] \n",
      "\n",
      "2019-02-22T00:01:41.424530: step 1040, loss 1.0947, acc [0.86826172 0.79208984 0.70908203 0.64677734]\n",
      "2019-02-22T00:01:42.661553: step 1080, loss 1.08735, acc [0.87578125 0.79101562 0.70917969 0.64667969]\n",
      "2019-02-22T00:01:43.876255: step 1120, loss 1.15113, acc [0.86396484 0.77861328 0.69804687 0.63173828]\n",
      "2019-02-22T00:01:45.086329: step 1160, loss 1.06141, acc [0.8765625  0.79648438 0.7125     0.65107422]\n",
      "2019-02-22T00:01:46.317021: step 1200, loss 1.07084, acc [0.86748047 0.79208984 0.71328125 0.64892578]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:01:50.455638: step 1200, loss 1.03945, acc [0.87136722 0.79358087 0.71645527 0.65101262] \n",
      "\n",
      "2019-02-22T00:01:51.777971: step 1240, loss 1.08822, acc [0.86933594 0.78525391 0.70722656 0.64462891]\n",
      "2019-02-22T00:01:53.016004: step 1280, loss 1.05715, acc [0.87080078 0.79560547 0.71220703 0.64697266]\n",
      "2019-02-22T00:01:54.253008: step 1320, loss 1.10369, acc [0.86630859 0.78896484 0.71005859 0.64755859]\n",
      "2019-02-22T00:01:55.481102: step 1360, loss 1.0798, acc [0.86425781 0.78916016 0.71191406 0.64335937]\n",
      "2019-02-22T00:01:56.706221: step 1400, loss 1.07825, acc [0.87128906 0.79023438 0.71298828 0.64941406]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:02:00.602332: step 1400, loss 1.0129, acc [0.87330937 0.79462203 0.72137072 0.65394588] \n",
      "\n",
      "2019-02-22T00:02:02.015688: step 1440, loss 1.0846, acc [0.87148437 0.79140625 0.70996094 0.64970703]\n",
      "2019-02-22T00:02:03.227415: step 1480, loss 1.05323, acc [0.86875    0.79042969 0.71513672 0.64921875]\n",
      "2019-02-22T00:02:04.442670: step 1520, loss 1.07241, acc [0.87382812 0.79052734 0.71542969 0.65585938]\n",
      "2019-02-22T00:02:05.649379: step 1560, loss 1.0656, acc [0.86972656 0.78974609 0.71416016 0.65361328]\n",
      "2019-02-22T00:02:07.053057: step 1600, loss 0.962908, acc [0.87907    0.8036409  0.72607521 0.66383464]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:02:11.045895: step 1600, loss 0.999484, acc [0.87082662 0.7982861  0.7231527  0.65616835] \n",
      "\n",
      "2019-02-22T00:02:12.315608: step 1640, loss 1.00846, acc [0.86767578 0.79462891 0.72529297 0.65839844]\n",
      "2019-02-22T00:02:13.492119: step 1680, loss 0.997137, acc [0.87480469 0.79726562 0.7296875  0.6671875 ]\n",
      "2019-02-22T00:02:14.662682: step 1720, loss 0.996089, acc [0.87275391 0.79160156 0.72060547 0.65449219]\n",
      "2019-02-22T00:02:15.913589: step 1760, loss 1.01255, acc [0.86835938 0.79140625 0.71611328 0.64648438]\n",
      "2019-02-22T00:02:17.088115: step 1800, loss 1.04683, acc [0.86572266 0.78789062 0.71298828 0.64775391]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:02:21.005568: step 1800, loss 0.972633, acc [0.87555186 0.79970768 0.7319825  0.66357657] \n",
      "\n",
      "2019-02-22T00:02:22.259898: step 1840, loss 0.971845, acc [0.87382812 0.80214844 0.73359375 0.6671875 ]\n",
      "2019-02-22T00:02:23.432937: step 1880, loss 1.00157, acc [0.87099609 0.79589844 0.72363281 0.65693359]\n",
      "2019-02-22T00:02:24.664008: step 1920, loss 1.00443, acc [0.875      0.80332031 0.72226563 0.65859375]\n",
      "2019-02-22T00:02:25.836584: step 1960, loss 0.966753, acc [0.8765625  0.80205078 0.73476562 0.66699219]\n",
      "2019-02-22T00:02:27.011572: step 2000, loss 0.998566, acc [0.87597656 0.79794922 0.72431641 0.65761719]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:02:30.942864: step 2000, loss 0.964257, acc [0.8762126  0.80444293 0.73454535 0.668472  ] \n",
      "\n",
      "2019-02-22T00:02:32.196761: step 2040, loss 0.974858, acc [0.87744141 0.80625    0.72841797 0.66552734]\n",
      "2019-02-22T00:02:33.383178: step 2080, loss 0.981871, acc [0.88037109 0.80175781 0.72900391 0.66972656]\n",
      "2019-02-22T00:02:34.572090: step 2120, loss 0.983855, acc [0.875      0.80341797 0.72724609 0.6625    ]\n",
      "2019-02-22T00:02:35.798199: step 2160, loss 0.9974, acc [0.87304688 0.79892578 0.72607422 0.66074219]\n",
      "2019-02-22T00:02:36.979670: step 2200, loss 0.974117, acc [0.87666016 0.80439453 0.72734375 0.66445312]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:02:40.995777: step 2200, loss 0.960054, acc [0.87570203 0.8051337  0.73276337 0.66918279] \n",
      "\n",
      "2019-02-22T00:02:42.267093: step 2240, loss 1.01763, acc [0.87207031 0.79013672 0.72197266 0.65869141]\n",
      "2019-02-22T00:02:43.505533: step 2280, loss 0.992383, acc [0.87900391 0.79892578 0.73076172 0.66484375]\n",
      "2019-02-22T00:02:44.685019: step 2320, loss 0.960347, acc [0.87402344 0.80664062 0.73388672 0.67216797]\n",
      "2019-02-22T00:02:45.866516: step 2360, loss 0.973372, acc [0.87460938 0.80341797 0.73154297 0.66933594]\n",
      "2019-02-22T00:02:47.053771: step 2400, loss 1.03082, acc [0.87050781 0.79775391 0.72460938 0.66201172]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:02:50.937940: step 2400, loss 0.947931, acc [0.88027711 0.80575439 0.73510597 0.66972339] \n",
      "\n",
      "2019-02-22T00:02:52.196291: step 2440, loss 0.991793, acc [0.87763672 0.79765625 0.72744141 0.66367188]\n",
      "2019-02-22T00:02:53.373298: step 2480, loss 0.97372, acc [0.88125    0.80673828 0.73046875 0.66865234]\n",
      "2019-02-22T00:02:54.593000: step 2520, loss 0.959726, acc [0.87890625 0.80615234 0.73486328 0.66923828]\n",
      "2019-02-22T00:02:55.817581: step 2560, loss 0.986938, acc [0.87597656 0.79863281 0.73320312 0.66738281]\n",
      "2019-02-22T00:02:56.988637: step 2600, loss 0.948528, acc [0.87978516 0.80800781 0.73515625 0.67041016]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:03:00.911657: step 2600, loss 0.94163, acc [0.87949624 0.80721601 0.73548639 0.67240637] \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-02-22T00:03:02.168972: step 2640, loss 0.972568, acc [0.87910156 0.80556641 0.72763672 0.66816406]\n",
      "2019-02-22T00:03:03.352924: step 2680, loss 0.995203, acc [0.87802734 0.80253906 0.728125   0.66337891]\n",
      "2019-02-22T00:03:04.530425: step 2720, loss 0.984882, acc [0.87880859 0.80634766 0.73076172 0.66689453]\n",
      "2019-02-22T00:03:05.701976: step 2760, loss 0.972547, acc [0.88242188 0.80537109 0.73007813 0.66660156]\n",
      "2019-02-22T00:03:06.876997: step 2800, loss 0.973306, acc [0.88222656 0.80947266 0.73242188 0.66992188]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:03:10.817713: step 2800, loss 0.924814, acc [0.87973651 0.80943848 0.74170329 0.67841304] \n",
      "\n",
      "2019-02-22T00:03:12.188159: step 2840, loss 1.00711, acc [0.87529297 0.79785156 0.72304687 0.66015625]\n"
     ]
    }
   ],
   "source": [
    "y_train = [train_y_label1_one_hot,train_y_label2_one_hot,train_y_label3_one_hot]\n",
    "y_test = [test_y_label1_one_hot,test_y_label2_one_hot,test_y_label3_one_hot]\n",
    "train(x_train, y_train,  x_test, y_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_y_label3_one_hot.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y_label3_one_hot.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
