{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.utils import shuffle\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "特征如下：['ITEM_NAME' 'TYPE'] \n",
      "\n",
      "数据数目:  499447\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('data/train/trainx.txt', sep='\\t')\n",
    "features = data.columns.values\n",
    "print(\"特征如下：{} \\n\".format(features))\n",
    "print(\"数据数目: \", len(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据清洗\n",
    "* 替换一些字符 类似于 （ ） ‘ ’ _"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ITEM_NAME</th>\n",
       "      <th>TYPE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>腾讯QQ黄钻三个月QQ黄钻3个月季卡官方自动充值可查时间可续费</td>\n",
       "      <td>本地生活--游戏充值--QQ充值</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>腾讯q币充值30元qq币30qb30q币自动充值</td>\n",
       "      <td>本地生活--游戏充值--QQ充值</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>腾讯QQ红钻包月卡 QQ红钻一个月QQ红钻1个月</td>\n",
       "      <td>本地生活--游戏充值--QQ充值</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>腾讯QQ蓝钻贵族31个月直充</td>\n",
       "      <td>本地生活--游戏充值--QQ充值</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>腾讯QQ币148元148QQ币148个直充148Q币148个Q币148个QQB★自动充值</td>\n",
       "      <td>本地生活--游戏充值--QQ充值</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      ITEM_NAME              TYPE\n",
       "0               腾讯QQ黄钻三个月QQ黄钻3个月季卡官方自动充值可查时间可续费  本地生活--游戏充值--QQ充值\n",
       "1                      腾讯q币充值30元qq币30qb30q币自动充值  本地生活--游戏充值--QQ充值\n",
       "2                      腾讯QQ红钻包月卡 QQ红钻一个月QQ红钻1个月  本地生活--游戏充值--QQ充值\n",
       "3                                腾讯QQ蓝钻贵族31个月直充  本地生活--游戏充值--QQ充值\n",
       "4  腾讯QQ币148元148QQ币148个直充148Q币148个Q币148个QQB★自动充值  本地生活--游戏充值--QQ充值"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "from zhon.hanzi import punctuation\n",
    "from string import punctuation as english_punc\n",
    "def  rm_punc(strs):\n",
    "    return re.sub(r\"[{}]+\".format(punctuation + english_punc),\" \",strs)\n",
    "data[features[0]]  = data[features[0]].apply(rm_punc)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练数据数目:   399558\n",
      "测试数据数目:   99889\n",
      "(399558,) (99889,)\n"
     ]
    }
   ],
   "source": [
    "# 因为有的label里本身就存在空格，所以先要把空格去掉，再去掉- - ， 再以空格划分\n",
    "data = shuffle(data)\n",
    "data_y = data[features[1]]\n",
    "\n",
    "# 取训练数据和测试数据\n",
    "train = data.sample(frac= 0.8).reset_index()\n",
    "test = data.sample(frac= 0.2).reset_index()\n",
    "\n",
    "\n",
    "print(\"训练数据数目:  \", len(train))\n",
    "print(\"测试数据数目:  \", len(test))\n",
    "\n",
    "train_x  = train[features[0]]\n",
    "train_y  = train[features[1]]\n",
    "\n",
    "test_x  = test[features[0]]\n",
    "test_y  = test[features[1]]\n",
    "\n",
    "print(train_x.shape, test_x.shape)\n",
    "\n",
    "# 分出数据集 训练集 测试集 数据集用来训练词向量\n",
    "data_y = data_y.apply(lambda x: x.replace(\" \", \"\").replace(\"--\", \" \").replace(\"/\", \"\")).tolist()\n",
    "data_y= [each.split() for each in data_y]\n",
    "\n",
    "train_y = train_y.apply(lambda x: x.replace(\" \", \"\").replace(\"--\", \" \").replace(\"/\", \"\")).tolist()\n",
    "train_y = [each.split() for each in train_y]\n",
    "\n",
    "test_y = test_y.apply(lambda x: x.replace(\" \", \"\").replace(\"--\", \" \").replace(\"/\", \"\")).tolist()\n",
    "test_y = [each.split() for each in test_y]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 分词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba\n",
    "def participle(data):\n",
    "    words = []\n",
    "    for i in range(len(data)):\n",
    "        result=[]\n",
    "        seg_list = jieba.cut(data.iloc[i])\n",
    "        for w in seg_list :#读取每一行分词\n",
    "            if w != \" \":\n",
    "                result.append(w)\n",
    "        words.append(result)#将该行分词写入列表形式的总分词列表\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\Richado\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.803 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "# 每条数据的分词结果是个长度不等的词列表\n",
    "word_data = participle(data[features[0]])\n",
    "word_data_train = participle(train_x)\n",
    "word_data_test = participle(test_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 标签分离(一个标签分成三个标签)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y = pd.DataFrame(train_y, columns=['label1', 'label2', 'label3'])\n",
    "test_y = pd.DataFrame(test_y, columns=['label1', 'label2', 'label3'])\n",
    "data_y = pd.DataFrame(data_y, columns=['label1', 'label2', 'label3'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_y[\"label1\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "191"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_y[\"label2\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1192"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_y[\"label3\"].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 标签onn_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "# 先生成唯一数组\n",
    "y_label1 = data_y['label1'].unique().tolist()\n",
    "y_label2 = data_y['label2'].unique().tolist()\n",
    "y_label3 = data_y['label3'].unique().tolist()\n",
    "\n",
    "# 获取在唯一数组中的索引(训练集和测试集各有3个标签需要处理)\n",
    "train_y_label1_map = train_y['label1'].apply(lambda x: y_label1.index(x))\n",
    "train_y_label2_map = train_y['label2'].apply(lambda x: y_label2.index(x))\n",
    "train_y_label3_map = train_y['label3'].apply(lambda x: y_label3.index(x))\n",
    "test_y_label1_map = test_y['label1'].apply(lambda x: y_label1.index(x))\n",
    "test_y_label2_map = test_y['label2'].apply(lambda x: y_label2.index(x))\n",
    "test_y_label3_map = test_y['label3'].apply(lambda x: y_label3.index(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生成对应one-hot(用做训练模型的标签)\n",
    "train_y_label1_one_hot = to_categorical(train_y_label1_map, 22)\n",
    "train_y_label2_one_hot = to_categorical(train_y_label2_map, 191)\n",
    "train_y_label3_one_hot = to_categorical(train_y_label3_map, 1192)\n",
    "test_y_label1_one_hot = to_categorical(test_y_label1_map, 22)\n",
    "test_y_label2_one_hot = to_categorical(test_y_label2_map, 191)\n",
    "test_y_label3_one_hot = to_categorical(test_y_label3_map, 1192)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 词向量训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "from gensim.models.word2vec import Word2Vec \n",
    "if os.path.exists(\"w2v_model\"):\n",
    "    model = Word2Vec.load(\"w2v_model\")\n",
    "    print(1)\n",
    "else:\n",
    "    sentences = word_data\n",
    "    model= Word2Vec(size=50, window=10, min_count = 1)\n",
    "    model.build_vocab(sentences)\n",
    "    model.train(sentences,total_examples = model.corpus_count,epochs = model.iter)\n",
    "    model.save(\"w2v_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 填充词向量(构造输入输出)（让每个句子拥有同样的维度）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\richado\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\ipykernel_launcher.py:2: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \n",
      "c:\\users\\richado\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\ipykernel_launcher.py:3: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing import sequence\n",
    "x_train = [model[word] for word in word_data_train]\n",
    "x_test = [model[word] for word in word_data_test]\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=20)\n",
    "x_test  = sequence.pad_sequences(x_test, maxlen=20) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 默认参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen = 10 # 一句话的填充的词的个数\n",
    "batch_size = 32 \n",
    "embedding_dims = 50 # 一个词的词向量长度\n",
    "epochs = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## TextCnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class TextCNN(object):\n",
    "    \"\"\"\n",
    "    A CNN for text classification.\n",
    "    Uses an embedding layer, followed by a convolutional, max-pooling and softmax layer.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "      self, maxlen, sequence_length, num_classes, filter_sizes, embedding_size, num_filters, l2_reg_lambda=0.0):\n",
    "\n",
    "        # Placeholders for input, output and dropout\n",
    "        self.input_x = tf.placeholder(tf.float32, [None, sequence_length, embedding_size], name=\"input_x\")\n",
    "        self.input_y1 = tf.placeholder(tf.float32, [None, num_classes[0]], name=\"input_y1\")\n",
    "        self.input_y2 = tf.placeholder(tf.float32, [None, num_classes[1]], name=\"input_y2\")\n",
    "        self.input_y3 = tf.placeholder(tf.float32, [None, num_classes[2]], name=\"input_y3\")\n",
    "        # Keeping track of l2 regularization loss (optional)\n",
    "        l2_loss = tf.constant(0.0)\n",
    "        \n",
    "        pooled_outputs = []\n",
    "        self.input_x_expand = tf.expand_dims(self.input_x, -1)\n",
    "        print(self.input_x_expand.shape)\n",
    "        for i, filter_size in enumerate(filter_sizes):\n",
    "            with tf.name_scope(\"conv-maxpool-%s\" % filter_size):\n",
    "                # Convolution Layer\n",
    "                filter_shape = [filter_size, embedding_size, 1, num_filters]\n",
    "                W = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.1), name=\"W\")\n",
    "                b = tf.Variable(tf.constant(0.1, shape=[num_filters]), name=\"b\")\n",
    "                print(W.shape)\n",
    "                conv = tf.nn.conv2d(\n",
    "                    self.input_x_expand,\n",
    "                    W,\n",
    "                    strides=[1, 1, 1, 1],\n",
    "                    padding=\"VALID\",\n",
    "                    name=\"conv\")\n",
    "                # Apply nonlinearity\n",
    "                h = tf.nn.relu(tf.nn.bias_add(conv, b), name=\"relu\")\n",
    "                # Maxpooling over the outputs\n",
    "                pooled = tf.nn.max_pool(\n",
    "                    h,\n",
    "                    ksize=[1, sequence_length - filter_size + 1, 1, 1],\n",
    "                    strides=[1, 1, 1, 1],\n",
    "                    padding='VALID',\n",
    "                    name=\"pool\")\n",
    "                pooled_outputs.append(pooled)\n",
    "\n",
    "        # Combine all the pooled features\n",
    "        num_filters_total = num_filters * len(filter_sizes)\n",
    "        self.h_pool = tf.concat(pooled_outputs, 3)\n",
    "        self.h_pool_flat = tf.reshape(self.h_pool, [-1, num_filters_total])\n",
    "        self.h_drop = self.h_pool_flat\n",
    "        \n",
    "        # Final (unnormalized) scores and predictions\n",
    "        \n",
    "        with tf.name_scope(\"output1\"):\n",
    "            W1 = tf.get_variable(\n",
    "                \"W11\",\n",
    "                shape=[num_filters_total, 64],\n",
    "                initializer=tf.contrib.layers.xavier_initializer())\n",
    "            b1 = tf.Variable(tf.constant(0.1, shape=[64]), name=\"b11\")\n",
    "            \n",
    "            self.output = tf.nn.relu(tf.nn.xw_plus_b(self.h_drop, W1, b1, name=\"output1\"))\n",
    "            \n",
    "            W2 = tf.get_variable(\n",
    "                \"W12\",\n",
    "                shape=[64, num_classes[0]],\n",
    "                initializer=tf.contrib.layers.xavier_initializer())\n",
    "            b2 = tf.Variable(tf.constant(0.1, shape=[num_classes[0]]), name=\"b12\")\n",
    "            l2_loss += tf.nn.l2_loss(W1)\n",
    "            l2_loss += tf.nn.l2_loss(b1)\n",
    "            self.scores1 = tf.nn.xw_plus_b(self.output, W2, b2, name=\"scores\")\n",
    "            self.predictions1 = tf.argmax(self.scores1, 1, name=\"predictions\")\n",
    "            \n",
    "        with tf.name_scope(\"output2\"):\n",
    "            W1 = tf.get_variable(\n",
    "                \"W21\",\n",
    "                shape=[num_filters_total, 128],\n",
    "                initializer=tf.contrib.layers.xavier_initializer())\n",
    "            b1 = tf.Variable(tf.constant(0.1, shape=[128]), name=\"b21\")\n",
    "            \n",
    "            self.output = tf.nn.relu(tf.nn.xw_plus_b(self.h_drop, W1, b1, name=\"output2\"))\n",
    "            \n",
    "            W2 = tf.get_variable(\n",
    "                \"W22\",\n",
    "                shape=[128, num_classes[1]],\n",
    "                initializer=tf.contrib.layers.xavier_initializer())\n",
    "            b2 = tf.Variable(tf.constant(0.1, shape=[num_classes[1]]), name=\"b22\")\n",
    "            l2_loss += tf.nn.l2_loss(W1)\n",
    "            l2_loss += tf.nn.l2_loss(b1)\n",
    "            self.scores2 = tf.nn.xw_plus_b(self.output, W2, b2, name=\"scores\")\n",
    "            self.predictions2 = tf.argmax(self.scores2, 1, name=\"predictions\")\n",
    "            \n",
    "        with tf.name_scope(\"output3\"):\n",
    "            W1 = tf.get_variable(\n",
    "                \"W31\",\n",
    "                shape=[num_filters_total, 256],\n",
    "                initializer=tf.contrib.layers.xavier_initializer())\n",
    "            b1 = tf.Variable(tf.constant(0.1, shape=[256]), name=\"b31\")\n",
    "            \n",
    "            self.output = tf.nn.relu(tf.nn.xw_plus_b(self.h_drop, W1, b1, name=\"output3\"))\n",
    "            \n",
    "            W2 = tf.get_variable(\n",
    "                \"W32\",\n",
    "                shape=[256, num_classes[2]],\n",
    "                initializer=tf.contrib.layers.xavier_initializer())\n",
    "            b2 = tf.Variable(tf.constant(0.1, shape=[num_classes[2]]), name=\"b32\")\n",
    "            l2_loss += tf.nn.l2_loss(W1)\n",
    "            l2_loss += tf.nn.l2_loss(b1)\n",
    "            self.scores3 = tf.nn.xw_plus_b(self.output, W2, b2, name=\"scores\")\n",
    "            self.predictions3 = tf.argmax(self.scores3, 1, name=\"predictions\")\n",
    "        # Calculate mean cross-entropy loss\n",
    "        \n",
    "        with tf.name_scope(\"loss\"):\n",
    "            losses1 = tf.nn.softmax_cross_entropy_with_logits(logits=self.scores1, labels=self.input_y1)\n",
    "            losses2 = tf.nn.softmax_cross_entropy_with_logits(logits=self.scores2, labels=self.input_y2)\n",
    "            losses3 = tf.nn.softmax_cross_entropy_with_logits(logits=self.scores3, labels=self.input_y3)\n",
    "            self.loss = tf.reduce_mean(losses1) + tf.reduce_mean(losses2) + tf.reduce_mean(losses3) + l2_reg_lambda * l2_loss\n",
    "            tf.summary.scalar('loss1',tf.reduce_mean(losses1))\n",
    "            tf.summary.scalar('loss2',tf.reduce_mean(losses2))\n",
    "            tf.summary.scalar('loss3',tf.reduce_mean(losses3))\n",
    "            tf.summary.scalar('loss',self.loss)\n",
    "        # Accuracy\n",
    "        with tf.name_scope(\"accuracy\"):\n",
    "            correct_predictions1 = tf.equal(self.predictions1, tf.argmax(self.input_y1, 1))\n",
    "            correct_predictions2 = tf.equal(self.predictions2, tf.argmax(self.input_y2, 1))\n",
    "            correct_predictions3 = tf.equal(self.predictions3, tf.argmax(self.input_y3, 1))\n",
    "#           单独的准确率\n",
    "            self.accuracy1 = tf.reduce_mean(tf.cast(correct_predictions1, \"float\"), name=\"accuracy1\")\n",
    "            self.accuracy2 = tf.reduce_mean(tf.cast(correct_predictions2, \"float\"), name=\"accuracy2\")\n",
    "            self.accuracy3 = tf.reduce_mean(tf.cast(correct_predictions3, \"float\"), name=\"accuracy3\")\n",
    "#           一起的准确率\n",
    "            self.acc = tf.reduce_mean(tf.cast(correct_predictions1, \"float\")*tf.cast(correct_predictions2, \"float\")*tf.cast(correct_predictions3, \"float\"))\n",
    "            tf.summary.scalar('acc1',self.accuracy1)\n",
    "            tf.summary.scalar('acc2',self.accuracy2)\n",
    "            tf.summary.scalar('acc3',self.accuracy3)\n",
    "            tf.summary.scalar('acc',self.acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def batch_iter(data, batch_size, num_epochs, shuffle=True):\n",
    "    \"\"\"\n",
    "    Generates a batch iterator for a dataset.\n",
    "    \"\"\"\n",
    "    data = np.array(data)\n",
    "    data_size = len(data)\n",
    "    num_batches_per_epoch = int((len(data)-1)/batch_size) + 1\n",
    "    for epoch in range(num_epochs):\n",
    "        # Shuffle the data at each epoch\n",
    "        if shuffle:\n",
    "            shuffle_indices = np.random.permutation(np.arange(data_size))\n",
    "            shuffled_data = data[shuffle_indices]\n",
    "        else:\n",
    "            shuffled_data = data\n",
    "  \n",
    "        for batch_num in range(num_batches_per_epoch):\n",
    "            start_index = batch_num * batch_size\n",
    "            end_index = np.min(((batch_num + 1) * batch_size, data_size))\n",
    "            yield shuffled_data[start_index:end_index]\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "from tensorflow.contrib import learn\n",
    "\n",
    "# 特征数这里是第三级标签的特征数\n",
    "max_features = 1192\n",
    "\n",
    "# 句子填充的长度\n",
    "maxlen = 16\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "# 词向量长度\n",
    "embedding_dims = 50\n",
    "\n",
    "filters = 64\n",
    "kernel_size = 5\n",
    "hidden_dims = 128\n",
    "epochs = 100\n",
    "\n",
    "# Parameters\n",
    "# ==================================================\n",
    "\n",
    "# Data loading params\n",
    "# tf.flags.DEFINE_float(\"dev_sample_percentage\", .1, \"Percentage of the training data to use for validation\")\n",
    "# tf.flags.DEFINE_string(\"positive_data_file\", \"./data/rt-polaritydata/rt-polarity.pos\", \"Data source for the positive data.\")\n",
    "# tf.flags.DEFINE_string(\"negative_data_file\", \"./data/rt-polaritydata/rt-polarity.neg\", \"Data source for the negative data.\")\n",
    "\n",
    "# Model Hyperparameters\n",
    "# tf.flags.DEFINE_integer(\"embedding_dim\", 128, \"Dimensionality of character embedding (default: 128)\")\n",
    "# tf.flags.DEFINE_string(\"filter_sizes\", \"3,4,5\", \"Comma-separated filter sizes (default: '3,4,5')\")\n",
    "# tf.flags.DEFINE_integer(\"num_filters\", 128, \"Number of filters per filter size (default: 128)\")\n",
    "# tf.flags.DEFINE_float(\"dropout_keep_prob\", 0.5, \"Dropout keep probability (default: 0.5)\")\n",
    "# tf.flags.DEFINE_float(\"l2_reg_lambda\", 0.0, \"L2 regularization lambda (default: 0.0)\")\n",
    "\n",
    "# Training parameters\n",
    "# tf.flags.DEFINE_integer(\"batch_size\", 64, \"Batch Size (default: 64)\")\n",
    "# tf.flags.DEFINE_integer(\"num_epochs\", 200, \"Number of training epochs (default: 200)\")\n",
    "# tf.flags.DEFINE_integer(\"evaluate_every\", 100, \"Evaluate model on dev set after this many steps (default: 100)\")\n",
    "# tf.flags.DEFINE_integer(\"checkpoint_every\", 100, \"Save model after this many steps (default: 100)\")\n",
    "# tf.flags.DEFINE_integer(\"num_checkpoints\", 5, \"Number of checkpoints to store (default: 5)\")\n",
    "# Misc Parameters\n",
    "# tf.flags.DEFINE_boolean(\"allow_soft_placement\", True, \"Allow device soft device placement\")\n",
    "# tf.flags.DEFINE_boolean(\"log_device_placement\", False, \"Log placement of ops on devices\")\n",
    "\n",
    "FLAGS = tf.flags.FLAGS\n",
    "\n",
    "def train(x_train, y_train, x_dev, y_dev):\n",
    "    # Training\n",
    "    # ==================================================\n",
    "\n",
    "    with tf.Graph().as_default():\n",
    "        session_conf = tf.ConfigProto(\n",
    "#           allow_soft_placement=FLAGS.allow_soft_placement,\n",
    "#           log_device_placement=FLAGS.log_device_placement\n",
    "        )\n",
    "        sess = tf.Session(config=session_conf)\n",
    "        with sess.as_default():\n",
    "            cnn = TextCNN(maxlen = 10, sequence_length = 20, num_classes = [22, 191, 1192], filter_sizes = [1,2, 3, 4, 5], embedding_size = 50, num_filters = 64)\n",
    "\n",
    "            # Define Training procedure\n",
    "\n",
    "            global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "#           学习率衰减\n",
    "            learning_rate = tf.train.exponential_decay(\n",
    "                0.01,\n",
    "                global_step,\n",
    "                x_train.shape[0] / 32,\n",
    "                0.9,\n",
    "                staircase=True,)\n",
    "            optimizer = tf.train.AdamOptimizer(1e-3)\n",
    "            grads_and_vars = optimizer.compute_gradients(cnn.loss)\n",
    "            train_op = optimizer.apply_gradients(grads_and_vars, global_step=global_step)\n",
    "\n",
    "            # Initialize all variables\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            \n",
    "            merged = tf.summary.merge_all()\n",
    "            \n",
    "            writer = tf.summary.FileWriter(\"fwwb\",sess.graph)\n",
    "            acc_sum = np.array([0.0,0.0,0.0,0.0])\n",
    "            loss_sum = 0\n",
    "            \n",
    "            def train_step(x_batch, y_batch1, y_batch2, y_batch3, loss_sum, acc_sum, interal = 10):\n",
    "  \n",
    "                \"\"\"\n",
    "                A single training step\n",
    "                \"\"\"\n",
    "                feed_dict = {\n",
    "                  cnn.input_x: x_batch,\n",
    "                  cnn.input_y1: y_batch1,\n",
    "                  cnn.input_y2: y_batch2,\n",
    "                  cnn.input_y3: y_batch3\n",
    "                }\n",
    "#         三个标签单独的准确率， 一起的准确率共四个准确率\n",
    "                rs, _, step,  loss, accuracy1, accuracy2, accuracy3, acc = sess.run(\n",
    "                    [merged, train_op, global_step, cnn.loss, cnn.accuracy1, cnn.accuracy2, cnn.accuracy3, cnn.acc],\n",
    "                    feed_dict)\n",
    "                time_str = datetime.datetime.now().isoformat()\n",
    "                acc_sum += [accuracy1, accuracy2, accuracy3, acc]\n",
    "                loss_sum += loss\n",
    "                if step % interal == 0:\n",
    "                    print(\"{}: step {}, loss {:g}, acc {}\".format(time_str, step, loss_sum/interal, acc_sum/interal))\n",
    "                    loss_sum = 0\n",
    "                    acc_sum = np.array([0.0,0.0,0.0,0.0])\n",
    "                    writer.add_summary(rs, step)\n",
    "                return loss_sum, acc_sum\n",
    "            # 评估步骤\n",
    "            def dev_step(x_batch, y_batch):\n",
    "                \"\"\"\n",
    "                Evaluates model on a dev set\n",
    "                \"\"\"\n",
    "\n",
    "                dev_batches = batch_iter(\n",
    "                    list(zip(x_batch, y_batch[0], y_batch[1], y_batch[2])), 640, 1)\n",
    "                step = 0\n",
    "                loss = 0\n",
    "                accuracy = np.array([0.0, 0.0, 0.0,0.0])\n",
    "                for batch in dev_batches:\n",
    "                    x_batch_dev, y_batch1_dev, y_batch2_dev, y_batch3_dev = zip(*batch)\n",
    "                    feed_dict = {\n",
    "                      cnn.input_x: x_batch_dev,\n",
    "                      cnn.input_y1: y_batch1_dev,\n",
    "                      cnn.input_y2: y_batch2_dev,\n",
    "                      cnn.input_y3: y_batch3_dev\n",
    "                    }\n",
    "                    step,  temp_loss, temp_accuracy1, temp_accuracy2, temp_accuracy3 , acc= sess.run(\n",
    "                        [global_step,cnn.loss, cnn.accuracy1, cnn.accuracy2, cnn.accuracy3, cnn.acc],\n",
    "                        feed_dict)\n",
    "                    accuracy[0] += temp_accuracy1 * len(x_batch_dev)\n",
    "                    accuracy[1] += temp_accuracy2 * len(x_batch_dev)\n",
    "                    accuracy[2] += temp_accuracy3 * len(x_batch_dev)\n",
    "                    accuracy[3] += acc * len(x_batch_dev)\n",
    "                    loss += temp_loss * len(x_batch_dev)\n",
    "                accuracy /= x_batch.shape[0]\n",
    "                loss /= x_batch.shape[0]\n",
    "                time_str = datetime.datetime.now().isoformat()                    \n",
    "\n",
    "                print(\"{}: step {}, loss {:g}, acc {} \".format(time_str, step, loss, accuracy))\n",
    "\n",
    "\n",
    "            # Generate batches\n",
    "            batches = batch_iter(\n",
    "                list(zip(x_train, y_train[0],y_train[1], y_train[2] )), 64, 200)\n",
    "            # Training loop. For each batch...\n",
    "            for batch in batches:\n",
    "                x_batch, y_batch1, y_batch2, y_batch3 = zip(*batch)\n",
    "                loss_sum, acc_sum = train_step(x_batch, y_batch1, y_batch2, y_batch3,  loss_sum, acc_sum, 40)\n",
    "                current_step = tf.train.global_step(sess, global_step)\n",
    "                if current_step % 200 == 0:\n",
    "                    print(\"\\nEvaluation:\")\n",
    "                    dev_step(x_dev, y_dev)\n",
    "                    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 20, 50, 1)\n",
      "(1, 50, 1, 64)\n",
      "(2, 50, 1, 64)\n",
      "(3, 50, 1, 64)\n",
      "(4, 50, 1, 64)\n",
      "(5, 50, 1, 64)\n",
      "2019-02-21T17:54:14.882373: step 40, loss 11.0231, acc [0.48085937 0.33789062 0.20820312 0.14960937]\n",
      "2019-02-21T17:54:15.461212: step 80, loss 6.87234, acc [0.74257812 0.534375   0.38632813 0.3234375 ]\n",
      "2019-02-21T17:54:16.004819: step 120, loss 5.98455, acc [0.76875    0.584375   0.4515625  0.38476562]\n",
      "2019-02-21T17:54:16.567282: step 160, loss 5.2285, acc [0.78007812 0.62734375 0.5046875  0.42109375]\n",
      "2019-02-21T17:54:17.101489: step 200, loss 4.79877, acc [0.8078125  0.67148438 0.53125    0.46054688]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-21T17:54:21.152368: step 200, loss 4.6558, acc [0.8064852  0.67089469 0.5420717  0.46208291] \n",
      "\n",
      "2019-02-21T17:54:21.816009: step 240, loss 4.51697, acc [0.8203125  0.68515625 0.55351562 0.4796875 ]\n",
      "2019-02-21T17:54:22.364543: step 280, loss 4.44487, acc [0.79765625 0.6796875  0.57734375 0.49726562]\n",
      "2019-02-21T17:54:22.946846: step 320, loss 4.28301, acc [0.81601563 0.70078125 0.5734375  0.49648437]\n",
      "2019-02-21T17:54:23.542580: step 360, loss 3.9549, acc [0.83671875 0.70585937 0.58203125 0.50585938]\n",
      "2019-02-21T17:54:24.090345: step 400, loss 3.74713, acc [0.83242187 0.725      0.60351562 0.534375  ]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-21T17:54:27.890741: step 400, loss 3.82005, acc [0.83200352 0.71519386 0.60356996 0.5296379 ] \n",
      "\n",
      "2019-02-21T17:54:28.547398: step 440, loss 3.81796, acc [0.83046875 0.709375   0.603125   0.52539062]\n",
      "2019-02-21T17:54:29.093491: step 480, loss 3.64756, acc [0.84726563 0.73125    0.61679688 0.5515625 ]\n",
      "2019-02-21T17:54:29.682245: step 520, loss 3.6845, acc [0.82929688 0.71171875 0.61875    0.54335937]\n",
      "2019-02-21T17:54:30.267042: step 560, loss 3.55102, acc [0.83828125 0.73671875 0.625      0.55742187]\n",
      "2019-02-21T17:54:30.853815: step 600, loss 3.60183, acc [0.83242187 0.72773438 0.63320312 0.55703125]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-21T17:54:34.793042: step 600, loss 3.42549, acc [0.84024267 0.73512599 0.63386359 0.56338536] \n",
      "\n",
      "2019-02-21T17:54:35.453712: step 640, loss 3.4627, acc [0.8375     0.725      0.6359375  0.56992188]\n",
      "2019-02-21T17:54:36.048913: step 680, loss 3.27866, acc [0.84921875 0.73789063 0.64804688 0.58046875]\n",
      "2019-02-21T17:54:36.608894: step 720, loss 3.2596, acc [0.84921875 0.75429687 0.64453125 0.58359375]\n",
      "2019-02-21T17:54:37.170117: step 760, loss 3.31494, acc [0.84765625 0.74648437 0.6484375  0.58046875]\n",
      "2019-02-21T17:54:37.755893: step 800, loss 3.23413, acc [0.85703125 0.7453125  0.63984375 0.5703125 ]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-21T17:54:41.871694: step 800, loss 3.27906, acc [0.83545736 0.74290462 0.64960105 0.56886144] \n",
      "\n",
      "2019-02-21T17:54:42.501120: step 840, loss 3.33139, acc [0.83984375 0.73046875 0.65078125 0.5734375 ]\n",
      "2019-02-21T17:54:43.125618: step 880, loss 3.41207, acc [0.83671875 0.73125    0.64179688 0.5671875 ]\n",
      "2019-02-21T17:54:43.723298: step 920, loss 3.29535, acc [0.84296875 0.75       0.65039062 0.58671875]\n",
      "2019-02-21T17:54:44.308623: step 960, loss 3.26142, acc [0.85234375 0.7328125  0.63085938 0.5609375 ]\n",
      "2019-02-21T17:54:44.915680: step 1000, loss 3.13449, acc [0.85       0.7515625  0.6578125  0.58867187]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-21T17:54:48.902028: step 1000, loss 3.10095, acc [0.85168537 0.75069327 0.65881128 0.58656109] \n",
      "\n",
      "2019-02-21T17:54:49.537214: step 1040, loss 3.02889, acc [0.8609375  0.76054687 0.66992188 0.6078125 ]\n",
      "2019-02-21T17:54:50.088291: step 1080, loss 3.0637, acc [0.85703125 0.75585938 0.66601562 0.603125  ]\n",
      "2019-02-21T17:54:50.644803: step 1120, loss 3.04992, acc [0.86132812 0.7609375  0.66796875 0.60078125]\n",
      "2019-02-21T17:54:51.195362: step 1160, loss 3.18891, acc [0.85429687 0.74492187 0.65039062 0.58085937]\n",
      "2019-02-21T17:54:51.753419: step 1200, loss 2.81191, acc [0.86835938 0.7734375  0.66015625 0.60273438]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-21T17:54:55.553796: step 1200, loss 2.94727, acc [0.85814254 0.76565988 0.66692028 0.60410055] \n",
      "\n",
      "2019-02-21T17:54:56.180579: step 1240, loss 3.03506, acc [0.86054688 0.76484375 0.65703125 0.59726563]\n",
      "2019-02-21T17:54:56.756931: step 1280, loss 2.90105, acc [0.86210937 0.76796875 0.67695313 0.61484375]\n",
      "2019-02-21T17:54:57.319395: step 1320, loss 2.92382, acc [0.853125   0.75859375 0.67617187 0.60703125]\n",
      "2019-02-21T17:54:57.868962: step 1360, loss 3.03813, acc [0.84609375 0.75507813 0.67304688 0.60273438]\n",
      "2019-02-21T17:54:58.431425: step 1400, loss 3.1152, acc [0.84453125 0.7578125  0.65898437 0.5890625 ]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-21T17:55:02.518992: step 1400, loss 2.8769, acc [0.86065533 0.76675109 0.67715164 0.61210944] \n",
      "\n",
      "2019-02-21T17:55:03.162761: step 1440, loss 2.69023, acc [0.87304688 0.77929688 0.6953125  0.63164062]\n",
      "2019-02-21T17:55:03.719768: step 1480, loss 3.03805, acc [0.85       0.76328125 0.66484375 0.59765625]\n",
      "2019-02-21T17:55:04.267352: step 1520, loss 2.86484, acc [0.86875    0.77734375 0.6890625  0.62382812]\n",
      "2019-02-21T17:55:04.810967: step 1560, loss 2.81893, acc [0.86289063 0.77148438 0.6796875  0.6140625 ]\n",
      "2019-02-21T17:55:05.397734: step 1600, loss 2.9584, acc [0.84179688 0.75742188 0.66289062 0.59140625]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-21T17:55:09.472370: step 1600, loss 2.79775, acc [0.86556077 0.77466988 0.68158656 0.61994814] \n",
      "\n",
      "2019-02-21T17:55:10.132544: step 1640, loss 2.72222, acc [0.86210937 0.7875     0.68554688 0.62460938]\n",
      "2019-02-21T17:55:10.739150: step 1680, loss 2.85869, acc [0.871875   0.77070313 0.68789062 0.6234375 ]\n",
      "2019-02-21T17:55:11.278798: step 1720, loss 2.91727, acc [0.86367187 0.77382812 0.68164062 0.6203125 ]\n",
      "2019-02-21T17:55:11.823917: step 1760, loss 2.801, acc [0.86328125 0.77617187 0.68867188 0.62851563]\n",
      "2019-02-21T17:55:12.411676: step 1800, loss 2.87184, acc [0.86054688 0.76914063 0.665625   0.61054688]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-21T17:55:16.529958: step 1800, loss 2.75116, acc [0.86493007 0.77588123 0.68315831 0.61812612] \n",
      "\n",
      "2019-02-21T17:55:17.166324: step 1840, loss 2.85536, acc [0.86875    0.76835937 0.67851562 0.61171875]\n",
      "2019-02-21T17:55:17.749123: step 1880, loss 2.67704, acc [0.87226563 0.78085938 0.68945312 0.625     ]\n",
      "2019-02-21T17:55:18.324510: step 1920, loss 2.77987, acc [0.8640625  0.7765625  0.68828125 0.62578125]\n",
      "2019-02-21T17:55:18.873059: step 1960, loss 2.79924, acc [0.86992187 0.77265625 0.66875    0.61835938]\n",
      "2019-02-21T17:55:19.417665: step 2000, loss 2.81296, acc [0.86054688 0.76835937 0.67578125 0.61640625]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-21T17:55:23.469729: step 2000, loss 2.67149, acc [0.86713252 0.78179779 0.69145752 0.63137082] \n",
      "\n",
      "2019-02-21T17:55:24.144449: step 2040, loss 2.72781, acc [0.86835938 0.78242188 0.68085938 0.62226563]\n",
      "2019-02-21T17:55:24.869105: step 2080, loss 2.72747, acc [0.87578125 0.78632813 0.69140625 0.634375  ]\n",
      "2019-02-21T17:55:25.797152: step 2120, loss 2.62366, acc [0.87890625 0.7796875  0.6921875  0.628125  ]\n",
      "2019-02-21T17:55:26.377879: step 2160, loss 2.89617, acc [0.86367187 0.7625     0.67070312 0.603125  ]\n",
      "2019-02-21T17:55:26.983633: step 2200, loss 2.74682, acc [0.86953125 0.7828125  0.6828125  0.6234375 ]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-21T17:55:31.048844: step 2200, loss 2.65491, acc [0.86724264 0.77918489 0.69788465 0.63256214] \n",
      "\n",
      "2019-02-21T17:55:31.694162: step 2240, loss 2.51258, acc [0.88203125 0.78476563 0.70429688 0.63789063]\n",
      "2019-02-21T17:55:32.276172: step 2280, loss 2.64628, acc [0.87734375 0.77695313 0.68789062 0.62851563]\n",
      "2019-02-21T17:55:32.863931: step 2320, loss 2.62533, acc [0.87109375 0.78515625 0.703125   0.63867188]\n",
      "2019-02-21T17:55:33.476236: step 2360, loss 2.80037, acc [0.8609375  0.77421875 0.68789062 0.62304688]\n",
      "2019-02-21T17:55:34.056636: step 2400, loss 2.56536, acc [0.8703125  0.78242188 0.7046875  0.63398438]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-21T17:55:38.035542: step 2400, loss 2.60006, acc [0.86961527 0.78708366 0.70025728 0.63895925] \n",
      "\n",
      "2019-02-21T17:55:38.711859: step 2440, loss 2.76695, acc [0.86875    0.77148438 0.6859375  0.62304688]\n",
      "2019-02-21T17:55:39.318389: step 2480, loss 2.64383, acc [0.87578125 0.78476563 0.70078125 0.63945312]\n",
      "2019-02-21T17:55:39.883333: step 2520, loss 2.848, acc [0.85742188 0.77070313 0.6734375  0.61132812]\n",
      "2019-02-21T17:55:40.491025: step 2560, loss 2.65984, acc [0.8765625  0.78046875 0.68945312 0.62851563]\n",
      "2019-02-21T17:55:41.059441: step 2600, loss 2.63769, acc [0.86875    0.78320312 0.696875   0.62734375]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-21T17:55:45.115229: step 2600, loss 2.57454, acc [0.87051628 0.78867543 0.70059766 0.63999039] \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-02-21T17:55:45.765980: step 2640, loss 2.58996, acc [0.86835938 0.78085938 0.6953125  0.63320312]\n",
      "2019-02-21T17:55:46.324473: step 2680, loss 2.66235, acc [0.86875    0.77929688 0.69882813 0.63945312]\n",
      "2019-02-21T17:55:46.901816: step 2720, loss 2.82524, acc [0.85234375 0.771875   0.68945312 0.62421875]\n",
      "2019-02-21T17:55:47.630935: step 2760, loss 2.6829, acc [0.86796875 0.78789062 0.69101563 0.63554687]\n",
      "2019-02-21T17:55:48.193399: step 2800, loss 2.60442, acc [0.8734375  0.7890625  0.70039063 0.64335937]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-21T17:55:52.616722: step 2800, loss 2.53341, acc [0.87479102 0.79108811 0.70527285 0.64375457] \n",
      "\n",
      "2019-02-21T17:55:53.269951: step 2840, loss 2.58395, acc [0.86992187 0.79179687 0.7015625  0.64492187]\n",
      "2019-02-21T17:55:53.847295: step 2880, loss 2.5707, acc [0.86640625 0.78945312 0.70078125 0.6421875 ]\n",
      "2019-02-21T17:55:54.396103: step 2920, loss 2.6122, acc [0.87421875 0.78710938 0.71054688 0.6453125 ]\n",
      "2019-02-21T17:55:54.959063: step 2960, loss 2.60406, acc [0.86757812 0.7921875  0.70625    0.64492187]\n",
      "2019-02-21T17:55:55.545332: step 3000, loss 2.65849, acc [0.86757812 0.778125   0.69296875 0.63359375]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-21T17:55:59.578799: step 3000, loss 2.51893, acc [0.87467089 0.78978666 0.70672446 0.6481995 ] \n",
      "\n",
      "2019-02-21T17:56:00.214561: step 3040, loss 2.51069, acc [0.88359375 0.79648438 0.71875    0.659375  ]\n",
      "2019-02-21T17:56:00.759168: step 3080, loss 2.62965, acc [0.86796875 0.77851563 0.69335938 0.63515625]\n",
      "2019-02-21T17:56:01.368256: step 3120, loss 2.54812, acc [0.86679688 0.80351562 0.70703125 0.64765625]\n",
      "2019-02-21T17:56:01.943119: step 3160, loss 2.72664, acc [0.8703125  0.778125   0.69492188 0.63320312]\n",
      "2019-02-21T17:56:02.539310: step 3200, loss 2.71695, acc [0.86757812 0.7703125  0.69648438 0.63359375]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-21T17:56:06.841114: step 3200, loss 2.51505, acc [0.87399013 0.78912593 0.70536295 0.6418024 ] \n",
      "\n",
      "2019-02-21T17:56:07.513687: step 3240, loss 2.58579, acc [0.87421875 0.7875     0.70078125 0.64296875]\n",
      "2019-02-21T17:56:08.071685: step 3280, loss 2.52604, acc [0.87773437 0.77851563 0.69882813 0.634375  ]\n",
      "2019-02-21T17:56:08.619765: step 3320, loss 2.48276, acc [0.87617188 0.79453125 0.71757812 0.65976563]\n",
      "2019-02-21T17:56:09.201076: step 3360, loss 2.56978, acc [0.87578125 0.78398437 0.70585937 0.64648438]\n",
      "2019-02-21T17:56:09.751669: step 3400, loss 2.56181, acc [0.8765625  0.7921875  0.7109375  0.64648438]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-21T17:56:13.769424: step 3400, loss 2.47002, acc [0.87634274 0.79330056 0.71286128 0.65217391] \n",
      "\n",
      "2019-02-21T17:56:14.394190: step 3440, loss 2.75099, acc [0.86953125 0.78398437 0.69492188 0.63671875]\n",
      "2019-02-21T17:56:14.947724: step 3480, loss 2.46763, acc [0.8796875  0.79335937 0.70625    0.653125  ]\n",
      "2019-02-21T17:56:15.489901: step 3520, loss 2.41342, acc [0.88242188 0.79414063 0.72148437 0.65820312]\n",
      "2019-02-21T17:56:16.035452: step 3560, loss 2.42147, acc [0.87734375 0.78710938 0.71289062 0.64375   ]\n",
      "2019-02-21T17:56:16.610315: step 3600, loss 2.56514, acc [0.87421875 0.78398437 0.69609375 0.63398438]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-21T17:56:21.047525: step 3600, loss 2.45488, acc [0.87789446 0.79322047 0.71334181 0.65077236] \n",
      "\n",
      "2019-02-21T17:56:21.793506: step 3640, loss 2.57211, acc [0.865625   0.78398437 0.7109375  0.64648438]\n",
      "2019-02-21T17:56:22.392675: step 3680, loss 2.55037, acc [0.87226563 0.78320312 0.69335938 0.63203125]\n",
      "2019-02-21T17:56:23.022593: step 3720, loss 2.43862, acc [0.87695312 0.80039063 0.70273438 0.64179688]\n",
      "2019-02-21T17:56:23.590016: step 3760, loss 2.55331, acc [0.86914062 0.79257813 0.70039063 0.64257812]\n",
      "2019-02-21T17:56:24.184719: step 3800, loss 2.40687, acc [0.87226563 0.79804688 0.71523437 0.65820312]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-21T17:56:28.263323: step 3800, loss 2.44871, acc [0.87238835 0.79311035 0.71503369 0.65268448] \n",
      "\n",
      "2019-02-21T17:56:28.966154: step 3840, loss 2.45856, acc [0.87148437 0.8015625  0.71367187 0.653125  ]\n",
      "2019-02-21T17:56:29.523685: step 3880, loss 2.37101, acc [0.87734375 0.80546875 0.71796875 0.65820312]\n",
      "2019-02-21T17:56:30.099014: step 3920, loss 2.57964, acc [0.875      0.79375    0.70507812 0.64335937]\n",
      "2019-02-21T17:56:30.712567: step 3960, loss 2.57705, acc [0.86484375 0.78945312 0.71640625 0.65351563]\n",
      "2019-02-21T17:56:31.281974: step 4000, loss 2.57891, acc [0.878125   0.7890625  0.71328125 0.653125  ]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-21T17:56:35.494992: step 4000, loss 2.41045, acc [0.87843507 0.79588343 0.72307261 0.6596422 ] \n",
      "\n",
      "2019-02-21T17:56:36.163103: step 4040, loss 2.49753, acc [0.87929687 0.79570312 0.71914062 0.6578125 ]\n",
      "2019-02-21T17:56:36.730031: step 4080, loss 2.45941, acc [0.87109375 0.79921875 0.7125     0.65390625]\n",
      "2019-02-21T17:56:37.315308: step 4120, loss 2.43926, acc [0.8796875  0.80273438 0.70742187 0.65273437]\n",
      "2019-02-21T17:56:37.857932: step 4160, loss 2.35686, acc [0.89296875 0.81015625 0.71953125 0.66679687]\n",
      "2019-02-21T17:56:38.413452: step 4200, loss 2.6266, acc [0.871875   0.7890625  0.70742187 0.6515625 ]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-21T17:56:42.344743: step 4200, loss 2.37576, acc [0.88181882 0.80067876 0.71817718 0.66210494] \n",
      "\n",
      "2019-02-21T17:56:43.000949: step 4240, loss 2.40437, acc [0.87890625 0.8046875  0.71367187 0.66328125]\n",
      "2019-02-21T17:56:43.585733: step 4280, loss 2.49298, acc [0.87734375 0.79257813 0.70859375 0.64882812]\n",
      "2019-02-21T17:56:44.146897: step 4320, loss 2.69273, acc [0.87109375 0.78320312 0.7        0.64414063]\n",
      "2019-02-21T17:56:44.715807: step 4360, loss 2.44324, acc [0.87773437 0.8        0.72226563 0.66289062]\n",
      "2019-02-21T17:56:45.276784: step 4400, loss 2.42469, acc [0.88476562 0.79960937 0.70742187 0.65078125]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-21T17:56:49.514600: step 4400, loss 2.35889, acc [0.88310024 0.79969767 0.71750643 0.65993252] \n",
      "\n",
      "2019-02-21T17:56:50.273481: step 4440, loss 2.44185, acc [0.88007813 0.79335937 0.71484375 0.65507812]\n",
      "2019-02-21T17:56:50.854792: step 4480, loss 2.38679, acc [0.87578125 0.8        0.72109375 0.66015625]\n",
      "2019-02-21T17:56:51.410806: step 4520, loss 2.43999, acc [0.88125    0.79648438 0.72109375 0.66171875]\n",
      "2019-02-21T17:56:51.960870: step 4560, loss 2.52011, acc [0.88046875 0.79335937 0.70703125 0.64882812]\n",
      "2019-02-21T17:56:52.542677: step 4600, loss 2.42806, acc [0.87695312 0.80234375 0.71210938 0.65234375]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-21T17:56:56.658479: step 4600, loss 2.36476, acc [0.87847511 0.7996376  0.71898808 0.65954209] \n",
      "\n",
      "2019-02-21T17:56:57.297326: step 4640, loss 2.45631, acc [0.8828125  0.7984375  0.72148437 0.66210938]\n",
      "2019-02-21T17:56:57.880125: step 4680, loss 2.42376, acc [0.88242188 0.79648438 0.71679688 0.659375  ]\n",
      "2019-02-21T17:56:58.448539: step 4720, loss 2.63842, acc [0.8625     0.77460938 0.70351562 0.63632813]\n",
      "2019-02-21T17:56:58.986735: step 4760, loss 2.40659, acc [0.88242188 0.80820313 0.71992188 0.66523438]\n",
      "2019-02-21T17:56:59.533826: step 4800, loss 2.51697, acc [0.87617188 0.79648438 0.72304687 0.6671875 ]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-21T17:57:03.560309: step 4800, loss 2.36465, acc [0.88016698 0.80157975 0.71708597 0.65632853] \n",
      "\n",
      "2019-02-21T17:57:04.234869: step 4840, loss 2.35986, acc [0.89179688 0.81289062 0.721875   0.66757813]\n",
      "2019-02-21T17:57:04.785924: step 4880, loss 2.49006, acc [0.87617188 0.78164062 0.703125   0.64140625]\n",
      "2019-02-21T17:57:05.509588: step 4920, loss 2.51659, acc [0.87851563 0.79570312 0.7078125  0.65234375]\n",
      "2019-02-21T17:57:06.248625: step 4960, loss 2.38071, acc [0.88671875 0.81054688 0.71601563 0.6671875 ]\n",
      "2019-02-21T17:57:06.919216: step 5000, loss 2.42389, acc [0.87929687 0.79570312 0.71015625 0.64804688]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-21T17:57:11.427887: step 5000, loss 2.33063, acc [0.8816186  0.80299132 0.72330287 0.66539859] \n",
      "\n",
      "2019-02-21T17:57:12.168379: step 5040, loss 2.33777, acc [0.89023438 0.815625   0.72421875 0.67421875]\n",
      "2019-02-21T17:57:12.745248: step 5080, loss 2.35365, acc [0.884375   0.80664062 0.72382813 0.67070312]\n",
      "2019-02-21T17:57:13.307215: step 5120, loss 2.41249, acc [0.87851563 0.80234375 0.73007813 0.6734375 ]\n",
      "2019-02-21T17:57:13.865710: step 5160, loss 2.38523, acc [0.87578125 0.80195313 0.71640625 0.65195313]\n",
      "2019-02-21T17:57:14.403373: step 5200, loss 2.41668, acc [0.88203125 0.80078125 0.72148437 0.66132813]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-21T17:57:18.894207: step 5200, loss 2.29468, acc [0.88398122 0.80734616 0.72848862 0.67343752] \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-02-21T17:57:19.647078: step 5240, loss 2.41528, acc [0.8703125  0.79882812 0.7328125  0.675     ]\n",
      "2019-02-21T17:57:20.203655: step 5280, loss 2.39058, acc [0.875      0.79375    0.71640625 0.65859375]\n",
      "2019-02-21T17:57:20.752192: step 5320, loss 2.37218, acc [0.88046875 0.80039063 0.72734375 0.66523438]\n",
      "2019-02-21T17:57:21.324598: step 5360, loss 2.50042, acc [0.875      0.79765625 0.70351562 0.64726562]\n",
      "2019-02-21T17:57:21.898079: step 5400, loss 2.33931, acc [0.8890625  0.80117187 0.71054688 0.65429688]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-21T17:57:26.009471: step 5400, loss 2.28386, acc [0.88305019 0.80595461 0.73155202 0.67160548] \n",
      "\n",
      "2019-02-21T17:57:26.658185: step 5440, loss 2.42056, acc [0.88046875 0.79023438 0.7109375  0.64609375]\n",
      "2019-02-21T17:57:27.236729: step 5480, loss 2.46054, acc [0.87929687 0.80117187 0.71445313 0.66015625]\n",
      "2019-02-21T17:57:27.792745: step 5520, loss 2.55436, acc [0.87773437 0.79101562 0.71171875 0.65039062]\n",
      "2019-02-21T17:57:28.349299: step 5560, loss 2.36054, acc [0.87695312 0.8015625  0.70546875 0.64609375]\n",
      "2019-02-21T17:57:28.899924: step 5600, loss 2.3266, acc [0.88710937 0.80585938 0.73203125 0.6671875 ]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-21T17:57:33.032416: step 5600, loss 2.2628, acc [0.88481214 0.80898798 0.73277338 0.67530959] \n",
      "\n",
      "2019-02-21T17:57:33.703008: step 5640, loss 2.39351, acc [0.88125    0.7859375  0.72148437 0.66015625]\n",
      "2019-02-21T17:57:34.302173: step 5680, loss 2.41461, acc [0.89023438 0.79335937 0.71640625 0.65664062]\n",
      "2019-02-21T17:57:34.962846: step 5720, loss 2.50401, acc [0.88125    0.79570312 0.70351562 0.64921875]\n",
      "2019-02-21T17:57:35.615583: step 5760, loss 2.32134, acc [0.88515625 0.8078125  0.73515625 0.67382812]\n",
      "2019-02-21T17:57:36.209787: step 5800, loss 2.33277, acc [0.88710937 0.79765625 0.72226563 0.66523438]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-21T17:57:40.494776: step 5800, loss 2.25061, acc [0.88502237 0.80825716 0.73284346 0.67599035] \n",
      "\n",
      "2019-02-21T17:57:41.187877: step 5840, loss 2.55762, acc [0.86523438 0.78554687 0.71640625 0.6484375 ]\n",
      "2019-02-21T17:57:41.748923: step 5880, loss 2.30591, acc [0.88476562 0.81015625 0.72265625 0.6625    ]\n",
      "2019-02-21T17:57:42.335191: step 5920, loss 2.40789, acc [0.87695312 0.80117187 0.7078125  0.64804688]\n",
      "2019-02-21T17:57:42.884265: step 5960, loss 2.38897, acc [0.86757812 0.80585938 0.70390625 0.64375   ]\n",
      "2019-02-21T17:57:43.433706: step 6000, loss 2.29379, acc [0.88945312 0.80976563 0.72929687 0.67695313]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-21T17:57:48.175547: step 6000, loss 2.25108, acc [0.88616364 0.80906807 0.73215269 0.67743195] \n",
      "\n",
      "2019-02-21T17:57:48.839656: step 6040, loss 2.47382, acc [0.87539062 0.79765625 0.72148437 0.66679687]\n",
      "2019-02-21T17:57:49.404103: step 6080, loss 2.43454, acc [0.87421875 0.78984375 0.7296875  0.67109375]\n",
      "2019-02-21T17:57:49.965078: step 6120, loss 2.32113, acc [0.884375   0.80351562 0.71523437 0.66171875]\n",
      "2019-02-21T17:57:50.501748: step 6160, loss 2.27612, acc [0.88359375 0.8109375  0.71835938 0.66484375]\n",
      "2019-02-21T17:57:51.038420: step 6200, loss 2.39303, acc [0.88203125 0.79179687 0.72382813 0.66210938]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-21T17:57:55.333773: step 6200, loss 2.25558, acc [0.88267978 0.8067555  0.7316221  0.67001372] \n",
      "\n",
      "2019-02-21T17:57:56.014781: step 6240, loss 2.50396, acc [0.8765625  0.78867188 0.7109375  0.65078125]\n",
      "2019-02-21T17:57:56.872579: step 6280, loss 2.22621, acc [0.88359375 0.8171875  0.72174479 0.66236979]\n",
      "2019-02-21T17:57:57.465795: step 6320, loss 2.22171, acc [0.8921875  0.80976563 0.73359375 0.67773438]\n",
      "2019-02-21T17:57:58.018337: step 6360, loss 2.27686, acc [0.88984375 0.80195313 0.72109375 0.66679687]\n",
      "2019-02-21T17:57:58.545090: step 6400, loss 2.21958, acc [0.88710937 0.81289062 0.7390625  0.6796875 ]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-21T17:58:02.702556: step 6400, loss 2.22242, acc [0.88960746 0.80845739 0.73506592 0.67674118] \n",
      "\n",
      "2019-02-21T17:58:03.335450: step 6440, loss 2.10569, acc [0.89648438 0.81640625 0.73242188 0.67773438]\n",
      "2019-02-21T17:58:03.911304: step 6480, loss 2.22947, acc [0.8875     0.81835938 0.72539062 0.66953125]\n",
      "2019-02-21T17:58:04.518904: step 6520, loss 2.14806, acc [0.90039062 0.81367188 0.74179688 0.68828125]\n",
      "2019-02-21T17:58:05.154774: step 6560, loss 2.34528, acc [0.87617188 0.80078125 0.73007813 0.67421875]\n",
      "2019-02-21T17:58:05.740054: step 6600, loss 2.12368, acc [0.88710937 0.82109375 0.7359375  0.67617187]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-21T17:58:10.120225: step 6600, loss 2.23709, acc [0.88497232 0.81109031 0.73139185 0.67481905] \n",
      "\n",
      "2019-02-21T17:58:10.837936: step 6640, loss 2.2095, acc [0.88164062 0.815625   0.7390625  0.6765625 ]\n",
      "2019-02-21T17:58:11.360719: step 6680, loss 2.23931, acc [0.88398438 0.80664062 0.73359375 0.67539063]\n",
      "2019-02-21T17:58:11.877055: step 6720, loss 2.06177, acc [0.8984375  0.8234375  0.74882812 0.690625  ]\n",
      "2019-02-21T17:58:12.494077: step 6760, loss 2.28411, acc [0.89179688 0.80820313 0.7359375  0.68242187]\n",
      "2019-02-21T17:58:13.024324: step 6800, loss 2.143, acc [0.8890625  0.81523437 0.74296875 0.6859375 ]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-21T17:58:17.081575: step 6800, loss 2.22054, acc [0.88570313 0.81376328 0.73341409 0.67772227] \n",
      "\n",
      "2019-02-21T17:58:17.735302: step 6840, loss 2.18916, acc [0.88398438 0.81054688 0.74296875 0.68632812]\n",
      "2019-02-21T17:58:18.345877: step 6880, loss 2.21625, acc [0.87773437 0.81015625 0.73789063 0.68046875]\n",
      "2019-02-21T17:58:18.927683: step 6920, loss 2.16733, acc [0.89101562 0.82109375 0.74375    0.68789062]\n",
      "2019-02-21T17:58:19.451955: step 6960, loss 2.22801, acc [0.8859375  0.79882812 0.73242188 0.66835937]\n",
      "2019-02-21T17:58:19.970771: step 7000, loss 2.17975, acc [0.88789063 0.82070312 0.74101562 0.68242187]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-21T17:58:24.295389: step 7000, loss 2.20218, acc [0.88986775 0.80970878 0.73390463 0.67662105] \n",
      "\n",
      "2019-02-21T17:58:25.094445: step 7040, loss 2.17893, acc [0.89648438 0.8109375  0.73789063 0.68203125]\n",
      "2019-02-21T17:58:25.684682: step 7080, loss 2.23182, acc [0.87890625 0.81835938 0.73359375 0.6828125 ]\n",
      "2019-02-21T17:58:26.258057: step 7120, loss 2.27056, acc [0.88632813 0.8109375  0.72578125 0.6703125 ]\n",
      "2019-02-21T17:58:26.805143: step 7160, loss 2.26351, acc [0.88710937 0.80585938 0.73046875 0.678125  ]\n",
      "2019-02-21T17:58:27.360666: step 7200, loss 2.2415, acc [0.89296875 0.80429688 0.73046875 0.6765625 ]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-21T17:58:31.728930: step 7200, loss 2.21347, acc [0.88716475 0.80903803 0.73362432 0.67344753] \n",
      "\n",
      "2019-02-21T17:58:32.399520: step 7240, loss 2.23452, acc [0.88632813 0.80859375 0.7390625  0.67421875]\n",
      "2019-02-21T17:58:33.069617: step 7280, loss 2.22359, acc [0.88789063 0.80195313 0.71992188 0.66210938]\n",
      "2019-02-21T17:58:33.642494: step 7320, loss 2.21472, acc [0.8859375  0.81601563 0.73828125 0.68671875]\n",
      "2019-02-21T17:58:34.206446: step 7360, loss 2.16248, acc [0.88671875 0.815625   0.73789063 0.68828125]\n",
      "2019-02-21T17:58:34.757502: step 7400, loss 2.40299, acc [0.87617188 0.79609375 0.72890625 0.66875   ]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-21T17:58:39.462987: step 7400, loss 2.19789, acc [0.88586331 0.81274214 0.73985123 0.68018501] \n",
      "\n",
      "2019-02-21T17:58:40.162454: step 7440, loss 2.37418, acc [0.875      0.79648438 0.72734375 0.671875  ]\n",
      "2019-02-21T17:58:40.689702: step 7480, loss 2.16384, acc [0.88945312 0.8125     0.734375   0.671875  ]\n",
      "2019-02-21T17:58:41.208021: step 7520, loss 2.20447, acc [0.88828125 0.81289062 0.73867187 0.68320313]\n",
      "2019-02-21T17:58:41.769493: step 7560, loss 2.18223, acc [0.8875     0.8171875  0.73242188 0.68242187]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-54-87d756c70f63>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0my_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtrain_y_label1_one_hot\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrain_y_label2_one_hot\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrain_y_label3_one_hot\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0my_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtest_y_label1_one_hot\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtest_y_label2_one_hot\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtest_y_label3_one_hot\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-53-7c38cab115a0>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(x_train, y_train, x_dev, y_dev)\u001b[0m\n\u001b[0;32m    148\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mbatches\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m                 \u001b[0mx_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_batch1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_batch2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_batch3\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 150\u001b[1;33m                 \u001b[0mloss_sum\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0macc_sum\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_batch1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_batch2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_batch3\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[0mloss_sum\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0macc_sum\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m40\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    151\u001b[0m                 \u001b[0mcurrent_step\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mglobal_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mglobal_step\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mcurrent_step\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m200\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-53-7c38cab115a0>\u001b[0m in \u001b[0;36mtrain_step\u001b[1;34m(x_batch, y_batch1, y_batch2, y_batch3, loss_sum, acc_sum, interal)\u001b[0m\n\u001b[0;32m     98\u001b[0m                 rs, _, step,  loss, accuracy1, accuracy2, accuracy3, acc = sess.run(\n\u001b[0;32m     99\u001b[0m                     \u001b[1;33m[\u001b[0m\u001b[0mmerged\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_op\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mglobal_step\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maccuracy1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maccuracy2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maccuracy3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0macc\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 100\u001b[1;33m                     feed_dict)\n\u001b[0m\u001b[0;32m    101\u001b[0m                 \u001b[0mtime_str\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misoformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    102\u001b[0m                 \u001b[0macc_sum\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0maccuracy1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccuracy2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccuracy3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0macc\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\richado\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    885\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    886\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 887\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    888\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    889\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\richado\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1108\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1109\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1110\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1111\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1112\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\richado\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1284\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1285\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1286\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1287\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1288\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\richado\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1290\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1291\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1292\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1293\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1294\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\richado\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1275\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1276\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m-> 1277\u001b[1;33m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1278\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1279\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\richado\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1365\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[0;32m   1366\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1367\u001b[1;33m         run_metadata)\n\u001b[0m\u001b[0;32m   1368\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1369\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "y_train = [train_y_label1_one_hot,train_y_label2_one_hot,train_y_label3_one_hot]\n",
    "y_test = [test_y_label1_one_hot,test_y_label2_one_hot,test_y_label3_one_hot]\n",
    "train(x_train, y_train,  x_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(99889, 1191)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_y_label3_one_hot.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(399558, 1192)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y_label3_one_hot.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
