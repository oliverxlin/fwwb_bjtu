{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.utils import shuffle\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "特征如下：['ITEM_NAME' 'TYPE'] \n",
      "\n",
      "数据数目:  499447\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('data/train/trainx.txt', sep='\\t')\n",
    "features = data.columns.values\n",
    "print(\"特征如下：{} \\n\".format(features))\n",
    "print(\"数据数目: \", len(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据清洗\n",
    "* 替换一些字符 类似于 （ ） ‘ ’ _"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ITEM_NAME</th>\n",
       "      <th>TYPE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>腾讯QQ黄钻三个月QQ黄钻3个月季卡官方自动充值可查时间可续费</td>\n",
       "      <td>本地生活--游戏充值--QQ充值</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>腾讯q币充值30元qq币30qb30q币自动充值</td>\n",
       "      <td>本地生活--游戏充值--QQ充值</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>腾讯QQ红钻包月卡 QQ红钻一个月QQ红钻1个月</td>\n",
       "      <td>本地生活--游戏充值--QQ充值</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>腾讯QQ蓝钻贵族31个月直充</td>\n",
       "      <td>本地生活--游戏充值--QQ充值</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>腾讯QQ币148元148QQ币148个直充148Q币148个Q币148个QQB★自动充值</td>\n",
       "      <td>本地生活--游戏充值--QQ充值</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      ITEM_NAME              TYPE\n",
       "0               腾讯QQ黄钻三个月QQ黄钻3个月季卡官方自动充值可查时间可续费  本地生活--游戏充值--QQ充值\n",
       "1                      腾讯q币充值30元qq币30qb30q币自动充值  本地生活--游戏充值--QQ充值\n",
       "2                      腾讯QQ红钻包月卡 QQ红钻一个月QQ红钻1个月  本地生活--游戏充值--QQ充值\n",
       "3                                腾讯QQ蓝钻贵族31个月直充  本地生活--游戏充值--QQ充值\n",
       "4  腾讯QQ币148元148QQ币148个直充148Q币148个Q币148个QQB★自动充值  本地生活--游戏充值--QQ充值"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "from zhon.hanzi import punctuation\n",
    "from string import punctuation as english_punc\n",
    "def  rm_punc(strs):\n",
    "    return re.sub(r\"[{}]+\".format(punctuation + english_punc),\" \",strs)\n",
    "data[features[0]]  = data[features[0]].apply(rm_punc)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练数据数目:   399558\n",
      "测试数据数目:   99889\n",
      "(399558,) (99889,)\n"
     ]
    }
   ],
   "source": [
    "# 因为有的label里本身就存在空格，所以先要把空格去掉，再去掉- - ， 再以空格划分\n",
    "data = shuffle(data)\n",
    "data_y = data[features[1]]\n",
    "\n",
    "# 取训练数据和测试数据\n",
    "train = data.sample(frac= 0.8).reset_index()\n",
    "test = data.sample(frac= 0.2).reset_index()\n",
    "\n",
    "\n",
    "print(\"训练数据数目:  \", len(train))\n",
    "print(\"测试数据数目:  \", len(test))\n",
    "\n",
    "train_x  = train[features[0]]\n",
    "train_y  = train[features[1]]\n",
    "\n",
    "test_x  = test[features[0]]\n",
    "test_y  = test[features[1]]\n",
    "\n",
    "print(train_x.shape, test_x.shape)\n",
    "\n",
    "# 分出数据集 训练集 测试集 数据集用来训练词向量\n",
    "data_y = data_y.apply(lambda x: x.replace(\" \", \"\").replace(\"--\", \" \").replace(\"/\", \"\")).tolist()\n",
    "data_y= [each.split() for each in data_y]\n",
    "\n",
    "train_y = train_y.apply(lambda x: x.replace(\" \", \"\").replace(\"--\", \" \").replace(\"/\", \"\")).tolist()\n",
    "train_y = [each.split() for each in train_y]\n",
    "\n",
    "test_y = test_y.apply(lambda x: x.replace(\" \", \"\").replace(\"--\", \" \").replace(\"/\", \"\")).tolist()\n",
    "test_y = [each.split() for each in test_y]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 分词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba\n",
    "from random import shuffle\n",
    "def participle(data, doshuffle = False):\n",
    "    words = []\n",
    "    for i in range(len(data)):\n",
    "        result=[]\n",
    "        seg_list = jieba.cut(data.iloc[i])\n",
    "        for w in seg_list :#读取每一行分词\n",
    "            if w != \" \":\n",
    "                result.append(w)\n",
    "        # 打乱每条数据的分词\n",
    "        if doshuffle:\n",
    "            shuffle(result)\n",
    "        words.append(result)#将该行分词写入列表形式的总分词列表\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\Richado\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.734 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "# 每条数据的分词结果是个长度不等的词列表\n",
    "word_data = participle(data[features[0]], doshuffle = False)\n",
    "word_data_train = participle(train_x,doshuffle = False)\n",
    "word_data_test = participle(test_x, doshuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'腾讯QQ黄钻三个月QQ黄钻3个月季卡官方自动充值可查时间可续费'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[features[0]][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['货到付款',\n",
       " '一步',\n",
       " '赢',\n",
       " '2014',\n",
       " '秋季',\n",
       " '松糕',\n",
       " '帆布鞋',\n",
       " '女',\n",
       " '韩版潮',\n",
       " '厚底',\n",
       " '松糕鞋',\n",
       " '学生',\n",
       " '高帮',\n",
       " '休闲',\n",
       " '板鞋',\n",
       " '黑',\n",
       " '白色',\n",
       " '35',\n",
       " '正规',\n",
       " '码']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from random import shuffle\n",
    "word_data[8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 标签分离(一个标签分成三个标签)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y = pd.DataFrame(train_y, columns=['label1', 'label2', 'label3'])\n",
    "test_y = pd.DataFrame(test_y, columns=['label1', 'label2', 'label3'])\n",
    "data_y = pd.DataFrame(data_y, columns=['label1', 'label2', 'label3'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_y[\"label1\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "191"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_y[\"label2\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1192"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_y[\"label3\"].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 标签onn_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import to_categorical\n",
    "# 先生成唯一数组\n",
    "y_label1 = data_y['label1'].unique().tolist()\n",
    "y_label2 = data_y['label2'].unique().tolist()\n",
    "y_label3 = data_y['label3'].unique().tolist()\n",
    "\n",
    "# 获取在唯一数组中的索引(训练集和测试集各有3个标签需要处理)\n",
    "train_y_label1_map = train_y['label1'].apply(lambda x: y_label1.index(x))\n",
    "train_y_label2_map = train_y['label2'].apply(lambda x: y_label2.index(x))\n",
    "train_y_label3_map = train_y['label3'].apply(lambda x: y_label3.index(x))\n",
    "test_y_label1_map = test_y['label1'].apply(lambda x: y_label1.index(x))\n",
    "test_y_label2_map = test_y['label2'].apply(lambda x: y_label2.index(x))\n",
    "test_y_label3_map = test_y['label3'].apply(lambda x: y_label3.index(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生成对应one-hot(用做训练模型的标签)\n",
    "train_y_label1_one_hot = to_categorical(train_y_label1_map, 22)\n",
    "train_y_label2_one_hot = to_categorical(train_y_label2_map, 191)\n",
    "train_y_label3_one_hot = to_categorical(train_y_label3_map, 1192)\n",
    "test_y_label1_one_hot = to_categorical(test_y_label1_map, 22)\n",
    "test_y_label2_one_hot = to_categorical(test_y_label2_map, 191)\n",
    "test_y_label3_one_hot = to_categorical(test_y_label3_map, 1192)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 词向量训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "from gensim.models.word2vec import Word2Vec \n",
    "if os.path.exists(\"w2v_model\"):\n",
    "    model = Word2Vec.load(\"w2v_model\")\n",
    "    print(1)\n",
    "else:\n",
    "    sentences = word_data\n",
    "    model= Word2Vec(size=50, window=10, min_count = 1)\n",
    "    model.build_vocab(sentences)\n",
    "    model.train(sentences,total_examples = model.corpus_count,epochs = model.iter)\n",
    "    model.save(\"w2v_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 填充词向量(构造输入输出)（让每个句子拥有同样的维度）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\richado\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\ipykernel_launcher.py:2: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \n",
      "c:\\users\\richado\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\ipykernel_launcher.py:3: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing import sequence\n",
    "x_train = [model[word] for word in word_data_train]\n",
    "x_test = [model[word] for word in word_data_test]\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=18)\n",
    "x_test  = sequence.pad_sequences(x_test, maxlen=18) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 默认参数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## TextCnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class TextCNN(object):\n",
    "    \"\"\"\n",
    "    A CNN for text classification.\n",
    "    Uses an embedding layer, followed by a convolutional, max-pooling and softmax layer.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "      self, maxlen, sequence_length, num_classes, filter_sizes, embedding_size, num_filters, l2_reg_lambda=0.0):\n",
    "\n",
    "        # Placeholders for input, output and dropout\n",
    "        self.input_x = tf.placeholder(tf.float32, [None, sequence_length, embedding_size], name=\"input_x\")\n",
    "        self.input_y1 = tf.placeholder(tf.float32, [None, num_classes[0]], name=\"input_y1\")\n",
    "        self.input_y2 = tf.placeholder(tf.float32, [None, num_classes[1]], name=\"input_y2\")\n",
    "        self.input_y3 = tf.placeholder(tf.float32, [None, num_classes[2]], name=\"input_y3\")\n",
    "        \n",
    "        # Keeping track of l2 regularization loss (optional)\n",
    "        l2_loss = tf.constant(0.0)\n",
    "        keep_prob = 0.9\n",
    "        pooled_outputs = []\n",
    "        self.input_x_expand = tf.expand_dims(self.input_x, -1)\n",
    "        print(self.input_x_expand.shape)\n",
    "        for i, filter_size in enumerate(filter_sizes):\n",
    "            with tf.name_scope(\"conv-maxpool-%s\" % filter_size):\n",
    "                # Convolution Layer\n",
    "                filter_shape = [filter_size, embedding_size, 1, num_filters]\n",
    "                W = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.1), name=\"W\")\n",
    "                b = tf.Variable(tf.constant(0.1, shape=[num_filters]), name=\"b\")\n",
    "                # 卷积\n",
    "                conv = tf.nn.conv2d(\n",
    "                    self.input_x_expand,\n",
    "                    W,\n",
    "                    strides=[1, 1, 1, 1],\n",
    "                    padding=\"VALID\",\n",
    "                    name=\"conv\")\n",
    "        \n",
    "        \n",
    "                h = tf.nn.relu(tf.nn.bias_add(conv, b), name=\"relu\")\n",
    "            \n",
    "                # 最大池化\n",
    "                pooled = tf.nn.max_pool(\n",
    "                    h,\n",
    "                    ksize=[1, sequence_length - filter_size + 1, 1, 1],\n",
    "                    strides=[1, 1, 1, 1],\n",
    "                    padding='VALID',\n",
    "                    name=\"pool\")\n",
    "                pooled = tf.nn.dropout(pooled, keep_prob)\n",
    "                pooled_outputs.append(pooled)\n",
    "                \n",
    "\n",
    "        # Combine all the pooled features\n",
    "        num_filters_total = num_filters * len(filter_sizes)\n",
    "        self.h_pool = tf.concat(pooled_outputs, 3)\n",
    "        print(self.h_pool.shape)\n",
    "        \n",
    "        self.h_pool_flat = tf.reshape(self.h_pool, [-1, num_filters_total])\n",
    "        self.h_drop = self.h_pool_flat\n",
    "        \n",
    "        # Final (unnormalized) scores and predictions\n",
    "        \n",
    "        with tf.name_scope(\"output1\"):\n",
    "            W1 = tf.get_variable(\n",
    "                \"W11\",\n",
    "                shape=[num_filters_total, 64],\n",
    "                initializer=tf.contrib.layers.xavier_initializer())\n",
    "            b1 = tf.Variable(tf.constant(0.1, shape=[64]), name=\"b11\")\n",
    "            self.output = tf.nn.xw_plus_b(self.h_drop, W1, b1, name=\"output1\")\n",
    "            self.output = tf.layers.batch_normalization(self.output, training=is_training)\n",
    "            self.output = tf.nn.relu(self.output)\n",
    "            self.output = tf.nn.dropout(self.output, keep_prob)\n",
    "            W2 = tf.get_variable(\n",
    "                \"W12\",\n",
    "                shape=[64, num_classes[0]],\n",
    "                initializer=tf.contrib.layers.xavier_initializer())\n",
    "            b2 = tf.Variable(tf.constant(0.1, shape=[num_classes[0]]), name=\"b12\")\n",
    "            l2_loss += tf.nn.l2_loss(W1)\n",
    "            l2_loss += tf.nn.l2_loss(b1)\n",
    "            self.scores1 = tf.nn.xw_plus_b(self.output, W2, b2, name=\"scores\")\n",
    "            self.predictions1 = tf.argmax(self.scores1, 1, name=\"predictions\")\n",
    "            \n",
    "        with tf.name_scope(\"output2\"):\n",
    "            W1 = tf.get_variable(\n",
    "                \"W21\",\n",
    "                shape=[num_filters_total, 128],\n",
    "                initializer=tf.contrib.layers.xavier_initializer())\n",
    "            b1 = tf.Variable(tf.constant(0.1, shape=[128]), name=\"b21\")\n",
    "            self.output = tf.nn.xw_plus_b(self.h_drop, W1, b1, name=\"output2\")\n",
    "            self.output = tf.layers.batch_normalization(self.output, training=is_training)\n",
    "            self.output = tf.nn.relu(self.output)\n",
    "            self.output = tf.nn.dropout(self.output, keep_prob)\n",
    "            W2 = tf.get_variable(\n",
    "                \"W22\",\n",
    "                shape=[128, num_classes[1]],\n",
    "                initializer=tf.contrib.layers.xavier_initializer())\n",
    "            b2 = tf.Variable(tf.constant(0.1, shape=[num_classes[1]]), name=\"b22\")\n",
    "            l2_loss += tf.nn.l2_loss(W1)\n",
    "            l2_loss += tf.nn.l2_loss(b1)\n",
    "            self.scores2 = tf.nn.xw_plus_b(self.output, W2, b2, name=\"scores\")\n",
    "            self.predictions2 = tf.argmax(self.scores2, 1, name=\"predictions\")\n",
    "            \n",
    "        with tf.name_scope(\"output3\"):\n",
    "            W1 = tf.get_variable(\n",
    "                \"W31\",\n",
    "                shape=[num_filters_total, 256],\n",
    "                initializer=tf.contrib.layers.xavier_initializer())\n",
    "            b1 = tf.Variable(tf.constant(0.1, shape=[256]), name=\"b31\")\n",
    "            self.output = tf.nn.xw_plus_b(self.h_drop, W1, b1, name=\"output3\")\n",
    "            self.output = tf.layers.batch_normalization(self.output, training=is_training)\n",
    "            self.output = tf.nn.relu(self.output)\n",
    "            self.output = tf.nn.dropout(self.output, keep_prob)\n",
    "            W2 = tf.get_variable(\n",
    "                \"W32\",\n",
    "                shape=[256, num_classes[2]],\n",
    "                initializer=tf.contrib.layers.xavier_initializer())\n",
    "            b2 = tf.Variable(tf.constant(0.1, shape=[num_classes[2]]), name=\"b32\")\n",
    "            l2_loss += tf.nn.l2_loss(W1)\n",
    "            l2_loss += tf.nn.l2_loss(b1)\n",
    "            self.scores3 = tf.nn.xw_plus_b(self.output, W2, b2, name=\"scores\")\n",
    "            self.predictions3 = tf.argmax(self.scores3, 1, name=\"predictions\")\n",
    "        # Calculate mean cross-entropy loss\n",
    "        \n",
    "        with tf.name_scope(\"loss\"):\n",
    "            losses1 = tf.nn.softmax_cross_entropy_with_logits(logits=self.scores1, labels=self.input_y1)\n",
    "            losses2 = tf.nn.softmax_cross_entropy_with_logits(logits=self.scores2, labels=self.input_y2)\n",
    "            losses3 = tf.nn.softmax_cross_entropy_with_logits(logits=self.scores3, labels=self.input_y3)\n",
    "            self.loss = 0.1 * tf.reduce_mean(losses1) + 0.2 * tf.reduce_mean(losses2) + 0.7*tf.reduce_mean(losses3) + l2_reg_lambda * l2_loss\n",
    "            tf.summary.scalar('loss1',tf.reduce_mean(losses1))\n",
    "            tf.summary.scalar('loss2',tf.reduce_mean(losses2))\n",
    "            tf.summary.scalar('loss3',tf.reduce_mean(losses3))\n",
    "            tf.summary.scalar('loss',self.loss)\n",
    "        # Accuracy\n",
    "        with tf.name_scope(\"accuracy\"):\n",
    "            correct_predictions1 = tf.equal(self.predictions1, tf.argmax(self.input_y1, 1))\n",
    "            correct_predictions2 = tf.equal(self.predictions2, tf.argmax(self.input_y2, 1))\n",
    "            correct_predictions3 = tf.equal(self.predictions3, tf.argmax(self.input_y3, 1))\n",
    "#           单独的准确率\n",
    "            self.accuracy1 = tf.reduce_mean(tf.cast(correct_predictions1, \"float\"), name=\"accuracy1\")\n",
    "            self.accuracy2 = tf.reduce_mean(tf.cast(correct_predictions2, \"float\"), name=\"accuracy2\")\n",
    "            self.accuracy3 = tf.reduce_mean(tf.cast(correct_predictions3, \"float\"), name=\"accuracy3\")\n",
    "#           一起的准确率\n",
    "            self.acc = tf.reduce_mean(tf.cast(correct_predictions1, \"float\")*tf.cast(correct_predictions2, \"float\")*tf.cast(correct_predictions3, \"float\"))\n",
    "            tf.summary.scalar('acc1',self.accuracy1)\n",
    "            tf.summary.scalar('acc2',self.accuracy2)\n",
    "            tf.summary.scalar('acc3',self.accuracy3)\n",
    "            tf.summary.scalar('acc',self.acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def batch_iter(data, batch_size, num_epochs, shuffle=True):\n",
    "    \"\"\"\n",
    "    Generates a batch iterator for a dataset.\n",
    "    \"\"\"\n",
    "    data = np.array(data)\n",
    "    data_size = len(data)\n",
    "    num_batches_per_epoch = int((len(data)-1)/batch_size) + 1\n",
    "    for epoch in range(num_epochs):\n",
    "        # Shuffle the data at each epoch\n",
    "        if shuffle:\n",
    "            shuffle_indices = np.random.permutation(np.arange(data_size))\n",
    "            shuffled_data = data[shuffle_indices]\n",
    "        else:\n",
    "            shuffled_data = data\n",
    "  \n",
    "        for batch_num in range(num_batches_per_epoch):\n",
    "            start_index = batch_num * batch_size\n",
    "            end_index = np.min(((batch_num + 1) * batch_size, data_size))\n",
    "            yield shuffled_data[start_index:end_index]\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "from tensorflow.contrib import learn\n",
    "\n",
    "# 特征数这里是第三级标签的特征数\n",
    "max_features = 1192\n",
    "\n",
    "# 句子填充的长度\n",
    "maxlen = 16\n",
    "\n",
    "batch_size = 256 \n",
    "\n",
    "epochs = 100\n",
    "\n",
    "is_training = True\n",
    "# 词向量长度\n",
    "embedding_dims = 50\n",
    "\n",
    "filters = 64\n",
    "kernel_size = 5\n",
    "hidden_dims = 128\n",
    "epochs = 100\n",
    "\n",
    "# Parameters\n",
    "# ==================================================\n",
    "\n",
    "# Data loading params\n",
    "# tf.flags.DEFINE_float(\"dev_sample_percentage\", .1, \"Percentage of the training data to use for validation\")\n",
    "# tf.flags.DEFINE_string(\"positive_data_file\", \"./data/rt-polaritydata/rt-polarity.pos\", \"Data source for the positive data.\")\n",
    "# tf.flags.DEFINE_string(\"negative_data_file\", \"./data/rt-polaritydata/rt-polarity.neg\", \"Data source for the negative data.\")\n",
    "\n",
    "# Model Hyperparameters\n",
    "# tf.flags.DEFINE_integer(\"embedding_dim\", 128, \"Dimensionality of character embedding (default: 128)\")\n",
    "# tf.flags.DEFINE_string(\"filter_sizes\", \"3,4,5\", \"Comma-separated filter sizes (default: '3,4,5')\")\n",
    "# tf.flags.DEFINE_integer(\"num_filters\", 128, \"Number of filters per filter size (default: 128)\")\n",
    "# tf.flags.DEFINE_float(\"dropout_keep_prob\", 0.5, \"Dropout keep probability (default: 0.5)\")\n",
    "# tf.flags.DEFINE_float(\"l2_reg_lambda\", 0.0, \"L2 regularization lambda (default: 0.0)\")\n",
    "\n",
    "# Training parameters\n",
    "# tf.flags.DEFINE_integer(\"batch_size\", 64, \"Batch Size (default: 64)\")\n",
    "# tf.flags.DEFINE_integer(\"num_epochs\", 200, \"Number of training epochs (default: 200)\")\n",
    "# tf.flags.DEFINE_integer(\"evaluate_every\", 100, \"Evaluate model on dev set after this many steps (default: 100)\")\n",
    "# tf.flags.DEFINE_integer(\"checkpoint_every\", 100, \"Save model after this many steps (default: 100)\")\n",
    "# tf.flags.DEFINE_integer(\"num_checkpoints\", 5, \"Number of checkpoints to store (default: 5)\")\n",
    "# Misc Parameters\n",
    "# tf.flags.DEFINE_boolean(\"allow_soft_placement\", True, \"Allow device soft device placement\")\n",
    "# tf.flags.DEFINE_boolean(\"log_device_placement\", False, \"Log placement of ops on devices\")\n",
    "\n",
    "FLAGS = tf.flags.FLAGS\n",
    "\n",
    "def train(x_train, y_train, x_dev, y_dev):\n",
    "    # Training\n",
    "    # ==================================================\n",
    "\n",
    "    with tf.Graph().as_default():\n",
    "        session_conf = tf.ConfigProto(\n",
    "#           allow_soft_placement=FLAGS.allow_soft_placement,\n",
    "#           log_device_placement=FLAGS.log_device_placement\n",
    "        )\n",
    "        sess = tf.Session(config=session_conf)\n",
    "        with sess.as_default():\n",
    "            cnn = TextCNN(maxlen = 10, sequence_length = 18, num_classes = [22, 191, 1192], filter_sizes = [1,2, 3, 4, 5], embedding_size = 50, num_filters = 64)\n",
    "\n",
    "            # Define Training procedure\n",
    "\n",
    "            global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "#           学习率衰减\n",
    "            learning_rate = tf.train.exponential_decay(\n",
    "                0.01,\n",
    "                global_step,\n",
    "                x_train.shape[0] / batch_size,\n",
    "                0.99,\n",
    "                staircase=True,)\n",
    "            optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "            grads_and_vars = optimizer.compute_gradients(cnn.loss)\n",
    "            train_op = optimizer.apply_gradients(grads_and_vars, global_step=global_step)\n",
    "\n",
    "            # Initialize all variables\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            \n",
    "            merged = tf.summary.merge_all()\n",
    "            \n",
    "            writer = tf.summary.FileWriter(\"fwwb\",sess.graph)\n",
    "            acc_sum = np.array([0.0,0.0,0.0,0.0])\n",
    "            loss_sum = 0\n",
    "            \n",
    "            def train_step(x_batch, y_batch1, y_batch2, y_batch3, loss_sum, acc_sum, interal = 10):\n",
    "  \n",
    "                \"\"\"\n",
    "                A single training step\n",
    "                \"\"\"\n",
    "                feed_dict = {\n",
    "                  cnn.input_x: x_batch,\n",
    "                  cnn.input_y1: y_batch1,\n",
    "                  cnn.input_y2: y_batch2,\n",
    "                  cnn.input_y3: y_batch3\n",
    "                }\n",
    "#         三个标签单独的准确率， 一起的准确率共四个准确率\n",
    "                rs, _, step,  loss, accuracy1, accuracy2, accuracy3, acc = sess.run(\n",
    "                    [merged, train_op, global_step, cnn.loss, cnn.accuracy1, cnn.accuracy2, cnn.accuracy3, cnn.acc],\n",
    "                    feed_dict)\n",
    "                time_str = datetime.datetime.now().isoformat()\n",
    "                acc_sum += [accuracy1, accuracy2, accuracy3, acc]\n",
    "                loss_sum += loss\n",
    "                if step % interal == 0:\n",
    "                    print(\"{}: step {}, loss {:g}, acc {}\".format(time_str, step, loss_sum/interal, acc_sum/interal))\n",
    "                    loss_sum = 0\n",
    "                    acc_sum = np.array([0.0,0.0,0.0,0.0])\n",
    "                    writer.add_summary(rs, step)\n",
    "                return loss_sum, acc_sum\n",
    "            # 评估步骤\n",
    "            def dev_step(x_batch, y_batch):\n",
    "                \"\"\"\n",
    "                Evaluates model on a dev set\n",
    "                \"\"\"\n",
    "\n",
    "                dev_batches = batch_iter(\n",
    "                    list(zip(x_batch, y_batch[0], y_batch[1], y_batch[2])), 640, 1)\n",
    "                step = 0\n",
    "                loss = 0\n",
    "                accuracy = np.array([0.0, 0.0, 0.0,0.0])\n",
    "                for batch in dev_batches:\n",
    "                    x_batch_dev, y_batch1_dev, y_batch2_dev, y_batch3_dev = zip(*batch)\n",
    "                    feed_dict = {\n",
    "                      cnn.input_x: x_batch_dev,\n",
    "                      cnn.input_y1: y_batch1_dev,\n",
    "                      cnn.input_y2: y_batch2_dev,\n",
    "                      cnn.input_y3: y_batch3_dev\n",
    "                    }\n",
    "                    step,  temp_loss, temp_accuracy1, temp_accuracy2, temp_accuracy3 , acc= sess.run(\n",
    "                        [global_step,cnn.loss, cnn.accuracy1, cnn.accuracy2, cnn.accuracy3, cnn.acc],\n",
    "                        feed_dict)\n",
    "                    accuracy[0] += temp_accuracy1 * len(x_batch_dev)\n",
    "                    accuracy[1] += temp_accuracy2 * len(x_batch_dev)\n",
    "                    accuracy[2] += temp_accuracy3 * len(x_batch_dev)\n",
    "                    accuracy[3] += acc * len(x_batch_dev)\n",
    "                    loss += temp_loss * len(x_batch_dev)\n",
    "                accuracy /= x_batch.shape[0]\n",
    "                loss /= x_batch.shape[0]\n",
    "                time_str = datetime.datetime.now().isoformat()                    \n",
    "\n",
    "                print(\"{}: step {}, loss {:g}, acc {} \".format(time_str, step, loss, accuracy))\n",
    "\n",
    "\n",
    "            # Generate batches\n",
    "            batches = batch_iter(\n",
    "                list(zip(x_train, y_train[0],y_train[1], y_train[2] )), batch_size, 200)\n",
    "            # Training loop. For each batch...\n",
    "            for batch in batches:\n",
    "                x_batch, y_batch1, y_batch2, y_batch3 = zip(*batch)\n",
    "                loss_sum, acc_sum = train_step(x_batch, y_batch1, y_batch2, y_batch3,  loss_sum, acc_sum, 40)\n",
    "                current_step = tf.train.global_step(sess, global_step)\n",
    "                if current_step % 200 == 0:\n",
    "                    print(\"\\nEvaluation:\")\n",
    "                    dev_step(x_dev, y_dev)\n",
    "                    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 18, 50, 1)\n",
      "(?, 1, 1, 320)\n",
      "2019-02-22T00:00:48.701899: step 40, loss 2.81225, acc [0.73701172 0.57236328 0.45078125 0.38056641]\n",
      "2019-02-22T00:00:50.010345: step 80, loss 1.75888, acc [0.82949219 0.71572266 0.60439453 0.53583984]\n",
      "2019-02-22T00:00:51.284568: step 120, loss 1.58793, acc [0.83134766 0.72978516 0.61855469 0.55146484]\n",
      "2019-02-22T00:00:52.542918: step 160, loss 1.46455, acc [0.8359375  0.74003906 0.6421875  0.57509766]\n",
      "2019-02-22T00:00:53.807220: step 200, loss 1.38436, acc [0.846875   0.75048828 0.65107422 0.58085937]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:00:58.070829: step 200, loss 1.35978, acc [0.84823154 0.74875112 0.65562775 0.58517955] \n",
      "\n",
      "2019-02-22T00:00:59.430365: step 240, loss 1.33695, acc [0.85214844 0.76015625 0.66240234 0.59619141]\n",
      "2019-02-22T00:01:00.733355: step 280, loss 1.31144, acc [0.85517578 0.75849609 0.66269531 0.59501953]\n",
      "2019-02-22T00:01:02.018489: step 320, loss 1.27594, acc [0.84853516 0.76416016 0.66933594 0.59785156]\n",
      "2019-02-22T00:01:03.300649: step 360, loss 1.25665, acc [0.85244141 0.76044922 0.67626953 0.60664063]\n",
      "2019-02-22T00:01:04.613556: step 400, loss 1.25641, acc [0.85166016 0.76435547 0.67519531 0.60908203]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:01:08.826574: step 400, loss 1.21134, acc [0.85786223 0.76475888 0.68175675 0.60811501] \n",
      "\n",
      "2019-02-22T00:01:10.191068: step 440, loss 1.19432, acc [0.86787109 0.77714844 0.69072266 0.62763672]\n",
      "2019-02-22T00:01:11.463803: step 480, loss 1.25266, acc [0.85605469 0.76298828 0.67197266 0.60761719]\n",
      "2019-02-22T00:01:12.752410: step 520, loss 1.23528, acc [0.86123047 0.77089844 0.67705078 0.61503906]\n",
      "2019-02-22T00:01:14.077732: step 560, loss 1.21237, acc [0.85859375 0.76787109 0.67421875 0.60888672]\n",
      "2019-02-22T00:01:15.397077: step 600, loss 1.1753, acc [0.86240234 0.77265625 0.68662109 0.62167969]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:01:19.417152: step 600, loss 1.13396, acc [0.86221706 0.77983562 0.69718387 0.62870787] \n",
      "\n",
      "2019-02-22T00:01:20.742461: step 640, loss 1.18418, acc [0.86005859 0.775      0.68652344 0.62246094]\n",
      "2019-02-22T00:01:22.022809: step 680, loss 1.13329, acc [0.86181641 0.78330078 0.69785156 0.62900391]\n",
      "2019-02-22T00:01:23.266768: step 720, loss 1.13199, acc [0.86728516 0.78085938 0.68730469 0.62353516]\n",
      "2019-02-22T00:01:24.515695: step 760, loss 1.17619, acc [0.865625   0.77421875 0.69179687 0.62773437]\n",
      "2019-02-22T00:01:25.762760: step 800, loss 1.15409, acc [0.86904297 0.77783203 0.69306641 0.63085938]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:01:29.772317: step 800, loss 1.08284, acc [0.86885443 0.78838511 0.70863659 0.64353432] \n",
      "\n",
      "2019-02-22T00:01:31.092370: step 840, loss 1.14353, acc [0.87138672 0.7875     0.70166016 0.64082031]\n",
      "2019-02-22T00:01:32.319472: step 880, loss 1.15033, acc [0.86181641 0.77998047 0.69658203 0.6328125 ]\n",
      "2019-02-22T00:01:33.550046: step 920, loss 1.1398, acc [0.86474609 0.77666016 0.69365234 0.62666016]\n",
      "2019-02-22T00:01:34.770700: step 960, loss 1.14015, acc [0.86142578 0.78154297 0.69814453 0.63095703]\n",
      "2019-02-22T00:01:36.007720: step 1000, loss 1.10537, acc [0.87050781 0.78964844 0.7046875  0.63759766]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:01:39.968771: step 1000, loss 1.05176, acc [0.87333941 0.79399133 0.71473335 0.65165334] \n",
      "\n",
      "2019-02-22T00:01:41.424530: step 1040, loss 1.0947, acc [0.86826172 0.79208984 0.70908203 0.64677734]\n",
      "2019-02-22T00:01:42.661553: step 1080, loss 1.08735, acc [0.87578125 0.79101562 0.70917969 0.64667969]\n",
      "2019-02-22T00:01:43.876255: step 1120, loss 1.15113, acc [0.86396484 0.77861328 0.69804687 0.63173828]\n",
      "2019-02-22T00:01:45.086329: step 1160, loss 1.06141, acc [0.8765625  0.79648438 0.7125     0.65107422]\n",
      "2019-02-22T00:01:46.317021: step 1200, loss 1.07084, acc [0.86748047 0.79208984 0.71328125 0.64892578]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:01:50.455638: step 1200, loss 1.03945, acc [0.87136722 0.79358087 0.71645527 0.65101262] \n",
      "\n",
      "2019-02-22T00:01:51.777971: step 1240, loss 1.08822, acc [0.86933594 0.78525391 0.70722656 0.64462891]\n",
      "2019-02-22T00:01:53.016004: step 1280, loss 1.05715, acc [0.87080078 0.79560547 0.71220703 0.64697266]\n",
      "2019-02-22T00:01:54.253008: step 1320, loss 1.10369, acc [0.86630859 0.78896484 0.71005859 0.64755859]\n",
      "2019-02-22T00:01:55.481102: step 1360, loss 1.0798, acc [0.86425781 0.78916016 0.71191406 0.64335937]\n",
      "2019-02-22T00:01:56.706221: step 1400, loss 1.07825, acc [0.87128906 0.79023438 0.71298828 0.64941406]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:02:00.602332: step 1400, loss 1.0129, acc [0.87330937 0.79462203 0.72137072 0.65394588] \n",
      "\n",
      "2019-02-22T00:02:02.015688: step 1440, loss 1.0846, acc [0.87148437 0.79140625 0.70996094 0.64970703]\n",
      "2019-02-22T00:02:03.227415: step 1480, loss 1.05323, acc [0.86875    0.79042969 0.71513672 0.64921875]\n",
      "2019-02-22T00:02:04.442670: step 1520, loss 1.07241, acc [0.87382812 0.79052734 0.71542969 0.65585938]\n",
      "2019-02-22T00:02:05.649379: step 1560, loss 1.0656, acc [0.86972656 0.78974609 0.71416016 0.65361328]\n",
      "2019-02-22T00:02:07.053057: step 1600, loss 0.962908, acc [0.87907    0.8036409  0.72607521 0.66383464]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:02:11.045895: step 1600, loss 0.999484, acc [0.87082662 0.7982861  0.7231527  0.65616835] \n",
      "\n",
      "2019-02-22T00:02:12.315608: step 1640, loss 1.00846, acc [0.86767578 0.79462891 0.72529297 0.65839844]\n",
      "2019-02-22T00:02:13.492119: step 1680, loss 0.997137, acc [0.87480469 0.79726562 0.7296875  0.6671875 ]\n",
      "2019-02-22T00:02:14.662682: step 1720, loss 0.996089, acc [0.87275391 0.79160156 0.72060547 0.65449219]\n",
      "2019-02-22T00:02:15.913589: step 1760, loss 1.01255, acc [0.86835938 0.79140625 0.71611328 0.64648438]\n",
      "2019-02-22T00:02:17.088115: step 1800, loss 1.04683, acc [0.86572266 0.78789062 0.71298828 0.64775391]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:02:21.005568: step 1800, loss 0.972633, acc [0.87555186 0.79970768 0.7319825  0.66357657] \n",
      "\n",
      "2019-02-22T00:02:22.259898: step 1840, loss 0.971845, acc [0.87382812 0.80214844 0.73359375 0.6671875 ]\n",
      "2019-02-22T00:02:23.432937: step 1880, loss 1.00157, acc [0.87099609 0.79589844 0.72363281 0.65693359]\n",
      "2019-02-22T00:02:24.664008: step 1920, loss 1.00443, acc [0.875      0.80332031 0.72226563 0.65859375]\n",
      "2019-02-22T00:02:25.836584: step 1960, loss 0.966753, acc [0.8765625  0.80205078 0.73476562 0.66699219]\n",
      "2019-02-22T00:02:27.011572: step 2000, loss 0.998566, acc [0.87597656 0.79794922 0.72431641 0.65761719]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:02:30.942864: step 2000, loss 0.964257, acc [0.8762126  0.80444293 0.73454535 0.668472  ] \n",
      "\n",
      "2019-02-22T00:02:32.196761: step 2040, loss 0.974858, acc [0.87744141 0.80625    0.72841797 0.66552734]\n",
      "2019-02-22T00:02:33.383178: step 2080, loss 0.981871, acc [0.88037109 0.80175781 0.72900391 0.66972656]\n",
      "2019-02-22T00:02:34.572090: step 2120, loss 0.983855, acc [0.875      0.80341797 0.72724609 0.6625    ]\n",
      "2019-02-22T00:02:35.798199: step 2160, loss 0.9974, acc [0.87304688 0.79892578 0.72607422 0.66074219]\n",
      "2019-02-22T00:02:36.979670: step 2200, loss 0.974117, acc [0.87666016 0.80439453 0.72734375 0.66445312]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:02:40.995777: step 2200, loss 0.960054, acc [0.87570203 0.8051337  0.73276337 0.66918279] \n",
      "\n",
      "2019-02-22T00:02:42.267093: step 2240, loss 1.01763, acc [0.87207031 0.79013672 0.72197266 0.65869141]\n",
      "2019-02-22T00:02:43.505533: step 2280, loss 0.992383, acc [0.87900391 0.79892578 0.73076172 0.66484375]\n",
      "2019-02-22T00:02:44.685019: step 2320, loss 0.960347, acc [0.87402344 0.80664062 0.73388672 0.67216797]\n",
      "2019-02-22T00:02:45.866516: step 2360, loss 0.973372, acc [0.87460938 0.80341797 0.73154297 0.66933594]\n",
      "2019-02-22T00:02:47.053771: step 2400, loss 1.03082, acc [0.87050781 0.79775391 0.72460938 0.66201172]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:02:50.937940: step 2400, loss 0.947931, acc [0.88027711 0.80575439 0.73510597 0.66972339] \n",
      "\n",
      "2019-02-22T00:02:52.196291: step 2440, loss 0.991793, acc [0.87763672 0.79765625 0.72744141 0.66367188]\n",
      "2019-02-22T00:02:53.373298: step 2480, loss 0.97372, acc [0.88125    0.80673828 0.73046875 0.66865234]\n",
      "2019-02-22T00:02:54.593000: step 2520, loss 0.959726, acc [0.87890625 0.80615234 0.73486328 0.66923828]\n",
      "2019-02-22T00:02:55.817581: step 2560, loss 0.986938, acc [0.87597656 0.79863281 0.73320312 0.66738281]\n",
      "2019-02-22T00:02:56.988637: step 2600, loss 0.948528, acc [0.87978516 0.80800781 0.73515625 0.67041016]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:03:00.911657: step 2600, loss 0.94163, acc [0.87949624 0.80721601 0.73548639 0.67240637] \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-02-22T00:03:02.168972: step 2640, loss 0.972568, acc [0.87910156 0.80556641 0.72763672 0.66816406]\n",
      "2019-02-22T00:03:03.352924: step 2680, loss 0.995203, acc [0.87802734 0.80253906 0.728125   0.66337891]\n",
      "2019-02-22T00:03:04.530425: step 2720, loss 0.984882, acc [0.87880859 0.80634766 0.73076172 0.66689453]\n",
      "2019-02-22T00:03:05.701976: step 2760, loss 0.972547, acc [0.88242188 0.80537109 0.73007813 0.66660156]\n",
      "2019-02-22T00:03:06.876997: step 2800, loss 0.973306, acc [0.88222656 0.80947266 0.73242188 0.66992188]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:03:10.817713: step 2800, loss 0.924814, acc [0.87973651 0.80943848 0.74170329 0.67841304] \n",
      "\n",
      "2019-02-22T00:03:12.188159: step 2840, loss 1.00711, acc [0.87529297 0.79785156 0.72304687 0.66015625]\n",
      "2019-02-22T00:03:13.356732: step 2880, loss 0.983258, acc [0.87734375 0.80390625 0.728125   0.66347656]\n",
      "2019-02-22T00:03:14.533739: step 2920, loss 0.9785, acc [0.88164062 0.80087891 0.73417969 0.66865234]\n",
      "2019-02-22T00:03:15.705289: step 2960, loss 0.973722, acc [0.88173828 0.80791016 0.734375   0.67011719]\n",
      "2019-02-22T00:03:16.877336: step 3000, loss 0.999726, acc [0.87421875 0.80185547 0.7296875  0.66298828]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:03:20.912784: step 3000, loss 0.915913, acc [0.88157855 0.81056973 0.74467659 0.67974452] \n",
      "\n",
      "2019-02-22T00:03:22.274303: step 3040, loss 0.972117, acc [0.88115234 0.80449219 0.73173828 0.6703125 ]\n",
      "2019-02-22T00:03:23.452301: step 3080, loss 0.979512, acc [0.88232422 0.80517578 0.73125    0.66953125]\n",
      "2019-02-22T00:03:24.623355: step 3120, loss 0.966646, acc [0.8765625  0.80205078 0.73642578 0.67138672]\n",
      "2019-02-22T00:03:25.943705: step 3160, loss 0.929521, acc [0.87471591 0.80491536 0.73697719 0.67231889]\n",
      "2019-02-22T00:03:27.166344: step 3200, loss 0.914311, acc [0.87607422 0.8046875  0.74394531 0.67695313]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:03:31.539115: step 3200, loss 0.911672, acc [0.88220925 0.81051968 0.74438627 0.67828289] \n",
      "\n",
      "2019-02-22T00:03:32.810383: step 3240, loss 0.924407, acc [0.88349609 0.80537109 0.74628906 0.68066406]\n",
      "2019-02-22T00:03:34.035439: step 3280, loss 0.913564, acc [0.88046875 0.81035156 0.74208984 0.67392578]\n",
      "2019-02-22T00:03:35.294780: step 3320, loss 0.923489, acc [0.87851563 0.81113281 0.74306641 0.67773438]\n",
      "2019-02-22T00:03:36.480220: step 3360, loss 0.904218, acc [0.87939453 0.80878906 0.74492187 0.67685547]\n",
      "2019-02-22T00:03:37.737081: step 3400, loss 0.919507, acc [0.88535156 0.80507812 0.74052734 0.67431641]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:03:41.687749: step 3400, loss 0.90665, acc [0.88244952 0.81226161 0.74327504 0.67839301] \n",
      "\n",
      "2019-02-22T00:03:43.002612: step 3440, loss 0.898933, acc [0.88564453 0.81044922 0.74335938 0.67705078]\n",
      "2019-02-22T00:03:44.184079: step 3480, loss 0.920922, acc [0.88720703 0.80722656 0.74345703 0.67792969]\n",
      "2019-02-22T00:03:45.362574: step 3520, loss 0.915158, acc [0.87939453 0.80800781 0.74492187 0.67871094]\n",
      "2019-02-22T00:03:46.596622: step 3560, loss 0.912093, acc [0.88447266 0.81201172 0.7453125  0.67734375]\n",
      "2019-02-22T00:03:47.786522: step 3600, loss 0.914852, acc [0.87705078 0.81123047 0.73857422 0.67841797]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:03:51.735172: step 3600, loss 0.903132, acc [0.88240947 0.81140065 0.74680896 0.68019502] \n",
      "\n",
      "2019-02-22T00:03:52.989556: step 3640, loss 0.93436, acc [0.88017578 0.80429688 0.73164063 0.66708984]\n",
      "2019-02-22T00:03:54.167058: step 3680, loss 0.909778, acc [0.88330078 0.81171875 0.74296875 0.67929688]\n",
      "2019-02-22T00:03:55.338112: step 3720, loss 0.920858, acc [0.88896484 0.81367188 0.73876953 0.67626953]\n",
      "2019-02-22T00:03:56.510654: step 3760, loss 0.915361, acc [0.88027344 0.80654297 0.74199219 0.67539063]\n",
      "2019-02-22T00:03:57.697087: step 3800, loss 0.913473, acc [0.87705078 0.81357422 0.74248047 0.67958984]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:04:01.678505: step 3800, loss 0.891554, acc [0.88314029 0.81254192 0.74780006 0.68232738] \n",
      "\n",
      "2019-02-22T00:04:02.929382: step 3840, loss 0.929667, acc [0.88369141 0.80810547 0.7359375  0.67480469]\n",
      "2019-02-22T00:04:04.114325: step 3880, loss 0.906746, acc [0.88203125 0.81357422 0.74755859 0.68427734]\n",
      "2019-02-22T00:04:05.379655: step 3920, loss 0.903644, acc [0.88339844 0.81142578 0.74667969 0.68261719]\n",
      "2019-02-22T00:04:06.550672: step 3960, loss 0.932828, acc [0.87587891 0.80820313 0.73964844 0.67519531]\n",
      "2019-02-22T00:04:07.717759: step 4000, loss 0.926675, acc [0.88105469 0.81064453 0.74042969 0.67460937]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:04:11.865802: step 4000, loss 0.885703, acc [0.88430158 0.81349298 0.74944188 0.68423951] \n",
      "\n",
      "2019-02-22T00:04:13.228310: step 4040, loss 0.939184, acc [0.87910156 0.80869141 0.74121094 0.67666016]\n",
      "2019-02-22T00:04:14.410277: step 4080, loss 0.92947, acc [0.8796875  0.81435547 0.74365234 0.67939453]\n",
      "2019-02-22T00:04:15.661685: step 4120, loss 0.912291, acc [0.87734375 0.81074219 0.74433594 0.68066406]\n",
      "2019-02-22T00:04:16.874403: step 4160, loss 0.932052, acc [0.88085938 0.80839844 0.740625   0.68164062]\n",
      "2019-02-22T00:04:18.121343: step 4200, loss 0.924847, acc [0.87978516 0.8015625  0.74384766 0.67724609]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:04:22.210363: step 4200, loss 0.886908, acc [0.88305019 0.81299242 0.74975222 0.68104596] \n",
      "\n",
      "2019-02-22T00:04:23.502936: step 4240, loss 0.899701, acc [0.88105469 0.81523437 0.74589844 0.67998047]\n",
      "2019-02-22T00:04:24.684440: step 4280, loss 0.900617, acc [0.88261719 0.80839844 0.75039062 0.68154297]\n",
      "2019-02-22T00:04:25.879766: step 4320, loss 0.92275, acc [0.8796875  0.81269531 0.74433594 0.68144531]\n",
      "2019-02-22T00:04:27.165893: step 4360, loss 0.920386, acc [0.87978516 0.80556641 0.74179688 0.67939453]\n",
      "2019-02-22T00:04:28.375138: step 4400, loss 0.917135, acc [0.88613281 0.81035156 0.74550781 0.684375  ]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:04:32.334256: step 4400, loss 0.872725, acc [0.88416142 0.81597573 0.75312597 0.68667221] \n",
      "\n",
      "2019-02-22T00:04:33.715066: step 4440, loss 0.914141, acc [0.88066406 0.80498047 0.74257812 0.67646484]\n",
      "2019-02-22T00:04:34.886649: step 4480, loss 0.932407, acc [0.87841797 0.80732422 0.74228516 0.67460937]\n",
      "2019-02-22T00:04:36.057671: step 4520, loss 0.925008, acc [0.88251953 0.81279297 0.74970703 0.68261719]\n",
      "2019-02-22T00:04:37.227237: step 4560, loss 0.932249, acc [0.87763672 0.80537109 0.74111328 0.67451172]\n",
      "2019-02-22T00:04:38.399778: step 4600, loss 0.936573, acc [0.87880859 0.81240234 0.74189453 0.67451172]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:04:42.295393: step 4600, loss 0.874419, acc [0.88609356 0.81659642 0.75167436 0.68671225] \n",
      "\n",
      "2019-02-22T00:04:43.584956: step 4640, loss 0.964896, acc [0.87939453 0.80488281 0.73222656 0.66728516]\n",
      "2019-02-22T00:04:44.774902: step 4680, loss 0.92624, acc [0.88613281 0.8125     0.74023438 0.67851562]\n",
      "2019-02-22T00:04:46.111081: step 4720, loss 0.844152, acc [0.8864968  0.81787504 0.76124132 0.69030934]\n",
      "2019-02-22T00:04:47.271222: step 4760, loss 0.849385, acc [0.88955078 0.81738281 0.76445312 0.69833984]\n",
      "2019-02-22T00:04:48.506261: step 4800, loss 0.867864, acc [0.88730469 0.81533203 0.74980469 0.68466797]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:04:52.390432: step 4800, loss 0.863723, acc [0.88361081 0.81519487 0.75592908 0.68854429] \n",
      "\n",
      "2019-02-22T00:04:53.631422: step 4840, loss 0.86779, acc [0.88408203 0.81923828 0.75566406 0.68984375]\n",
      "2019-02-22T00:04:54.819340: step 4880, loss 0.838986, acc [0.88115234 0.82119141 0.75703125 0.69052734]\n",
      "2019-02-22T00:04:56.007257: step 4920, loss 0.872466, acc [0.88222656 0.81357422 0.75048828 0.68310547]\n",
      "2019-02-22T00:04:57.171374: step 4960, loss 0.897215, acc [0.87949219 0.80673828 0.75107422 0.68007812]\n",
      "2019-02-22T00:04:58.330520: step 5000, loss 0.86583, acc [0.88212891 0.81835938 0.75117188 0.68066406]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:05:02.255362: step 5000, loss 0.86086, acc [0.88562304 0.81661644 0.75865211 0.69187799] \n",
      "\n",
      "2019-02-22T00:05:03.509248: step 5040, loss 0.865444, acc [0.88300781 0.81855469 0.75546875 0.68730469]\n",
      "2019-02-22T00:05:04.699646: step 5080, loss 0.858211, acc [0.88525391 0.81416016 0.76210937 0.69423828]\n",
      "2019-02-22T00:05:05.933195: step 5120, loss 0.879588, acc [0.87626953 0.80869141 0.75068359 0.68339844]\n",
      "2019-02-22T00:05:07.108715: step 5160, loss 0.855192, acc [0.88447266 0.81933594 0.75605469 0.69150391]\n",
      "2019-02-22T00:05:08.300600: step 5200, loss 0.854247, acc [0.88789063 0.81884766 0.75419922 0.68847656]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:05:12.273595: step 5200, loss 0.864938, acc [0.88591336 0.81463424 0.75524833 0.68768333] \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-02-22T00:05:13.524466: step 5240, loss 0.881746, acc [0.88535156 0.81308594 0.75244141 0.68583984]\n",
      "2019-02-22T00:05:14.701968: step 5280, loss 0.877933, acc [0.88388672 0.81298828 0.74853516 0.68105469]\n",
      "2019-02-22T00:05:15.929566: step 5320, loss 0.87886, acc [0.88457031 0.81435547 0.75566406 0.68505859]\n",
      "2019-02-22T00:05:17.098140: step 5360, loss 0.868386, acc [0.89111328 0.81992188 0.75527344 0.68925781]\n",
      "2019-02-22T00:05:18.260267: step 5400, loss 0.880052, acc [0.88652344 0.81738281 0.75185547 0.68769531]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:05:22.149431: step 5400, loss 0.859703, acc [0.88581325 0.81609587 0.75650972 0.68893472] \n",
      "\n",
      "2019-02-22T00:05:23.387410: step 5440, loss 0.892311, acc [0.88085938 0.81396484 0.74892578 0.6828125 ]\n",
      "2019-02-22T00:05:24.545570: step 5480, loss 0.89723, acc [0.87792969 0.81210938 0.746875   0.68320313]\n",
      "2019-02-22T00:05:25.770689: step 5520, loss 0.885205, acc [0.88691406 0.81386719 0.75224609 0.68789062]\n",
      "2019-02-22T00:05:26.944719: step 5560, loss 0.893426, acc [0.88408203 0.81123047 0.74707031 0.68671875]\n",
      "2019-02-22T00:05:28.105357: step 5600, loss 0.891747, acc [0.88134766 0.81474609 0.74824219 0.68261719]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:05:32.025240: step 5600, loss 0.852083, acc [0.88624373 0.8173873  0.75862207 0.69104706] \n",
      "\n",
      "2019-02-22T00:05:33.274661: step 5640, loss 0.899464, acc [0.87773437 0.81318359 0.74580078 0.67822266]\n",
      "2019-02-22T00:05:34.434308: step 5680, loss 0.902884, acc [0.88837891 0.81123047 0.74609375 0.68232422]\n",
      "2019-02-22T00:05:35.644050: step 5720, loss 0.899397, acc [0.87958984 0.81191406 0.75205078 0.68642578]\n",
      "2019-02-22T00:05:36.825023: step 5760, loss 0.863691, acc [0.88535156 0.82011719 0.75996094 0.69228516]\n",
      "2019-02-22T00:05:37.988143: step 5800, loss 0.878269, acc [0.88828125 0.81660156 0.74707031 0.684375  ]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:05:41.886732: step 5800, loss 0.846451, acc [0.88483217 0.81896906 0.75948303 0.69186797] \n",
      "\n",
      "2019-02-22T00:05:43.135127: step 5840, loss 0.904666, acc [0.87626953 0.81152344 0.74453125 0.67880859]\n",
      "2019-02-22T00:05:44.301718: step 5880, loss 0.894336, acc [0.884375   0.81708984 0.74638672 0.68144531]\n",
      "2019-02-22T00:05:45.468805: step 5920, loss 0.867172, acc [0.88183594 0.81464844 0.75224609 0.68046875]\n",
      "2019-02-22T00:05:46.635891: step 5960, loss 0.866414, acc [0.88837891 0.82099609 0.75449219 0.68935547]\n",
      "2019-02-22T00:05:47.808432: step 6000, loss 0.874049, acc [0.88027344 0.81513672 0.75273437 0.68867188]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:05:51.709995: step 6000, loss 0.843785, acc [0.88638388 0.82113146 0.75984343 0.69353983] \n",
      "\n",
      "2019-02-22T00:05:53.024361: step 6040, loss 0.876389, acc [0.88466797 0.81298828 0.74589844 0.67998047]\n",
      "2019-02-22T00:05:54.183015: step 6080, loss 0.857852, acc [0.88837891 0.81591797 0.75449219 0.69072266]\n",
      "2019-02-22T00:05:55.341669: step 6120, loss 0.872465, acc [0.88271484 0.81767578 0.75126953 0.68710938]\n",
      "2019-02-22T00:05:56.500325: step 6160, loss 0.847904, acc [0.88964844 0.82050781 0.76240234 0.69658203]\n",
      "2019-02-22T00:05:57.657987: step 6200, loss 0.896616, acc [0.87744141 0.81259766 0.75048828 0.68164062]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:06:01.582886: step 6200, loss 0.836581, acc [0.88791559 0.82030053 0.76385788 0.69616274] \n",
      "\n",
      "2019-02-22T00:06:02.912107: step 6240, loss 0.897791, acc [0.87978516 0.81132812 0.74658203 0.68125   ]\n",
      "2019-02-22T00:06:04.206665: step 6280, loss 0.819853, acc [0.88897865 0.81163688 0.76259667 0.69115964]\n",
      "2019-02-22T00:06:05.363336: step 6320, loss 0.818751, acc [0.88603516 0.81914062 0.76337891 0.69169922]\n",
      "2019-02-22T00:06:06.533398: step 6360, loss 0.790161, acc [0.89277344 0.82548828 0.77070313 0.70410156]\n",
      "2019-02-22T00:06:07.693045: step 6400, loss 0.813068, acc [0.89013672 0.8234375  0.76435547 0.70039063]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:06:11.653632: step 6400, loss 0.83579, acc [0.88728488 0.81982    0.76272663 0.694591  ] \n",
      "\n",
      "2019-02-22T00:06:12.907484: step 6440, loss 0.833202, acc [0.88427734 0.81503906 0.75644531 0.68525391]\n",
      "2019-02-22T00:06:14.071597: step 6480, loss 0.816527, acc [0.8890625  0.82138672 0.76552734 0.69902344]\n",
      "2019-02-22T00:06:15.232234: step 6520, loss 0.837436, acc [0.88642578 0.82197266 0.75888672 0.69130859]\n",
      "2019-02-22T00:06:16.425608: step 6560, loss 0.814844, acc [0.88740234 0.82001953 0.75966797 0.6890625 ]\n",
      "2019-02-22T00:06:17.662630: step 6600, loss 0.834289, acc [0.88447266 0.82070312 0.76455078 0.69746094]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:06:21.786864: step 6600, loss 0.826508, acc [0.88805574 0.82097128 0.76666099 0.69816496] \n",
      "\n",
      "2019-02-22T00:06:23.130032: step 6640, loss 0.831919, acc [0.88701172 0.81679687 0.75751953 0.68681641]\n",
      "2019-02-22T00:06:24.301084: step 6680, loss 0.859778, acc [0.8828125  0.81464844 0.75371094 0.68662109]\n",
      "2019-02-22T00:06:25.477596: step 6720, loss 0.821861, acc [0.88867188 0.82041016 0.76152344 0.69560547]\n",
      "2019-02-22T00:06:26.663034: step 6760, loss 0.821729, acc [0.88925781 0.82431641 0.76132813 0.69423828]\n",
      "2019-02-22T00:06:27.910970: step 6800, loss 0.84436, acc [0.88505859 0.81757813 0.75878906 0.69101563]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:06:31.900822: step 6800, loss 0.823753, acc [0.8888266  0.82192233 0.76683118 0.69754428] \n",
      "\n",
      "2019-02-22T00:06:33.153184: step 6840, loss 0.844163, acc [0.88505859 0.81523437 0.75820312 0.6890625 ]\n",
      "2019-02-22T00:06:34.318286: step 6880, loss 0.839455, acc [0.88798828 0.81884766 0.75595703 0.68828125]\n",
      "2019-02-22T00:06:35.482398: step 6920, loss 0.853194, acc [0.88193359 0.81806641 0.75966797 0.69248047]\n",
      "2019-02-22T00:06:36.658908: step 6960, loss 0.830092, acc [0.89052734 0.82324219 0.76220703 0.69726562]\n",
      "2019-02-22T00:06:37.847322: step 7000, loss 0.87893, acc [0.87890625 0.81191406 0.75117188 0.68203125]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:06:41.703716: step 7000, loss 0.828436, acc [0.88691447 0.82206249 0.76348747 0.69374005] \n",
      "\n",
      "2019-02-22T00:06:42.941729: step 7040, loss 0.837357, acc [0.88359375 0.82675781 0.76435547 0.69423828]\n",
      "2019-02-22T00:06:44.152962: step 7080, loss 0.838678, acc [0.88466797 0.82128906 0.75751953 0.69228516]\n",
      "2019-02-22T00:06:45.317071: step 7120, loss 0.866724, acc [0.88476562 0.81728516 0.75732422 0.69160156]\n",
      "2019-02-22T00:06:46.482173: step 7160, loss 0.850694, acc [0.88837891 0.81513672 0.75634766 0.69414062]\n",
      "2019-02-22T00:06:47.657692: step 7200, loss 0.861721, acc [0.88291016 0.81357422 0.75585938 0.68701172]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:06:51.562198: step 7200, loss 0.82647, acc [0.8863038  0.82237283 0.76469882 0.69697364] \n",
      "\n",
      "2019-02-22T00:06:52.803683: step 7240, loss 0.861699, acc [0.88203125 0.8171875  0.75576172 0.68759766]\n",
      "2019-02-22T00:06:53.967793: step 7280, loss 0.873125, acc [0.88398438 0.81171875 0.74765625 0.68125   ]\n",
      "2019-02-22T00:06:55.195392: step 7320, loss 0.840945, acc [0.88671875 0.82333984 0.76044922 0.69492188]\n",
      "2019-02-22T00:06:56.363470: step 7360, loss 0.83101, acc [0.89160156 0.82089844 0.76181641 0.69482422]\n",
      "2019-02-22T00:06:57.593054: step 7400, loss 0.870565, acc [0.88505859 0.81435547 0.75820312 0.690625  ]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:07:01.467304: step 7400, loss 0.82268, acc [0.88762527 0.8223428  0.76480894 0.69887575] \n",
      "\n",
      "2019-02-22T00:07:02.711765: step 7440, loss 0.855821, acc [0.88203125 0.81923828 0.76035156 0.69130859]\n",
      "2019-02-22T00:07:03.958707: step 7480, loss 0.873993, acc [0.88261719 0.81357422 0.75302734 0.68408203]\n",
      "2019-02-22T00:07:05.125795: step 7520, loss 0.868569, acc [0.88271484 0.81191406 0.74921875 0.68085938]\n",
      "2019-02-22T00:07:06.292882: step 7560, loss 0.859309, acc [0.89033203 0.81777344 0.76054687 0.69580078]\n",
      "2019-02-22T00:07:07.467407: step 7600, loss 0.859949, acc [0.88515625 0.81474609 0.75390625 0.68847656]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:07:11.386296: step 7600, loss 0.817082, acc [0.88865641 0.82131166 0.76583007 0.69815495] \n",
      "\n",
      "2019-02-22T00:07:12.638697: step 7640, loss 0.864099, acc [0.88232422 0.81855469 0.75830078 0.68652344]\n",
      "2019-02-22T00:07:13.813717: step 7680, loss 0.841395, acc [0.8890625  0.82177734 0.75625    0.69277344]\n",
      "2019-02-22T00:07:15.061653: step 7720, loss 0.845821, acc [0.88642578 0.81884766 0.75742188 0.69267578]\n",
      "2019-02-22T00:07:16.226753: step 7760, loss 0.852303, acc [0.88369141 0.81728516 0.76220703 0.69296875]\n",
      "2019-02-22T00:07:17.398799: step 7800, loss 0.873019, acc [0.88740234 0.81982422 0.75693359 0.69472656]\n",
      "\n",
      "Evaluation:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-02-22T00:07:21.302813: step 7800, loss 0.814288, acc [0.88762527 0.82127161 0.7673918  0.69849533] \n",
      "\n",
      "2019-02-22T00:07:22.755097: step 7840, loss 0.804678, acc [0.88698015 0.82155244 0.77107106 0.70030086]\n",
      "2019-02-22T00:07:23.913256: step 7880, loss 0.776009, acc [0.88916016 0.82431641 0.77666016 0.70078125]\n",
      "2019-02-22T00:07:25.072406: step 7920, loss 0.806369, acc [0.88730469 0.81855469 0.76865234 0.69667969]\n",
      "2019-02-22T00:07:26.227093: step 7960, loss 0.801337, acc [0.89433594 0.82011719 0.76904297 0.69941406]\n",
      "2019-02-22T00:07:27.392690: step 8000, loss 0.773807, acc [0.89306641 0.82402344 0.77529297 0.70429688]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:07:31.273931: step 8000, loss 0.81201, acc [0.88786553 0.82430498 0.76961427 0.7013485 ] \n",
      "\n",
      "2019-02-22T00:07:32.521820: step 8040, loss 0.777576, acc [0.89160156 0.82783203 0.77138672 0.70126953]\n",
      "2019-02-22T00:07:33.690890: step 8080, loss 0.797261, acc [0.89042969 0.8234375  0.76796875 0.69560547]\n",
      "2019-02-22T00:07:34.866407: step 8120, loss 0.808513, acc [0.88525391 0.81962891 0.76748047 0.69482422]\n",
      "2019-02-22T00:07:36.066231: step 8160, loss 0.807741, acc [0.88955078 0.81982422 0.76904297 0.69863281]\n",
      "2019-02-22T00:07:37.244228: step 8200, loss 0.831422, acc [0.88378906 0.81992188 0.75986328 0.6921875 ]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:07:41.164609: step 8200, loss 0.810674, acc [0.88830602 0.82389452 0.76995465 0.6991861 ] \n",
      "\n",
      "2019-02-22T00:07:42.406588: step 8240, loss 0.803775, acc [0.89130859 0.82109375 0.76708984 0.70205078]\n",
      "2019-02-22T00:07:43.563756: step 8280, loss 0.819319, acc [0.88251953 0.81845703 0.76689453 0.69707031]\n",
      "2019-02-22T00:07:44.783419: step 8320, loss 0.83043, acc [0.88271484 0.81494141 0.75927734 0.68779297]\n",
      "2019-02-22T00:07:45.945049: step 8360, loss 0.814004, acc [0.89140625 0.82539063 0.76933594 0.70097656]\n",
      "2019-02-22T00:07:47.105725: step 8400, loss 0.820826, acc [0.88662109 0.82519531 0.7625     0.69814453]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:07:51.053379: step 8400, loss 0.805724, acc [0.88814584 0.82380442 0.76910371 0.70010712] \n",
      "\n",
      "2019-02-22T00:07:52.302273: step 8440, loss 0.810653, acc [0.88964844 0.82001953 0.76533203 0.69462891]\n",
      "2019-02-22T00:07:53.461957: step 8480, loss 0.808737, acc [0.88378906 0.82050781 0.76796875 0.69394531]\n",
      "2019-02-22T00:07:54.616109: step 8520, loss 0.815158, acc [0.88369141 0.81845703 0.76708984 0.69091797]\n",
      "2019-02-22T00:07:55.841227: step 8560, loss 0.837591, acc [0.88828125 0.82265625 0.75576172 0.69003906]\n",
      "2019-02-22T00:07:56.997898: step 8600, loss 0.816931, acc [0.89160156 0.82363281 0.76826172 0.70048828]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:08:00.895000: step 8600, loss 0.803498, acc [0.88824595 0.8230636  0.77208702 0.70312046] \n",
      "\n",
      "2019-02-22T00:08:02.119585: step 8640, loss 0.818764, acc [0.88486328 0.82304687 0.76474609 0.69833984]\n",
      "2019-02-22T00:08:03.279765: step 8680, loss 0.817814, acc [0.88896484 0.81855469 0.76728516 0.69404297]\n",
      "2019-02-22T00:08:04.505342: step 8720, loss 0.83147, acc [0.88681641 0.82001953 0.76298828 0.69423828]\n",
      "2019-02-22T00:08:05.664989: step 8760, loss 0.813059, acc [0.88691406 0.81884766 0.76621094 0.69570312]\n",
      "2019-02-22T00:08:06.895067: step 8800, loss 0.829808, acc [0.89003906 0.82148438 0.76650391 0.69755859]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:08:10.823414: step 8800, loss 0.802022, acc [0.88839612 0.82443512 0.772077   0.70165884] \n",
      "\n",
      "2019-02-22T00:08:12.115460: step 8840, loss 0.828617, acc [0.88408203 0.82255859 0.76289063 0.69335938]\n",
      "2019-02-22T00:08:13.278578: step 8880, loss 0.817567, acc [0.88476562 0.82392578 0.76474609 0.69453125]\n",
      "2019-02-22T00:08:14.504192: step 8920, loss 0.828573, acc [0.88144531 0.82207031 0.7640625  0.69443359]\n",
      "2019-02-22T00:08:15.663384: step 8960, loss 0.839985, acc [0.88359375 0.81396484 0.76396484 0.69541016]\n",
      "2019-02-22T00:08:16.822989: step 9000, loss 0.805562, acc [0.88808594 0.82402344 0.76865234 0.69453125]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:08:20.773127: step 9000, loss 0.804719, acc [0.88923705 0.82394458 0.76887345 0.69969666] \n",
      "\n",
      "2019-02-22T00:08:22.016597: step 9040, loss 0.843449, acc [0.88242188 0.81806641 0.76572266 0.69541016]\n",
      "2019-02-22T00:08:23.177732: step 9080, loss 0.803067, acc [0.88837891 0.82041016 0.76669922 0.69648438]\n",
      "2019-02-22T00:08:24.391441: step 9120, loss 0.834614, acc [0.88349609 0.81894531 0.76533203 0.69511719]\n",
      "2019-02-22T00:08:25.549601: step 9160, loss 0.838463, acc [0.88691406 0.82363281 0.76914063 0.70136719]\n",
      "2019-02-22T00:08:26.769263: step 9200, loss 0.829394, acc [0.89091797 0.81855469 0.76220703 0.69394531]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:08:30.700553: step 9200, loss 0.795312, acc [0.88790557 0.82404469 0.77286788 0.70232959] \n",
      "\n",
      "2019-02-22T00:08:31.947496: step 9240, loss 0.849852, acc [0.88330078 0.82128906 0.75351563 0.68681641]\n",
      "2019-02-22T00:08:33.105158: step 9280, loss 0.822167, acc [0.88603516 0.82109375 0.76582031 0.69394531]\n",
      "2019-02-22T00:08:34.266292: step 9320, loss 0.810768, acc [0.88701172 0.82089844 0.76796875 0.69736328]\n",
      "2019-02-22T00:08:35.421474: step 9360, loss 0.836656, acc [0.88183594 0.81953125 0.76210937 0.69277344]\n",
      "2019-02-22T00:08:36.720002: step 9400, loss 0.786528, acc [0.88947877 0.82267203 0.77538964 0.70035117]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:08:40.700083: step 9400, loss 0.793706, acc [0.88904684 0.82368429 0.77386899 0.70280011] \n",
      "\n",
      "2019-02-22T00:08:41.931651: step 9440, loss 0.756706, acc [0.88818359 0.82958984 0.77578125 0.70419922]\n",
      "2019-02-22T00:08:43.087329: step 9480, loss 0.779548, acc [0.88691406 0.82783203 0.77177734 0.70048828]\n",
      "2019-02-22T00:08:44.244990: step 9520, loss 0.793375, acc [0.88554687 0.82021484 0.77441406 0.70166016]\n",
      "2019-02-22T00:08:45.459692: step 9560, loss 0.772671, acc [0.88916016 0.83027344 0.77919922 0.70888672]\n",
      "2019-02-22T00:08:46.616364: step 9600, loss 0.772964, acc [0.89267578 0.83359375 0.775      0.7078125 ]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:08:50.566004: step 9600, loss 0.791221, acc [0.89005796 0.8266676  0.77375887 0.70476229] \n",
      "\n",
      "2019-02-22T00:08:51.957779: step 9640, loss 0.786516, acc [0.88955078 0.82080078 0.77246094 0.70322266]\n",
      "2019-02-22T00:08:53.122881: step 9680, loss 0.812465, acc [0.88574219 0.81425781 0.7703125  0.696875  ]\n",
      "2019-02-22T00:08:54.280545: step 9720, loss 0.766177, acc [0.89345703 0.83037109 0.77421875 0.70732422]\n",
      "2019-02-22T00:08:55.445647: step 9760, loss 0.793284, acc [0.88730469 0.82021484 0.77167969 0.69853516]\n",
      "2019-02-22T00:08:56.605294: step 9800, loss 0.79509, acc [0.89042969 0.82792969 0.77177734 0.70546875]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:09:00.536627: step 9800, loss 0.79207, acc [0.88862637 0.82451521 0.77343852 0.70196919] \n",
      "\n",
      "2019-02-22T00:09:01.781542: step 9840, loss 0.794116, acc [0.88759766 0.8234375  0.77060547 0.69746094]\n",
      "2019-02-22T00:09:02.944658: step 9880, loss 0.778344, acc [0.88701172 0.82353516 0.77314453 0.69980469]\n",
      "2019-02-22T00:09:04.201523: step 9920, loss 0.755291, acc [0.89169922 0.83144531 0.78242188 0.71289062]\n",
      "2019-02-22T00:09:05.367616: step 9960, loss 0.763601, acc [0.89248047 0.83027344 0.77421875 0.70322266]\n",
      "2019-02-22T00:09:06.533710: step 10000, loss 0.813247, acc [0.88730469 0.82060547 0.76738281 0.69931641]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:09:10.836008: step 10000, loss 0.783326, acc [0.88994784 0.82572656 0.77540069 0.70318053] \n",
      "\n",
      "2019-02-22T00:09:12.124341: step 10040, loss 0.788189, acc [0.89023438 0.82558594 0.77226562 0.69902344]\n",
      "2019-02-22T00:09:13.318484: step 10080, loss 0.808427, acc [0.88984375 0.82265625 0.76855469 0.70136719]\n",
      "2019-02-22T00:09:14.546083: step 10120, loss 0.794975, acc [0.88994141 0.82988281 0.76875    0.70429688]\n",
      "2019-02-22T00:09:15.766737: step 10160, loss 0.794663, acc [0.88876953 0.81914062 0.77197266 0.696875  ]\n",
      "2019-02-22T00:09:17.005744: step 10200, loss 0.798832, acc [0.89326172 0.82871094 0.76972656 0.70166016]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:09:21.188009: step 10200, loss 0.79027, acc [0.88903683 0.8253161  0.77423941 0.70241969] \n",
      "\n",
      "2019-02-22T00:09:22.446856: step 10240, loss 0.803142, acc [0.88847656 0.81943359 0.76611328 0.69453125]\n",
      "2019-02-22T00:09:23.677489: step 10280, loss 0.787388, acc [0.88759766 0.82050781 0.77099609 0.70097656]\n",
      "2019-02-22T00:09:24.847492: step 10320, loss 0.814935, acc [0.88466797 0.8203125  0.76660156 0.696875  ]\n",
      "2019-02-22T00:09:26.088978: step 10360, loss 0.806314, acc [0.88339844 0.82021484 0.76923828 0.69970703]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-02-22T00:09:27.263010: step 10400, loss 0.782049, acc [0.88964844 0.82460937 0.77861328 0.70566406]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:09:31.272712: step 10400, loss 0.783966, acc [0.88958744 0.82525603 0.77608145 0.70271   ] \n",
      "\n",
      "2019-02-22T00:09:32.654025: step 10440, loss 0.801961, acc [0.89199219 0.82382813 0.76914063 0.69931641]\n",
      "2019-02-22T00:09:33.821112: step 10480, loss 0.818616, acc [0.88359375 0.82138672 0.76464844 0.69492188]\n",
      "2019-02-22T00:09:34.990181: step 10520, loss 0.81765, acc [0.89267578 0.82373047 0.76367188 0.69257813]\n",
      "2019-02-22T00:09:36.220756: step 10560, loss 0.817349, acc [0.88603516 0.81503906 0.76816406 0.69404297]\n",
      "2019-02-22T00:09:37.386851: step 10600, loss 0.831481, acc [0.88427734 0.81923828 0.76259766 0.69511719]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:09:41.289868: step 10600, loss 0.780058, acc [0.89039834 0.82685781 0.77550081 0.70563325] \n",
      "\n",
      "2019-02-22T00:09:42.545243: step 10640, loss 0.813036, acc [0.88955078 0.82070312 0.76601562 0.6953125 ]\n",
      "2019-02-22T00:09:43.708857: step 10680, loss 0.803096, acc [0.89160156 0.82373047 0.77177734 0.70761719]\n",
      "2019-02-22T00:09:44.947370: step 10720, loss 0.796593, acc [0.88916016 0.81777344 0.77324219 0.69833984]\n",
      "2019-02-22T00:09:46.111478: step 10760, loss 0.811351, acc [0.88486328 0.82089844 0.76904297 0.69882813]\n",
      "2019-02-22T00:09:47.344532: step 10800, loss 0.812314, acc [0.89052734 0.82822266 0.76669922 0.69824219]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:09:51.240652: step 10800, loss 0.77722, acc [0.89173983 0.82600687 0.77590125 0.70637408] \n",
      "\n",
      "2019-02-22T00:09:52.498002: step 10840, loss 0.818155, acc [0.89560547 0.82246094 0.76416016 0.69716797]\n",
      "2019-02-22T00:09:53.728043: step 10880, loss 0.814272, acc [0.88857422 0.82685547 0.77314453 0.70234375]\n",
      "2019-02-22T00:09:54.895625: step 10920, loss 0.810422, acc [0.88212891 0.82158203 0.76503906 0.69414062]\n",
      "2019-02-22T00:09:56.198438: step 10960, loss 0.74132, acc [0.8918462  0.83115826 0.78368056 0.71268643]\n",
      "2019-02-22T00:09:57.420083: step 11000, loss 0.743433, acc [0.89150391 0.83300781 0.78271484 0.71162109]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:10:01.492735: step 11000, loss 0.778976, acc [0.88890669 0.8259468  0.77760314 0.70400144] \n",
      "\n",
      "2019-02-22T00:10:02.841853: step 11040, loss 0.752044, acc [0.88857422 0.82529297 0.7796875  0.70556641]\n",
      "2019-02-22T00:10:04.006460: step 11080, loss 0.747699, acc [0.89121094 0.82958984 0.77958984 0.70566406]\n",
      "2019-02-22T00:10:05.164121: step 11120, loss 0.746902, acc [0.88740234 0.82998047 0.7796875  0.70576172]\n",
      "2019-02-22T00:10:06.322776: step 11160, loss 0.765617, acc [0.88662109 0.82451172 0.77783203 0.70507812]\n",
      "2019-02-22T00:10:07.483910: step 11200, loss 0.747007, acc [0.89189453 0.82792969 0.78300781 0.70986328]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:10:11.422176: step 11200, loss 0.77747, acc [0.89045841 0.8263072  0.77828389 0.70602369] \n",
      "\n",
      "2019-02-22T00:10:12.675569: step 11240, loss 0.756781, acc [0.89072266 0.82226562 0.78408203 0.70898438]\n",
      "2019-02-22T00:10:13.836174: step 11280, loss 0.739139, acc [0.89257812 0.82636719 0.78056641 0.70380859]\n",
      "2019-02-22T00:10:15.010203: step 11320, loss 0.75212, acc [0.8859375  0.82421875 0.78447266 0.70488281]\n",
      "2019-02-22T00:10:16.330554: step 11360, loss 0.758288, acc [0.88320312 0.81992188 0.77861328 0.69980469]\n",
      "2019-02-22T00:10:17.546248: step 11400, loss 0.779465, acc [0.88964844 0.82128906 0.77597656 0.70185547]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:10:21.752842: step 11400, loss 0.781055, acc [0.88890668 0.82830942 0.77647189 0.70347085] \n",
      "\n",
      "2019-02-22T00:10:23.111359: step 11440, loss 0.775372, acc [0.89042969 0.82460937 0.77421875 0.70292969]\n",
      "2019-02-22T00:10:24.291838: step 11480, loss 0.780678, acc [0.89013672 0.82421875 0.77431641 0.69892578]\n",
      "2019-02-22T00:10:25.601277: step 11520, loss 0.752391, acc [0.89111328 0.83007812 0.78339844 0.709375  ]\n",
      "2019-02-22T00:10:26.794652: step 11560, loss 0.797643, acc [0.88779297 0.81816406 0.77167969 0.69716797]\n",
      "2019-02-22T00:10:27.981080: step 11600, loss 0.777388, acc [0.89941406 0.83007812 0.77890625 0.71142578]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:10:32.310163: step 11600, loss 0.775888, acc [0.88966753 0.82762867 0.77817377 0.70569332] \n",
      "\n",
      "2019-02-22T00:10:33.686562: step 11640, loss 0.766852, acc [0.89462891 0.82841797 0.77753906 0.70625   ]\n",
      "2019-02-22T00:10:34.897792: step 11680, loss 0.790517, acc [0.88515625 0.81972656 0.76708984 0.69628906]\n",
      "2019-02-22T00:10:36.085708: step 11720, loss 0.778423, acc [0.89130859 0.83017578 0.77167969 0.70605469]\n",
      "2019-02-22T00:10:37.313307: step 11760, loss 0.782743, acc [0.890625   0.825      0.77324219 0.70166016]\n",
      "2019-02-22T00:10:38.545370: step 11800, loss 0.78216, acc [0.89160156 0.82978516 0.77109375 0.70146484]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:10:42.675599: step 11800, loss 0.7708, acc [0.88991781 0.82746849 0.77881449 0.7082862 ] \n",
      "\n",
      "2019-02-22T00:10:43.931427: step 11840, loss 0.788851, acc [0.8828125  0.8234375  0.77412109 0.70263672]\n",
      "2019-02-22T00:10:45.170928: step 11880, loss 0.788264, acc [0.88730469 0.82441406 0.77050781 0.69921875]\n",
      "2019-02-22T00:10:46.342479: step 11920, loss 0.769961, acc [0.89658203 0.82744141 0.77753906 0.70517578]\n",
      "2019-02-22T00:10:47.597855: step 11960, loss 0.787773, acc [0.88564453 0.82617188 0.77197266 0.69980469]\n",
      "2019-02-22T00:10:48.772876: step 12000, loss 0.790654, acc [0.89472656 0.82158203 0.77597656 0.70498047]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:10:52.752310: step 12000, loss 0.773174, acc [0.89029823 0.82717817 0.77893461 0.70507263] \n",
      "\n",
      "2019-02-22T00:10:54.051797: step 12040, loss 0.82139, acc [0.88154297 0.81992188 0.76103516 0.69091797]\n",
      "2019-02-22T00:10:55.282866: step 12080, loss 0.78852, acc [0.88681641 0.82109375 0.77480469 0.69814453]\n",
      "2019-02-22T00:10:56.461362: step 12120, loss 0.778947, acc [0.89052734 0.82539063 0.77998047 0.70761719]\n",
      "2019-02-22T00:10:57.627950: step 12160, loss 0.802297, acc [0.89101562 0.82226562 0.77080078 0.70039063]\n",
      "2019-02-22T00:10:58.844636: step 12200, loss 0.777734, acc [0.89257812 0.82597656 0.77041016 0.70253906]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:11:02.755128: step 12200, loss 0.7681, acc [0.89065863 0.82809919 0.7793751  0.70799588] \n",
      "\n",
      "2019-02-22T00:11:04.001047: step 12240, loss 0.790986, acc [0.88916016 0.82558594 0.76904297 0.70429688]\n",
      "2019-02-22T00:11:05.195413: step 12280, loss 0.801974, acc [0.89228516 0.82851562 0.77392578 0.70703125]\n",
      "2019-02-22T00:11:06.429953: step 12320, loss 0.778699, acc [0.89179688 0.825      0.77597656 0.70488281]\n",
      "2019-02-22T00:11:07.600512: step 12360, loss 0.807814, acc [0.88876953 0.82402344 0.76982422 0.69990234]\n",
      "2019-02-22T00:11:08.783967: step 12400, loss 0.785989, acc [0.88691406 0.82382813 0.77451172 0.70175781]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:11:12.696956: step 12400, loss 0.766773, acc [0.89267087 0.82939062 0.77930503 0.70826617] \n",
      "\n",
      "2019-02-22T00:11:13.938391: step 12440, loss 0.789704, acc [0.88505859 0.82695312 0.77363281 0.69921875]\n",
      "2019-02-22T00:11:15.165991: step 12480, loss 0.775176, acc [0.88974609 0.82646484 0.77470703 0.70419922]\n",
      "2019-02-22T00:11:16.467989: step 12520, loss 0.725876, acc [0.89581459 0.83489583 0.78736091 0.71713621]\n",
      "2019-02-22T00:11:17.668802: step 12560, loss 0.731047, acc [0.88779297 0.82841797 0.784375   0.70537109]\n",
      "2019-02-22T00:11:18.842338: step 12600, loss 0.728416, acc [0.89335937 0.83164063 0.78994141 0.71416016]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:11:22.809339: step 12600, loss 0.76118, acc [0.89158966 0.82950074 0.78224829 0.71059876] \n",
      "\n",
      "2019-02-22T00:11:24.115304: step 12640, loss 0.729545, acc [0.89326172 0.83007812 0.78652344 0.71484375]\n",
      "2019-02-22T00:11:25.365224: step 12680, loss 0.739593, acc [0.88730469 0.82890625 0.78496094 0.71240234]\n",
      "2019-02-22T00:11:26.583893: step 12720, loss 0.75497, acc [0.89560547 0.82890625 0.78105469 0.70859375]\n",
      "2019-02-22T00:11:27.768835: step 12760, loss 0.728559, acc [0.89355469 0.83457031 0.78564453 0.70888672]\n",
      "2019-02-22T00:11:29.015780: step 12800, loss 0.73537, acc [0.88955078 0.82695312 0.79042969 0.71103516]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:11:33.093425: step 12800, loss 0.766404, acc [0.89147954 0.82830942 0.78270881 0.70929732] \n",
      "\n",
      "2019-02-22T00:11:34.421178: step 12840, loss 0.759198, acc [0.88984375 0.82568359 0.77626953 0.70224609]\n",
      "2019-02-22T00:11:35.603642: step 12880, loss 0.737003, acc [0.89414063 0.82988281 0.78037109 0.70927734]\n",
      "2019-02-22T00:11:36.773702: step 12920, loss 0.78389, acc [0.88720703 0.82587891 0.76982422 0.69433594]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-02-22T00:11:37.947733: step 12960, loss 0.751417, acc [0.88769531 0.82509766 0.77919922 0.70234375]\n",
      "2019-02-22T00:11:39.122755: step 13000, loss 0.767495, acc [0.88759766 0.81894531 0.77207031 0.6953125 ]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:11:43.052590: step 13000, loss 0.762621, acc [0.89100902 0.82730831 0.78165764 0.70733514] \n",
      "\n",
      "2019-02-22T00:11:44.308427: step 13040, loss 0.74205, acc [0.89335937 0.83339844 0.78203125 0.71083984]\n",
      "2019-02-22T00:11:45.477995: step 13080, loss 0.730545, acc [0.89140625 0.8296875  0.78535156 0.71152344]\n",
      "2019-02-22T00:11:46.732875: step 13120, loss 0.773903, acc [0.89042969 0.828125   0.77724609 0.70732422]\n",
      "2019-02-22T00:11:47.906903: step 13160, loss 0.767868, acc [0.89345703 0.82880859 0.77460938 0.70458984]\n",
      "2019-02-22T00:11:49.071511: step 13200, loss 0.752243, acc [0.88623047 0.82978516 0.77822266 0.70439453]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:11:52.946289: step 13200, loss 0.761669, acc [0.8906286  0.82963089 0.78176776 0.70859654] \n",
      "\n",
      "2019-02-22T00:11:54.195222: step 13240, loss 0.743103, acc [0.89140625 0.82763672 0.78154297 0.71054688]\n",
      "2019-02-22T00:11:55.366771: step 13280, loss 0.751168, acc [0.89033203 0.82910156 0.78095703 0.71035156]\n",
      "2019-02-22T00:11:56.534314: step 13320, loss 0.742143, acc [0.89521484 0.83935547 0.78339844 0.71591797]\n",
      "2019-02-22T00:11:57.692970: step 13360, loss 0.757373, acc [0.89121094 0.82890625 0.78056641 0.70751953]\n",
      "2019-02-22T00:11:58.857080: step 13400, loss 0.753794, acc [0.89609375 0.83173828 0.77919922 0.70693359]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:12:02.863761: step 13400, loss 0.766117, acc [0.89224039 0.82784891 0.78060647 0.70771557] \n",
      "\n",
      "2019-02-22T00:12:04.112688: step 13440, loss 0.791205, acc [0.88789063 0.82109375 0.77304688 0.69912109]\n",
      "2019-02-22T00:12:05.278783: step 13480, loss 0.743833, acc [0.89433594 0.83271484 0.78134766 0.71416016]\n",
      "2019-02-22T00:12:06.446861: step 13520, loss 0.766002, acc [0.8890625  0.82958984 0.77568359 0.70605469]\n",
      "2019-02-22T00:12:07.611468: step 13560, loss 0.767649, acc [0.88740234 0.82470703 0.77460938 0.70126953]\n",
      "2019-02-22T00:12:08.773099: step 13600, loss 0.781905, acc [0.88759766 0.82636719 0.77568359 0.70146484]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:12:12.730713: step 13600, loss 0.758758, acc [0.89148955 0.8286498  0.7824385  0.70925728] \n",
      "\n",
      "2019-02-22T00:12:13.962738: step 13640, loss 0.758984, acc [0.88847656 0.83115234 0.77919922 0.70888672]\n",
      "2019-02-22T00:12:15.125361: step 13680, loss 0.795653, acc [0.88828125 0.82119141 0.77089844 0.69794922]\n",
      "2019-02-22T00:12:16.290464: step 13720, loss 0.786122, acc [0.88466797 0.82539063 0.77167969 0.69804687]\n",
      "2019-02-22T00:12:17.455567: step 13760, loss 0.793981, acc [0.88496094 0.82558594 0.77373047 0.70234375]\n",
      "2019-02-22T00:12:18.620172: step 13800, loss 0.776931, acc [0.88486328 0.82548828 0.77568359 0.70419922]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:12:22.543068: step 13800, loss 0.758279, acc [0.89214028 0.82942066 0.78295908 0.7104486 ] \n",
      "\n",
      "2019-02-22T00:12:23.798405: step 13840, loss 0.766878, acc [0.89287109 0.82714844 0.77783203 0.70498047]\n",
      "2019-02-22T00:12:24.963010: step 13880, loss 0.783888, acc [0.89150391 0.81982422 0.7734375  0.70087891]\n",
      "2019-02-22T00:12:26.131089: step 13920, loss 0.785317, acc [0.89101562 0.82431641 0.77509766 0.70654297]\n",
      "2019-02-22T00:12:27.298174: step 13960, loss 0.749432, acc [0.89521484 0.83369141 0.78193359 0.71005859]\n",
      "2019-02-22T00:12:28.463279: step 14000, loss 0.808876, acc [0.88515625 0.82158203 0.76679688 0.69541016]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:12:32.366297: step 14000, loss 0.755552, acc [0.89100902 0.83041176 0.78241849 0.70997808] \n",
      "\n",
      "2019-02-22T00:12:33.617702: step 14040, loss 0.798577, acc [0.88828125 0.82177734 0.76552734 0.69560547]\n",
      "2019-02-22T00:12:34.907797: step 14080, loss 0.71768, acc [0.89375197 0.8307775  0.78883266 0.71333452]\n",
      "2019-02-22T00:12:36.070915: step 14120, loss 0.720579, acc [0.89335937 0.83574219 0.78847656 0.71591797]\n",
      "2019-02-22T00:12:37.227090: step 14160, loss 0.70288, acc [0.89111328 0.83896484 0.79179687 0.71806641]\n",
      "2019-02-22T00:12:38.384751: step 14200, loss 0.728696, acc [0.89482422 0.82900391 0.78964844 0.71298828]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:12:42.249081: step 14200, loss 0.758151, acc [0.89281102 0.82788896 0.78282894 0.70924727] \n",
      "\n",
      "2019-02-22T00:12:43.527273: step 14240, loss 0.728883, acc [0.89453125 0.83066406 0.7890625  0.71474609]\n",
      "2019-02-22T00:12:44.686423: step 14280, loss 0.709967, acc [0.89736328 0.83417969 0.79443359 0.71826172]\n",
      "2019-02-22T00:12:45.851028: step 14320, loss 0.729093, acc [0.89179688 0.82910156 0.78388672 0.71162109]\n",
      "2019-02-22T00:12:47.013651: step 14360, loss 0.726778, acc [0.89306641 0.83564453 0.78642578 0.71386719]\n",
      "2019-02-22T00:12:48.174786: step 14400, loss 0.725001, acc [0.89101562 0.83007812 0.78369141 0.70830078]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:12:52.031413: step 14400, loss 0.755521, acc [0.89148955 0.82857972 0.78467099 0.71074893] \n",
      "\n",
      "2019-02-22T00:12:53.309603: step 14440, loss 0.754337, acc [0.89033203 0.82148438 0.78330078 0.70507812]\n",
      "2019-02-22T00:12:54.468258: step 14480, loss 0.755401, acc [0.88955078 0.83144531 0.77978516 0.70751953]\n",
      "2019-02-22T00:12:55.629887: step 14520, loss 0.76323, acc [0.88935547 0.82597656 0.77900391 0.70195312]\n",
      "2019-02-22T00:12:56.785565: step 14560, loss 0.739999, acc [0.89384766 0.82773438 0.78691406 0.71044922]\n",
      "2019-02-22T00:12:57.942733: step 14600, loss 0.742824, acc [0.88818359 0.82988281 0.78320312 0.70625   ]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:13:01.884969: step 14600, loss 0.752604, acc [0.89128933 0.82952077 0.7835998  0.70997808] \n",
      "\n",
      "2019-02-22T00:13:03.115508: step 14640, loss 0.750652, acc [0.890625   0.82714844 0.77998047 0.70419922]\n",
      "2019-02-22T00:13:04.272676: step 14680, loss 0.755014, acc [0.890625   0.82587891 0.7859375  0.70712891]\n",
      "2019-02-22T00:13:05.424882: step 14720, loss 0.727367, acc [0.89355469 0.83554688 0.78759766 0.71386719]\n",
      "2019-02-22T00:13:06.583040: step 14760, loss 0.738194, acc [0.88964844 0.82861328 0.78369141 0.70488281]\n",
      "2019-02-22T00:13:07.740207: step 14800, loss 0.749575, acc [0.88798828 0.83222656 0.78085938 0.70771484]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:13:11.669512: step 14800, loss 0.756087, acc [0.89237053 0.82890008 0.78440068 0.7106288 ] \n",
      "\n",
      "2019-02-22T00:13:12.925383: step 14840, loss 0.739631, acc [0.89521484 0.82822266 0.78085938 0.70458984]\n",
      "2019-02-22T00:13:14.086517: step 14880, loss 0.724194, acc [0.89863281 0.8296875  0.78173828 0.71357422]\n",
      "2019-02-22T00:13:15.244179: step 14920, loss 0.764496, acc [0.89013672 0.825      0.78134766 0.70732422]\n",
      "2019-02-22T00:13:16.404818: step 14960, loss 0.746993, acc [0.88955078 0.83007812 0.77773437 0.70898438]\n",
      "2019-02-22T00:13:17.561489: step 15000, loss 0.740404, acc [0.88974609 0.82851562 0.78300781 0.70732422]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:13:21.435290: step 15000, loss 0.758996, acc [0.89058855 0.82776882 0.78418044 0.70813603] \n",
      "\n",
      "2019-02-22T00:13:22.775929: step 15040, loss 0.778325, acc [0.88466797 0.81757813 0.77714844 0.69902344]\n",
      "2019-02-22T00:13:23.935079: step 15080, loss 0.746889, acc [0.89023438 0.83261719 0.78066406 0.71035156]\n",
      "2019-02-22T00:13:25.092247: step 15120, loss 0.768119, acc [0.88779297 0.83095703 0.77832031 0.70488281]\n",
      "2019-02-22T00:13:26.254868: step 15160, loss 0.756945, acc [0.89248047 0.82783203 0.77890625 0.70683594]\n",
      "2019-02-22T00:13:27.415010: step 15200, loss 0.752881, acc [0.89306641 0.82705078 0.78359375 0.70878906]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:13:31.330461: step 15200, loss 0.747596, acc [0.8916197  0.8301815  0.78722382 0.71123947] \n",
      "\n",
      "2019-02-22T00:13:32.615563: step 15240, loss 0.75335, acc [0.89179688 0.83173828 0.78398437 0.71015625]\n",
      "2019-02-22T00:13:33.774713: step 15280, loss 0.768823, acc [0.88867188 0.82412109 0.77597656 0.70546875]\n",
      "2019-02-22T00:13:34.930925: step 15320, loss 0.744952, acc [0.89121094 0.83017578 0.78056641 0.70898438]\n",
      "2019-02-22T00:13:36.090038: step 15360, loss 0.757329, acc [0.89648438 0.82392578 0.78076172 0.70654297]\n",
      "2019-02-22T00:13:37.244724: step 15400, loss 0.757978, acc [0.89208984 0.82919922 0.78007812 0.70791016]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:13:41.118026: step 15400, loss 0.743009, acc [0.89226041 0.83007138 0.78650302 0.71291133] \n",
      "\n",
      "2019-02-22T00:13:42.405597: step 15440, loss 0.771245, acc [0.88798828 0.82285156 0.78007812 0.70302734]\n",
      "2019-02-22T00:13:43.560779: step 15480, loss 0.767987, acc [0.89335937 0.82412109 0.77558594 0.70791016]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-02-22T00:13:44.721914: step 15520, loss 0.765824, acc [0.89101562 0.82333984 0.78662109 0.70957031]\n",
      "2019-02-22T00:13:45.878089: step 15560, loss 0.740275, acc [0.89404297 0.834375   0.78505859 0.71269531]\n",
      "2019-02-22T00:13:47.037734: step 15600, loss 0.743006, acc [0.89238281 0.8234375  0.77988281 0.70449219]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:13:50.905536: step 15600, loss 0.744857, acc [0.89018811 0.82931054 0.78598244 0.70998809] \n",
      "\n",
      "2019-02-22T00:13:52.292351: step 15640, loss 0.713352, acc [0.89542397 0.83537918 0.79153843 0.71706913]\n",
      "2019-02-22T00:13:53.445053: step 15680, loss 0.675824, acc [0.89384766 0.83183594 0.79765625 0.71787109]\n",
      "2019-02-22T00:13:54.608667: step 15720, loss 0.686561, acc [0.89423828 0.8328125  0.79677734 0.71728516]\n",
      "2019-02-22T00:13:55.763353: step 15760, loss 0.714248, acc [0.89033203 0.82802734 0.78974609 0.70927734]\n",
      "2019-02-22T00:13:56.923496: step 15800, loss 0.729152, acc [0.88505859 0.82832031 0.78681641 0.70957031]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:14:00.787384: step 15800, loss 0.748744, acc [0.89230046 0.83209362 0.78602249 0.71295137] \n",
      "\n",
      "2019-02-22T00:14:02.031297: step 15840, loss 0.738398, acc [0.89042969 0.82685547 0.78154297 0.70644531]\n",
      "2019-02-22T00:14:03.194911: step 15880, loss 0.749171, acc [0.88681641 0.82509766 0.78134766 0.70449219]\n",
      "2019-02-22T00:14:04.367950: step 15920, loss 0.716843, acc [0.8921875  0.83242187 0.79326172 0.71142578]\n",
      "2019-02-22T00:14:05.532060: step 15960, loss 0.710575, acc [0.89384766 0.83105469 0.79335937 0.71611328]\n",
      "2019-02-22T00:14:06.694186: step 16000, loss 0.700648, acc [0.89580078 0.83847656 0.79267578 0.72109375]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:14:10.630485: step 16000, loss 0.746587, acc [0.89260079 0.83096237 0.78771436 0.71356205] \n",
      "\n",
      "2019-02-22T00:14:11.935411: step 16040, loss 0.718185, acc [0.89521484 0.83330078 0.78769531 0.71367187]\n",
      "2019-02-22T00:14:13.108450: step 16080, loss 0.724906, acc [0.88759766 0.82607422 0.78662109 0.70771484]\n",
      "2019-02-22T00:14:14.273056: step 16120, loss 0.745751, acc [0.88369141 0.82138672 0.78369141 0.70185547]\n",
      "2019-02-22T00:14:15.441438: step 16160, loss 0.715265, acc [0.89414063 0.83271484 0.78818359 0.71806641]\n",
      "2019-02-22T00:14:16.612493: step 16200, loss 0.692687, acc [0.89326172 0.83808594 0.7984375  0.7234375 ]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:14:20.478840: step 16200, loss 0.748478, acc [0.8916197  0.82942066 0.78575218 0.71188019] \n",
      "\n",
      "2019-02-22T00:14:21.808086: step 16240, loss 0.721006, acc [0.8953125  0.83847656 0.78730469 0.71855469]\n",
      "2019-02-22T00:14:22.969219: step 16280, loss 0.725822, acc [0.89296875 0.83144531 0.78837891 0.71230469]\n",
      "2019-02-22T00:14:24.128368: step 16320, loss 0.757958, acc [0.88779297 0.82285156 0.78173828 0.70390625]\n",
      "2019-02-22T00:14:25.289505: step 16360, loss 0.713579, acc [0.89130859 0.83515625 0.78798828 0.71376953]\n",
      "2019-02-22T00:14:26.453670: step 16400, loss 0.741889, acc [0.890625   0.82919922 0.78144531 0.70576172]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:14:30.401809: step 16400, loss 0.741123, acc [0.89268087 0.82918039 0.78789456 0.71141968] \n",
      "\n",
      "2019-02-22T00:14:31.645734: step 16440, loss 0.728102, acc [0.89238281 0.82890625 0.78271484 0.71035156]\n",
      "2019-02-22T00:14:32.807861: step 16480, loss 0.703736, acc [0.89804688 0.84208984 0.79355469 0.72158203]\n",
      "2019-02-22T00:14:33.969989: step 16520, loss 0.744975, acc [0.89208984 0.83339844 0.77792969 0.70849609]\n",
      "2019-02-22T00:14:35.133601: step 16560, loss 0.756946, acc [0.89140625 0.82587891 0.7828125  0.70810547]\n",
      "2019-02-22T00:14:36.297713: step 16600, loss 0.743573, acc [0.89228516 0.82617188 0.78144531 0.70517578]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:14:40.148651: step 16600, loss 0.73663, acc [0.89275095 0.8322538  0.78964651 0.71597473] \n",
      "\n",
      "2019-02-22T00:14:41.398072: step 16640, loss 0.7413, acc [0.89492187 0.83310547 0.78515625 0.70820313]\n",
      "2019-02-22T00:14:42.561687: step 16680, loss 0.754794, acc [0.89101562 0.82949219 0.78046875 0.70371094]\n",
      "2019-02-22T00:14:43.736709: step 16720, loss 0.753576, acc [0.88662109 0.82832031 0.77822266 0.70605469]\n",
      "2019-02-22T00:14:44.902309: step 16760, loss 0.741356, acc [0.88994141 0.82792969 0.77675781 0.70527344]\n",
      "2019-02-22T00:14:46.085266: step 16800, loss 0.727387, acc [0.89511719 0.83173828 0.78398437 0.7125    ]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:14:50.212972: step 16800, loss 0.734761, acc [0.89265084 0.83331498 0.78850524 0.71310154] \n",
      "\n",
      "2019-02-22T00:14:51.494140: step 16840, loss 0.761234, acc [0.88662109 0.82275391 0.77958984 0.7015625 ]\n",
      "2019-02-22T00:14:52.672633: step 16880, loss 0.750779, acc [0.89130859 0.83232422 0.77871094 0.70664063]\n",
      "2019-02-22T00:14:53.871464: step 16920, loss 0.746817, acc [0.89189453 0.82978516 0.78144531 0.70556641]\n",
      "2019-02-22T00:14:55.131301: step 16960, loss 0.734604, acc [0.89248047 0.83476562 0.78408203 0.71132812]\n",
      "2019-02-22T00:14:56.302851: step 17000, loss 0.760035, acc [0.89277344 0.82705078 0.78037109 0.70761719]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:15:00.361117: step 17000, loss 0.736073, acc [0.89338166 0.83213367 0.79075774 0.71518386] \n",
      "\n",
      "2019-02-22T00:15:01.631373: step 17040, loss 0.7623, acc [0.88183594 0.82539063 0.78408203 0.70498047]\n",
      "2019-02-22T00:15:02.800939: step 17080, loss 0.758559, acc [0.89248047 0.82539063 0.78076172 0.70585937]\n",
      "2019-02-22T00:15:03.970505: step 17120, loss 0.774882, acc [0.88867188 0.82441406 0.77548828 0.70400391]\n",
      "2019-02-22T00:15:05.137590: step 17160, loss 0.741063, acc [0.89326172 0.83076172 0.78535156 0.71357422]\n",
      "2019-02-22T00:15:06.434133: step 17200, loss 0.693696, acc [0.89341363 0.83282828 0.79377269 0.72071595]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:15:10.269200: step 17200, loss 0.735441, acc [0.89254072 0.83203356 0.78976664 0.71573447] \n",
      "\n",
      "2019-02-22T00:15:11.523085: step 17240, loss 0.699015, acc [0.89326172 0.83193359 0.80234375 0.72011719]\n",
      "2019-02-22T00:15:12.688188: step 17280, loss 0.690949, acc [0.89638672 0.83388672 0.79423828 0.72128906]\n",
      "2019-02-22T00:15:13.869658: step 17320, loss 0.6972, acc [0.88740234 0.83144531 0.7953125  0.71621094]\n",
      "2019-02-22T00:15:15.036249: step 17360, loss 0.70173, acc [0.89667969 0.83496094 0.78847656 0.71503906]\n",
      "2019-02-22T00:15:16.197383: step 17400, loss 0.709359, acc [0.89306641 0.83154297 0.79365234 0.71464844]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:15:20.063201: step 17400, loss 0.738217, acc [0.89318143 0.83164312 0.78884562 0.71340188] \n",
      "\n",
      "2019-02-22T00:15:21.304192: step 17440, loss 0.674791, acc [0.89882812 0.83935547 0.79453125 0.721875  ]\n",
      "2019-02-22T00:15:22.471277: step 17480, loss 0.680942, acc [0.89296875 0.83818359 0.79775391 0.71708984]\n",
      "2019-02-22T00:15:23.642333: step 17520, loss 0.731762, acc [0.88613281 0.83173828 0.78720703 0.70966797]\n",
      "2019-02-22T00:15:24.804459: step 17560, loss 0.705714, acc [0.89003906 0.83447266 0.79013672 0.71484375]\n",
      "2019-02-22T00:15:25.965098: step 17600, loss 0.734128, acc [0.89160156 0.82744141 0.78642578 0.70859375]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:15:29.884518: step 17600, loss 0.738147, acc [0.89280101 0.83121265 0.7870236  0.71197029] \n",
      "\n",
      "2019-02-22T00:15:31.119027: step 17640, loss 0.734327, acc [0.88955078 0.82675781 0.78330078 0.70214844]\n",
      "2019-02-22T00:15:32.280161: step 17680, loss 0.695329, acc [0.89033203 0.83564453 0.79794922 0.71796875]\n",
      "2019-02-22T00:15:33.443775: step 17720, loss 0.713445, acc [0.89150391 0.8359375  0.78779297 0.71513672]\n",
      "2019-02-22T00:15:34.646077: step 17760, loss 0.728731, acc [0.88789063 0.82714844 0.78828125 0.70615234]\n",
      "2019-02-22T00:15:35.931706: step 17800, loss 0.725952, acc [0.88837891 0.83242187 0.78652344 0.70927734]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:15:40.174032: step 17800, loss 0.737202, acc [0.89295118 0.82971098 0.7895464  0.7141427 ] \n",
      "\n",
      "2019-02-22T00:15:41.446724: step 17840, loss 0.706208, acc [0.89462891 0.82832031 0.79267578 0.71113281]\n",
      "2019-02-22T00:15:42.629186: step 17880, loss 0.731059, acc [0.89033203 0.82705078 0.78564453 0.71132812]\n",
      "2019-02-22T00:15:43.810160: step 17920, loss 0.728544, acc [0.89365234 0.82714844 0.78583984 0.70917969]\n",
      "2019-02-22T00:15:45.053629: step 17960, loss 0.711228, acc [0.89189453 0.83037109 0.78574219 0.7109375 ]\n",
      "2019-02-22T00:15:46.225180: step 18000, loss 0.709189, acc [0.8921875  0.83642578 0.79384766 0.72021484]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:15:50.294855: step 18000, loss 0.735769, acc [0.89354183 0.83125269 0.78875552 0.71334181] \n",
      "\n",
      "2019-02-22T00:15:51.552710: step 18040, loss 0.748585, acc [0.89160156 0.82607422 0.77988281 0.71181641]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-02-22T00:15:52.728228: step 18080, loss 0.725128, acc [0.89580078 0.83222656 0.78837891 0.71230469]\n",
      "2019-02-22T00:15:53.947890: step 18120, loss 0.741455, acc [0.88896484 0.82695312 0.78613281 0.70546875]\n",
      "2019-02-22T00:15:55.192848: step 18160, loss 0.747934, acc [0.89121094 0.81767578 0.78164062 0.70371094]\n",
      "2019-02-22T00:15:56.376303: step 18200, loss 0.732489, acc [0.89697266 0.83046875 0.78339844 0.71054688]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:16:00.432088: step 18200, loss 0.729693, acc [0.89266085 0.83157305 0.79053749 0.7146833 ] \n",
      "\n",
      "2019-02-22T00:16:01.683992: step 18240, loss 0.727784, acc [0.89130859 0.82841797 0.78808594 0.71142578]\n",
      "2019-02-22T00:16:02.855540: step 18280, loss 0.735653, acc [0.88730469 0.82470703 0.79003906 0.70908203]\n",
      "2019-02-22T00:16:04.028579: step 18320, loss 0.723779, acc [0.89384766 0.83339844 0.790625   0.71650391]\n",
      "2019-02-22T00:16:05.206082: step 18360, loss 0.738364, acc [0.89492187 0.83457031 0.78457031 0.71660156]\n",
      "2019-02-22T00:16:06.427727: step 18400, loss 0.73951, acc [0.89013672 0.83164063 0.78349609 0.71337891]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:16:10.272219: step 18400, loss 0.730313, acc [0.89336163 0.83229385 0.7906276  0.7155843 ] \n",
      "\n",
      "2019-02-22T00:16:11.526104: step 18440, loss 0.722087, acc [0.89658203 0.83544922 0.78222656 0.71386719]\n",
      "2019-02-22T00:16:12.693687: step 18480, loss 0.727958, acc [0.89921875 0.83623047 0.78818359 0.71943359]\n",
      "2019-02-22T00:16:13.862823: step 18520, loss 0.73748, acc [0.89052734 0.83164063 0.78505859 0.71005859]\n",
      "2019-02-22T00:16:15.028850: step 18560, loss 0.739738, acc [0.89023438 0.83242187 0.78515625 0.70917969]\n",
      "2019-02-22T00:16:16.193953: step 18600, loss 0.746946, acc [0.89130859 0.82841797 0.78154297 0.70703125]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:16:20.095518: step 18600, loss 0.727597, acc [0.8942326  0.83224379 0.79073772 0.71536405] \n",
      "\n",
      "2019-02-22T00:16:21.338459: step 18640, loss 0.735402, acc [0.89169922 0.82802734 0.78369141 0.70927734]\n",
      "2019-02-22T00:16:22.506064: step 18680, loss 0.728213, acc [0.89775391 0.83154297 0.78515625 0.70986328]\n",
      "2019-02-22T00:16:23.672632: step 18720, loss 0.717046, acc [0.89589844 0.83300781 0.78955078 0.71464844]\n",
      "2019-02-22T00:16:24.965701: step 18760, loss 0.69011, acc [0.89444247 0.83400016 0.79645478 0.7193606 ]\n",
      "2019-02-22T00:16:26.123859: step 18800, loss 0.67656, acc [0.89414063 0.83457031 0.79541016 0.71835938]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:16:30.032335: step 18800, loss 0.725407, acc [0.89322147 0.8320736  0.79198911 0.7163952 ] \n",
      "\n",
      "2019-02-22T00:16:31.274316: step 18840, loss 0.680701, acc [0.89716797 0.83515625 0.79453125 0.71855469]\n",
      "2019-02-22T00:16:32.435452: step 18880, loss 0.683556, acc [0.89013672 0.83310547 0.79375    0.71425781]\n",
      "2019-02-22T00:16:33.596586: step 18920, loss 0.682531, acc [0.89433594 0.83671875 0.79511719 0.71289062]\n",
      "2019-02-22T00:16:34.753751: step 18960, loss 0.675957, acc [0.89169922 0.83681641 0.79882812 0.71835938]\n",
      "2019-02-22T00:16:35.911415: step 19000, loss 0.678838, acc [0.89628906 0.83710938 0.79921875 0.72070312]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:16:39.824353: step 19000, loss 0.733104, acc [0.89287109 0.83212366 0.79106808 0.7163051 ] \n",
      "\n",
      "2019-02-22T00:16:41.061375: step 19040, loss 0.715, acc [0.8890625  0.82988281 0.79316406 0.7125    ]\n",
      "2019-02-22T00:16:42.222508: step 19080, loss 0.688211, acc [0.89238281 0.83496094 0.79765625 0.71816406]\n",
      "2019-02-22T00:16:43.379180: step 19120, loss 0.688172, acc [0.89677734 0.83173828 0.796875   0.72128906]\n",
      "2019-02-22T00:16:44.539323: step 19160, loss 0.705857, acc [0.89023438 0.834375   0.79414063 0.71396484]\n",
      "2019-02-22T00:16:45.701448: step 19200, loss 0.702277, acc [0.88876953 0.82636719 0.79394531 0.70888672]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:16:49.586148: step 19200, loss 0.733419, acc [0.89392225 0.83051187 0.79115819 0.71476339] \n",
      "\n",
      "2019-02-22T00:16:50.856370: step 19240, loss 0.692793, acc [0.89287109 0.83066406 0.79628906 0.71699219]\n",
      "2019-02-22T00:16:52.009568: step 19280, loss 0.691594, acc [0.89277344 0.82832031 0.79169922 0.71025391]\n",
      "2019-02-22T00:16:53.187565: step 19320, loss 0.694306, acc [0.89658203 0.83691406 0.79355469 0.72070312]\n",
      "2019-02-22T00:16:54.357629: step 19360, loss 0.701598, acc [0.89208984 0.83310547 0.79345703 0.71679688]\n",
      "2019-02-22T00:16:55.518761: step 19400, loss 0.724145, acc [0.89199219 0.83261719 0.78330078 0.709375  ]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:16:59.378132: step 19400, loss 0.729433, acc [0.89254072 0.83102244 0.79104806 0.71269109] \n",
      "\n",
      "2019-02-22T00:17:00.617636: step 19440, loss 0.715334, acc [0.890625   0.83271484 0.79501953 0.71523437]\n",
      "2019-02-22T00:17:01.778274: step 19480, loss 0.699422, acc [0.89199219 0.83183594 0.79394531 0.71416016]\n",
      "2019-02-22T00:17:02.943376: step 19520, loss 0.683443, acc [0.89384766 0.834375   0.79746094 0.71904297]\n",
      "2019-02-22T00:17:04.175438: step 19560, loss 0.739503, acc [0.88828125 0.8265625  0.78154297 0.70683594]\n",
      "2019-02-22T00:17:05.338062: step 19600, loss 0.717728, acc [0.88955078 0.82841797 0.78642578 0.70996094]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:17:09.194454: step 19600, loss 0.728034, acc [0.89331157 0.83317482 0.79039734 0.71482345] \n",
      "\n",
      "2019-02-22T00:17:10.431478: step 19640, loss 0.742485, acc [0.88857422 0.82617188 0.78427734 0.70820313]\n",
      "2019-02-22T00:17:11.590130: step 19680, loss 0.713129, acc [0.89775391 0.83447266 0.78828125 0.71523437]\n",
      "2019-02-22T00:17:12.753249: step 19720, loss 0.734403, acc [0.89257812 0.82753906 0.78574219 0.71191406]\n",
      "2019-02-22T00:17:13.925295: step 19760, loss 0.695184, acc [0.89628906 0.83310547 0.79580078 0.7171875 ]\n",
      "2019-02-22T00:17:15.159342: step 19800, loss 0.7438, acc [0.89150391 0.83134766 0.78017578 0.70703125]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:17:19.043513: step 19800, loss 0.725678, acc [0.8936019  0.83320485 0.79122826 0.71587462] \n",
      "\n",
      "2019-02-22T00:17:20.294918: step 19840, loss 0.71568, acc [0.8921875  0.83134766 0.78642578 0.70947266]\n",
      "2019-02-22T00:17:21.455062: step 19880, loss 0.716189, acc [0.89521484 0.82978516 0.78867188 0.70976562]\n",
      "2019-02-22T00:17:22.619172: step 19920, loss 0.720588, acc [0.89443359 0.83066406 0.78359375 0.71162109]\n",
      "2019-02-22T00:17:23.780804: step 19960, loss 0.744728, acc [0.89101562 0.82724609 0.78261719 0.71181641]\n",
      "2019-02-22T00:17:24.944417: step 20000, loss 0.721424, acc [0.89287109 0.83632812 0.78925781 0.71464844]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:17:28.894586: step 20000, loss 0.726502, acc [0.89345173 0.83263422 0.79190902 0.71570443] \n",
      "\n",
      "2019-02-22T00:17:30.153898: step 20040, loss 0.740681, acc [0.89267578 0.82851562 0.78378906 0.70761719]\n",
      "2019-02-22T00:17:31.323465: step 20080, loss 0.713571, acc [0.89306641 0.83349609 0.78740234 0.71181641]\n",
      "2019-02-22T00:17:32.486084: step 20120, loss 0.731419, acc [0.89335937 0.83183594 0.78330078 0.71132812]\n",
      "2019-02-22T00:17:33.648709: step 20160, loss 0.731286, acc [0.89248047 0.82958984 0.78632813 0.71054688]\n",
      "2019-02-22T00:17:34.813313: step 20200, loss 0.723026, acc [0.89199219 0.82802734 0.78925781 0.71396484]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:17:38.667229: step 20200, loss 0.723187, acc [0.89420256 0.83173322 0.79355084 0.71712601] \n",
      "\n",
      "2019-02-22T00:17:39.909705: step 20240, loss 0.723181, acc [0.89697266 0.83994141 0.78808594 0.71845703]\n",
      "2019-02-22T00:17:41.072328: step 20280, loss 0.756592, acc [0.89111328 0.82714844 0.78339844 0.70820313]\n",
      "2019-02-22T00:17:42.373830: step 20320, loss 0.666991, acc [0.89546934 0.83756116 0.79700028 0.72200225]\n",
      "2019-02-22T00:17:43.530007: step 20360, loss 0.658971, acc [0.89384766 0.83427734 0.80458984 0.72324219]\n",
      "2019-02-22T00:17:44.692131: step 20400, loss 0.695276, acc [0.88603516 0.83408203 0.78964844 0.70917969]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:17:48.598127: step 20400, loss 0.727164, acc [0.89403237 0.83339507 0.79254973 0.71562434] \n",
      "\n",
      "2019-02-22T00:17:49.849533: step 20440, loss 0.674625, acc [0.88701172 0.83193359 0.79667969 0.71601563]\n",
      "2019-02-22T00:17:51.012194: step 20480, loss 0.686084, acc [0.89609375 0.83476562 0.79160156 0.71386719]\n",
      "2019-02-22T00:17:52.164361: step 20520, loss 0.681895, acc [0.89482422 0.83798828 0.79707031 0.71787109]\n",
      "2019-02-22T00:17:53.322520: step 20560, loss 0.668198, acc [0.89296875 0.83886719 0.79990234 0.72451172]\n",
      "2019-02-22T00:17:54.480679: step 20600, loss 0.702557, acc [0.88789063 0.82666016 0.78710938 0.70371094]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:17:58.315300: step 20600, loss 0.727488, acc [0.89361191 0.83199351 0.79397131 0.71728619] \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-02-22T00:17:59.560704: step 20640, loss 0.686854, acc [0.89394531 0.83417969 0.79941406 0.71865234]\n",
      "2019-02-22T00:18:00.718367: step 20680, loss 0.690234, acc [0.89492187 0.83798828 0.79306641 0.715625  ]\n",
      "2019-02-22T00:18:01.880491: step 20720, loss 0.702688, acc [0.89306641 0.83251953 0.78994141 0.71416016]\n",
      "2019-02-22T00:18:03.045593: step 20760, loss 0.701977, acc [0.89023438 0.82910156 0.79511719 0.71328125]\n",
      "2019-02-22T00:18:04.206233: step 20800, loss 0.694495, acc [0.89189453 0.83310547 0.79560547 0.71904297]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:18:08.118219: step 20800, loss 0.726644, acc [0.89283104 0.83159307 0.7935108  0.71635515] \n",
      "\n",
      "2019-02-22T00:18:09.362641: step 20840, loss 0.703784, acc [0.89228516 0.83222656 0.79296875 0.71533203]\n",
      "2019-02-22T00:18:10.523776: step 20880, loss 0.70453, acc [0.89335937 0.83095703 0.79189453 0.71230469]\n",
      "2019-02-22T00:18:11.682927: step 20920, loss 0.702717, acc [0.89394531 0.82910156 0.79404297 0.71679688]\n",
      "2019-02-22T00:18:12.841582: step 20960, loss 0.681705, acc [0.89570313 0.83720703 0.79697266 0.71972656]\n",
      "2019-02-22T00:18:14.002716: step 21000, loss 0.697204, acc [0.89091797 0.83515625 0.79189453 0.71552734]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:18:17.843785: step 21000, loss 0.726412, acc [0.89354183 0.83286448 0.79407142 0.71631511] \n",
      "\n",
      "2019-02-22T00:18:19.077780: step 21040, loss 0.710782, acc [0.89433594 0.83447266 0.78886719 0.71230469]\n",
      "2019-02-22T00:18:20.239906: step 21080, loss 0.696869, acc [0.89482422 0.83232422 0.79111328 0.71035156]\n",
      "2019-02-22T00:18:21.399057: step 21120, loss 0.722759, acc [0.89384766 0.8328125  0.78974609 0.71279297]\n",
      "2019-02-22T00:18:22.563185: step 21160, loss 0.677414, acc [0.89697266 0.8375     0.79228516 0.71728516]\n",
      "2019-02-22T00:18:23.726284: step 21200, loss 0.738942, acc [0.89091797 0.82841797 0.78193359 0.70742187]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:18:27.612938: step 21200, loss 0.722614, acc [0.89335162 0.83308472 0.79381113 0.71575449] \n",
      "\n",
      "2019-02-22T00:18:28.849960: step 21240, loss 0.717737, acc [0.89335937 0.83193359 0.78925781 0.70986328]\n",
      "2019-02-22T00:18:30.010596: step 21280, loss 0.705478, acc [0.89208984 0.83427734 0.78896484 0.71445313]\n",
      "2019-02-22T00:18:31.179668: step 21320, loss 0.713771, acc [0.89550781 0.83359375 0.79003906 0.71757812]\n",
      "2019-02-22T00:18:32.340304: step 21360, loss 0.712536, acc [0.88759766 0.83300781 0.79306641 0.71337891]\n",
      "2019-02-22T00:18:33.507390: step 21400, loss 0.683059, acc [0.8984375  0.83779297 0.79814453 0.72324219]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:18:37.466997: step 21400, loss 0.723354, acc [0.89367198 0.83256414 0.79079779 0.71454314] \n",
      "\n",
      "2019-02-22T00:18:38.717865: step 21440, loss 0.726931, acc [0.89423828 0.83427734 0.78710938 0.70908203]\n",
      "2019-02-22T00:18:39.878007: step 21480, loss 0.713844, acc [0.89072266 0.82919922 0.79257813 0.71523437]\n",
      "2019-02-22T00:18:41.039638: step 21520, loss 0.718207, acc [0.89589844 0.83310547 0.78466797 0.71181641]\n",
      "2019-02-22T00:18:42.207257: step 21560, loss 0.7185, acc [0.89355469 0.82841797 0.78740234 0.70966797]\n",
      "2019-02-22T00:18:43.368849: step 21600, loss 0.711407, acc [0.89189453 0.83154297 0.78769531 0.71005859]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:18:47.248059: step 21600, loss 0.717294, acc [0.89372203 0.83332499 0.795493   0.71889798] \n",
      "\n",
      "2019-02-22T00:18:48.484089: step 21640, loss 0.709572, acc [0.89208984 0.83222656 0.79316406 0.71494141]\n",
      "2019-02-22T00:18:49.647257: step 21680, loss 0.698779, acc [0.89453125 0.83291016 0.79414063 0.71699219]\n",
      "2019-02-22T00:18:50.804375: step 21720, loss 0.719533, acc [0.89599609 0.83222656 0.79023438 0.71699219]\n",
      "2019-02-22T00:18:51.965509: step 21760, loss 0.701648, acc [0.89472656 0.83427734 0.79521484 0.72011719]\n",
      "2019-02-22T00:18:53.132596: step 21800, loss 0.716667, acc [0.89121094 0.83183594 0.78388672 0.71191406]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:18:56.990534: step 21800, loss 0.719358, acc [0.89410246 0.83283445 0.79462203 0.71791689] \n",
      "\n",
      "2019-02-22T00:18:58.232460: step 21840, loss 0.717424, acc [0.89375    0.83408203 0.79013672 0.71611328]\n",
      "2019-02-22T00:18:59.519082: step 21880, loss 0.696016, acc [0.89457367 0.83249487 0.79232461 0.7142588 ]\n",
      "2019-02-22T00:19:00.679226: step 21920, loss 0.646564, acc [0.89814453 0.84130859 0.80751953 0.72646484]\n",
      "2019-02-22T00:19:01.835894: step 21960, loss 0.677505, acc [0.89658203 0.83691406 0.79707031 0.71728516]\n",
      "2019-02-22T00:19:02.997527: step 22000, loss 0.663555, acc [0.89775391 0.83525391 0.79990234 0.72001953]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:19:06.900049: step 22000, loss 0.711609, acc [0.89437275 0.83447627 0.79739511 0.72107039] \n",
      "\n",
      "2019-02-22T00:19:08.138063: step 22040, loss 0.672679, acc [0.89306641 0.83623047 0.80078125 0.72236328]\n",
      "2019-02-22T00:19:09.297707: step 22080, loss 0.676614, acc [0.89443359 0.83300781 0.79716797 0.7140625 ]\n",
      "2019-02-22T00:19:10.456858: step 22120, loss 0.672706, acc [0.89501953 0.83369141 0.79921875 0.71982422]\n",
      "2019-02-22T00:19:11.612090: step 22160, loss 0.673754, acc [0.88916016 0.83164063 0.79726562 0.71308594]\n",
      "2019-02-22T00:19:12.784087: step 22200, loss 0.677659, acc [0.89238281 0.83466797 0.79804688 0.71855469]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:19:16.774899: step 22200, loss 0.719308, acc [0.8925207  0.83407582 0.79490234 0.71600477] \n",
      "\n",
      "2019-02-22T00:19:18.023825: step 22240, loss 0.695095, acc [0.89482422 0.83320313 0.79599609 0.71953125]\n",
      "2019-02-22T00:19:19.186447: step 22280, loss 0.694102, acc [0.89453125 0.83691406 0.79472656 0.71503906]\n",
      "2019-02-22T00:19:20.350558: step 22320, loss 0.670577, acc [0.89912109 0.83408203 0.80078125 0.72128906]\n",
      "2019-02-22T00:19:21.516155: step 22360, loss 0.669134, acc [0.89521484 0.83212891 0.79677734 0.71757812]\n",
      "2019-02-22T00:19:22.670409: step 22400, loss 0.68066, acc [0.89257812 0.83496094 0.80146484 0.72119141]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:19:26.574388: step 22400, loss 0.719349, acc [0.89408243 0.8337855  0.79505251 0.7168457 ] \n",
      "\n",
      "2019-02-22T00:19:27.835186: step 22440, loss 0.70453, acc [0.88691406 0.82675781 0.79101562 0.709375  ]\n",
      "2019-02-22T00:19:29.000785: step 22480, loss 0.704261, acc [0.89267578 0.82988281 0.79482422 0.71582031]\n",
      "2019-02-22T00:19:30.169854: step 22520, loss 0.671385, acc [0.89267578 0.83925781 0.79638672 0.71855469]\n",
      "2019-02-22T00:19:31.329004: step 22560, loss 0.67644, acc [0.9        0.84111328 0.79580078 0.72089844]\n",
      "2019-02-22T00:19:32.494509: step 22600, loss 0.709533, acc [0.89355469 0.82998047 0.79394531 0.71748047]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:19:36.408938: step 22600, loss 0.717163, acc [0.89255073 0.83529718 0.79496241 0.7164853 ] \n",
      "\n",
      "2019-02-22T00:19:37.647447: step 22640, loss 0.708887, acc [0.89492187 0.83457031 0.78916016 0.71298828]\n",
      "2019-02-22T00:19:38.804117: step 22680, loss 0.703384, acc [0.88994141 0.82783203 0.79443359 0.71103516]\n",
      "2019-02-22T00:19:40.015843: step 22720, loss 0.695168, acc [0.89248047 0.83183594 0.79287109 0.71386719]\n",
      "2019-02-22T00:19:41.177475: step 22760, loss 0.705714, acc [0.89824219 0.83496094 0.79423828 0.71582031]\n",
      "2019-02-22T00:19:42.340592: step 22800, loss 0.710613, acc [0.89072266 0.83808594 0.79150391 0.71777344]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:19:46.239641: step 22800, loss 0.72005, acc [0.8942326  0.83249407 0.79454194 0.71708597] \n",
      "\n",
      "2019-02-22T00:19:47.475177: step 22840, loss 0.722233, acc [0.88671875 0.82666016 0.78085938 0.70566406]\n",
      "2019-02-22T00:19:48.631846: step 22880, loss 0.688691, acc [0.89248047 0.83388672 0.79316406 0.71484375]\n",
      "2019-02-22T00:19:49.855973: step 22920, loss 0.696887, acc [0.89541016 0.83837891 0.79316406 0.72167969]\n",
      "2019-02-22T00:19:51.010164: step 22960, loss 0.688346, acc [0.88994141 0.83554688 0.79560547 0.71757812]\n",
      "2019-02-22T00:19:52.164849: step 23000, loss 0.713132, acc [0.89052734 0.82998047 0.79287109 0.71416016]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:19:56.102133: step 23000, loss 0.716599, acc [0.89312136 0.83460641 0.79418154 0.7172061 ] \n",
      "\n",
      "2019-02-22T00:19:57.338123: step 23040, loss 0.712164, acc [0.89121094 0.83017578 0.78896484 0.71269531]\n",
      "2019-02-22T00:19:58.512152: step 23080, loss 0.714195, acc [0.89755859 0.83466797 0.78671875 0.71494141]\n",
      "2019-02-22T00:19:59.687174: step 23120, loss 0.698, acc [0.88994141 0.83105469 0.79306641 0.71728516]\n",
      "2019-02-22T00:20:00.871125: step 23160, loss 0.683582, acc [0.89384766 0.84033203 0.79960937 0.72255859]\n",
      "2019-02-22T00:20:02.039699: step 23200, loss 0.692367, acc [0.89541016 0.83896484 0.79433594 0.72226563]\n",
      "\n",
      "Evaluation:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-02-22T00:20:05.968511: step 23200, loss 0.716598, acc [0.89254072 0.83210363 0.79547297 0.71467329] \n",
      "\n",
      "2019-02-22T00:20:07.210493: step 23240, loss 0.74024, acc [0.89306641 0.82568359 0.78623047 0.70986328]\n",
      "2019-02-22T00:20:08.384026: step 23280, loss 0.70753, acc [0.89453125 0.83105469 0.7890625  0.71582031]\n",
      "2019-02-22T00:20:09.552601: step 23320, loss 0.717738, acc [0.89169922 0.82724609 0.79179687 0.71416016]\n",
      "2019-02-22T00:20:10.723656: step 23360, loss 0.725622, acc [0.89111328 0.83066406 0.78095703 0.70820313]\n",
      "2019-02-22T00:20:11.926949: step 23400, loss 0.699684, acc [0.89443359 0.83681641 0.79667969 0.72128906]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:20:15.945040: step 23400, loss 0.710024, acc [0.89458299 0.83503689 0.79766541 0.72038963] \n",
      "\n",
      "2019-02-22T00:20:17.406254: step 23440, loss 0.662512, acc [0.89314927 0.83096098 0.80307173 0.71906072]\n",
      "2019-02-22T00:20:18.576811: step 23480, loss 0.657123, acc [0.89609375 0.83779297 0.80478516 0.72597656]\n",
      "2019-02-22T00:20:19.745387: step 23520, loss 0.658364, acc [0.89599609 0.83876953 0.80205078 0.72265625]\n",
      "2019-02-22T00:20:20.907512: step 23560, loss 0.669078, acc [0.89560547 0.84052734 0.79892578 0.71943359]\n",
      "2019-02-22T00:20:22.073607: step 23600, loss 0.680513, acc [0.89199219 0.83154297 0.79970703 0.72099609]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:20:25.929505: step 23600, loss 0.716456, acc [0.89366197 0.8329746  0.79462203 0.71565438] \n",
      "\n",
      "2019-02-22T00:20:27.176946: step 23640, loss 0.639934, acc [0.89619141 0.83740234 0.8046875  0.72509766]\n",
      "2019-02-22T00:20:28.347998: step 23680, loss 0.663199, acc [0.88730469 0.83574219 0.80683594 0.72304687]\n",
      "2019-02-22T00:20:29.514589: step 23720, loss 0.675741, acc [0.89443359 0.83916016 0.79648438 0.72236328]\n",
      "2019-02-22T00:20:30.682667: step 23760, loss 0.667013, acc [0.89648438 0.84072266 0.80205078 0.72675781]\n",
      "2019-02-22T00:20:31.857192: step 23800, loss 0.665723, acc [0.89638672 0.83710938 0.79824219 0.71826172]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:20:35.762740: step 23800, loss 0.713122, acc [0.89370201 0.8331548  0.79718488 0.71884792] \n",
      "\n",
      "2019-02-22T00:20:37.023521: step 23840, loss 0.679209, acc [0.89609375 0.83310547 0.80244141 0.72167969]\n",
      "2019-02-22T00:20:38.191106: step 23880, loss 0.681452, acc [0.89257812 0.83476562 0.79716797 0.71884766]\n",
      "2019-02-22T00:20:39.353229: step 23920, loss 0.653323, acc [0.89785156 0.83994141 0.80419922 0.72626953]\n",
      "2019-02-22T00:20:40.518336: step 23960, loss 0.693883, acc [0.89316406 0.83457031 0.7953125  0.71503906]\n",
      "2019-02-22T00:20:41.682940: step 24000, loss 0.683386, acc [0.89052734 0.83320313 0.79296875 0.71435547]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:20:45.503126: step 24000, loss 0.715177, acc [0.89348177 0.83322488 0.79605362 0.71714603] \n",
      "\n",
      "2019-02-22T00:20:46.741140: step 24040, loss 0.67884, acc [0.89521484 0.83574219 0.79550781 0.7171875 ]\n",
      "2019-02-22T00:20:47.906243: step 24080, loss 0.694497, acc [0.89121094 0.82724609 0.79853516 0.71503906]\n",
      "2019-02-22T00:20:49.069360: step 24120, loss 0.704797, acc [0.88671875 0.83037109 0.796875   0.71435547]\n",
      "2019-02-22T00:20:50.225038: step 24160, loss 0.688911, acc [0.89345703 0.83583984 0.79570312 0.71884766]\n",
      "2019-02-22T00:20:51.407005: step 24200, loss 0.693165, acc [0.89316406 0.82783203 0.79882812 0.71494141]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:20:55.339289: step 24200, loss 0.71289, acc [0.89467309 0.83301464 0.79651413 0.71736628] \n",
      "\n",
      "2019-02-22T00:20:56.600119: step 24240, loss 0.688506, acc [0.890625   0.83261719 0.79677734 0.71513672]\n",
      "2019-02-22T00:20:57.779604: step 24280, loss 0.68381, acc [0.89238281 0.83388672 0.79892578 0.71787109]\n",
      "2019-02-22T00:20:58.951650: step 24320, loss 0.686242, acc [0.89228516 0.83476562 0.79794922 0.72011719]\n",
      "2019-02-22T00:21:00.123202: step 24360, loss 0.696826, acc [0.89492187 0.83056641 0.79365234 0.71347656]\n",
      "2019-02-22T00:21:01.297232: step 24400, loss 0.69858, acc [0.89736328 0.83671875 0.79453125 0.71855469]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:21:05.195289: step 24400, loss 0.709902, acc [0.89537387 0.83559751 0.79619377 0.71915827] \n",
      "\n",
      "2019-02-22T00:21:06.452151: step 24440, loss 0.705288, acc [0.8921875  0.82988281 0.79726562 0.71767578]\n",
      "2019-02-22T00:21:07.619736: step 24480, loss 0.691592, acc [0.89492187 0.83242187 0.79667969 0.71386719]\n",
      "2019-02-22T00:21:08.792773: step 24520, loss 0.719942, acc [0.89238281 0.83251953 0.78896484 0.70830078]\n",
      "2019-02-22T00:21:09.961347: step 24560, loss 0.691849, acc [0.89150391 0.83662109 0.79316406 0.71542969]\n",
      "2019-02-22T00:21:11.217714: step 24600, loss 0.694128, acc [0.89414063 0.83642578 0.79472656 0.71435547]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:21:15.164876: step 24600, loss 0.707511, acc [0.89479322 0.83525714 0.79867653 0.72036961] \n",
      "\n",
      "2019-02-22T00:21:16.424716: step 24640, loss 0.680991, acc [0.89609375 0.83828125 0.79482422 0.72089844]\n",
      "2019-02-22T00:21:17.599736: step 24680, loss 0.68373, acc [0.89570313 0.83369141 0.79550781 0.71777344]\n",
      "2019-02-22T00:21:18.775751: step 24720, loss 0.718399, acc [0.88798828 0.82929688 0.78935547 0.70908203]\n",
      "2019-02-22T00:21:19.956725: step 24760, loss 0.69464, acc [0.89658203 0.83339844 0.79169922 0.71474609]\n",
      "2019-02-22T00:21:21.127779: step 24800, loss 0.711737, acc [0.89550781 0.8328125  0.79033203 0.71757812]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:21:24.996574: step 24800, loss 0.709478, acc [0.89411247 0.83353523 0.79715484 0.71847751] \n",
      "\n",
      "2019-02-22T00:21:26.250487: step 24840, loss 0.700248, acc [0.89130859 0.83154297 0.79501953 0.715625  ]\n",
      "2019-02-22T00:21:27.421064: step 24880, loss 0.69531, acc [0.89580078 0.83535156 0.79716797 0.72412109]\n",
      "2019-02-22T00:21:28.598522: step 24920, loss 0.705502, acc [0.89052734 0.83486328 0.79091797 0.71328125]\n",
      "2019-02-22T00:21:29.765607: step 24960, loss 0.675379, acc [0.89619141 0.83798828 0.8015625  0.72421875]\n",
      "2019-02-22T00:21:31.069093: step 25000, loss 0.646471, acc [0.89900864 0.84041686 0.80654198 0.72242148]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:21:35.012784: step 25000, loss 0.708757, acc [0.89453293 0.83547738 0.79762536 0.71951866] \n",
      "\n",
      "2019-02-22T00:21:36.276095: step 25040, loss 0.632813, acc [0.89814453 0.84111328 0.80605469 0.72363281]\n",
      "2019-02-22T00:21:37.446157: step 25080, loss 0.642341, acc [0.89677734 0.83652344 0.80917969 0.73212891]\n",
      "2019-02-22T00:21:38.678217: step 25120, loss 0.655045, acc [0.89042969 0.83564453 0.80136719 0.71660156]\n",
      "2019-02-22T00:21:39.840344: step 25160, loss 0.657899, acc [0.89560547 0.83730469 0.80410156 0.71953125]\n",
      "2019-02-22T00:21:41.004503: step 25200, loss 0.6477, acc [0.89580078 0.83779297 0.80966797 0.72646484]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:21:44.878706: step 25200, loss 0.714602, acc [0.89503349 0.83511698 0.79625384 0.71992912] \n",
      "\n",
      "2019-02-22T00:21:46.131599: step 25240, loss 0.659064, acc [0.89599609 0.83818359 0.80566406 0.72607422]\n",
      "2019-02-22T00:21:47.299181: step 25280, loss 0.661888, acc [0.89335937 0.83730469 0.803125   0.72099609]\n",
      "2019-02-22T00:21:48.461804: step 25320, loss 0.676498, acc [0.89042969 0.82871094 0.79716797 0.71064453]\n",
      "2019-02-22T00:21:49.679482: step 25360, loss 0.656418, acc [0.89746094 0.83535156 0.80585938 0.72333984]\n",
      "2019-02-22T00:21:50.843593: step 25400, loss 0.653996, acc [0.8984375  0.84404297 0.80146484 0.72421875]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:21:54.727764: step 25400, loss 0.709303, acc [0.89400234 0.83510697 0.79761535 0.71815715] \n",
      "\n",
      "2019-02-22T00:21:55.977185: step 25440, loss 0.708403, acc [0.88691406 0.82910156 0.79658203 0.71455078]\n",
      "2019-02-22T00:21:57.141296: step 25480, loss 0.665112, acc [0.89257812 0.83857422 0.79980469 0.71855469]\n",
      "2019-02-22T00:21:58.307389: step 25520, loss 0.703549, acc [0.89082031 0.82949219 0.79199219 0.71123047]\n",
      "2019-02-22T00:21:59.478445: step 25560, loss 0.649575, acc [0.89980469 0.84150391 0.80537109 0.72724609]\n",
      "2019-02-22T00:22:00.656443: step 25600, loss 0.665056, acc [0.89804688 0.83701172 0.80205078 0.72119141]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:22:04.588265: step 25600, loss 0.708737, acc [0.89469311 0.83446626 0.79787564 0.71849753] \n",
      "\n",
      "2019-02-22T00:22:05.865924: step 25640, loss 0.681098, acc [0.89072266 0.83330078 0.79755859 0.71308594]\n",
      "2019-02-22T00:22:07.035492: step 25680, loss 0.677245, acc [0.89052734 0.83056641 0.80107422 0.71679688]\n",
      "2019-02-22T00:22:08.215472: step 25720, loss 0.67498, acc [0.89453125 0.83300781 0.80429688 0.72128906]\n",
      "2019-02-22T00:22:09.383055: step 25760, loss 0.678443, acc [0.89775391 0.83505859 0.79833984 0.72138672]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-02-22T00:22:10.553117: step 25800, loss 0.686817, acc [0.89140625 0.83681641 0.79511719 0.71621094]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:22:14.473495: step 25800, loss 0.703968, acc [0.89401236 0.83525714 0.80062869 0.72119052] \n",
      "\n",
      "2019-02-22T00:22:15.717463: step 25840, loss 0.676137, acc [0.89667969 0.83603516 0.79970703 0.71933594]\n",
      "2019-02-22T00:22:16.881572: step 25880, loss 0.682139, acc [0.89179688 0.83320313 0.79628906 0.71601563]\n",
      "2019-02-22T00:22:18.043699: step 25920, loss 0.700917, acc [0.89199219 0.83203125 0.79443359 0.71386719]\n",
      "2019-02-22T00:22:19.206369: step 25960, loss 0.677293, acc [0.89853516 0.83740234 0.79560547 0.71835938]\n",
      "2019-02-22T00:22:20.375391: step 26000, loss 0.683682, acc [0.89101562 0.83320313 0.80039063 0.71708984]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:22:24.299292: step 26000, loss 0.704677, acc [0.89408243 0.83677882 0.79876663 0.71941855] \n",
      "\n",
      "2019-02-22T00:22:25.561562: step 26040, loss 0.697941, acc [0.89189453 0.83134766 0.79638672 0.71582031]\n",
      "2019-02-22T00:22:26.731622: step 26080, loss 0.692017, acc [0.89394531 0.83837891 0.79326172 0.7171875 ]\n",
      "2019-02-22T00:22:27.903725: step 26120, loss 0.697178, acc [0.89248047 0.83066406 0.79335937 0.71318359]\n",
      "2019-02-22T00:22:29.071746: step 26160, loss 0.66198, acc [0.90185547 0.84160156 0.80087891 0.72607422]\n",
      "2019-02-22T00:22:30.253712: step 26200, loss 0.677038, acc [0.89599609 0.83222656 0.79658203 0.71318359]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:22:34.152763: step 26200, loss 0.706974, acc [0.89457298 0.8344162  0.79944739 0.72134069] \n",
      "\n",
      "2019-02-22T00:22:35.408137: step 26240, loss 0.674619, acc [0.89785156 0.84013672 0.79863281 0.7234375 ]\n",
      "2019-02-22T00:22:36.579192: step 26280, loss 0.694385, acc [0.89306641 0.83085937 0.79130859 0.71318359]\n",
      "2019-02-22T00:22:37.745782: step 26320, loss 0.711864, acc [0.89267578 0.82773438 0.78535156 0.71015625]\n",
      "2019-02-22T00:22:38.918821: step 26360, loss 0.692944, acc [0.89609375 0.83408203 0.7953125  0.71630859]\n",
      "2019-02-22T00:22:40.133039: step 26400, loss 0.678948, acc [0.89619141 0.83408203 0.79580078 0.71582031]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:22:44.008270: step 26400, loss 0.699913, acc [0.89556408 0.83651854 0.79988788 0.7215309 ] \n",
      "\n",
      "2019-02-22T00:22:45.283979: step 26440, loss 0.700219, acc [0.89423828 0.83486328 0.78876953 0.715625  ]\n",
      "2019-02-22T00:22:46.455530: step 26480, loss 0.694484, acc [0.89423828 0.83535156 0.79365234 0.71630859]\n",
      "2019-02-22T00:22:47.626089: step 26520, loss 0.709935, acc [0.88720703 0.83056641 0.79726562 0.71582031]\n",
      "2019-02-22T00:22:48.926599: step 26560, loss 0.656538, acc [0.89321832 0.83588917 0.804215   0.72370778]\n",
      "2019-02-22T00:22:50.089221: step 26600, loss 0.631609, acc [0.89921875 0.84033203 0.80869141 0.72705078]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:22:53.995710: step 26600, loss 0.698225, acc [0.89514361 0.83598795 0.80105918 0.72220164] \n",
      "\n",
      "2019-02-22T00:22:55.334910: step 26640, loss 0.637711, acc [0.89707031 0.83525391 0.80996094 0.72734375]\n",
      "2019-02-22T00:22:56.494060: step 26680, loss 0.646167, acc [0.89443359 0.83242187 0.80527344 0.721875  ]\n",
      "2019-02-22T00:22:57.658172: step 26720, loss 0.637745, acc [0.89589844 0.84013672 0.80507812 0.72089844]\n",
      "2019-02-22T00:22:58.832695: step 26760, loss 0.637878, acc [0.89638672 0.84169922 0.80654297 0.72919922]\n",
      "2019-02-22T00:22:59.994862: step 26800, loss 0.635077, acc [0.89863281 0.83994141 0.81152344 0.73125   ]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:23:03.876554: step 26800, loss 0.700465, acc [0.8945029  0.8370291  0.79956752 0.71948863] \n",
      "\n",
      "2019-02-22T00:23:05.110064: step 26840, loss 0.635922, acc [0.89765625 0.84189453 0.80800781 0.72734375]\n",
      "2019-02-22T00:23:06.274670: step 26880, loss 0.65032, acc [0.89326172 0.83632812 0.80488281 0.72119141]\n",
      "2019-02-22T00:23:07.436299: step 26920, loss 0.653011, acc [0.8953125  0.83662109 0.80371094 0.72080078]\n",
      "2019-02-22T00:23:08.604874: step 26960, loss 0.661801, acc [0.90048828 0.83886719 0.80302734 0.72587891]\n",
      "2019-02-22T00:23:09.768985: step 27000, loss 0.671459, acc [0.89169922 0.83310547 0.79785156 0.7171875 ]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:23:13.720660: step 27000, loss 0.706941, acc [0.8949534  0.83622821 0.79832614 0.71978897] \n",
      "\n",
      "2019-02-22T00:23:14.966066: step 27040, loss 0.684908, acc [0.88701172 0.82744141 0.79912109 0.71503906]\n",
      "2019-02-22T00:23:16.129679: step 27080, loss 0.674088, acc [0.89560547 0.83798828 0.79628906 0.7203125 ]\n",
      "2019-02-22T00:23:17.325039: step 27120, loss 0.69358, acc [0.88710937 0.82929688 0.79326172 0.70908203]\n",
      "2019-02-22T00:23:18.557100: step 27160, loss 0.661741, acc [0.89873047 0.83349609 0.80517578 0.72080078]\n",
      "2019-02-22T00:23:19.764362: step 27200, loss 0.6656, acc [0.89287109 0.83476562 0.80419922 0.72207031]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:23:23.687717: step 27200, loss 0.701595, acc [0.8946831  0.83412588 0.79915706 0.72048974] \n",
      "\n",
      "2019-02-22T00:23:24.932179: step 27240, loss 0.680882, acc [0.89150391 0.83193359 0.79941406 0.71796875]\n",
      "2019-02-22T00:23:26.090835: step 27280, loss 0.684534, acc [0.89130859 0.83925781 0.79423828 0.71894531]\n",
      "2019-02-22T00:23:27.295660: step 27320, loss 0.674324, acc [0.89658203 0.83271484 0.79443359 0.71445313]\n",
      "2019-02-22T00:23:28.487007: step 27360, loss 0.672697, acc [0.89628906 0.83623047 0.80078125 0.72353516]\n",
      "2019-02-22T00:23:29.649628: step 27400, loss 0.669839, acc [0.89238281 0.83173828 0.80087891 0.71962891]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:23:33.526857: step 27400, loss 0.701207, acc [0.89454294 0.83566759 0.79994794 0.72072   ] \n",
      "\n",
      "2019-02-22T00:23:34.792646: step 27440, loss 0.667371, acc [0.89355469 0.8328125  0.79619141 0.71455078]\n",
      "2019-02-22T00:23:35.957252: step 27480, loss 0.692788, acc [0.88955078 0.83056641 0.79472656 0.71582031]\n",
      "2019-02-22T00:23:37.121364: step 27520, loss 0.688071, acc [0.89082031 0.83525391 0.79433594 0.71650391]\n",
      "2019-02-22T00:23:38.282495: step 27560, loss 0.685544, acc [0.89121094 0.82617188 0.79453125 0.71425781]\n",
      "2019-02-22T00:23:39.464463: step 27600, loss 0.688638, acc [0.89462891 0.83535156 0.79560547 0.71181641]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:23:43.438409: step 27600, loss 0.701829, acc [0.89500345 0.833335   0.80163982 0.72106038] \n",
      "\n",
      "2019-02-22T00:23:44.677416: step 27640, loss 0.696027, acc [0.89560547 0.83271484 0.79316406 0.71425781]\n",
      "2019-02-22T00:23:45.837557: step 27680, loss 0.678622, acc [0.89511719 0.83779297 0.79589844 0.71845703]\n",
      "2019-02-22T00:23:46.998196: step 27720, loss 0.690524, acc [0.89072266 0.83544922 0.79619141 0.71757812]\n",
      "2019-02-22T00:23:48.163297: step 27760, loss 0.690609, acc [0.89462891 0.83671875 0.79550781 0.71982422]\n",
      "2019-02-22T00:23:49.326419: step 27800, loss 0.681248, acc [0.89667969 0.83642578 0.79960937 0.72158203]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:23:53.242367: step 27800, loss 0.702788, acc [0.89607464 0.83583778 0.79892681 0.72046972] \n",
      "\n",
      "2019-02-22T00:23:54.482826: step 27840, loss 0.670811, acc [0.89238281 0.83330078 0.80175781 0.72246094]\n",
      "2019-02-22T00:23:55.645449: step 27880, loss 0.662137, acc [0.89804688 0.83212891 0.79853516 0.71738281]\n",
      "2019-02-22T00:23:56.803109: step 27920, loss 0.699019, acc [0.890625   0.83076172 0.79296875 0.71630859]\n",
      "2019-02-22T00:23:57.959285: step 27960, loss 0.695204, acc [0.89550781 0.83925781 0.79394531 0.71591797]\n",
      "2019-02-22T00:23:59.114962: step 28000, loss 0.674679, acc [0.89736328 0.83242187 0.79882812 0.71757812]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:24:03.040302: step 28000, loss 0.701449, acc [0.89478321 0.83585781 0.79851636 0.72000921] \n",
      "\n",
      "2019-02-22T00:24:04.273354: step 28040, loss 0.676719, acc [0.89384766 0.83925781 0.79853516 0.72021484]\n",
      "2019-02-22T00:24:05.500457: step 28080, loss 0.687074, acc [0.89521484 0.83554688 0.79863281 0.72001953]\n",
      "2019-02-22T00:24:06.794022: step 28120, loss 0.655553, acc [0.89752703 0.83763711 0.80097064 0.72305477]\n",
      "2019-02-22T00:24:07.952679: step 28160, loss 0.615943, acc [0.89873047 0.84599609 0.81464844 0.73466797]\n",
      "2019-02-22T00:24:09.111828: step 28200, loss 0.634719, acc [0.89619141 0.84287109 0.8125     0.72822266]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:24:12.988607: step 28200, loss 0.696276, acc [0.89503348 0.83641842 0.80258086 0.7229725 ] \n",
      "\n",
      "2019-02-22T00:24:14.228558: step 28240, loss 0.629296, acc [0.89404297 0.83388672 0.81123047 0.72548828]\n",
      "2019-02-22T00:24:15.401101: step 28280, loss 0.652139, acc [0.89628906 0.83242187 0.80693359 0.721875  ]\n",
      "2019-02-22T00:24:16.562730: step 28320, loss 0.628161, acc [0.89921875 0.84091797 0.81435547 0.73134766]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-02-22T00:24:17.728824: step 28360, loss 0.652755, acc [0.89609375 0.83457031 0.80927734 0.72158203]\n",
      "2019-02-22T00:24:18.891942: step 28400, loss 0.64628, acc [0.89560547 0.84003906 0.8046875  0.72724609]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:24:22.769666: step 28400, loss 0.700599, acc [0.89439278 0.83584779 0.8015297  0.72089019] \n",
      "\n",
      "2019-02-22T00:24:24.009168: step 28440, loss 0.63583, acc [0.89453125 0.83359375 0.80957031 0.72363281]\n",
      "2019-02-22T00:24:25.170302: step 28480, loss 0.651858, acc [0.89462891 0.83623047 0.80439453 0.72294922]\n",
      "2019-02-22T00:24:26.339869: step 28520, loss 0.649544, acc [0.90009766 0.84433594 0.80039063 0.72275391]\n",
      "2019-02-22T00:24:27.502491: step 28560, loss 0.670534, acc [0.88828125 0.8359375  0.80009766 0.71728516]\n",
      "2019-02-22T00:24:28.661641: step 28600, loss 0.65895, acc [0.89824219 0.84111328 0.79960937 0.721875  ]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:24:32.534436: step 28600, loss 0.70441, acc [0.89510356 0.83740952 0.79826608 0.72040965] \n",
      "\n",
      "2019-02-22T00:24:33.839874: step 28640, loss 0.66225, acc [0.896875   0.83828125 0.80175781 0.71972656]\n",
      "2019-02-22T00:24:35.007455: step 28680, loss 0.640783, acc [0.89560547 0.84140625 0.80947266 0.7265625 ]\n",
      "2019-02-22T00:24:36.172061: step 28720, loss 0.669454, acc [0.89375    0.83212891 0.80166016 0.71923828]\n",
      "2019-02-22T00:24:37.335677: step 28760, loss 0.653651, acc [0.8953125  0.83515625 0.80283203 0.72041016]\n",
      "2019-02-22T00:24:38.504870: step 28800, loss 0.666678, acc [0.89765625 0.83701172 0.80400391 0.72226563]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:24:42.394991: step 28800, loss 0.698215, acc [0.89544394 0.83687894 0.80002803 0.72076004] \n",
      "\n",
      "2019-02-22T00:24:43.633501: step 28840, loss 0.662905, acc [0.89208984 0.83183594 0.80244141 0.71601563]\n",
      "2019-02-22T00:24:44.848700: step 28880, loss 0.669191, acc [0.89248047 0.83583984 0.80283203 0.71943359]\n",
      "2019-02-22T00:24:46.009337: step 28920, loss 0.659144, acc [0.89873047 0.84003906 0.80390625 0.72597656]\n",
      "2019-02-22T00:24:47.168983: step 28960, loss 0.658871, acc [0.89580078 0.83701172 0.80332031 0.72587891]\n",
      "2019-02-22T00:24:48.334583: step 29000, loss 0.671046, acc [0.89619141 0.83701172 0.803125   0.72333984]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:24:52.232190: step 29000, loss 0.698343, acc [0.89551402 0.83443622 0.80181001 0.72087016] \n",
      "\n",
      "2019-02-22T00:24:53.481071: step 29040, loss 0.667501, acc [0.89316406 0.8359375  0.79833984 0.71708984]\n",
      "2019-02-22T00:24:54.644235: step 29080, loss 0.670102, acc [0.89482422 0.83251953 0.80068359 0.72099609]\n",
      "2019-02-22T00:24:55.806315: step 29120, loss 0.683282, acc [0.89101562 0.83994141 0.79980469 0.71943359]\n",
      "2019-02-22T00:24:56.961004: step 29160, loss 0.662551, acc [0.89804688 0.84130859 0.79824219 0.71943359]\n",
      "2019-02-22T00:24:58.120152: step 29200, loss 0.702469, acc [0.88847656 0.83007812 0.79179687 0.71132812]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:25:02.097604: step 29200, loss 0.699744, acc [0.89409244 0.83588784 0.79964761 0.71928841] \n",
      "\n",
      "2019-02-22T00:25:03.340048: step 29240, loss 0.669825, acc [0.89951172 0.84013672 0.79824219 0.72285156]\n",
      "2019-02-22T00:25:04.498207: step 29280, loss 0.670528, acc [0.89794922 0.83720703 0.79882812 0.71953125]\n",
      "2019-02-22T00:25:05.666820: step 29320, loss 0.684916, acc [0.88828125 0.83017578 0.79833984 0.71337891]\n",
      "2019-02-22T00:25:06.828907: step 29360, loss 0.691711, acc [0.88701172 0.8296875  0.79042969 0.70908203]\n",
      "2019-02-22T00:25:07.989545: step 29400, loss 0.685686, acc [0.89873047 0.83769531 0.79628906 0.71835938]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:25:11.959526: step 29400, loss 0.693079, acc [0.89510357 0.83707916 0.8011693  0.721621  ] \n",
      "\n",
      "2019-02-22T00:25:13.261026: step 29440, loss 0.668641, acc [0.89863281 0.83535156 0.79658203 0.71777344]\n",
      "2019-02-22T00:25:14.463328: step 29480, loss 0.687448, acc [0.89121094 0.83876953 0.80068359 0.72207031]\n",
      "2019-02-22T00:25:15.636368: step 29520, loss 0.690035, acc [0.89589844 0.834375   0.79169922 0.71533203]\n",
      "2019-02-22T00:25:16.803951: step 29560, loss 0.709053, acc [0.89433594 0.83730469 0.79736328 0.71826172]\n",
      "2019-02-22T00:25:17.973516: step 29600, loss 0.672102, acc [0.89345703 0.83505859 0.80019531 0.71914062]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:25:21.872603: step 29600, loss 0.692406, acc [0.89466308 0.83676881 0.80227052 0.72102033] \n",
      "\n",
      "2019-02-22T00:25:23.117524: step 29640, loss 0.704158, acc [0.89130859 0.828125   0.79287109 0.71484375]\n",
      "2019-02-22T00:25:24.416050: step 29680, loss 0.650448, acc [0.89522372 0.83795869 0.80388849 0.72000375]\n",
      "2019-02-22T00:25:25.581154: step 29720, loss 0.639258, acc [0.90126953 0.83496094 0.80517578 0.72255859]\n",
      "2019-02-22T00:25:26.800867: step 29760, loss 0.629102, acc [0.8921875  0.83554688 0.81054688 0.725     ]\n",
      "2019-02-22T00:25:27.961454: step 29800, loss 0.652421, acc [0.89814453 0.83710938 0.80458984 0.72304687]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:25:31.909609: step 29800, loss 0.698945, acc [0.89625484 0.83448628 0.80101913 0.72051978] \n",
      "\n",
      "2019-02-22T00:25:33.143158: step 29840, loss 0.640826, acc [0.88994141 0.83710938 0.81230469 0.72666016]\n",
      "2019-02-22T00:25:34.307270: step 29880, loss 0.651058, acc [0.89345703 0.83515625 0.81015625 0.725     ]\n",
      "2019-02-22T00:25:35.470882: step 29920, loss 0.626209, acc [0.89990234 0.83642578 0.80498047 0.72275391]\n",
      "2019-02-22T00:25:36.641487: step 29960, loss 0.642275, acc [0.89453125 0.83779297 0.80449219 0.72333984]\n",
      "2019-02-22T00:25:37.808529: step 30000, loss 0.639862, acc [0.89560547 0.83701172 0.80810547 0.72314453]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:25:41.698155: step 30000, loss 0.698692, acc [0.89553404 0.83629829 0.80072881 0.72013935] \n",
      "\n",
      "2019-02-22T00:25:42.942617: step 30040, loss 0.640669, acc [0.89707031 0.84160156 0.80595703 0.72226563]\n",
      "2019-02-22T00:25:44.145910: step 30080, loss 0.634819, acc [0.89697266 0.83818359 0.81015625 0.72626953]\n",
      "2019-02-22T00:25:45.331350: step 30120, loss 0.647097, acc [0.89404297 0.83349609 0.80761719 0.72050781]\n",
      "2019-02-22T00:25:46.492979: step 30160, loss 0.644569, acc [0.896875   0.83945313 0.80771484 0.72421875]\n",
      "2019-02-22T00:25:47.653617: step 30200, loss 0.647358, acc [0.89609375 0.83984375 0.80615234 0.72275391]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:25:51.553166: step 30200, loss 0.694194, acc [0.89370201 0.83615814 0.8028812  0.72059986] \n",
      "\n",
      "2019-02-22T00:25:52.794153: step 30240, loss 0.663513, acc [0.88710937 0.83457031 0.80175781 0.71660156]\n",
      "2019-02-22T00:25:53.971165: step 30280, loss 0.658978, acc [0.89306641 0.83769531 0.79970703 0.71601563]\n",
      "2019-02-22T00:25:55.136264: step 30320, loss 0.670782, acc [0.89169922 0.83525391 0.80292969 0.71865234]\n",
      "2019-02-22T00:25:56.299878: step 30360, loss 0.656791, acc [0.89208984 0.83886719 0.79990234 0.71660156]\n",
      "2019-02-22T00:25:57.462996: step 30400, loss 0.653329, acc [0.89814453 0.83867187 0.80136719 0.72373047]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:26:01.413170: step 30400, loss 0.698014, acc [0.89534383 0.83518706 0.79842625 0.71922834] \n",
      "\n",
      "2019-02-22T00:26:02.657100: step 30440, loss 0.669433, acc [0.89267578 0.8359375  0.80419922 0.72138672]\n",
      "2019-02-22T00:26:03.818234: step 30480, loss 0.66543, acc [0.89365234 0.83505859 0.79873047 0.72158203]\n",
      "2019-02-22T00:26:04.989288: step 30520, loss 0.678828, acc [0.89326172 0.83115234 0.79726562 0.71738281]\n",
      "2019-02-22T00:26:06.235241: step 30560, loss 0.651777, acc [0.89951172 0.83935547 0.80839844 0.72958984]\n",
      "2019-02-22T00:26:07.406789: step 30600, loss 0.676718, acc [0.89443359 0.82910156 0.79814453 0.71513672]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:26:11.301408: step 30600, loss 0.695404, acc [0.89604461 0.83591787 0.80138954 0.72128062] \n",
      "\n",
      "2019-02-22T00:26:12.584029: step 30640, loss 0.663824, acc [0.89443359 0.83369141 0.80390625 0.72402344]\n",
      "2019-02-22T00:26:13.747662: step 30680, loss 0.662768, acc [0.89326172 0.83681641 0.79921875 0.72167969]\n",
      "2019-02-22T00:26:14.911755: step 30720, loss 0.658934, acc [0.89394531 0.84042969 0.80214844 0.72158203]\n",
      "2019-02-22T00:26:16.076360: step 30760, loss 0.653353, acc [0.89785156 0.83701172 0.803125   0.72519531]\n",
      "2019-02-22T00:26:17.255847: step 30800, loss 0.66708, acc [0.89345703 0.83515625 0.80205078 0.71914062]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:26:21.173747: step 30800, loss 0.696211, acc [0.89595451 0.83600797 0.80181001 0.72151088] \n",
      "\n",
      "2019-02-22T00:26:22.420687: step 30840, loss 0.650566, acc [0.89980469 0.84130859 0.80292969 0.72431641]\n",
      "2019-02-22T00:26:23.591246: step 30880, loss 0.657823, acc [0.89599609 0.83847656 0.80898437 0.72695312]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-02-22T00:26:24.748908: step 30920, loss 0.677787, acc [0.89111328 0.83847656 0.80195313 0.71962891]\n",
      "2019-02-22T00:26:25.911037: step 30960, loss 0.666002, acc [0.89462891 0.83671875 0.79863281 0.71865234]\n",
      "2019-02-22T00:26:27.069193: step 31000, loss 0.688174, acc [0.89101562 0.82919922 0.79853516 0.71640625]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:26:31.001985: step 31000, loss 0.693238, acc [0.89652514 0.8359479  0.80254082 0.72210154] \n",
      "\n",
      "2019-02-22T00:26:32.237025: step 31040, loss 0.682692, acc [0.89599609 0.83554688 0.79882812 0.71591797]\n",
      "2019-02-22T00:26:33.401631: step 31080, loss 0.652483, acc [0.89716797 0.83935547 0.80390625 0.72402344]\n",
      "2019-02-22T00:26:34.559294: step 31120, loss 0.67296, acc [0.89384766 0.83105469 0.80117187 0.71796875]\n",
      "2019-02-22T00:26:35.718940: step 31160, loss 0.681983, acc [0.89580078 0.83173828 0.79833984 0.71708984]\n",
      "2019-02-22T00:26:36.912315: step 31200, loss 0.680399, acc [0.89199219 0.83359375 0.79375    0.71386719]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:26:41.062837: step 31200, loss 0.693456, acc [0.89446285 0.83686892 0.80224049 0.72106038] \n",
      "\n",
      "2019-02-22T00:26:42.510163: step 31240, loss 0.663151, acc [0.89378748 0.83835425 0.80446753 0.72383404]\n",
      "2019-02-22T00:26:43.709986: step 31280, loss 0.617026, acc [0.89462891 0.84267578 0.81386719 0.73154297]\n",
      "2019-02-22T00:26:44.934607: step 31320, loss 0.623621, acc [0.89677734 0.83564453 0.80957031 0.725     ]\n",
      "2019-02-22T00:26:46.159230: step 31360, loss 0.627679, acc [0.89882812 0.83886719 0.81425781 0.72958984]\n",
      "2019-02-22T00:26:47.333754: step 31400, loss 0.647919, acc [0.89296875 0.83671875 0.81083984 0.72607422]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:26:51.291378: step 31400, loss 0.690673, acc [0.89682547 0.83689896 0.80397241 0.72372333] \n",
      "\n",
      "2019-02-22T00:26:52.553652: step 31440, loss 0.630745, acc [0.89384766 0.84082031 0.81416016 0.72724609]\n",
      "2019-02-22T00:26:53.723714: step 31480, loss 0.617532, acc [0.89707031 0.83691406 0.81142578 0.72792969]\n",
      "2019-02-22T00:26:54.895762: step 31520, loss 0.629639, acc [0.89707031 0.84238281 0.80859375 0.72568359]\n",
      "2019-02-22T00:26:56.141710: step 31560, loss 0.626088, acc [0.89628906 0.84238281 0.81230469 0.73007813]\n",
      "2019-02-22T00:26:57.312766: step 31600, loss 0.649244, acc [0.89658203 0.83359375 0.81005859 0.72763672]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:27:01.188504: step 31600, loss 0.699934, acc [0.8957643  0.83557749 0.80200022 0.72140076] \n",
      "\n",
      "2019-02-22T00:27:02.434476: step 31640, loss 0.646102, acc [0.89443359 0.83896484 0.80732422 0.72519531]\n",
      "2019-02-22T00:27:03.612452: step 31680, loss 0.637704, acc [0.89921875 0.83867187 0.80449219 0.72285156]\n",
      "2019-02-22T00:27:04.777058: step 31720, loss 0.631177, acc [0.90048828 0.84365234 0.81171875 0.7328125 ]\n",
      "2019-02-22T00:27:05.996262: step 31760, loss 0.675138, acc [0.89111328 0.82998047 0.79931641 0.71396484]\n",
      "2019-02-22T00:27:07.163312: step 31800, loss 0.647597, acc [0.8984375  0.84130859 0.80761719 0.7265625 ]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:27:11.086212: step 31800, loss 0.694581, acc [0.8961247  0.83696903 0.80373214 0.72299252] \n",
      "\n",
      "2019-02-22T00:27:12.383743: step 31840, loss 0.623579, acc [0.89580078 0.84160156 0.81005859 0.72578125]\n",
      "2019-02-22T00:27:13.585524: step 31880, loss 0.648506, acc [0.89560547 0.84199219 0.80595703 0.728125  ]\n",
      "2019-02-22T00:27:14.760533: step 31920, loss 0.65603, acc [0.89384766 0.83466797 0.80683594 0.72294922]\n",
      "2019-02-22T00:27:15.929602: step 31960, loss 0.644978, acc [0.89755859 0.83798828 0.80390625 0.72314453]\n",
      "2019-02-22T00:27:17.155712: step 32000, loss 0.658981, acc [0.89648438 0.83525391 0.80507812 0.71914062]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:27:21.034428: step 32000, loss 0.697294, acc [0.89563415 0.83663867 0.80223048 0.72230175] \n",
      "\n",
      "2019-02-22T00:27:22.284347: step 32040, loss 0.675445, acc [0.89335937 0.83007812 0.79863281 0.715625  ]\n",
      "2019-02-22T00:27:23.445481: step 32080, loss 0.669844, acc [0.89580078 0.83583984 0.80175781 0.71914062]\n",
      "2019-02-22T00:27:24.609590: step 32120, loss 0.638818, acc [0.89755859 0.84316406 0.80791016 0.72890625]\n",
      "2019-02-22T00:27:25.811892: step 32160, loss 0.661138, acc [0.89580078 0.83681641 0.80263672 0.71826172]\n",
      "2019-02-22T00:27:26.979513: step 32200, loss 0.655889, acc [0.89746094 0.84082031 0.80419922 0.72861328]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:27:30.874062: step 32200, loss 0.69253, acc [0.89608465 0.83715925 0.80319154 0.7236933 ] \n",
      "\n",
      "2019-02-22T00:27:32.112075: step 32240, loss 0.654022, acc [0.89990234 0.8359375  0.80732422 0.72041016]\n",
      "2019-02-22T00:27:33.277223: step 32280, loss 0.678385, acc [0.88984375 0.83095703 0.79560547 0.71269531]\n",
      "2019-02-22T00:27:34.485433: step 32320, loss 0.651488, acc [0.89472656 0.83544922 0.80048828 0.71494141]\n",
      "2019-02-22T00:27:35.647062: step 32360, loss 0.683857, acc [0.89570313 0.83613281 0.79462891 0.71396484]\n",
      "2019-02-22T00:27:36.807703: step 32400, loss 0.661752, acc [0.89277344 0.83554688 0.80253906 0.71914062]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:27:40.761311: step 32400, loss 0.691633, acc [0.89525373 0.83658861 0.80246073 0.72219163] \n",
      "\n",
      "2019-02-22T00:27:42.006764: step 32440, loss 0.659154, acc [0.89736328 0.834375   0.79970703 0.71816406]\n",
      "2019-02-22T00:27:43.167900: step 32480, loss 0.668703, acc [0.89707031 0.83417969 0.80224609 0.72138672]\n",
      "2019-02-22T00:27:44.397977: step 32520, loss 0.673029, acc [0.89248047 0.83759766 0.79814453 0.71757812]\n",
      "2019-02-22T00:27:45.571019: step 32560, loss 0.662369, acc [0.89736328 0.83710938 0.80332031 0.72275391]\n",
      "2019-02-22T00:27:46.734630: step 32600, loss 0.677532, acc [0.89384766 0.83681641 0.80078125 0.72041016]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:27:50.649058: step 32600, loss 0.688983, acc [0.89590445 0.83780997 0.80271101 0.72149086] \n",
      "\n",
      "2019-02-22T00:27:51.924272: step 32640, loss 0.680884, acc [0.89570313 0.84013672 0.80039063 0.72392578]\n",
      "2019-02-22T00:27:53.126078: step 32680, loss 0.674183, acc [0.89560547 0.83183594 0.79609375 0.71728516]\n",
      "2019-02-22T00:27:54.376491: step 32720, loss 0.675261, acc [0.89199219 0.82832031 0.79794922 0.71875   ]\n",
      "2019-02-22T00:27:55.609545: step 32760, loss 0.671974, acc [0.89492187 0.83164063 0.79511719 0.71289062]\n",
      "2019-02-22T00:27:56.992889: step 32800, loss 0.622685, acc [0.90412721 0.84684343 0.81566939 0.73666746]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:28:00.979007: step 32800, loss 0.687836, acc [0.8947732  0.83749962 0.80319154 0.7226121 ] \n",
      "\n",
      "2019-02-22T00:28:02.243308: step 32840, loss 0.604366, acc [0.90175781 0.84423828 0.81689453 0.73554688]\n",
      "2019-02-22T00:28:03.480348: step 32880, loss 0.649987, acc [0.89384766 0.83300781 0.80673828 0.721875  ]\n",
      "2019-02-22T00:28:04.693048: step 32920, loss 0.606692, acc [0.90234375 0.853125   0.81552734 0.73994141]\n",
      "2019-02-22T00:28:05.893861: step 32960, loss 0.61072, acc [0.89921875 0.84980469 0.81787109 0.73798828]\n",
      "2019-02-22T00:28:07.104596: step 33000, loss 0.63584, acc [0.89365234 0.83701172 0.80761719 0.72431641]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:28:10.998223: step 33000, loss 0.693532, acc [0.89532381 0.8347766  0.80349188 0.72079008] \n",
      "\n",
      "2019-02-22T00:28:12.263018: step 33040, loss 0.639109, acc [0.89130859 0.83789062 0.80625    0.72314453]\n",
      "2019-02-22T00:28:13.432556: step 33080, loss 0.641071, acc [0.89873047 0.83886719 0.80410156 0.72119141]\n",
      "2019-02-22T00:28:14.602618: step 33120, loss 0.62124, acc [0.89365234 0.83779297 0.81259766 0.72304687]\n",
      "2019-02-22T00:28:15.796984: step 33160, loss 0.629717, acc [0.89697266 0.83955078 0.81455078 0.72880859]\n",
      "2019-02-22T00:28:17.003750: step 33200, loss 0.653411, acc [0.890625   0.83144531 0.80566406 0.7171875 ]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:28:20.930087: step 33200, loss 0.692299, acc [0.89564417 0.83654857 0.80317152 0.72251199] \n",
      "\n",
      "2019-02-22T00:28:22.177024: step 33240, loss 0.656719, acc [0.89316406 0.83535156 0.80644531 0.71953125]\n",
      "2019-02-22T00:28:23.348078: step 33280, loss 0.640977, acc [0.90029297 0.83779297 0.809375   0.72402344]\n",
      "2019-02-22T00:28:24.513220: step 33320, loss 0.628775, acc [0.89882812 0.84023437 0.80722656 0.72548828]\n",
      "2019-02-22T00:28:25.744250: step 33360, loss 0.649438, acc [0.89443359 0.83291016 0.80546875 0.72216797]\n",
      "2019-02-22T00:28:26.904393: step 33400, loss 0.644984, acc [0.8953125  0.83662109 0.80664062 0.72255859]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:28:30.768264: step 33400, loss 0.693661, acc [0.89664528 0.83598795 0.8022505  0.72106038] \n",
      "\n",
      "2019-02-22T00:28:32.156529: step 33440, loss 0.637833, acc [0.90029297 0.84521484 0.81015625 0.73056641]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-02-22T00:28:33.318200: step 33480, loss 0.650971, acc [0.89599609 0.83964844 0.81103516 0.72695312]\n",
      "2019-02-22T00:28:34.480781: step 33520, loss 0.660451, acc [0.89306641 0.83417969 0.80166016 0.72314453]\n",
      "2019-02-22T00:28:35.642412: step 33560, loss 0.647712, acc [0.89550781 0.83935547 0.80927734 0.72675781]\n",
      "2019-02-22T00:28:36.802603: step 33600, loss 0.629162, acc [0.90068359 0.84326172 0.81142578 0.72783203]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:28:40.701108: step 33600, loss 0.689329, acc [0.8957643  0.83737949 0.80446296 0.72343301] \n",
      "\n",
      "2019-02-22T00:28:41.960451: step 33640, loss 0.63818, acc [0.90146484 0.84658203 0.80546875 0.73134766]\n",
      "2019-02-22T00:28:43.147376: step 33680, loss 0.656755, acc [0.89541016 0.83896484 0.80429688 0.72216797]\n",
      "2019-02-22T00:28:44.325872: step 33720, loss 0.656623, acc [0.89306641 0.8328125  0.80458984 0.7203125 ]\n",
      "2019-02-22T00:28:45.544047: step 33760, loss 0.657227, acc [0.89794922 0.83720703 0.79960937 0.72177734]\n",
      "2019-02-22T00:28:46.750811: step 33800, loss 0.665072, acc [0.89570313 0.83105469 0.80683594 0.72480469]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:28:50.737703: step 33800, loss 0.692584, acc [0.89632492 0.83718928 0.80254082 0.72224169] \n",
      "\n",
      "2019-02-22T00:28:52.023782: step 33840, loss 0.665166, acc [0.89609375 0.83164063 0.80605469 0.72265625]\n",
      "2019-02-22T00:28:53.198308: step 33880, loss 0.678339, acc [0.8890625  0.83007812 0.79677734 0.71259766]\n",
      "2019-02-22T00:28:54.372352: step 33920, loss 0.635392, acc [0.89912109 0.83994141 0.809375   0.72871094]\n",
      "2019-02-22T00:28:55.564719: step 33960, loss 0.671929, acc [0.89238281 0.83378906 0.80019531 0.71787109]\n",
      "2019-02-22T00:28:56.729326: step 34000, loss 0.662446, acc [0.89824219 0.84003906 0.80400391 0.72587891]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:29:00.615997: step 34000, loss 0.688412, acc [0.8971158  0.83842065 0.80523381 0.72535515] \n",
      "\n",
      "2019-02-22T00:29:02.033045: step 34040, loss 0.655169, acc [0.89296875 0.834375   0.80087891 0.71416016]\n",
      "2019-02-22T00:29:03.191701: step 34080, loss 0.649382, acc [0.89697266 0.84003906 0.80292969 0.72148437]\n",
      "2019-02-22T00:29:04.367219: step 34120, loss 0.663618, acc [0.896875   0.83291016 0.80302734 0.72158203]\n",
      "2019-02-22T00:29:05.529841: step 34160, loss 0.640708, acc [0.9        0.83886719 0.80957031 0.72695312]\n",
      "2019-02-22T00:29:06.690977: step 34200, loss 0.667551, acc [0.88876953 0.83076172 0.80400391 0.71914062]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:29:10.612347: step 34200, loss 0.682243, acc [0.89506352 0.83585781 0.80653525 0.72320275] \n",
      "\n",
      "2019-02-22T00:29:11.857801: step 34240, loss 0.671906, acc [0.89599609 0.82978516 0.80009766 0.71855469]\n",
      "2019-02-22T00:29:13.043238: step 34280, loss 0.674713, acc [0.89335937 0.83408203 0.79931641 0.71669922]\n",
      "2019-02-22T00:29:14.265878: step 34320, loss 0.670976, acc [0.88613281 0.83378906 0.80078125 0.71777344]\n",
      "2019-02-22T00:29:15.577794: step 34360, loss 0.652163, acc [0.89326369 0.83648694 0.80331933 0.71746962]\n",
      "2019-02-22T00:29:16.761747: step 34400, loss 0.600508, acc [0.89492187 0.84667969 0.82441406 0.73720703]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:29:20.695048: step 34400, loss 0.687195, acc [0.89580434 0.8376598  0.80439287 0.72281232] \n",
      "\n",
      "2019-02-22T00:29:22.100728: step 34440, loss 0.598602, acc [0.90253906 0.84394531 0.81933594 0.73173828]\n",
      "2019-02-22T00:29:23.263305: step 34480, loss 0.615197, acc [0.89873047 0.84238281 0.81679687 0.73525391]\n",
      "2019-02-22T00:29:24.422950: step 34520, loss 0.59805, acc [0.90048828 0.84550781 0.81826172 0.73632812]\n",
      "2019-02-22T00:29:25.576646: step 34560, loss 0.631338, acc [0.89804688 0.83925781 0.81416016 0.73212891]\n",
      "2019-02-22T00:29:26.739764: step 34600, loss 0.632696, acc [0.89658203 0.83505859 0.81298828 0.72626953]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:29:30.637326: step 34600, loss 0.692253, acc [0.89559411 0.83696903 0.80399243 0.72227172] \n",
      "\n",
      "2019-02-22T00:29:31.874844: step 34640, loss 0.624414, acc [0.89130859 0.83417969 0.81728516 0.72753906]\n",
      "2019-02-22T00:29:33.055818: step 34680, loss 0.641716, acc [0.89472656 0.83496094 0.80634766 0.7203125 ]\n",
      "2019-02-22T00:29:34.232329: step 34720, loss 0.633929, acc [0.89326172 0.83515625 0.80615234 0.71523437]\n",
      "2019-02-22T00:29:35.420743: step 34760, loss 0.62577, acc [0.89677734 0.83857422 0.8109375  0.72636719]\n",
      "2019-02-22T00:29:36.640901: step 34800, loss 0.646273, acc [0.89775391 0.83828125 0.80888672 0.72568359]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:29:40.546896: step 34800, loss 0.689566, acc [0.89582436 0.83690897 0.8039624  0.72157094] \n",
      "\n",
      "2019-02-22T00:29:41.782939: step 34840, loss 0.623786, acc [0.89560547 0.84365234 0.81337891 0.73183594]\n",
      "2019-02-22T00:29:42.945052: step 34880, loss 0.641973, acc [0.89667969 0.83896484 0.80566406 0.72177734]\n",
      "2019-02-22T00:29:44.175626: step 34920, loss 0.630196, acc [0.90117187 0.84375    0.81005859 0.73398438]\n",
      "2019-02-22T00:29:45.335769: step 34960, loss 0.635829, acc [0.89453125 0.83798828 0.80664062 0.72294922]\n",
      "2019-02-22T00:29:46.496408: step 35000, loss 0.647126, acc [0.89892578 0.84238281 0.79990234 0.71884766]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:29:50.473375: step 35000, loss 0.690359, acc [0.89593449 0.83751965 0.80295128 0.72192133] \n",
      "\n",
      "2019-02-22T00:29:51.714814: step 35040, loss 0.639928, acc [0.89951172 0.83779297 0.80830078 0.72646484]\n",
      "2019-02-22T00:29:52.876446: step 35080, loss 0.640641, acc [0.89404297 0.83691406 0.80830078 0.72714844]\n",
      "2019-02-22T00:29:54.037083: step 35120, loss 0.655161, acc [0.89501953 0.834375   0.80458984 0.7203125 ]\n",
      "2019-02-22T00:29:55.203675: step 35160, loss 0.663541, acc [0.89521484 0.83193359 0.79814453 0.71748047]\n",
      "2019-02-22T00:29:56.413914: step 35200, loss 0.644755, acc [0.89042969 0.83769531 0.81103516 0.72617188]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:30:00.282753: step 35200, loss 0.692839, acc [0.89553404 0.836939   0.80379221 0.72181121] \n",
      "\n",
      "2019-02-22T00:30:01.528660: step 35240, loss 0.659739, acc [0.89169922 0.8375     0.80302734 0.72236328]\n",
      "2019-02-22T00:30:02.686859: step 35280, loss 0.638419, acc [0.90244141 0.84423828 0.80957031 0.73164063]\n",
      "2019-02-22T00:30:03.862829: step 35320, loss 0.64778, acc [0.89609375 0.83710938 0.81044922 0.72978516]\n",
      "2019-02-22T00:30:05.052733: step 35360, loss 0.677395, acc [0.89287109 0.83105469 0.79560547 0.715625  ]\n",
      "2019-02-22T00:30:06.209900: step 35400, loss 0.657131, acc [0.89462891 0.8390625  0.80322266 0.72392578]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:30:10.118868: step 35400, loss 0.685363, acc [0.89661524 0.83645847 0.80482335 0.72294247] \n",
      "\n",
      "2019-02-22T00:30:11.354900: step 35440, loss 0.64239, acc [0.89746094 0.83935547 0.80439453 0.72470703]\n",
      "2019-02-22T00:30:12.522481: step 35480, loss 0.654725, acc [0.89521484 0.83681641 0.80009766 0.71484375]\n",
      "2019-02-22T00:30:13.682129: step 35520, loss 0.650466, acc [0.89726562 0.83642578 0.80449219 0.7234375 ]\n",
      "2019-02-22T00:30:14.860126: step 35560, loss 0.652641, acc [0.89609375 0.83808594 0.80244141 0.72158203]\n",
      "2019-02-22T00:30:16.022252: step 35600, loss 0.647323, acc [0.89794922 0.84023437 0.80810547 0.72666016]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:30:19.982328: step 35600, loss 0.685338, acc [0.8955841  0.83789006 0.80480333 0.72272222] \n",
      "\n",
      "2019-02-22T00:30:21.231734: step 35640, loss 0.666389, acc [0.89179688 0.83671875 0.803125   0.72236328]\n",
      "2019-02-22T00:30:22.391876: step 35680, loss 0.68831, acc [0.8953125  0.82871094 0.79296875 0.71289062]\n",
      "2019-02-22T00:30:23.554002: step 35720, loss 0.675652, acc [0.89130859 0.83349609 0.79677734 0.71630859]\n",
      "2019-02-22T00:30:24.713153: step 35760, loss 0.656424, acc [0.8953125  0.83671875 0.80449219 0.72392578]\n",
      "2019-02-22T00:30:25.870320: step 35800, loss 0.662396, acc [0.89755859 0.83701172 0.79824219 0.72255859]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:30:29.768874: step 35800, loss 0.68315, acc [0.89542392 0.83698906 0.80436284 0.72264213] \n",
      "\n",
      "2019-02-22T00:30:31.000475: step 35840, loss 0.635977, acc [0.89697266 0.84462891 0.80966797 0.72753906]\n",
      "2019-02-22T00:30:32.157110: step 35880, loss 0.652687, acc [0.89580078 0.83583984 0.81044922 0.73193359]\n",
      "2019-02-22T00:30:33.448197: step 35920, loss 0.647068, acc [0.89517144 0.83905461 0.80539378 0.72224984]\n",
      "2019-02-22T00:30:34.607842: step 35960, loss 0.5939, acc [0.89384766 0.84179688 0.81914062 0.73486328]\n",
      "2019-02-22T00:30:35.767986: step 36000, loss 0.597691, acc [0.8984375  0.83847656 0.82041016 0.73154297]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:30:39.670059: step 36000, loss 0.68663, acc [0.89709578 0.83831053 0.80571434 0.72388351] \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-02-22T00:30:40.901080: step 36040, loss 0.617585, acc [0.89638672 0.84355469 0.81523437 0.73017578]\n",
      "2019-02-22T00:30:42.062216: step 36080, loss 0.617944, acc [0.89482422 0.84228516 0.81904297 0.72773438]\n",
      "2019-02-22T00:30:43.241704: step 36120, loss 0.634788, acc [0.89941406 0.83935547 0.81220703 0.72841797]\n",
      "2019-02-22T00:30:44.403344: step 36160, loss 0.629097, acc [0.89589844 0.83652344 0.81328125 0.72558594]\n",
      "2019-02-22T00:30:45.626962: step 36200, loss 0.618095, acc [0.89794922 0.83789062 0.81533203 0.72763672]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:30:49.788397: step 36200, loss 0.684062, acc [0.89532381 0.83864089 0.80645516 0.72426393] \n",
      "\n",
      "2019-02-22T00:30:51.253090: step 36240, loss 0.6387, acc [0.89414063 0.83808594 0.80664062 0.72304687]\n",
      "2019-02-22T00:30:52.436538: step 36280, loss 0.612846, acc [0.90322266 0.84316406 0.81542969 0.73310547]\n",
      "2019-02-22T00:30:53.679512: step 36320, loss 0.624825, acc [0.89472656 0.83867187 0.80849609 0.72617188]\n",
      "2019-02-22T00:30:54.861479: step 36360, loss 0.630586, acc [0.89882812 0.83554688 0.81455078 0.72734375]\n",
      "2019-02-22T00:30:56.101765: step 36400, loss 0.645051, acc [0.89355469 0.83994141 0.803125   0.72294922]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:31:00.034588: step 36400, loss 0.688178, acc [0.89705573 0.83748961 0.80559421 0.72363323] \n",
      "\n",
      "2019-02-22T00:31:01.284461: step 36440, loss 0.617254, acc [0.89853516 0.84580078 0.81298828 0.73115234]\n",
      "2019-02-22T00:31:02.447084: step 36480, loss 0.64992, acc [0.89296875 0.83417969 0.80537109 0.71621094]\n",
      "2019-02-22T00:31:03.617469: step 36520, loss 0.628029, acc [0.89775391 0.84423828 0.809375   0.72851562]\n",
      "2019-02-22T00:31:04.778066: step 36560, loss 0.650897, acc [0.89501953 0.83710938 0.80322266 0.71748047]\n",
      "2019-02-22T00:31:05.939202: step 36600, loss 0.632535, acc [0.89589844 0.84101563 0.80996094 0.72382813]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:31:09.849355: step 36600, loss 0.68901, acc [0.89631491 0.83766982 0.8051337  0.72390353] \n",
      "\n",
      "2019-02-22T00:31:11.088852: step 36640, loss 0.648203, acc [0.89335937 0.83320313 0.80390625 0.71699219]\n",
      "2019-02-22T00:31:12.257884: step 36680, loss 0.635884, acc [0.89628906 0.84189453 0.80761719 0.72734375]\n",
      "2019-02-22T00:31:13.420009: step 36720, loss 0.618721, acc [0.89775391 0.84326172 0.81289062 0.72958984]\n",
      "2019-02-22T00:31:14.620329: step 36760, loss 0.609927, acc [0.90097656 0.84765625 0.81542969 0.73701172]\n",
      "2019-02-22T00:31:15.831064: step 36800, loss 0.638635, acc [0.90009766 0.8453125  0.80439453 0.72744141]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:31:19.826831: step 36800, loss 0.686106, acc [0.8962148  0.83732944 0.80418264 0.72276227] \n",
      "\n",
      "2019-02-22T00:31:21.085185: step 36840, loss 0.64099, acc [0.89648438 0.83652344 0.80732422 0.72207031]\n",
      "2019-02-22T00:31:22.273598: step 36880, loss 0.647274, acc [0.89658203 0.83837891 0.80507812 0.72236328]\n",
      "2019-02-22T00:31:23.450109: step 36920, loss 0.661193, acc [0.88916016 0.82978516 0.79980469 0.71386719]\n",
      "2019-02-22T00:31:24.613721: step 36960, loss 0.650579, acc [0.89853516 0.83837891 0.80390625 0.72392578]\n",
      "2019-02-22T00:31:25.838344: step 37000, loss 0.658451, acc [0.89580078 0.83818359 0.80537109 0.721875  ]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:31:29.720064: step 37000, loss 0.683363, acc [0.89551402 0.83749962 0.80491345 0.72344302] \n",
      "\n",
      "2019-02-22T00:31:30.964496: step 37040, loss 0.641886, acc [0.89443359 0.83876953 0.80761719 0.72558594]\n",
      "2019-02-22T00:31:32.180190: step 37080, loss 0.647521, acc [0.89462891 0.83916016 0.80615234 0.72451172]\n",
      "2019-02-22T00:31:33.342813: step 37120, loss 0.659351, acc [0.89462891 0.83486328 0.80527344 0.72050781]\n",
      "2019-02-22T00:31:34.504939: step 37160, loss 0.643524, acc [0.89082031 0.834375   0.80576172 0.72148437]\n",
      "2019-02-22T00:31:35.666569: step 37200, loss 0.63273, acc [0.89648438 0.84003906 0.80917969 0.72333984]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:31:39.610756: step 37200, loss 0.686434, acc [0.89651513 0.83742955 0.80436284 0.72335292] \n",
      "\n",
      "2019-02-22T00:31:40.885066: step 37240, loss 0.668132, acc [0.89208984 0.83876953 0.79902344 0.72041016]\n",
      "2019-02-22T00:31:42.053142: step 37280, loss 0.675814, acc [0.89755859 0.83183594 0.79736328 0.71835938]\n",
      "2019-02-22T00:31:43.225725: step 37320, loss 0.632341, acc [0.90341797 0.84648437 0.81269531 0.73251953]\n",
      "2019-02-22T00:31:44.394755: step 37360, loss 0.679655, acc [0.89101562 0.83476562 0.79736328 0.71455078]\n",
      "2019-02-22T00:31:45.564322: step 37400, loss 0.641416, acc [0.89443359 0.83964844 0.80888672 0.72460938]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:31:49.718856: step 37400, loss 0.681041, acc [0.89569422 0.83800018 0.80663536 0.72538518] \n",
      "\n",
      "2019-02-22T00:31:50.981129: step 37440, loss 0.646476, acc [0.89853516 0.84179688 0.80742187 0.72822266]\n",
      "2019-02-22T00:31:52.284157: step 37480, loss 0.628214, acc [0.89621705 0.83547684 0.81054589 0.72550111]\n",
      "2019-02-22T00:31:53.453688: step 37520, loss 0.603733, acc [0.89951172 0.83925781 0.82021484 0.73056641]\n",
      "2019-02-22T00:31:54.686740: step 37560, loss 0.622533, acc [0.89472656 0.83945313 0.81503906 0.72597656]\n",
      "2019-02-22T00:31:55.850355: step 37600, loss 0.595932, acc [0.9        0.84580078 0.81474609 0.73359375]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:31:59.771726: step 37600, loss 0.683646, acc [0.89619478 0.83893121 0.80601467 0.72434402] \n",
      "\n",
      "2019-02-22T00:32:01.028093: step 37640, loss 0.633539, acc [0.89443359 0.83369141 0.81269531 0.72255859]\n",
      "2019-02-22T00:32:02.190714: step 37680, loss 0.603142, acc [0.89902344 0.83623047 0.81396484 0.72548828]\n",
      "2019-02-22T00:32:03.357800: step 37720, loss 0.606699, acc [0.89736328 0.84335938 0.82207031 0.73857422]\n",
      "2019-02-22T00:32:04.537287: step 37760, loss 0.623591, acc [0.89541016 0.83554688 0.81630859 0.72783203]\n",
      "2019-02-22T00:32:05.708838: step 37800, loss 0.625273, acc [0.89501953 0.84121094 0.8125     0.728125  ]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:32:09.628758: step 37800, loss 0.687164, acc [0.89562414 0.83746959 0.80507363 0.72267216] \n",
      "\n",
      "2019-02-22T00:32:10.867230: step 37840, loss 0.621775, acc [0.89892578 0.84541016 0.81269531 0.73193359]\n",
      "2019-02-22T00:32:12.031340: step 37880, loss 0.630118, acc [0.89326172 0.84316406 0.81289062 0.72714844]\n",
      "2019-02-22T00:32:13.196441: step 37920, loss 0.623157, acc [0.90175781 0.84375    0.81201172 0.73066406]\n",
      "2019-02-22T00:32:14.362576: step 37960, loss 0.610202, acc [0.89570313 0.83984375 0.81523437 0.72753906]\n",
      "2019-02-22T00:32:15.525656: step 38000, loss 0.633631, acc [0.89345703 0.83955078 0.80800781 0.72568359]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:32:19.531346: step 38000, loss 0.685186, acc [0.89602459 0.83769985 0.80598464 0.72314269] \n",
      "\n",
      "2019-02-22T00:32:20.783745: step 38040, loss 0.62506, acc [0.89716797 0.8390625  0.80927734 0.72353516]\n",
      "2019-02-22T00:32:21.953309: step 38080, loss 0.634758, acc [0.89375    0.83574219 0.81259766 0.72265625]\n",
      "2019-02-22T00:32:23.134781: step 38120, loss 0.64294, acc [0.89414063 0.83603516 0.80537109 0.72167969]\n",
      "2019-02-22T00:32:24.386683: step 38160, loss 0.6317, acc [0.89648438 0.84003906 0.80820313 0.72792969]\n",
      "2019-02-22T00:32:25.550297: step 38200, loss 0.635814, acc [0.89873047 0.83984375 0.80605469 0.72314453]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:32:29.457560: step 38200, loss 0.68207, acc [0.89582437 0.83812031 0.8064852  0.72313268] \n",
      "\n",
      "2019-02-22T00:32:30.700535: step 38240, loss 0.62082, acc [0.89853516 0.84208984 0.81230469 0.72998047]\n",
      "2019-02-22T00:32:31.865139: step 38280, loss 0.646449, acc [0.89589844 0.83652344 0.80732422 0.72421875]\n",
      "2019-02-22T00:32:33.029251: step 38320, loss 0.632159, acc [0.89755859 0.84277344 0.80800781 0.72353516]\n",
      "2019-02-22T00:32:34.189393: step 38360, loss 0.640766, acc [0.89755859 0.83769531 0.81025391 0.72587891]\n",
      "2019-02-22T00:32:35.354496: step 38400, loss 0.652581, acc [0.89521484 0.8359375  0.80390625 0.71982422]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:32:39.236715: step 38400, loss 0.682115, acc [0.89642503 0.83853077 0.8068456  0.7254953 ] \n",
      "\n",
      "2019-02-22T00:32:40.557577: step 38440, loss 0.632492, acc [0.89326172 0.8359375  0.81191406 0.725     ]\n",
      "2019-02-22T00:32:41.723125: step 38480, loss 0.638014, acc [0.89599609 0.83730469 0.80292969 0.71894531]\n",
      "2019-02-22T00:32:42.888725: step 38520, loss 0.648698, acc [0.89580078 0.83730469 0.80732422 0.72294922]\n",
      "2019-02-22T00:32:44.053332: step 38560, loss 0.651855, acc [0.89355469 0.83515625 0.80283203 0.71904297]\n",
      "2019-02-22T00:32:45.214466: step 38600, loss 0.637563, acc [0.89267578 0.84082031 0.80791016 0.72392578]\n",
      "\n",
      "Evaluation:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-02-22T00:32:49.186923: step 38600, loss 0.681558, acc [0.89537386 0.83930163 0.80677552 0.72454424] \n",
      "\n",
      "2019-02-22T00:32:50.433727: step 38640, loss 0.620938, acc [0.89804688 0.84326172 0.81240234 0.72822266]\n",
      "2019-02-22T00:32:51.600317: step 38680, loss 0.635624, acc [0.89414063 0.84189453 0.80673828 0.72382813]\n",
      "2019-02-22T00:32:52.838828: step 38720, loss 0.640075, acc [0.89775391 0.83759766 0.80917969 0.72539062]\n",
      "2019-02-22T00:32:54.007400: step 38760, loss 0.647033, acc [0.89189453 0.83320313 0.8078125  0.72041016]\n",
      "2019-02-22T00:32:55.179944: step 38800, loss 0.636934, acc [0.89570313 0.83916016 0.80908203 0.72402344]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:32:59.349313: step 38800, loss 0.676828, acc [0.89430268 0.8381103  0.80874771 0.72413379] \n",
      "\n",
      "2019-02-22T00:33:00.619072: step 38840, loss 0.647756, acc [0.89335937 0.83769531 0.80634766 0.72246094]\n",
      "2019-02-22T00:33:01.819390: step 38880, loss 0.66242, acc [0.88798828 0.83564453 0.80253906 0.71630859]\n",
      "2019-02-22T00:33:03.013757: step 38920, loss 0.642382, acc [0.89375    0.840625   0.81025391 0.72636719]\n",
      "2019-02-22T00:33:04.280043: step 38960, loss 0.658118, acc [0.89619141 0.83935547 0.79921875 0.71972656]\n",
      "2019-02-22T00:33:05.472920: step 39000, loss 0.662629, acc [0.89638672 0.83115234 0.80185547 0.71582031]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:33:09.593728: step 39000, loss 0.677407, acc [0.89593449 0.83922153 0.80806695 0.72521499] \n",
      "\n",
      "2019-02-22T00:33:11.016209: step 39040, loss 0.633959, acc [0.89484987 0.8363084  0.81228003 0.72750454]\n",
      "2019-02-22T00:33:12.271086: step 39080, loss 0.591675, acc [0.89482422 0.84775391 0.82324219 0.73476562]\n",
      "2019-02-22T00:33:13.459007: step 39120, loss 0.617369, acc [0.89423828 0.84042969 0.81152344 0.72597656]\n",
      "2019-02-22T00:33:14.645940: step 39160, loss 0.602766, acc [0.90048828 0.83798828 0.8171875  0.72841797]\n",
      "2019-02-22T00:33:15.883951: step 39200, loss 0.602913, acc [0.89667969 0.84501953 0.81669922 0.73242188]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:33:20.522070: step 39200, loss 0.682051, acc [0.89694561 0.83915146 0.80924826 0.72752756] \n",
      "\n",
      "2019-02-22T00:33:21.834449: step 39240, loss 0.599099, acc [0.90126953 0.84160156 0.81865234 0.73125   ]\n",
      "2019-02-22T00:33:23.135456: step 39280, loss 0.610985, acc [0.89248047 0.84150391 0.81816406 0.73232422]\n",
      "2019-02-22T00:33:24.394798: step 39320, loss 0.619293, acc [0.89404297 0.84091797 0.81191406 0.72324219]\n",
      "2019-02-22T00:33:25.670508: step 39360, loss 0.630285, acc [0.89316406 0.84042969 0.80849609 0.72099609]\n",
      "2019-02-22T00:33:26.968042: step 39400, loss 0.62514, acc [0.89892578 0.83681641 0.81621094 0.73017578]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:33:31.489077: step 39400, loss 0.676817, acc [0.89635496 0.8381103  0.80802691 0.72436405] \n",
      "\n",
      "2019-02-22T00:33:32.797521: step 39440, loss 0.646042, acc [0.89648438 0.82958984 0.80869141 0.71767578]\n",
      "2019-02-22T00:33:33.986967: step 39480, loss 0.620263, acc [0.89199219 0.84267578 0.81748047 0.73046875]\n",
      "2019-02-22T00:33:35.176830: step 39520, loss 0.633214, acc [0.89775391 0.83837891 0.81025391 0.7234375 ]\n",
      "2019-02-22T00:33:36.437660: step 39560, loss 0.612398, acc [0.90244141 0.84257812 0.81699219 0.73515625]\n",
      "2019-02-22T00:33:37.649883: step 39600, loss 0.614084, acc [0.89960938 0.84189453 0.81445312 0.72861328]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:33:41.844085: step 39600, loss 0.683272, acc [0.89559411 0.83859084 0.80678553 0.72412378] \n",
      "\n",
      "2019-02-22T00:33:43.119269: step 39640, loss 0.633239, acc [0.89863281 0.83984375 0.81005859 0.72636719]\n",
      "2019-02-22T00:33:44.322561: step 39680, loss 0.629196, acc [0.89921875 0.84238281 0.81298828 0.73095703]\n",
      "2019-02-22T00:33:45.615137: step 39720, loss 0.625408, acc [0.89814453 0.83652344 0.80732422 0.72734375]\n",
      "2019-02-22T00:33:46.831822: step 39760, loss 0.651568, acc [0.89228516 0.83564453 0.80615234 0.721875  ]\n",
      "2019-02-22T00:33:48.131836: step 39800, loss 0.627581, acc [0.89316406 0.83916016 0.81142578 0.7296875 ]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:33:52.286824: step 39800, loss 0.680293, acc [0.89518365 0.8382905  0.80767652 0.72453423] \n",
      "\n",
      "2019-02-22T00:33:53.573940: step 39840, loss 0.655332, acc [0.890625   0.83496094 0.80722656 0.71943359]\n",
      "2019-02-22T00:33:54.764835: step 39880, loss 0.619768, acc [0.89462891 0.84033203 0.80908203 0.72529297]\n",
      "2019-02-22T00:33:55.956720: step 39920, loss 0.624565, acc [0.89794922 0.84091797 0.81318359 0.72519531]\n",
      "2019-02-22T00:33:57.220031: step 39960, loss 0.636015, acc [0.89775391 0.8390625  0.81298828 0.72929687]\n",
      "2019-02-22T00:33:58.406462: step 40000, loss 0.614234, acc [0.90009766 0.84628906 0.81162109 0.73339844]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:34:02.410696: step 40000, loss 0.677997, acc [0.89593449 0.83951186 0.80838731 0.72602589] \n",
      "\n",
      "2019-02-22T00:34:03.684886: step 40040, loss 0.653126, acc [0.89345703 0.8359375  0.80097656 0.71738281]\n",
      "2019-02-22T00:34:04.871812: step 40080, loss 0.635, acc [0.89707031 0.84111328 0.81044922 0.72509766]\n",
      "2019-02-22T00:34:06.056755: step 40120, loss 0.639265, acc [0.89726562 0.83847656 0.80585938 0.72441406]\n",
      "2019-02-22T00:34:07.241202: step 40160, loss 0.641589, acc [0.89492187 0.84052734 0.81083984 0.72744141]\n",
      "2019-02-22T00:34:08.469294: step 40200, loss 0.644833, acc [0.89355469 0.83564453 0.80927734 0.72392578]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:34:12.473001: step 40200, loss 0.675034, acc [0.89649511 0.83844067 0.80923825 0.72610598] \n",
      "\n",
      "2019-02-22T00:34:13.730856: step 40240, loss 0.626336, acc [0.89912109 0.84404297 0.81269531 0.72890625]\n",
      "2019-02-22T00:34:14.916791: step 40280, loss 0.622674, acc [0.89589844 0.83730469 0.81542969 0.73134766]\n",
      "2019-02-22T00:34:16.100244: step 40320, loss 0.645567, acc [0.89365234 0.83242187 0.80673828 0.72099609]\n",
      "2019-02-22T00:34:17.376947: step 40360, loss 0.647594, acc [0.89199219 0.83417969 0.80654297 0.71982422]\n",
      "2019-02-22T00:34:18.575777: step 40400, loss 0.637746, acc [0.89257812 0.84228516 0.81083984 0.72597656]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:34:22.610236: step 40400, loss 0.672696, acc [0.89743615 0.83787003 0.80876773 0.72569552] \n",
      "\n",
      "2019-02-22T00:34:23.928601: step 40440, loss 0.644616, acc [0.89794922 0.84033203 0.80849609 0.72617188]\n",
      "2019-02-22T00:34:25.114039: step 40480, loss 0.623781, acc [0.8984375  0.83964844 0.80576172 0.72597656]\n",
      "2019-02-22T00:34:26.337174: step 40520, loss 0.634953, acc [0.89482422 0.84189453 0.80859375 0.72636719]\n",
      "2019-02-22T00:34:27.545924: step 40560, loss 0.633009, acc [0.90039062 0.83671875 0.81142578 0.72587891]\n",
      "2019-02-22T00:34:28.852882: step 40600, loss 0.627512, acc [0.8988301  0.84244397 0.81388988 0.73001006]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:34:33.276204: step 40600, loss 0.678141, acc [0.89584439 0.83740952 0.80780667 0.72434402] \n",
      "\n",
      "2019-02-22T00:34:34.771642: step 40640, loss 0.579262, acc [0.89804688 0.84453125 0.82509766 0.73466797]\n",
      "2019-02-22T00:34:36.140600: step 40680, loss 0.604173, acc [0.89951172 0.84355469 0.82011719 0.73623047]\n",
      "2019-02-22T00:34:37.416310: step 40720, loss 0.588748, acc [0.89648438 0.84277344 0.81953125 0.73466797]\n",
      "2019-02-22T00:34:38.718308: step 40760, loss 0.59833, acc [0.89912109 0.84199219 0.81513672 0.72783203]\n",
      "2019-02-22T00:34:39.988563: step 40800, loss 0.600661, acc [0.89375    0.84150391 0.81884766 0.73271484]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:34:44.273501: step 40800, loss 0.678976, acc [0.89647509 0.8370291  0.80888787 0.72539519] \n",
      "\n",
      "2019-02-22T00:34:45.588394: step 40840, loss 0.618948, acc [0.89628906 0.84140625 0.81347656 0.72744141]\n",
      "2019-02-22T00:34:46.806568: step 40880, loss 0.619923, acc [0.89785156 0.83779297 0.81005859 0.721875  ]\n",
      "2019-02-22T00:34:48.002919: step 40920, loss 0.595288, acc [0.89804688 0.84345703 0.82138672 0.73388672]\n",
      "2019-02-22T00:34:49.291528: step 40960, loss 0.59451, acc [0.90458984 0.84501953 0.81591797 0.73408203]\n",
      "2019-02-22T00:34:50.550372: step 41000, loss 0.618162, acc [0.90039062 0.84306641 0.81464844 0.734375  ]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:34:54.621533: step 41000, loss 0.679401, acc [0.89606463 0.8390113  0.80768653 0.72596582] \n",
      "\n",
      "2019-02-22T00:34:55.947835: step 41040, loss 0.623585, acc [0.89150391 0.83837891 0.80908203 0.72021484]\n",
      "2019-02-22T00:34:57.203706: step 41080, loss 0.616989, acc [0.89443359 0.83984375 0.81572266 0.72636719]\n",
      "2019-02-22T00:34:58.692697: step 41120, loss 0.622287, acc [0.8984375  0.84052734 0.81181641 0.72753906]\n",
      "2019-02-22T00:34:59.987253: step 41160, loss 0.631269, acc [0.89091797 0.83916016 0.81376953 0.72548828]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-02-22T00:35:01.168724: step 41200, loss 0.625448, acc [0.89482422 0.83925781 0.81347656 0.72958984]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:35:05.199710: step 41200, loss 0.680121, acc [0.8952237  0.83804023 0.80823714 0.72352311] \n",
      "\n",
      "2019-02-22T00:35:06.474925: step 41240, loss 0.635586, acc [0.89482422 0.84013672 0.80869141 0.7265625 ]\n",
      "2019-02-22T00:35:07.656395: step 41280, loss 0.607037, acc [0.89960938 0.84257812 0.81728516 0.73505859]\n",
      "2019-02-22T00:35:08.919209: step 41320, loss 0.618998, acc [0.89833984 0.84169922 0.81103516 0.72333984]\n",
      "2019-02-22T00:35:10.099191: step 41360, loss 0.635985, acc [0.89521484 0.83847656 0.80830078 0.72509766]\n",
      "2019-02-22T00:35:11.314390: step 41400, loss 0.632143, acc [0.89472656 0.83720703 0.80996094 0.72470703]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:35:15.693107: step 41400, loss 0.6783, acc [0.89749622 0.83797015 0.80915817 0.72598585] \n",
      "\n",
      "2019-02-22T00:35:17.053102: step 41440, loss 0.637712, acc [0.896875   0.8390625  0.81191406 0.73261719]\n",
      "2019-02-22T00:35:18.271276: step 41480, loss 0.640095, acc [0.89521484 0.84160156 0.80615234 0.72753906]\n",
      "2019-02-22T00:35:19.504827: step 41520, loss 0.607798, acc [0.90048828 0.84345703 0.81542969 0.73291016]\n",
      "2019-02-22T00:35:20.712088: step 41560, loss 0.650669, acc [0.89667969 0.83544922 0.80800781 0.71972656]\n",
      "2019-02-22T00:35:21.944646: step 41600, loss 0.615563, acc [0.90087891 0.8453125  0.81542969 0.73369141]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:35:25.940946: step 41600, loss 0.679558, acc [0.896395   0.83742955 0.80894793 0.72661655] \n",
      "\n",
      "2019-02-22T00:35:27.194304: step 41640, loss 0.619953, acc [0.89814453 0.84013672 0.80976563 0.72841797]\n",
      "2019-02-22T00:35:28.422396: step 41680, loss 0.629491, acc [0.89658203 0.84091797 0.81191406 0.72578125]\n",
      "2019-02-22T00:35:29.663884: step 41720, loss 0.653463, acc [0.89160156 0.83212891 0.80195313 0.71552734]\n",
      "2019-02-22T00:35:30.964394: step 41760, loss 0.636758, acc [0.89462891 0.83671875 0.80878906 0.72392578]\n",
      "2019-02-22T00:35:32.151854: step 41800, loss 0.63314, acc [0.89150391 0.84042969 0.81162109 0.72275391]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:35:36.192762: step 41800, loss 0.673899, acc [0.8949534  0.83750964 0.808107   0.72312267] \n",
      "\n",
      "2019-02-22T00:35:37.451073: step 41840, loss 0.636893, acc [0.89541016 0.83789062 0.80917969 0.72226563]\n",
      "2019-02-22T00:35:38.625599: step 41880, loss 0.633505, acc [0.89667969 0.83994141 0.80957031 0.72265625]\n",
      "2019-02-22T00:35:39.906268: step 41920, loss 0.640892, acc [0.89619141 0.83857422 0.80400391 0.71894531]\n",
      "2019-02-22T00:35:41.074843: step 41960, loss 0.667359, acc [0.89384766 0.82988281 0.80078125 0.71738281]\n",
      "2019-02-22T00:35:42.312363: step 42000, loss 0.627903, acc [0.90068359 0.84042969 0.80751953 0.73037109]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:35:46.379556: step 42000, loss 0.678293, acc [0.89600456 0.83844067 0.80763648 0.72552533] \n",
      "\n",
      "2019-02-22T00:35:47.687506: step 42040, loss 0.645173, acc [0.88798828 0.84248047 0.80791016 0.72626953]\n",
      "2019-02-22T00:35:48.950320: step 42080, loss 0.646857, acc [0.89892578 0.84111328 0.80742187 0.7265625 ]\n",
      "2019-02-22T00:35:50.251823: step 42120, loss 0.646592, acc [0.89941406 0.84023437 0.80810547 0.72880859]\n",
      "2019-02-22T00:35:51.627228: step 42160, loss 0.654688, acc [0.89239662 0.83382161 0.80143525 0.71393722]\n",
      "2019-02-22T00:35:52.811178: step 42200, loss 0.58537, acc [0.89941406 0.84638672 0.82021484 0.73427734]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:35:56.845681: step 42200, loss 0.672703, acc [0.8956742  0.83985224 0.81063981 0.72818829] \n",
      "\n",
      "2019-02-22T00:35:58.100515: step 42240, loss 0.596314, acc [0.89951172 0.84101563 0.82070312 0.73583984]\n",
      "2019-02-22T00:35:59.356881: step 42280, loss 0.599472, acc [0.89462891 0.84316406 0.82236328 0.7359375 ]\n",
      "2019-02-22T00:36:00.580016: step 42320, loss 0.600847, acc [0.89628906 0.83808594 0.81748047 0.72802734]\n",
      "2019-02-22T00:36:01.840350: step 42360, loss 0.593292, acc [0.89824219 0.83554688 0.82285156 0.73320312]\n",
      "2019-02-22T00:36:03.010908: step 42400, loss 0.609025, acc [0.89648438 0.83701172 0.81845703 0.725     ]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:36:07.254183: step 42400, loss 0.677101, acc [0.8958544  0.83854078 0.80847741 0.72399363] \n",
      "\n",
      "2019-02-22T00:36:08.541796: step 42440, loss 0.61161, acc [0.89404297 0.84287109 0.81591797 0.72939453]\n",
      "2019-02-22T00:36:09.766418: step 42480, loss 0.617486, acc [0.89873047 0.83964844 0.80878906 0.72529297]\n",
      "2019-02-22T00:36:10.997985: step 42520, loss 0.626523, acc [0.89707031 0.83945313 0.80957031 0.72578125]\n",
      "2019-02-22T00:36:12.251871: step 42560, loss 0.626183, acc [0.89892578 0.84052734 0.81210938 0.72558594]\n",
      "2019-02-22T00:36:13.430366: step 42600, loss 0.577634, acc [0.90302734 0.85087891 0.82519531 0.74394531]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:36:17.538231: step 42600, loss 0.678657, acc [0.89715584 0.83903133 0.80975883 0.72741743] \n",
      "\n",
      "2019-02-22T00:36:18.831798: step 42640, loss 0.626565, acc [0.89814453 0.83681641 0.81416016 0.72587891]\n",
      "2019-02-22T00:36:20.043028: step 42680, loss 0.606332, acc [0.89902344 0.84277344 0.81552734 0.73398438]\n",
      "2019-02-22T00:36:21.218051: step 42720, loss 0.607657, acc [0.896875   0.84111328 0.8203125  0.73427734]\n",
      "2019-02-22T00:36:22.430273: step 42760, loss 0.635326, acc [0.89667969 0.83613281 0.80585938 0.71796875]\n",
      "2019-02-22T00:36:23.633566: step 42800, loss 0.630792, acc [0.89619141 0.83701172 0.81318359 0.72587891]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:36:27.872875: step 42800, loss 0.679206, acc [0.89732603 0.83854078 0.80788675 0.72517495] \n",
      "\n",
      "2019-02-22T00:36:29.163959: step 42840, loss 0.595616, acc [0.89541016 0.84287109 0.82236328 0.73320312]\n",
      "2019-02-22T00:36:30.355846: step 42880, loss 0.629985, acc [0.89482422 0.84033203 0.81367188 0.7265625 ]\n",
      "2019-02-22T00:36:31.532852: step 42920, loss 0.610225, acc [0.89755859 0.83945313 0.81542969 0.72900391]\n",
      "2019-02-22T00:36:32.783272: step 42960, loss 0.613559, acc [0.89648438 0.84326172 0.81630859 0.73378906]\n",
      "2019-02-22T00:36:33.983089: step 43000, loss 0.639503, acc [0.89179688 0.83857422 0.81025391 0.72304687]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:36:38.445097: step 43000, loss 0.675932, acc [0.89598455 0.83956192 0.80920822 0.72500476] \n",
      "\n",
      "2019-02-22T00:36:39.836872: step 43040, loss 0.627407, acc [0.90566406 0.84511719 0.81044922 0.72919922]\n",
      "2019-02-22T00:36:41.056535: step 43080, loss 0.614587, acc [0.89628906 0.84462891 0.80966797 0.72666016]\n",
      "2019-02-22T00:36:42.332740: step 43120, loss 0.631045, acc [0.89677734 0.84277344 0.81240234 0.72714844]\n",
      "2019-02-22T00:36:43.517682: step 43160, loss 0.615701, acc [0.89765625 0.84160156 0.81308594 0.73027344]\n",
      "2019-02-22T00:36:44.773554: step 43200, loss 0.617152, acc [0.89873047 0.84638672 0.81132812 0.72861328]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:36:48.926556: step 43200, loss 0.676267, acc [0.89735607 0.84024267 0.80886784 0.7272072 ] \n",
      "\n",
      "2019-02-22T00:36:50.201769: step 43240, loss 0.630819, acc [0.89892578 0.83642578 0.81474609 0.72548828]\n",
      "2019-02-22T00:36:51.384231: step 43280, loss 0.644838, acc [0.89589844 0.82998047 0.809375   0.72089844]\n",
      "2019-02-22T00:36:52.644565: step 43320, loss 0.635159, acc [0.89179688 0.83691406 0.81220703 0.72451172]\n",
      "2019-02-22T00:36:53.819588: step 43360, loss 0.652129, acc [0.89394531 0.83652344 0.80830078 0.72275391]\n",
      "2019-02-22T00:36:54.992627: step 43400, loss 0.640393, acc [0.89638672 0.84189453 0.80771484 0.72822266]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:36:58.973516: step 43400, loss 0.675343, acc [0.89667531 0.83882109 0.80880778 0.726126  ] \n",
      "\n",
      "2019-02-22T00:37:00.258650: step 43440, loss 0.637484, acc [0.88779297 0.83457031 0.80986328 0.72050781]\n",
      "2019-02-22T00:37:01.433673: step 43480, loss 0.632867, acc [0.89541016 0.84130859 0.80986328 0.725     ]\n",
      "2019-02-22T00:37:02.625064: step 43520, loss 0.638303, acc [0.89472656 0.83623047 0.81113281 0.72207031]\n",
      "2019-02-22T00:37:03.852662: step 43560, loss 0.633349, acc [0.89501953 0.83769531 0.80878906 0.72333984]\n",
      "2019-02-22T00:37:05.032148: step 43600, loss 0.622808, acc [0.89677734 0.83818359 0.8109375  0.72998047]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:37:09.048751: step 43600, loss 0.675745, acc [0.8961247  0.83931163 0.8078367  0.72493468] \n",
      "\n",
      "2019-02-22T00:37:10.307101: step 43640, loss 0.636198, acc [0.89814453 0.83271484 0.81162109 0.72275391]\n",
      "2019-02-22T00:37:11.543130: step 43680, loss 0.6374, acc [0.89287109 0.83896484 0.80869141 0.72412109]\n",
      "2019-02-22T00:37:12.857033: step 43720, loss 0.631282, acc [0.8929944  0.83634391 0.81317077 0.7230597 ]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-02-22T00:37:14.041479: step 43760, loss 0.582133, acc [0.89970703 0.84414062 0.82470703 0.73535156]\n",
      "2019-02-22T00:37:15.222453: step 43800, loss 0.579224, acc [0.8984375  0.84521484 0.82490234 0.73652344]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:37:19.173088: step 43800, loss 0.676942, acc [0.89751624 0.83947181 0.81085004 0.7273874 ] \n",
      "\n",
      "2019-02-22T00:37:20.434910: step 43840, loss 0.591303, acc [0.89921875 0.8421875  0.82138672 0.73564453]\n",
      "2019-02-22T00:37:21.604477: step 43880, loss 0.588341, acc [0.90097656 0.84238281 0.82255859 0.73457031]\n",
      "2019-02-22T00:37:22.769578: step 43920, loss 0.591237, acc [0.89599609 0.84482422 0.82353516 0.73789063]\n",
      "2019-02-22T00:37:24.003625: step 43960, loss 0.594403, acc [0.89697266 0.84189453 0.82441406 0.73349609]\n",
      "2019-02-22T00:37:25.173192: step 44000, loss 0.609684, acc [0.89677734 0.8390625  0.81689453 0.72890625]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:37:29.171938: step 44000, loss 0.668793, acc [0.8970257  0.83866091 0.81283224 0.72803812] \n",
      "\n",
      "2019-02-22T00:37:30.428800: step 44040, loss 0.59932, acc [0.90283203 0.84423828 0.82226562 0.73925781]\n",
      "2019-02-22T00:37:31.597870: step 44080, loss 0.598517, acc [0.89970703 0.84042969 0.81669922 0.73203125]\n",
      "2019-02-22T00:37:32.765453: step 44120, loss 0.601002, acc [0.89765625 0.84130859 0.82080078 0.73466797]\n",
      "2019-02-22T00:37:33.932042: step 44160, loss 0.620623, acc [0.90205078 0.84042969 0.81318359 0.72919922]\n",
      "2019-02-22T00:37:35.098633: step 44200, loss 0.602125, acc [0.89775391 0.84306641 0.81748047 0.73134766]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:37:39.023973: step 44200, loss 0.679255, acc [0.8971158  0.83834056 0.80821712 0.72518496] \n",
      "\n",
      "2019-02-22T00:37:40.262978: step 44240, loss 0.621452, acc [0.89404297 0.83847656 0.81181641 0.72353516]\n",
      "2019-02-22T00:37:41.427584: step 44280, loss 0.608615, acc [0.89453125 0.83886719 0.81796875 0.72910156]\n",
      "2019-02-22T00:37:42.596655: step 44320, loss 0.617366, acc [0.89189453 0.83818359 0.81123047 0.72119141]\n",
      "2019-02-22T00:37:43.763741: step 44360, loss 0.619973, acc [0.89091797 0.83339844 0.81298828 0.71855469]\n",
      "2019-02-22T00:37:44.928347: step 44400, loss 0.626286, acc [0.89628906 0.84052734 0.81074219 0.72470703]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:37:48.849222: step 44400, loss 0.678217, acc [0.89550401 0.83905135 0.80845739 0.72582567] \n",
      "\n",
      "2019-02-22T00:37:50.102116: step 44440, loss 0.611861, acc [0.89765625 0.84013672 0.81347656 0.72744141]\n",
      "2019-02-22T00:37:51.269698: step 44480, loss 0.618412, acc [0.9        0.83857422 0.81064453 0.72607422]\n",
      "2019-02-22T00:37:52.440753: step 44520, loss 0.644466, acc [0.89667969 0.83603516 0.80722656 0.72060547]\n",
      "2019-02-22T00:37:53.605359: step 44560, loss 0.61155, acc [0.89697266 0.84082031 0.81835938 0.73144531]\n",
      "2019-02-22T00:37:54.771452: step 44600, loss 0.637413, acc [0.89638672 0.83544922 0.81240234 0.72509766]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:37:58.694311: step 44600, loss 0.675823, acc [0.89592448 0.83925157 0.80991901 0.72619608] \n",
      "\n",
      "2019-02-22T00:37:59.946214: step 44640, loss 0.632243, acc [0.8953125  0.83808594 0.81113281 0.72578125]\n",
      "2019-02-22T00:38:01.124212: step 44680, loss 0.631411, acc [0.89541016 0.83564453 0.80917969 0.72109375]\n",
      "2019-02-22T00:38:02.299731: step 44720, loss 0.611672, acc [0.8984375  0.8359375  0.81611328 0.72460938]\n",
      "2019-02-22T00:38:03.464833: step 44760, loss 0.632218, acc [0.89824219 0.83916016 0.80869141 0.72568359]\n",
      "2019-02-22T00:38:04.634895: step 44800, loss 0.637432, acc [0.89707031 0.83945313 0.80869141 0.72509766]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:38:08.584585: step 44800, loss 0.673542, acc [0.89777653 0.83771987 0.80991901 0.72669664] \n",
      "\n",
      "2019-02-22T00:38:09.822093: step 44840, loss 0.613624, acc [0.89970703 0.83935547 0.81337891 0.72871094]\n",
      "2019-02-22T00:38:10.988646: step 44880, loss 0.627733, acc [0.89199219 0.83564453 0.8078125  0.71904297]\n",
      "2019-02-22T00:38:12.159205: step 44920, loss 0.633515, acc [0.89921875 0.83759766 0.80859375 0.72783203]\n",
      "2019-02-22T00:38:13.327283: step 44960, loss 0.639534, acc [0.89697266 0.83691406 0.81308594 0.72851562]\n",
      "2019-02-22T00:38:14.496850: step 45000, loss 0.636544, acc [0.89755859 0.83867187 0.80673828 0.72373047]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:38:18.419213: step 45000, loss 0.670891, acc [0.89646508 0.83879106 0.81112034 0.72678673] \n",
      "\n",
      "2019-02-22T00:38:19.670618: step 45040, loss 0.621345, acc [0.89472656 0.84287109 0.81767578 0.73388672]\n",
      "2019-02-22T00:38:20.836712: step 45080, loss 0.619448, acc [0.89775391 0.84121094 0.80859375 0.72412109]\n",
      "2019-02-22T00:38:22.003302: step 45120, loss 0.634354, acc [0.89414063 0.83701172 0.81025391 0.72373047]\n",
      "2019-02-22T00:38:23.221476: step 45160, loss 0.651769, acc [0.89375    0.83681641 0.80292969 0.72246094]\n",
      "2019-02-22T00:38:24.401954: step 45200, loss 0.616068, acc [0.89697266 0.84257812 0.80976563 0.728125  ]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:38:28.303983: step 45200, loss 0.669602, acc [0.89690556 0.84033277 0.81004915 0.72615604] \n",
      "\n",
      "2019-02-22T00:38:29.555388: step 45240, loss 0.63373, acc [0.89492187 0.83554688 0.81181641 0.72792969]\n",
      "2019-02-22T00:38:30.867802: step 45280, loss 0.59937, acc [0.89747475 0.84356357 0.81955197 0.73696536]\n",
      "2019-02-22T00:38:32.041336: step 45320, loss 0.573685, acc [0.89785156 0.84013672 0.82695312 0.73134766]\n",
      "2019-02-22T00:38:33.202470: step 45360, loss 0.59527, acc [0.89833984 0.84316406 0.82226562 0.73515625]\n",
      "2019-02-22T00:38:34.369061: step 45400, loss 0.583269, acc [0.90244141 0.84365234 0.81835938 0.73525391]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:38:38.289935: step 45400, loss 0.672421, acc [0.89584439 0.839642   0.81045961 0.72579563] \n",
      "\n",
      "2019-02-22T00:38:39.536382: step 45440, loss 0.593291, acc [0.89892578 0.84091797 0.81494141 0.72675781]\n",
      "2019-02-22T00:38:40.707435: step 45480, loss 0.577417, acc [0.89833984 0.84384766 0.82705078 0.73564453]\n",
      "2019-02-22T00:38:41.884939: step 45520, loss 0.608557, acc [0.89736328 0.8421875  0.81894531 0.7328125 ]\n",
      "2019-02-22T00:38:43.056488: step 45560, loss 0.59682, acc [0.89257812 0.84091797 0.81542969 0.72666016]\n",
      "2019-02-22T00:38:44.226550: step 45600, loss 0.607408, acc [0.90136719 0.84189453 0.81796875 0.73320312]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:38:48.153378: step 45600, loss 0.675762, acc [0.89588443 0.83788005 0.80976884 0.72628618] \n",
      "\n",
      "2019-02-22T00:38:49.392880: step 45640, loss 0.578428, acc [0.90351563 0.84521484 0.82617188 0.73779297]\n",
      "2019-02-22T00:38:50.554014: step 45680, loss 0.610621, acc [0.89580078 0.83769531 0.81796875 0.72744141]\n",
      "2019-02-22T00:38:51.722092: step 45720, loss 0.587589, acc [0.89707031 0.84287109 0.82529297 0.73349609]\n",
      "2019-02-22T00:38:52.913483: step 45760, loss 0.60928, acc [0.89755859 0.84013672 0.80976563 0.72207031]\n",
      "2019-02-22T00:38:54.180264: step 45800, loss 0.609824, acc [0.89726562 0.83857422 0.81826172 0.73046875]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:38:58.175045: step 45800, loss 0.673762, acc [0.89616474 0.83969206 0.8106298  0.72608595] \n",
      "\n",
      "2019-02-22T00:38:59.438353: step 45840, loss 0.610137, acc [0.89628906 0.83818359 0.81611328 0.73144531]\n",
      "2019-02-22T00:39:00.613376: step 45880, loss 0.617345, acc [0.89648438 0.83876953 0.80644531 0.71933594]\n",
      "2019-02-22T00:39:01.792862: step 45920, loss 0.619009, acc [0.89462891 0.84003906 0.81455078 0.72958984]\n",
      "2019-02-22T00:39:02.966397: step 45960, loss 0.617761, acc [0.89257812 0.84306641 0.81357422 0.7296875 ]\n",
      "2019-02-22T00:39:04.133979: step 46000, loss 0.620003, acc [0.89775391 0.84267578 0.81416016 0.73037109]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:39:08.068246: step 46000, loss 0.672519, acc [0.89679544 0.83923155 0.80973881 0.72645637] \n",
      "\n",
      "2019-02-22T00:39:09.333043: step 46040, loss 0.626852, acc [0.90029297 0.83808594 0.80800781 0.72753906]\n",
      "2019-02-22T00:39:10.504098: step 46080, loss 0.619021, acc [0.89609375 0.83554688 0.8125     0.72460938]\n",
      "2019-02-22T00:39:11.683593: step 46120, loss 0.609135, acc [0.90019531 0.84052734 0.81328125 0.73378906]\n",
      "2019-02-22T00:39:12.863576: step 46160, loss 0.59962, acc [0.89833984 0.84384766 0.81601563 0.73144531]\n",
      "2019-02-22T00:39:14.034630: step 46200, loss 0.639572, acc [0.89375    0.83271484 0.81083984 0.7234375 ]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:39:17.994190: step 46200, loss 0.674264, acc [0.89555406 0.83906136 0.80899799 0.7250448 ] \n",
      "\n",
      "2019-02-22T00:39:19.241133: step 46240, loss 0.623734, acc [0.8984375  0.84521484 0.81416016 0.73125   ]\n",
      "2019-02-22T00:39:20.410701: step 46280, loss 0.626014, acc [0.89414063 0.8375     0.80830078 0.72080078]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-02-22T00:39:21.650201: step 46320, loss 0.612741, acc [0.90351563 0.84375    0.81044922 0.73115234]\n",
      "2019-02-22T00:39:22.838616: step 46360, loss 0.656566, acc [0.88916016 0.83330078 0.80595703 0.71914062]\n",
      "2019-02-22T00:39:24.008183: step 46400, loss 0.624658, acc [0.89443359 0.83652344 0.81650391 0.72666016]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:39:27.905248: step 46400, loss 0.671057, acc [0.89719589 0.84075324 0.81107029 0.7282884 ] \n",
      "\n",
      "2019-02-22T00:39:29.149713: step 46440, loss 0.647912, acc [0.89511719 0.83359375 0.80751953 0.71904297]\n",
      "2019-02-22T00:39:30.318285: step 46480, loss 0.636415, acc [0.89248047 0.83720703 0.81240234 0.72705078]\n",
      "2019-02-22T00:39:31.482893: step 46520, loss 0.626245, acc [0.89179688 0.84111328 0.81591797 0.72822266]\n",
      "2019-02-22T00:39:32.648987: step 46560, loss 0.633298, acc [0.89550781 0.83613281 0.81298828 0.72988281]\n",
      "2019-02-22T00:39:33.814088: step 46600, loss 0.62278, acc [0.89355469 0.84433594 0.81113281 0.7234375 ]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:39:37.749348: step 46600, loss 0.668299, acc [0.89718588 0.84017259 0.81195127 0.72830842] \n",
      "\n",
      "2019-02-22T00:39:38.995296: step 46640, loss 0.610997, acc [0.89306641 0.84638672 0.81757813 0.73378906]\n",
      "2019-02-22T00:39:40.157422: step 46680, loss 0.647884, acc [0.89414063 0.83916016 0.80888672 0.72431641]\n",
      "2019-02-22T00:39:41.317071: step 46720, loss 0.628234, acc [0.89892578 0.83886719 0.81230469 0.72978516]\n",
      "2019-02-22T00:39:42.483662: step 46760, loss 0.650486, acc [0.89462891 0.84052734 0.80615234 0.72333984]\n",
      "2019-02-22T00:39:43.648762: step 46800, loss 0.614321, acc [0.89287109 0.84091797 0.81142578 0.72734375]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:39:47.508628: step 46800, loss 0.670057, acc [0.89677542 0.83932165 0.8112605  0.72692689] \n",
      "\n",
      "2019-02-22T00:39:49.035315: step 46840, loss 0.627016, acc [0.89404001 0.83686573 0.81168423 0.72485795]\n",
      "2019-02-22T00:39:50.196450: step 46880, loss 0.596316, acc [0.89521484 0.83857422 0.82197266 0.72958984]\n",
      "2019-02-22T00:39:51.360560: step 46920, loss 0.611079, acc [0.89160156 0.83710938 0.81689453 0.72392578]\n",
      "2019-02-22T00:39:52.522189: step 46960, loss 0.580866, acc [0.89462891 0.84238281 0.82548828 0.73427734]\n",
      "2019-02-22T00:39:53.681838: step 47000, loss 0.577601, acc [0.89619141 0.84501953 0.82529297 0.73232422]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:39:57.521368: step 47000, loss 0.673802, acc [0.89645507 0.838741   0.811711   0.7269369 ] \n",
      "\n",
      "2019-02-22T00:39:58.759412: step 47040, loss 0.593332, acc [0.89589844 0.84140625 0.81767578 0.73046875]\n",
      "2019-02-22T00:39:59.920019: step 47080, loss 0.572341, acc [0.89980469 0.84160156 0.82695312 0.73486328]\n",
      "2019-02-22T00:40:01.078674: step 47120, loss 0.60002, acc [0.90185547 0.83876953 0.81630859 0.72626953]\n",
      "2019-02-22T00:40:02.239807: step 47160, loss 0.590886, acc [0.89960938 0.8453125  0.82119141 0.73583984]\n",
      "2019-02-22T00:40:03.403423: step 47200, loss 0.582385, acc [0.90097656 0.84277344 0.82626953 0.73398438]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:40:07.292587: step 47200, loss 0.671518, acc [0.89723593 0.83976214 0.81194126 0.72815826] \n",
      "\n",
      "2019-02-22T00:40:08.546934: step 47240, loss 0.595203, acc [0.89472656 0.83671875 0.81972656 0.72880859]\n",
      "2019-02-22T00:40:09.709594: step 47280, loss 0.623301, acc [0.89082031 0.83925781 0.81347656 0.72705078]\n",
      "2019-02-22T00:40:10.876148: step 47320, loss 0.615308, acc [0.89755859 0.84091797 0.81318359 0.72617188]\n",
      "2019-02-22T00:40:12.086420: step 47360, loss 0.595321, acc [0.89570313 0.84179688 0.82177734 0.73427734]\n",
      "2019-02-22T00:40:13.254962: step 47400, loss 0.60206, acc [0.90292969 0.84326172 0.81484375 0.73105469]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:40:17.117811: step 47400, loss 0.669614, acc [0.89595451 0.8409034  0.81224159 0.72877894] \n",
      "\n",
      "2019-02-22T00:40:18.359298: step 47440, loss 0.619468, acc [0.89492187 0.83544922 0.81210938 0.7234375 ]\n",
      "2019-02-22T00:40:19.522416: step 47480, loss 0.601849, acc [0.89638672 0.83896484 0.81845703 0.72919922]\n",
      "2019-02-22T00:40:20.686032: step 47520, loss 0.624189, acc [0.90175781 0.84238281 0.81113281 0.72714844]\n",
      "2019-02-22T00:40:21.840738: step 47560, loss 0.603307, acc [0.89726562 0.84414062 0.81328125 0.72958984]\n",
      "2019-02-22T00:40:23.003835: step 47600, loss 0.610622, acc [0.90126953 0.84423828 0.81191406 0.72998047]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:40:26.923253: step 47600, loss 0.669006, acc [0.89761635 0.8385608  0.81101022 0.72658651] \n",
      "\n",
      "2019-02-22T00:40:28.166196: step 47640, loss 0.621199, acc [0.89345703 0.84072266 0.81425781 0.72636719]\n",
      "2019-02-22T00:40:29.419089: step 47680, loss 0.59822, acc [0.89941406 0.83896484 0.81904297 0.73144531]\n",
      "2019-02-22T00:40:30.622881: step 47720, loss 0.618466, acc [0.89609375 0.84257812 0.81210938 0.72714844]\n",
      "2019-02-22T00:40:31.831137: step 47760, loss 0.633662, acc [0.89501953 0.83740234 0.80996094 0.72304687]\n",
      "2019-02-22T00:40:33.042863: step 47800, loss 0.60345, acc [0.89482422 0.84316406 0.81826172 0.72958984]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:40:37.271800: step 47800, loss 0.669264, acc [0.89686552 0.83957192 0.81218152 0.72786793] \n",
      "\n",
      "2019-02-22T00:40:38.540517: step 47840, loss 0.628752, acc [0.89746094 0.84091797 0.81484375 0.73125   ]\n",
      "2019-02-22T00:40:39.718020: step 47880, loss 0.624699, acc [0.89482422 0.84482422 0.81630859 0.73447266]\n",
      "2019-02-22T00:40:40.894035: step 47920, loss 0.602897, acc [0.89746094 0.84072266 0.81933594 0.73330078]\n",
      "2019-02-22T00:40:42.069557: step 47960, loss 0.612156, acc [0.89335937 0.84130859 0.81660156 0.72919922]\n",
      "2019-02-22T00:40:43.295662: step 48000, loss 0.63188, acc [0.89267578 0.83759766 0.81201172 0.72460938]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:40:47.198219: step 48000, loss 0.669928, acc [0.89664528 0.83925157 0.81132056 0.7262161 ] \n",
      "\n",
      "2019-02-22T00:40:48.465958: step 48040, loss 0.613272, acc [0.8984375  0.84023437 0.81494141 0.72988281]\n",
      "2019-02-22T00:40:49.640981: step 48080, loss 0.607323, acc [0.90107422 0.84931641 0.81816406 0.734375  ]\n",
      "2019-02-22T00:40:50.810281: step 48120, loss 0.603007, acc [0.9        0.84765625 0.81582031 0.73447266]\n",
      "2019-02-22T00:40:51.987287: step 48160, loss 0.629535, acc [0.89423828 0.83847656 0.81054688 0.72402344]\n",
      "2019-02-22T00:40:53.160327: step 48200, loss 0.646059, acc [0.89658203 0.83769531 0.81396484 0.72617188]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:40:57.034577: step 48200, loss 0.670212, acc [0.89694561 0.8393717  0.81102023 0.72688684] \n",
      "\n",
      "2019-02-22T00:40:58.291438: step 48240, loss 0.63944, acc [0.89287109 0.83222656 0.80839844 0.71953125]\n",
      "2019-02-22T00:40:59.458523: step 48280, loss 0.638868, acc [0.89726562 0.83945313 0.81025391 0.72675781]\n",
      "2019-02-22T00:41:00.634577: step 48320, loss 0.645224, acc [0.89726562 0.83759766 0.80986328 0.72714844]\n",
      "2019-02-22T00:41:01.804602: step 48360, loss 0.624472, acc [0.89755859 0.8421875  0.80556641 0.72255859]\n",
      "2019-02-22T00:41:03.103127: step 48400, loss 0.611609, acc [0.89495936 0.83875671 0.81459024 0.72587989]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:41:07.035905: step 48400, loss 0.668826, acc [0.89660523 0.84004245 0.81167096 0.7267567 ] \n",
      "\n",
      "2019-02-22T00:41:08.289793: step 48440, loss 0.585911, acc [0.89794922 0.84482422 0.82060547 0.734375  ]\n",
      "2019-02-22T00:41:09.455429: step 48480, loss 0.572959, acc [0.89707031 0.84619141 0.82685547 0.73974609]\n",
      "2019-02-22T00:41:10.625452: step 48520, loss 0.580792, acc [0.90332031 0.84160156 0.82255859 0.73779297]\n",
      "2019-02-22T00:41:11.807608: step 48560, loss 0.589915, acc [0.90029297 0.84052734 0.82509766 0.73681641]\n",
      "2019-02-22T00:41:12.985110: step 48600, loss 0.589423, acc [0.89892578 0.84521484 0.82324219 0.73623047]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:41:16.921858: step 48600, loss 0.670499, acc [0.89637498 0.83860085 0.81192123 0.72640631] \n",
      "\n",
      "2019-02-22T00:41:18.169792: step 48640, loss 0.577293, acc [0.90039062 0.84599609 0.82480469 0.73662109]\n",
      "2019-02-22T00:41:19.335886: step 48680, loss 0.590609, acc [0.89882812 0.84882813 0.82431641 0.73427734]\n",
      "2019-02-22T00:41:20.510411: step 48720, loss 0.593065, acc [0.89511719 0.83886719 0.82041016 0.72832031]\n",
      "2019-02-22T00:41:21.676011: step 48760, loss 0.583923, acc [0.89912109 0.84433594 0.82578125 0.7375    ]\n",
      "2019-02-22T00:41:22.840122: step 48800, loss 0.578589, acc [0.89746094 0.84433594 0.81884766 0.72763672]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:41:26.705940: step 48800, loss 0.673471, acc [0.89789667 0.83932165 0.81217151 0.72894913] \n",
      "\n",
      "2019-02-22T00:41:27.950899: step 48840, loss 0.611689, acc [0.89775391 0.84414062 0.81884766 0.73134766]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-02-22T00:41:29.116495: step 48880, loss 0.604057, acc [0.89609375 0.84228516 0.81894531 0.73222656]\n",
      "2019-02-22T00:41:30.283087: step 48920, loss 0.602183, acc [0.90175781 0.84335938 0.81855469 0.73125   ]\n",
      "2019-02-22T00:41:31.445261: step 48960, loss 0.595825, acc [0.90322266 0.85117188 0.81796875 0.73613281]\n",
      "2019-02-22T00:41:32.614779: step 49000, loss 0.614837, acc [0.89511719 0.83925781 0.81523437 0.72714844]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:41:36.572358: step 49000, loss 0.667175, acc [0.8981069  0.83968205 0.81338286 0.72993022] \n",
      "\n",
      "2019-02-22T00:41:37.823269: step 49040, loss 0.619376, acc [0.89433594 0.83994141 0.81650391 0.73056641]\n",
      "2019-02-22T00:41:38.994323: step 49080, loss 0.624574, acc [0.89814453 0.83359375 0.81191406 0.72138672]\n",
      "2019-02-22T00:41:40.162399: step 49120, loss 0.601985, acc [0.89951172 0.83974609 0.82138672 0.73271484]\n",
      "2019-02-22T00:41:41.326510: step 49160, loss 0.612847, acc [0.89775391 0.83642578 0.81474609 0.73037109]\n",
      "2019-02-22T00:41:42.494589: step 49200, loss 0.592252, acc [0.89804688 0.84443359 0.81894531 0.73193359]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:41:46.359416: step 49200, loss 0.668837, acc [0.89649511 0.83951186 0.81275215 0.72824836] \n",
      "\n",
      "2019-02-22T00:41:47.609830: step 49240, loss 0.624344, acc [0.89677734 0.8421875  0.81162109 0.73115234]\n",
      "2019-02-22T00:41:48.779892: step 49280, loss 0.63083, acc [0.89394531 0.83271484 0.80732422 0.71767578]\n",
      "2019-02-22T00:41:49.948467: step 49320, loss 0.613537, acc [0.89257812 0.84140625 0.81884766 0.73212891]\n",
      "2019-02-22T00:41:51.118032: step 49360, loss 0.611133, acc [0.89550781 0.83984375 0.81738281 0.73291016]\n",
      "2019-02-22T00:41:52.282640: step 49400, loss 0.601985, acc [0.90019531 0.83896484 0.81826172 0.72861328]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:41:56.110806: step 49400, loss 0.669646, acc [0.89667531 0.83898127 0.81150076 0.72636627] \n",
      "\n",
      "2019-02-22T00:41:57.350265: step 49440, loss 0.60828, acc [0.89853516 0.84091797 0.81445312 0.72773438]\n",
      "2019-02-22T00:41:58.512392: step 49480, loss 0.607005, acc [0.90097656 0.84804687 0.81542969 0.73222656]\n",
      "2019-02-22T00:41:59.675012: step 49520, loss 0.622577, acc [0.89306641 0.83710938 0.81425781 0.72519531]\n",
      "2019-02-22T00:42:00.841604: step 49560, loss 0.628165, acc [0.89033203 0.83896484 0.81289062 0.72480469]\n",
      "2019-02-22T00:42:02.005714: step 49600, loss 0.632516, acc [0.89716797 0.83769531 0.8109375  0.7234375 ]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:42:05.867565: step 49600, loss 0.661228, acc [0.89739611 0.8402727  0.81479442 0.72977004] \n",
      "\n",
      "2019-02-22T00:42:07.120457: step 49640, loss 0.606975, acc [0.90175781 0.84550781 0.81806641 0.73583984]\n",
      "2019-02-22T00:42:08.283081: step 49680, loss 0.608466, acc [0.89550781 0.84501953 0.81660156 0.72988281]\n",
      "2019-02-22T00:42:09.451654: step 49720, loss 0.640651, acc [0.89453125 0.8359375  0.80712891 0.72402344]\n",
      "2019-02-22T00:42:10.615269: step 49760, loss 0.617717, acc [0.89667969 0.83798828 0.80917969 0.72412109]\n",
      "2019-02-22T00:42:11.778425: step 49800, loss 0.653887, acc [0.89453125 0.83525391 0.80634766 0.71923828]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:42:15.633791: step 49800, loss 0.665766, acc [0.89691558 0.84095346 0.81301245 0.7280181 ] \n",
      "\n",
      "2019-02-22T00:42:16.891645: step 49840, loss 0.601649, acc [0.90126953 0.84677734 0.81513672 0.734375  ]\n",
      "2019-02-22T00:42:18.057243: step 49880, loss 0.611434, acc [0.89570313 0.83876953 0.81328125 0.72675781]\n",
      "2019-02-22T00:42:19.213462: step 49920, loss 0.617601, acc [0.89560547 0.83818359 0.81601563 0.72578125]\n",
      "2019-02-22T00:42:20.542200: step 49960, loss 0.618055, acc [0.89545257 0.83743983 0.81288273 0.72284564]\n",
      "2019-02-22T00:42:21.748966: step 50000, loss 0.57051, acc [0.90126953 0.84384766 0.82666016 0.73574219]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:42:25.855880: step 50000, loss 0.669956, acc [0.89671535 0.83861086 0.81230165 0.72724725] \n",
      "\n",
      "2019-02-22T00:42:27.132543: step 50040, loss 0.574522, acc [0.89736328 0.84501953 0.82919922 0.73808594]\n",
      "2019-02-22T00:42:28.356172: step 50080, loss 0.591571, acc [0.89990234 0.84492188 0.82275391 0.73623047]\n",
      "2019-02-22T00:42:29.547067: step 50120, loss 0.580205, acc [0.9        0.84609375 0.82236328 0.73867187]\n",
      "2019-02-22T00:42:30.732506: step 50160, loss 0.585634, acc [0.90107422 0.84316406 0.8265625  0.7390625 ]\n",
      "2019-02-22T00:42:31.914508: step 50200, loss 0.591161, acc [0.89902344 0.84101563 0.82460937 0.73916016]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:42:35.872050: step 50200, loss 0.668097, acc [0.89779656 0.84114367 0.81364314 0.73024056] \n",
      "\n",
      "2019-02-22T00:42:37.232574: step 50240, loss 0.576849, acc [0.89941406 0.84765625 0.82304687 0.73525391]\n",
      "2019-02-22T00:42:38.517213: step 50280, loss 0.58852, acc [0.89541016 0.84394531 0.82246094 0.73300781]\n",
      "2019-02-22T00:42:39.734892: step 50320, loss 0.586691, acc [0.89580078 0.84042969 0.8234375  0.73271484]\n",
      "2019-02-22T00:42:40.925292: step 50360, loss 0.603252, acc [0.89501953 0.83701172 0.81972656 0.72587891]\n",
      "2019-02-22T00:42:42.125609: step 50400, loss 0.610733, acc [0.89658203 0.84355469 0.81552734 0.73144531]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:42:46.143699: step 50400, loss 0.666672, acc [0.89736608 0.8413539  0.81188119 0.72892911] \n",
      "\n",
      "2019-02-22T00:42:47.427344: step 50440, loss 0.602141, acc [0.89316406 0.84082031 0.81982422 0.72958984]\n",
      "2019-02-22T00:42:48.601871: step 50480, loss 0.580616, acc [0.89990234 0.84296875 0.82441406 0.73242188]\n",
      "2019-02-22T00:42:49.775404: step 50520, loss 0.592152, acc [0.89589844 0.84384766 0.81757813 0.73164063]\n",
      "2019-02-22T00:42:50.948444: step 50560, loss 0.600077, acc [0.89912109 0.84443359 0.81972656 0.73330078]\n",
      "2019-02-22T00:42:52.119003: step 50600, loss 0.621039, acc [0.8953125  0.83798828 0.81230469 0.725     ]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:42:55.971923: step 50600, loss 0.669559, acc [0.89755629 0.84015257 0.81245182 0.72925948] \n",
      "\n",
      "2019-02-22T00:42:57.214403: step 50640, loss 0.603751, acc [0.89726562 0.84326172 0.8171875  0.73085937]\n",
      "2019-02-22T00:42:58.380001: step 50680, loss 0.602995, acc [0.89472656 0.83847656 0.81640625 0.72792969]\n",
      "2019-02-22T00:42:59.556017: step 50720, loss 0.596707, acc [0.89677734 0.84814453 0.81894531 0.73076172]\n",
      "2019-02-22T00:43:00.722110: step 50760, loss 0.586181, acc [0.89853516 0.84941406 0.82441406 0.73896484]\n",
      "2019-02-22T00:43:01.889692: step 50800, loss 0.625829, acc [0.89599609 0.83505859 0.8140625  0.7265625 ]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:43:05.746585: step 50800, loss 0.668209, acc [0.89700568 0.84015257 0.81244181 0.72731732] \n",
      "\n",
      "2019-02-22T00:43:06.996998: step 50840, loss 0.630643, acc [0.89365234 0.83183594 0.80869141 0.72226563]\n",
      "2019-02-22T00:43:08.169043: step 50880, loss 0.605306, acc [0.89716797 0.84140625 0.82041016 0.73076172]\n",
      "2019-02-22T00:43:09.337618: step 50920, loss 0.620858, acc [0.89443359 0.83476562 0.81425781 0.72333984]\n",
      "2019-02-22T00:43:10.501231: step 50960, loss 0.608397, acc [0.89414063 0.8359375  0.81767578 0.7296875 ]\n",
      "2019-02-22T00:43:11.662366: step 51000, loss 0.610652, acc [0.89492187 0.84277344 0.81611328 0.72714844]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:43:15.601098: step 51000, loss 0.666578, acc [0.89731602 0.84043288 0.81182112 0.72793801] \n",
      "\n",
      "2019-02-22T00:43:16.857462: step 51040, loss 0.63797, acc [0.89384766 0.83818359 0.80927734 0.71816406]\n",
      "2019-02-22T00:43:18.025046: step 51080, loss 0.607652, acc [0.89472656 0.83994141 0.81503906 0.72734375]\n",
      "2019-02-22T00:43:19.196146: step 51120, loss 0.635304, acc [0.89619141 0.83759766 0.80703125 0.72216797]\n",
      "2019-02-22T00:43:20.364178: step 51160, loss 0.624009, acc [0.89755859 0.83925781 0.81601563 0.72949219]\n",
      "2019-02-22T00:43:21.528783: step 51200, loss 0.618155, acc [0.89794922 0.83378906 0.81044922 0.72128906]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:43:25.407046: step 51200, loss 0.667069, acc [0.89766641 0.83996236 0.81227162 0.7281082 ] \n",
      "\n",
      "2019-02-22T00:43:26.663864: step 51240, loss 0.607838, acc [0.90029297 0.84677734 0.81962891 0.73574219]\n",
      "2019-02-22T00:43:27.829462: step 51280, loss 0.613174, acc [0.90009766 0.84257812 0.81367188 0.73007813]\n",
      "2019-02-22T00:43:28.996549: step 51320, loss 0.632207, acc [0.89628906 0.83935547 0.80917969 0.725     ]\n",
      "2019-02-22T00:43:30.190419: step 51360, loss 0.629938, acc [0.89453125 0.83847656 0.80878906 0.72763672]\n",
      "2019-02-22T00:43:31.371395: step 51400, loss 0.603744, acc [0.90361328 0.84833984 0.81503906 0.73544922]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:43:35.268955: step 51400, loss 0.66278, acc [0.89745618 0.8391014  0.81334281 0.72816827] \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-02-22T00:43:36.527353: step 51440, loss 0.62476, acc [0.896875   0.84130859 0.81171875 0.72988281]\n",
      "2019-02-22T00:43:37.702330: step 51480, loss 0.615447, acc [0.89433594 0.83925781 0.81923828 0.72958984]\n",
      "2019-02-22T00:43:39.033114: step 51520, loss 0.593052, acc [0.90114524 0.8415986  0.81863261 0.73149661]\n",
      "2019-02-22T00:43:40.207622: step 51560, loss 0.570147, acc [0.90322266 0.84853516 0.82714844 0.73974609]\n",
      "2019-02-22T00:43:41.383638: step 51600, loss 0.582337, acc [0.89736328 0.84091797 0.82324219 0.73085937]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:43:45.334271: step 51600, loss 0.661862, acc [0.89719589 0.84173432 0.81349298 0.72842855] \n",
      "\n",
      "2019-02-22T00:43:46.586669: step 51640, loss 0.581511, acc [0.89609375 0.83886719 0.82109375 0.72832031]\n",
      "2019-02-22T00:43:47.762684: step 51680, loss 0.587847, acc [0.89482422 0.83994141 0.82451172 0.73349609]\n",
      "2019-02-22T00:43:48.930762: step 51720, loss 0.572437, acc [0.90234375 0.84736328 0.82578125 0.73730469]\n",
      "2019-02-22T00:43:50.099336: step 51760, loss 0.588335, acc [0.89746094 0.84335938 0.82148438 0.73398438]\n",
      "2019-02-22T00:43:51.264438: step 51800, loss 0.580801, acc [0.89755859 0.84453125 0.8265625  0.73564453]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:43:55.109921: step 51800, loss 0.671079, acc [0.89673538 0.83963199 0.81337284 0.72717717] \n",
      "\n",
      "2019-02-22T00:43:56.362814: step 51840, loss 0.60987, acc [0.89697266 0.83720703 0.81347656 0.72402344]\n",
      "2019-02-22T00:43:57.532382: step 51880, loss 0.590565, acc [0.90078125 0.83964844 0.82119141 0.73173828]\n",
      "2019-02-22T00:43:58.695995: step 51920, loss 0.589414, acc [0.89921875 0.84511719 0.81962891 0.73183594]\n",
      "2019-02-22T00:43:59.859114: step 51960, loss 0.578896, acc [0.89941406 0.84453125 0.82470703 0.73632812]\n",
      "2019-02-22T00:44:01.038103: step 52000, loss 0.609122, acc [0.89335937 0.83876953 0.81289062 0.72568359]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:44:04.888052: step 52000, loss 0.667781, acc [0.89673538 0.84059306 0.81246183 0.72809819] \n",
      "\n",
      "2019-02-22T00:44:06.132511: step 52040, loss 0.589301, acc [0.89628906 0.84072266 0.82216797 0.73144531]\n",
      "2019-02-22T00:44:07.300618: step 52080, loss 0.591074, acc [0.89882812 0.84267578 0.821875   0.73535156]\n",
      "2019-02-22T00:44:08.467180: step 52120, loss 0.594394, acc [0.89853516 0.84121094 0.82421875 0.73310547]\n",
      "2019-02-22T00:44:09.646667: step 52160, loss 0.584458, acc [0.89560547 0.84326172 0.82011719 0.73242188]\n",
      "2019-02-22T00:44:10.809830: step 52200, loss 0.585387, acc [0.90185547 0.84863281 0.82392578 0.73964844]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:44:14.714837: step 52200, loss 0.670167, acc [0.8970257  0.8392816  0.81176105 0.7268468 ] \n",
      "\n",
      "2019-02-22T00:44:15.958803: step 52240, loss 0.599842, acc [0.89921875 0.84638672 0.81621094 0.73222656]\n",
      "2019-02-22T00:44:17.127826: step 52280, loss 0.602973, acc [0.89833984 0.84208984 0.82255859 0.73691406]\n",
      "2019-02-22T00:44:18.317730: step 52320, loss 0.605973, acc [0.89707031 0.840625   0.81416016 0.72978516]\n",
      "2019-02-22T00:44:19.487789: step 52360, loss 0.58678, acc [0.89804688 0.84863281 0.82089844 0.73408203]\n",
      "2019-02-22T00:44:20.656860: step 52400, loss 0.626246, acc [0.89746094 0.84121094 0.81132812 0.72685547]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:44:24.512800: step 52400, loss 0.667585, acc [0.8966653  0.84094345 0.81375326 0.72929952] \n",
      "\n",
      "2019-02-22T00:44:25.772598: step 52440, loss 0.621363, acc [0.89521484 0.83789062 0.81796875 0.72861328]\n",
      "2019-02-22T00:44:26.937762: step 52480, loss 0.616051, acc [0.89667969 0.84277344 0.81650391 0.73095703]\n",
      "2019-02-22T00:44:28.101811: step 52520, loss 0.612705, acc [0.89746094 0.84335938 0.81298828 0.72617188]\n",
      "2019-02-22T00:44:29.260479: step 52560, loss 0.604209, acc [0.9        0.84150391 0.81630859 0.72646484]\n",
      "2019-02-22T00:44:30.425565: step 52600, loss 0.601643, acc [0.89882812 0.83974609 0.81777344 0.72939453]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:44:34.308795: step 52600, loss 0.663065, acc [0.89791669 0.84019261 0.81282223 0.72888907] \n",
      "\n",
      "2019-02-22T00:44:35.550229: step 52640, loss 0.61161, acc [0.89628906 0.83710938 0.81416016 0.72412109]\n",
      "2019-02-22T00:44:36.711862: step 52680, loss 0.606705, acc [0.89580078 0.84414062 0.81884766 0.73457031]\n",
      "2019-02-22T00:44:37.874484: step 52720, loss 0.593943, acc [0.90097656 0.84453125 0.82324219 0.73769531]\n",
      "2019-02-22T00:44:39.038593: step 52760, loss 0.625758, acc [0.89375    0.83974609 0.80947266 0.72246094]\n",
      "2019-02-22T00:44:40.205680: step 52800, loss 0.612528, acc [0.89736328 0.83945313 0.8171875  0.73125   ]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:44:44.055132: step 52800, loss 0.663021, acc [0.8971158  0.84166425 0.81328274 0.72933957] \n",
      "\n",
      "2019-02-22T00:44:45.319433: step 52840, loss 0.612687, acc [0.89785156 0.84042969 0.81259766 0.72841797]\n",
      "2019-02-22T00:44:46.480567: step 52880, loss 0.627552, acc [0.89951172 0.83916016 0.81552734 0.73613281]\n",
      "2019-02-22T00:44:47.645174: step 52920, loss 0.615411, acc [0.89082031 0.83652344 0.81337891 0.72158203]\n",
      "2019-02-22T00:44:48.809780: step 52960, loss 0.62742, acc [0.89697266 0.83896484 0.80957031 0.72275391]\n",
      "2019-02-22T00:44:49.969427: step 53000, loss 0.618937, acc [0.90126953 0.8421875  0.81337891 0.72988281]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:44:53.808958: step 53000, loss 0.661968, acc [0.89730601 0.84033277 0.81202134 0.7274775 ] \n",
      "\n",
      "2019-02-22T00:44:55.054410: step 53040, loss 0.633993, acc [0.89316406 0.83691406 0.81083984 0.72255859]\n",
      "2019-02-22T00:44:56.347481: step 53080, loss 0.620829, acc [0.89602174 0.8393239  0.81205611 0.724782  ]\n",
      "2019-02-22T00:44:57.510104: step 53120, loss 0.569477, acc [0.90263672 0.84501953 0.83076172 0.7421875 ]\n",
      "2019-02-22T00:44:58.669751: step 53160, loss 0.556312, acc [0.9015625  0.85009766 0.83105469 0.74619141]\n",
      "2019-02-22T00:44:59.830886: step 53200, loss 0.579186, acc [0.89697266 0.84169922 0.825      0.73447266]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:45:03.712575: step 53200, loss 0.664582, acc [0.89714583 0.84024267 0.81452412 0.72834847] \n",
      "\n",
      "2019-02-22T00:45:04.952573: step 53240, loss 0.569536, acc [0.89677734 0.84501953 0.82695312 0.73359375]\n",
      "2019-02-22T00:45:06.112220: step 53280, loss 0.585692, acc [0.90097656 0.84589844 0.82773438 0.73818359]\n",
      "2019-02-22T00:45:07.271369: step 53320, loss 0.595439, acc [0.90087891 0.83720703 0.82363281 0.73066406]\n",
      "2019-02-22T00:45:08.430580: step 53360, loss 0.603477, acc [0.890625   0.83681641 0.82265625 0.72832031]\n",
      "2019-02-22T00:45:09.589175: step 53400, loss 0.574681, acc [0.89521484 0.84677734 0.82529297 0.73242188]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:45:13.525921: step 53400, loss 0.664955, acc [0.89768643 0.84093344 0.81353302 0.72958984] \n",
      "\n",
      "2019-02-22T00:45:14.758478: step 53440, loss 0.583373, acc [0.89775391 0.84736328 0.82441406 0.73583984]\n",
      "2019-02-22T00:45:15.926168: step 53480, loss 0.60329, acc [0.89667969 0.84287109 0.82128906 0.73095703]\n",
      "2019-02-22T00:45:17.089284: step 53520, loss 0.603622, acc [0.89384766 0.83945313 0.81748047 0.72519531]\n",
      "2019-02-22T00:45:18.250420: step 53560, loss 0.608054, acc [0.89716797 0.84003906 0.81816406 0.73007813]\n",
      "2019-02-22T00:45:19.408577: step 53600, loss 0.601366, acc [0.90214844 0.84111328 0.82207031 0.73681641]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:45:23.256542: step 53600, loss 0.664204, acc [0.89674539 0.83877104 0.8138734  0.72753757] \n",
      "\n",
      "2019-02-22T00:45:24.495050: step 53640, loss 0.61503, acc [0.89824219 0.83339844 0.81425781 0.721875  ]\n",
      "2019-02-22T00:45:25.659657: step 53680, loss 0.596541, acc [0.89775391 0.84423828 0.82128906 0.73720703]\n",
      "2019-02-22T00:45:26.821784: step 53720, loss 0.591012, acc [0.89970703 0.84570312 0.82138672 0.73925781]\n",
      "2019-02-22T00:45:27.981430: step 53760, loss 0.587256, acc [0.90175781 0.84648437 0.81992188 0.73564453]\n",
      "2019-02-22T00:45:29.140580: step 53800, loss 0.601635, acc [0.89677734 0.83935547 0.81601563 0.72587891]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:45:32.979616: step 53800, loss 0.667041, acc [0.89811691 0.84015257 0.81406361 0.72831843] \n",
      "\n",
      "2019-02-22T00:45:34.225566: step 53840, loss 0.600491, acc [0.89980469 0.84335938 0.82080078 0.73417969]\n",
      "2019-02-22T00:45:35.383722: step 53880, loss 0.614905, acc [0.89726562 0.83652344 0.81474609 0.72792969]\n",
      "2019-02-22T00:45:36.546387: step 53920, loss 0.601749, acc [0.90068359 0.84003906 0.81826172 0.73105469]\n",
      "2019-02-22T00:45:37.704505: step 53960, loss 0.59109, acc [0.89833984 0.8484375  0.81767578 0.73095703]\n",
      "2019-02-22T00:45:38.863159: step 54000, loss 0.588291, acc [0.89697266 0.84072266 0.81630859 0.72949219]\n",
      "\n",
      "Evaluation:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-02-22T00:45:42.721042: step 54000, loss 0.662697, acc [0.89743616 0.83960196 0.81413369 0.72825837] \n",
      "\n",
      "2019-02-22T00:45:43.965007: step 54040, loss 0.593509, acc [0.89814453 0.84316406 0.81660156 0.73037109]\n",
      "2019-02-22T00:45:45.124158: step 54080, loss 0.613646, acc [0.89755859 0.84541016 0.81171875 0.72841797]\n",
      "2019-02-22T00:45:46.285298: step 54120, loss 0.592983, acc [0.90048828 0.84453125 0.82050781 0.73173828]\n",
      "2019-02-22T00:45:47.442458: step 54160, loss 0.612097, acc [0.90039062 0.84023437 0.81542969 0.73115234]\n",
      "2019-02-22T00:45:48.600121: step 54200, loss 0.619605, acc [0.89189453 0.83789062 0.8125     0.72353516]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:45:52.447092: step 54200, loss 0.660775, acc [0.89728599 0.84069317 0.81417373 0.72910931] \n",
      "\n",
      "2019-02-22T00:45:53.684610: step 54240, loss 0.619746, acc [0.89111328 0.83613281 0.81083984 0.71835938]\n",
      "2019-02-22T00:45:54.842767: step 54280, loss 0.589585, acc [0.90039062 0.84707031 0.81992188 0.73886719]\n",
      "2019-02-22T00:45:56.005391: step 54320, loss 0.611753, acc [0.89550781 0.83740234 0.81240234 0.72304687]\n",
      "2019-02-22T00:45:57.168510: step 54360, loss 0.621886, acc [0.89755859 0.84482422 0.81123047 0.72666016]\n",
      "2019-02-22T00:45:58.330139: step 54400, loss 0.622096, acc [0.89394531 0.83818359 0.81210938 0.72275391]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:46:02.177606: step 54400, loss 0.65947, acc [0.89722592 0.83943177 0.8146843  0.72886904] \n",
      "\n",
      "2019-02-22T00:46:03.422068: step 54440, loss 0.607688, acc [0.89433594 0.83466797 0.81835938 0.72617188]\n",
      "2019-02-22T00:46:04.583202: step 54480, loss 0.604, acc [0.89746094 0.83837891 0.81777344 0.72802734]\n",
      "2019-02-22T00:46:05.758722: step 54520, loss 0.624318, acc [0.89951172 0.84130859 0.81171875 0.72910156]\n",
      "2019-02-22T00:46:06.923327: step 54560, loss 0.587408, acc [0.90068359 0.84482422 0.82138672 0.73798828]\n",
      "2019-02-22T00:46:08.095374: step 54600, loss 0.600743, acc [0.89794922 0.84599609 0.81699219 0.73398438]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:46:12.092630: step 54600, loss 0.660172, acc [0.8968455  0.84032276 0.81471433 0.72926949] \n",
      "\n",
      "2019-02-22T00:46:13.532518: step 54640, loss 0.629656, acc [0.89576428 0.83972143 0.80537701 0.72195194]\n",
      "2019-02-22T00:46:14.710020: step 54680, loss 0.572057, acc [0.896875   0.84365234 0.82900391 0.73955078]\n",
      "2019-02-22T00:46:15.881074: step 54720, loss 0.565501, acc [0.89960938 0.84511719 0.83222656 0.73955078]\n",
      "2019-02-22T00:46:17.047167: step 54760, loss 0.552893, acc [0.90380859 0.85283203 0.83447266 0.74580078]\n",
      "2019-02-22T00:46:18.217231: step 54800, loss 0.555785, acc [0.90371094 0.85429687 0.82919922 0.74228516]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:46:22.056313: step 54800, loss 0.660224, acc [0.89700568 0.84067315 0.81565538 0.72908929] \n",
      "\n",
      "2019-02-22T00:46:23.314614: step 54840, loss 0.572899, acc [0.90380859 0.84746094 0.82871094 0.74248047]\n",
      "2019-02-22T00:46:24.480708: step 54880, loss 0.598002, acc [0.89707031 0.83945313 0.81884766 0.72773438]\n",
      "2019-02-22T00:46:25.651270: step 54920, loss 0.589655, acc [0.89238281 0.84248047 0.81943359 0.72539062]\n",
      "2019-02-22T00:46:26.815379: step 54960, loss 0.591169, acc [0.89824219 0.83457031 0.82148438 0.73076172]\n",
      "2019-02-22T00:46:27.981473: step 55000, loss 0.582895, acc [0.89833984 0.84335938 0.8234375  0.73320312]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:46:31.807167: step 55000, loss 0.662105, acc [0.89602459 0.84112365 0.81484448 0.72867883] \n",
      "\n",
      "2019-02-22T00:46:33.043147: step 55040, loss 0.580254, acc [0.89775391 0.84609375 0.82138672 0.73398438]\n",
      "2019-02-22T00:46:34.212713: step 55080, loss 0.613489, acc [0.89326172 0.83857422 0.81728516 0.72636719]\n",
      "2019-02-22T00:46:35.378310: step 55120, loss 0.584509, acc [0.89765625 0.84443359 0.82373047 0.73515625]\n",
      "2019-02-22T00:46:36.549366: step 55160, loss 0.606445, acc [0.89707031 0.83896484 0.81757813 0.73076172]\n",
      "2019-02-22T00:46:37.710498: step 55200, loss 0.593331, acc [0.89814453 0.84492188 0.81533203 0.72636719]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:46:41.597647: step 55200, loss 0.664234, acc [0.89736608 0.84013255 0.81455415 0.73025058] \n",
      "\n",
      "2019-02-22T00:46:42.854507: step 55240, loss 0.598965, acc [0.89550781 0.83818359 0.82080078 0.72714844]\n",
      "2019-02-22T00:46:44.039945: step 55280, loss 0.588416, acc [0.90214844 0.84335938 0.82226562 0.73691406]\n",
      "2019-02-22T00:46:45.215464: step 55320, loss 0.572398, acc [0.89609375 0.84365234 0.83271484 0.74179688]\n",
      "2019-02-22T00:46:46.384038: step 55360, loss 0.588748, acc [0.89892578 0.84208984 0.82050781 0.734375  ]\n",
      "2019-02-22T00:46:47.560052: step 55400, loss 0.612029, acc [0.89414063 0.83261719 0.8171875  0.72158203]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:46:51.498785: step 55400, loss 0.664942, acc [0.89673537 0.83986225 0.81254192 0.72559541] \n",
      "\n",
      "2019-02-22T00:46:52.751747: step 55440, loss 0.59298, acc [0.89580078 0.84111328 0.82363281 0.73134766]\n",
      "2019-02-22T00:46:53.926773: step 55480, loss 0.605979, acc [0.89726562 0.84130859 0.81679687 0.73232422]\n",
      "2019-02-22T00:46:55.100799: step 55520, loss 0.60325, acc [0.89833984 0.84189453 0.81435547 0.72929687]\n",
      "2019-02-22T00:46:56.274831: step 55560, loss 0.594902, acc [0.89736328 0.84394531 0.82128906 0.73076172]\n",
      "2019-02-22T00:46:57.447868: step 55600, loss 0.586788, acc [0.89892578 0.84677734 0.82353516 0.73740234]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:47:01.359816: step 55600, loss 0.666842, acc [0.89737609 0.8411737  0.81345293 0.72878896] \n",
      "\n",
      "2019-02-22T00:47:02.619158: step 55640, loss 0.584717, acc [0.89873047 0.84414062 0.81972656 0.73164063]\n",
      "2019-02-22T00:47:03.796164: step 55680, loss 0.61823, acc [0.89433594 0.83447266 0.81728516 0.72578125]\n",
      "2019-02-22T00:47:04.974658: step 55720, loss 0.606584, acc [0.89707031 0.8390625  0.81689453 0.73027344]\n",
      "2019-02-22T00:47:06.149187: step 55760, loss 0.620561, acc [0.89199219 0.83779297 0.81113281 0.71992188]\n",
      "2019-02-22T00:47:07.338590: step 55800, loss 0.613281, acc [0.89550781 0.84306641 0.81699219 0.72910156]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:47:11.223256: step 55800, loss 0.66282, acc [0.89708577 0.84033277 0.81275215 0.72786793] \n",
      "\n",
      "2019-02-22T00:47:12.479126: step 55840, loss 0.600371, acc [0.90029297 0.8453125  0.81816406 0.73310547]\n",
      "2019-02-22T00:47:13.664070: step 55880, loss 0.620664, acc [0.89306641 0.83916016 0.8125     0.72402344]\n",
      "2019-02-22T00:47:14.833637: step 55920, loss 0.61189, acc [0.89755859 0.84023437 0.81591797 0.72851562]\n",
      "2019-02-22T00:47:16.023571: step 55960, loss 0.599063, acc [0.90029297 0.84052734 0.82275391 0.73515625]\n",
      "2019-02-22T00:47:17.199552: step 56000, loss 0.601006, acc [0.89716797 0.84638672 0.81953125 0.73515625]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:47:21.069342: step 56000, loss 0.660199, acc [0.89737609 0.84091342 0.81478441 0.72986014] \n",
      "\n",
      "2019-02-22T00:47:22.327193: step 56040, loss 0.631286, acc [0.89462891 0.83671875 0.80898437 0.72148437]\n",
      "2019-02-22T00:47:23.496267: step 56080, loss 0.591067, acc [0.9        0.84570312 0.81845703 0.73242188]\n",
      "2019-02-22T00:47:24.662855: step 56120, loss 0.604166, acc [0.89716797 0.84277344 0.81796875 0.73349609]\n",
      "2019-02-22T00:47:25.831428: step 56160, loss 0.618433, acc [0.89580078 0.83994141 0.81181641 0.72597656]\n",
      "2019-02-22T00:47:27.135411: step 56200, loss 0.616296, acc [0.89327553 0.83540779 0.81295869 0.72373146]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:47:30.982382: step 56200, loss 0.661855, acc [0.89603459 0.84022265 0.8136932  0.72759764] \n",
      "\n",
      "2019-02-22T00:47:32.241228: step 56240, loss 0.568924, acc [0.90117187 0.84521484 0.82861328 0.73916016]\n",
      "2019-02-22T00:47:33.405337: step 56280, loss 0.554553, acc [0.89814453 0.8484375  0.83212891 0.74199219]\n",
      "2019-02-22T00:47:34.570936: step 56320, loss 0.55635, acc [0.90507812 0.84384766 0.83535156 0.74697266]\n",
      "2019-02-22T00:47:35.743973: step 56360, loss 0.5658, acc [0.89931641 0.84609375 0.83066406 0.73896484]\n",
      "2019-02-22T00:47:36.918007: step 56400, loss 0.568855, acc [0.89765625 0.84931641 0.82685547 0.73974609]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:47:40.780352: step 56400, loss 0.66757, acc [0.89690557 0.8401826  0.81204137 0.72641632] \n",
      "\n",
      "2019-02-22T00:47:42.025309: step 56440, loss 0.587155, acc [0.90068359 0.84169922 0.82392578 0.73369141]\n",
      "2019-02-22T00:47:43.198349: step 56480, loss 0.566594, acc [0.89931641 0.84912109 0.82724609 0.74248047]\n",
      "2019-02-22T00:47:44.368908: step 56520, loss 0.599494, acc [0.89306641 0.84169922 0.81806641 0.72753906]\n",
      "2019-02-22T00:47:45.536047: step 56560, loss 0.587825, acc [0.89248047 0.83935547 0.82128906 0.728125  ]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-02-22T00:47:46.702088: step 56600, loss 0.578952, acc [0.9015625  0.84658203 0.82363281 0.73486328]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:47:50.544594: step 56600, loss 0.667885, acc [0.89649511 0.84042287 0.81272212 0.72667661] \n",
      "\n",
      "2019-02-22T00:47:51.803440: step 56640, loss 0.582857, acc [0.89736328 0.83759766 0.81806641 0.72519531]\n",
      "2019-02-22T00:47:52.969534: step 56680, loss 0.583286, acc [0.89589844 0.84316406 0.82539063 0.73349609]\n",
      "2019-02-22T00:47:54.139100: step 56720, loss 0.579029, acc [0.90029297 0.84814453 0.82451172 0.7390625 ]\n",
      "2019-02-22T00:47:55.305194: step 56760, loss 0.580497, acc [0.90048828 0.84453125 0.82412109 0.73398438]\n",
      "2019-02-22T00:47:56.475753: step 56800, loss 0.58823, acc [0.89414063 0.84306641 0.82421875 0.73808594]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:48:00.340135: step 56800, loss 0.663441, acc [0.89687553 0.84007248 0.81509476 0.72772778] \n",
      "\n",
      "2019-02-22T00:48:01.594466: step 56840, loss 0.599391, acc [0.89433594 0.8390625  0.81767578 0.72675781]\n",
      "2019-02-22T00:48:02.763535: step 56880, loss 0.585402, acc [0.89980469 0.84804687 0.82636719 0.73867187]\n",
      "2019-02-22T00:48:03.928637: step 56920, loss 0.592527, acc [0.89707031 0.84091797 0.82275391 0.73515625]\n",
      "2019-02-22T00:48:05.089781: step 56960, loss 0.627936, acc [0.89404297 0.83261719 0.80888672 0.71982422]\n",
      "2019-02-22T00:48:06.251898: step 57000, loss 0.62076, acc [0.89375    0.83730469 0.81416016 0.72236328]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:48:10.112758: step 57000, loss 0.662505, acc [0.89691558 0.84013255 0.81448408 0.72704702] \n",
      "\n",
      "2019-02-22T00:48:11.360692: step 57040, loss 0.587712, acc [0.90009766 0.84169922 0.82080078 0.72841797]\n",
      "2019-02-22T00:48:12.530755: step 57080, loss 0.597178, acc [0.89824219 0.84052734 0.81982422 0.72871094]\n",
      "2019-02-22T00:48:13.700321: step 57120, loss 0.588235, acc [0.90068359 0.84882813 0.81845703 0.73457031]\n",
      "2019-02-22T00:48:14.870879: step 57160, loss 0.596962, acc [0.8984375  0.84501953 0.82216797 0.73417969]\n",
      "2019-02-22T00:48:16.040942: step 57200, loss 0.579458, acc [0.90039062 0.84609375 0.82207031 0.73339844]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:48:19.911720: step 57200, loss 0.663151, acc [0.89759633 0.84083333 0.81385338 0.72908929] \n",
      "\n",
      "2019-02-22T00:48:21.163171: step 57240, loss 0.585975, acc [0.89746094 0.84658203 0.81689453 0.73222656]\n",
      "2019-02-22T00:48:22.338645: step 57280, loss 0.581899, acc [0.89804688 0.84511719 0.82275391 0.734375  ]\n",
      "2019-02-22T00:48:23.510691: step 57320, loss 0.605978, acc [0.90009766 0.84462891 0.8171875  0.73095703]\n",
      "2019-02-22T00:48:24.677775: step 57360, loss 0.61069, acc [0.89746094 0.84101563 0.81533203 0.72880859]\n",
      "2019-02-22T00:48:25.849840: step 57400, loss 0.61909, acc [0.89082031 0.83613281 0.81386719 0.72138672]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:48:29.676958: step 57400, loss 0.662359, acc [0.89780657 0.83975212 0.81496461 0.72925948] \n",
      "\n",
      "2019-02-22T00:48:30.921914: step 57440, loss 0.608858, acc [0.90283203 0.84052734 0.81923828 0.72919922]\n",
      "2019-02-22T00:48:32.089001: step 57480, loss 0.610593, acc [0.89130859 0.8359375  0.81582031 0.72402344]\n",
      "2019-02-22T00:48:33.257576: step 57520, loss 0.609768, acc [0.89599609 0.83955078 0.81367188 0.72783203]\n",
      "2019-02-22T00:48:34.424664: step 57560, loss 0.622123, acc [0.89072266 0.83691406 0.81357422 0.7234375 ]\n",
      "2019-02-22T00:48:35.596212: step 57600, loss 0.59598, acc [0.89980469 0.84423828 0.81708984 0.73330078]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:48:39.476911: step 57600, loss 0.65788, acc [0.89851735 0.84163422 0.81601578 0.73075113] \n",
      "\n",
      "2019-02-22T00:48:40.755102: step 57640, loss 0.61301, acc [0.90234375 0.84433594 0.81962891 0.73515625]\n",
      "2019-02-22T00:48:41.922684: step 57680, loss 0.620038, acc [0.89443359 0.83837891 0.81669922 0.7265625 ]\n",
      "2019-02-22T00:48:43.090265: step 57720, loss 0.610545, acc [0.89785156 0.84023437 0.81806641 0.73193359]\n",
      "2019-02-22T00:48:44.389784: step 57760, loss 0.609753, acc [0.89874921 0.83862354 0.81796086 0.73072719]\n",
      "2019-02-22T00:48:45.552903: step 57800, loss 0.58018, acc [0.90107422 0.84189453 0.82431641 0.73994141]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:48:49.449473: step 57800, loss 0.658587, acc [0.89757631 0.84194456 0.81532501 0.72947972] \n",
      "\n",
      "2019-02-22T00:48:50.694927: step 57840, loss 0.544965, acc [0.90039062 0.84619141 0.83222656 0.73779297]\n",
      "2019-02-22T00:48:51.865484: step 57880, loss 0.579399, acc [0.89492187 0.84638672 0.82353516 0.7328125 ]\n",
      "2019-02-22T00:48:53.030091: step 57920, loss 0.559684, acc [0.89550781 0.84394531 0.83027344 0.734375  ]\n",
      "2019-02-22T00:48:54.195690: step 57960, loss 0.56701, acc [0.90400391 0.84912109 0.82636719 0.74277344]\n",
      "2019-02-22T00:48:55.359305: step 58000, loss 0.56077, acc [0.89746094 0.84990234 0.82949219 0.74003906]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:48:59.219215: step 58000, loss 0.662366, acc [0.89760634 0.84164422 0.81455415 0.73029062] \n",
      "\n",
      "2019-02-22T00:49:00.461648: step 58040, loss 0.558833, acc [0.90126953 0.85273438 0.82802734 0.73974609]\n",
      "2019-02-22T00:49:01.631215: step 58080, loss 0.58665, acc [0.89775391 0.84355469 0.82714844 0.73554688]\n",
      "2019-02-22T00:49:02.797806: step 58120, loss 0.58349, acc [0.8953125  0.84365234 0.82060547 0.73232422]\n",
      "2019-02-22T00:49:03.963900: step 58160, loss 0.595095, acc [0.89785156 0.84101563 0.81962891 0.73017578]\n",
      "2019-02-22T00:49:05.122555: step 58200, loss 0.574432, acc [0.89990234 0.84394531 0.82236328 0.73515625]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:49:09.008237: step 58200, loss 0.665322, acc [0.89709577 0.84104356 0.81452412 0.72919941] \n",
      "\n",
      "2019-02-22T00:49:10.268051: step 58240, loss 0.585737, acc [0.89707031 0.84599609 0.82958984 0.73798828]\n",
      "2019-02-22T00:49:11.445593: step 58280, loss 0.579075, acc [0.89755859 0.83867187 0.828125   0.73242188]\n",
      "2019-02-22T00:49:12.611647: step 58320, loss 0.585901, acc [0.89951172 0.84472656 0.81621094 0.72949219]\n",
      "2019-02-22T00:49:13.787663: step 58360, loss 0.59604, acc [0.89287109 0.84013672 0.81767578 0.72285156]\n",
      "2019-02-22T00:49:14.955245: step 58400, loss 0.587132, acc [0.89990234 0.84306641 0.82001953 0.73291016]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:49:18.965276: step 58400, loss 0.665442, acc [0.89816697 0.84033277 0.81372323 0.72860876] \n",
      "\n",
      "2019-02-22T00:49:20.213211: step 58440, loss 0.588339, acc [0.89873047 0.84589844 0.82226562 0.73251953]\n",
      "2019-02-22T00:49:21.379801: step 58480, loss 0.592392, acc [0.89853516 0.84257812 0.81992188 0.72978516]\n",
      "2019-02-22T00:49:22.546887: step 58520, loss 0.592814, acc [0.89853516 0.84228516 0.82138672 0.73183594]\n",
      "2019-02-22T00:49:23.714966: step 58560, loss 0.586461, acc [0.89746094 0.84189453 0.81621094 0.728125  ]\n",
      "2019-02-22T00:49:24.884066: step 58600, loss 0.585412, acc [0.89980469 0.84638672 0.82138672 0.73710937]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:49:28.738993: step 58600, loss 0.666682, acc [0.89679544 0.84033277 0.81374326 0.72842855] \n",
      "\n",
      "2019-02-22T00:49:29.978445: step 58640, loss 0.580372, acc [0.90019531 0.84101563 0.82148438 0.73212891]\n",
      "2019-02-22T00:49:31.146027: step 58680, loss 0.614289, acc [0.89238281 0.83974609 0.81494141 0.72558594]\n",
      "2019-02-22T00:49:32.310138: step 58720, loss 0.616894, acc [0.89697266 0.84023437 0.81445312 0.73212891]\n",
      "2019-02-22T00:49:33.470821: step 58760, loss 0.602443, acc [0.89423828 0.83916016 0.81972656 0.73300781]\n",
      "2019-02-22T00:49:34.630917: step 58800, loss 0.608865, acc [0.89267578 0.83662109 0.81328125 0.72294922]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:49:38.470449: step 58800, loss 0.661343, acc [0.89757631 0.84119373 0.81523491 0.73076114] \n",
      "\n",
      "2019-02-22T00:49:39.709950: step 58840, loss 0.620443, acc [0.89238281 0.83417969 0.81728516 0.72666016]\n",
      "2019-02-22T00:49:40.878029: step 58880, loss 0.605977, acc [0.89433594 0.83574219 0.80957031 0.72011719]\n",
      "2019-02-22T00:49:42.048091: step 58920, loss 0.601743, acc [0.90224609 0.83925781 0.81826172 0.73242188]\n",
      "2019-02-22T00:49:43.215693: step 58960, loss 0.592922, acc [0.89863281 0.84052734 0.81933594 0.73242188]\n",
      "2019-02-22T00:49:44.375848: step 59000, loss 0.597977, acc [0.89726562 0.84433594 0.81757813 0.73125   ]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:49:48.245601: step 59000, loss 0.658009, acc [0.89764639 0.83979217 0.81618597 0.72973   ] \n",
      "\n",
      "2019-02-22T00:49:49.484661: step 59040, loss 0.600926, acc [0.89619141 0.8453125  0.81767578 0.72861328]\n",
      "2019-02-22T00:49:50.642805: step 59080, loss 0.599515, acc [0.90087891 0.84375    0.81816406 0.73330078]\n",
      "2019-02-22T00:49:51.807870: step 59120, loss 0.600689, acc [0.90107422 0.84462891 0.82138672 0.73544922]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-02-22T00:49:52.966524: step 59160, loss 0.604378, acc [0.89658203 0.84326172 0.8203125  0.72958984]\n",
      "2019-02-22T00:49:54.127658: step 59200, loss 0.608129, acc [0.89794922 0.84150391 0.8171875  0.73037109]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:49:57.979589: step 59200, loss 0.658363, acc [0.89845729 0.8399123  0.81437396 0.7277478 ] \n",
      "\n",
      "2019-02-22T00:49:59.212643: step 59240, loss 0.603783, acc [0.89814453 0.84306641 0.81835938 0.73388672]\n",
      "2019-02-22T00:50:00.376755: step 59280, loss 0.598426, acc [0.89755859 0.84257812 0.81650391 0.72783203]\n",
      "2019-02-22T00:50:01.673294: step 59320, loss 0.612793, acc [0.89873244 0.84166272 0.81540996 0.7306177 ]\n",
      "2019-02-22T00:50:02.831950: step 59360, loss 0.5733, acc [0.89736328 0.84423828 0.82529297 0.73720703]\n",
      "2019-02-22T00:50:03.992588: step 59400, loss 0.560764, acc [0.89511719 0.84287109 0.83027344 0.73232422]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:50:07.833607: step 59400, loss 0.660946, acc [0.89793671 0.84076325 0.81670654 0.7307211 ] \n",
      "\n",
      "2019-02-22T00:50:09.078566: step 59440, loss 0.578724, acc [0.88886719 0.83847656 0.82431641 0.72900391]\n",
      "2019-02-22T00:50:10.238211: step 59480, loss 0.584336, acc [0.89951172 0.84667969 0.82919922 0.74072266]\n",
      "2019-02-22T00:50:11.397361: step 59520, loss 0.584334, acc [0.90449219 0.84394531 0.82382813 0.73320312]\n",
      "2019-02-22T00:50:12.551056: step 59560, loss 0.566078, acc [0.8984375  0.84384766 0.82744141 0.73466797]\n",
      "2019-02-22T00:50:13.727566: step 59600, loss 0.589245, acc [0.89804688 0.84121094 0.81845703 0.72998047]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:50:17.565114: step 59600, loss 0.660585, acc [0.89839722 0.8408133  0.81586561 0.73016048] \n",
      "\n",
      "2019-02-22T00:50:18.810567: step 59640, loss 0.567084, acc [0.90009766 0.84833984 0.82958984 0.73818359]\n",
      "2019-02-22T00:50:19.969718: step 59680, loss 0.572029, acc [0.89902344 0.84755859 0.82636719 0.73769531]\n",
      "2019-02-22T00:50:21.128371: step 59720, loss 0.582425, acc [0.9015625  0.846875   0.82207031 0.73652344]\n",
      "2019-02-22T00:50:22.290001: step 59760, loss 0.582724, acc [0.89824219 0.84472656 0.82441406 0.73427734]\n",
      "2019-02-22T00:50:23.449153: step 59800, loss 0.577664, acc [0.89599609 0.83984375 0.82392578 0.73242188]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:50:27.303118: step 59800, loss 0.660796, acc [0.89793671 0.84161419 0.81619598 0.72956983] \n",
      "\n",
      "2019-02-22T00:50:28.537609: step 59840, loss 0.593048, acc [0.8953125  0.84443359 0.82216797 0.73330078]\n",
      "2019-02-22T00:50:29.695271: step 59880, loss 0.575832, acc [0.90097656 0.84892578 0.82939453 0.74091797]\n",
      "2019-02-22T00:50:30.858298: step 59920, loss 0.610472, acc [0.89501953 0.83691406 0.81542969 0.72949219]\n",
      "2019-02-22T00:50:32.013481: step 59960, loss 0.585425, acc [0.89970703 0.84267578 0.82480469 0.73837891]\n",
      "2019-02-22T00:50:33.172314: step 60000, loss 0.576422, acc [0.90361328 0.85185547 0.82451172 0.74228516]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:50:37.020772: step 60000, loss 0.662548, acc [0.8979267  0.84156414 0.81493458 0.7300904 ] \n",
      "\n",
      "2019-02-22T00:50:38.255809: step 60040, loss 0.577101, acc [0.89902344 0.84296875 0.82929688 0.73916016]\n",
      "2019-02-22T00:50:39.414958: step 60080, loss 0.58074, acc [0.89892578 0.84892578 0.82519531 0.73896484]\n",
      "2019-02-22T00:50:40.573118: step 60120, loss 0.595543, acc [0.8921875  0.84541016 0.82109375 0.73369141]\n",
      "2019-02-22T00:50:41.735244: step 60160, loss 0.594487, acc [0.89414063 0.83984375 0.81982422 0.72636719]\n",
      "2019-02-22T00:50:42.911756: step 60200, loss 0.589173, acc [0.89628906 0.84316406 0.82080078 0.73339844]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:50:46.828662: step 60200, loss 0.65648, acc [0.89875762 0.84227493 0.81557529 0.7302706 ] \n",
      "\n",
      "2019-02-22T00:50:48.075108: step 60240, loss 0.592573, acc [0.90361328 0.84179688 0.82080078 0.73193359]\n",
      "2019-02-22T00:50:49.246658: step 60280, loss 0.602154, acc [0.89521484 0.8359375  0.81806641 0.72705078]\n",
      "2019-02-22T00:50:50.416271: step 60320, loss 0.579044, acc [0.89833984 0.84365234 0.81728516 0.73037109]\n",
      "2019-02-22T00:50:51.584301: step 60360, loss 0.563086, acc [0.90126953 0.8453125  0.82841797 0.73720703]\n",
      "2019-02-22T00:50:52.753372: step 60400, loss 0.600307, acc [0.903125   0.84873047 0.8171875  0.73496094]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:50:56.612246: step 60400, loss 0.655809, acc [0.89782659 0.84132387 0.81552523 0.72925948] \n",
      "\n",
      "2019-02-22T00:50:57.858199: step 60440, loss 0.57579, acc [0.89726562 0.84833984 0.82451172 0.73574219]\n",
      "2019-02-22T00:50:59.025780: step 60480, loss 0.603698, acc [0.89824219 0.83896484 0.81914062 0.734375  ]\n",
      "2019-02-22T00:51:00.192397: step 60520, loss 0.613703, acc [0.89804688 0.83847656 0.81181641 0.72246094]\n",
      "2019-02-22T00:51:01.360449: step 60560, loss 0.598662, acc [0.89267578 0.84140625 0.81640625 0.72392578]\n",
      "2019-02-22T00:51:02.523567: step 60600, loss 0.596606, acc [0.90009766 0.84101563 0.81933594 0.73261719]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:51:06.370579: step 60600, loss 0.652512, acc [0.89775651 0.84130385 0.81674659 0.72978005] \n",
      "\n",
      "2019-02-22T00:51:07.616487: step 60640, loss 0.596893, acc [0.89765625 0.84150391 0.81875    0.73339844]\n",
      "2019-02-22T00:51:08.778667: step 60680, loss 0.587671, acc [0.89697266 0.84482422 0.82167969 0.73642578]\n",
      "2019-02-22T00:51:09.943271: step 60720, loss 0.610091, acc [0.89814453 0.83896484 0.80957031 0.72099609]\n",
      "2019-02-22T00:51:11.106836: step 60760, loss 0.628164, acc [0.89199219 0.83261719 0.80869141 0.71679688]\n",
      "2019-02-22T00:51:12.271441: step 60800, loss 0.610352, acc [0.89521484 0.84013672 0.81748047 0.72929687]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:51:16.228524: step 60800, loss 0.65981, acc [0.89705573 0.84226491 0.81655638 0.72950976] \n",
      "\n",
      "2019-02-22T00:51:17.489849: step 60840, loss 0.590879, acc [0.89677734 0.84160156 0.81992188 0.73017578]\n",
      "2019-02-22T00:51:18.800775: step 60880, loss 0.58859, acc [0.90320293 0.84455887 0.81923236 0.731177  ]\n",
      "2019-02-22T00:51:19.970384: step 60920, loss 0.572385, acc [0.9        0.84492188 0.83154297 0.73994141]\n",
      "2019-02-22T00:51:21.131036: step 60960, loss 0.531931, acc [0.90068359 0.85341797 0.84023437 0.74980469]\n",
      "2019-02-22T00:51:22.295091: step 61000, loss 0.557658, acc [0.8984375  0.84746094 0.82607422 0.73388672]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:51:26.153025: step 61000, loss 0.652751, acc [0.89764639 0.84192453 0.81840843 0.73226281] \n",
      "\n",
      "2019-02-22T00:51:27.395986: step 61040, loss 0.571537, acc [0.89462891 0.83974609 0.82294922 0.72617188]\n",
      "2019-02-22T00:51:28.556091: step 61080, loss 0.575815, acc [0.90117187 0.84238281 0.82617188 0.73720703]\n",
      "2019-02-22T00:51:29.723232: step 61120, loss 0.587949, acc [0.89941406 0.8421875  0.82509766 0.73515625]\n",
      "2019-02-22T00:51:30.887343: step 61160, loss 0.568759, acc [0.89697266 0.84648437 0.83027344 0.7390625 ]\n",
      "2019-02-22T00:51:32.048475: step 61200, loss 0.55053, acc [0.90371094 0.84697266 0.83212891 0.74072266]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:51:35.933144: step 61200, loss 0.663177, acc [0.89921814 0.84111363 0.81688674 0.72973   ] \n",
      "\n",
      "2019-02-22T00:51:37.180084: step 61240, loss 0.563623, acc [0.89853516 0.84707031 0.83076172 0.74306641]\n",
      "2019-02-22T00:51:38.341262: step 61280, loss 0.592863, acc [0.89619141 0.84023437 0.82333984 0.72929687]\n",
      "2019-02-22T00:51:39.505827: step 61320, loss 0.570674, acc [0.89853516 0.84257812 0.82539063 0.73554688]\n",
      "2019-02-22T00:51:40.674400: step 61360, loss 0.56641, acc [0.90273437 0.84443359 0.82783203 0.7359375 ]\n",
      "2019-02-22T00:51:41.837519: step 61400, loss 0.563957, acc [0.90166016 0.84824219 0.82978516 0.740625  ]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:51:45.732105: step 61400, loss 0.659711, acc [0.89847731 0.840543   0.81617596 0.73017049] \n",
      "\n",
      "2019-02-22T00:51:47.006327: step 61440, loss 0.590774, acc [0.89423828 0.84150391 0.82207031 0.72792969]\n",
      "2019-02-22T00:51:48.173985: step 61480, loss 0.593122, acc [0.89628906 0.83994141 0.82148438 0.72958984]\n",
      "2019-02-22T00:51:49.338046: step 61520, loss 0.581866, acc [0.89755859 0.84472656 0.82255859 0.73427734]\n",
      "2019-02-22T00:51:50.501162: step 61560, loss 0.582546, acc [0.89833984 0.84394531 0.82568359 0.74033203]\n",
      "2019-02-22T00:51:51.660784: step 61600, loss 0.599479, acc [0.89697266 0.84111328 0.81777344 0.72597656]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:51:55.537514: step 61600, loss 0.659564, acc [0.89697564 0.84102354 0.81570543 0.72919941] \n",
      "\n",
      "2019-02-22T00:51:56.782472: step 61640, loss 0.585985, acc [0.89667969 0.84111328 0.82275391 0.72929687]\n",
      "2019-02-22T00:51:57.947080: step 61680, loss 0.580772, acc [0.90136719 0.83935547 0.82333984 0.73417969]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-02-22T00:51:59.112678: step 61720, loss 0.596003, acc [0.89599609 0.83681641 0.81767578 0.72949219]\n",
      "2019-02-22T00:52:00.279268: step 61760, loss 0.588781, acc [0.89794922 0.84091797 0.82138672 0.73222656]\n",
      "2019-02-22T00:52:01.446356: step 61800, loss 0.599589, acc [0.8953125  0.84375    0.81513672 0.7234375 ]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:52:05.301261: step 61800, loss 0.65993, acc [0.89779655 0.84173432 0.81653636 0.7310815 ] \n",
      "\n",
      "2019-02-22T00:52:06.545226: step 61840, loss 0.580726, acc [0.90087891 0.84511719 0.82265625 0.73271484]\n",
      "2019-02-22T00:52:07.704875: step 61880, loss 0.579115, acc [0.90078125 0.84746094 0.82158203 0.73398438]\n",
      "2019-02-22T00:52:08.864024: step 61920, loss 0.595061, acc [0.89833984 0.84023437 0.82333984 0.73222656]\n",
      "2019-02-22T00:52:10.027141: step 61960, loss 0.585369, acc [0.89746094 0.84443359 0.82050781 0.72851562]\n",
      "2019-02-22T00:52:11.183813: step 62000, loss 0.579875, acc [0.9        0.84521484 0.82363281 0.73242188]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:52:15.038222: step 62000, loss 0.657933, acc [0.8974762  0.84055301 0.81561534 0.72829841] \n",
      "\n",
      "2019-02-22T00:52:16.270781: step 62040, loss 0.607352, acc [0.89775391 0.84082031 0.81972656 0.73017578]\n",
      "2019-02-22T00:52:17.436876: step 62080, loss 0.60356, acc [0.89619141 0.83955078 0.81572266 0.73115234]\n",
      "2019-02-22T00:52:18.598506: step 62120, loss 0.611776, acc [0.8984375  0.84326172 0.8171875  0.72880859]\n",
      "2019-02-22T00:52:19.759144: step 62160, loss 0.603557, acc [0.89589844 0.83642578 0.81796875 0.72626953]\n",
      "2019-02-22T00:52:20.919782: step 62200, loss 0.595954, acc [0.89404297 0.84042969 0.81933594 0.72998047]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:52:24.835234: step 62200, loss 0.655734, acc [0.89706575 0.84176436 0.81553524 0.72950976] \n",
      "\n",
      "2019-02-22T00:52:26.085120: step 62240, loss 0.616884, acc [0.8984375  0.83984375 0.80888672 0.72207031]\n",
      "2019-02-22T00:52:27.281965: step 62280, loss 0.609897, acc [0.88886719 0.83837891 0.81552734 0.72226563]\n",
      "2019-02-22T00:52:28.473355: step 62320, loss 0.605826, acc [0.90029297 0.84296875 0.81982422 0.7359375 ]\n",
      "2019-02-22T00:52:29.657306: step 62360, loss 0.60961, acc [0.89931641 0.84296875 0.82128906 0.7359375 ]\n",
      "2019-02-22T00:52:30.834313: step 62400, loss 0.587229, acc [0.89804688 0.84072266 0.82207031 0.73632812]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:52:34.821652: step 62400, loss 0.656424, acc [0.89793671 0.84194456 0.81427384 0.72913935] \n",
      "\n",
      "2019-02-22T00:52:36.088482: step 62440, loss 0.588206, acc [0.90227667 0.84283953 0.82508976 0.73724649]\n",
      "2019-02-22T00:52:37.401342: step 62480, loss 0.556816, acc [0.89707031 0.84550781 0.83261719 0.74042969]\n",
      "2019-02-22T00:52:38.567436: step 62520, loss 0.545395, acc [0.90576172 0.84726563 0.83330078 0.74912109]\n",
      "2019-02-22T00:52:39.737499: step 62560, loss 0.549354, acc [0.89990234 0.84716797 0.83447266 0.74345703]\n",
      "2019-02-22T00:52:40.903595: step 62600, loss 0.576238, acc [0.89824219 0.84423828 0.82636719 0.73769531]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:52:44.864150: step 62600, loss 0.657563, acc [0.89681546 0.8404529  0.81677662 0.7283785 ] \n",
      "\n",
      "2019-02-22T00:52:46.106130: step 62640, loss 0.569839, acc [0.9        0.84248047 0.82871094 0.73652344]\n",
      "2019-02-22T00:52:47.281651: step 62680, loss 0.560317, acc [0.90146484 0.84785156 0.83320313 0.74306641]\n",
      "2019-02-22T00:52:48.476556: step 62720, loss 0.57763, acc [0.89511719 0.84443359 0.82626953 0.73447266]\n",
      "2019-02-22T00:52:49.649551: step 62760, loss 0.577924, acc [0.90029297 0.84501953 0.82246094 0.73515625]\n",
      "2019-02-22T00:52:50.817627: step 62800, loss 0.5688, acc [0.89824219 0.84589844 0.82783203 0.73476562]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:52:54.740983: step 62800, loss 0.661569, acc [0.89695562 0.84092342 0.81499464 0.72793801] \n",
      "\n",
      "2019-02-22T00:52:55.996357: step 62840, loss 0.586863, acc [0.89921875 0.84589844 0.82460937 0.73271484]\n",
      "2019-02-22T00:52:57.167907: step 62880, loss 0.580331, acc [0.89814453 0.84521484 0.82480469 0.73574219]\n",
      "2019-02-22T00:52:58.340449: step 62920, loss 0.568808, acc [0.89882812 0.84716797 0.82539063 0.73730469]\n",
      "2019-02-22T00:52:59.510511: step 62960, loss 0.575509, acc [0.89902344 0.84345703 0.82441406 0.73515625]\n",
      "2019-02-22T00:53:00.680079: step 63000, loss 0.573103, acc [0.89873047 0.84824219 0.8265625  0.73691406]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:53:04.557349: step 63000, loss 0.660808, acc [0.89767642 0.84165424 0.81566539 0.72816827] \n",
      "\n",
      "2019-02-22T00:53:05.816151: step 63040, loss 0.559002, acc [0.90214844 0.84892578 0.83037109 0.73847656]\n",
      "2019-02-22T00:53:06.981750: step 63080, loss 0.567515, acc [0.90039062 0.84345703 0.82871094 0.73515625]\n",
      "2019-02-22T00:53:08.146356: step 63120, loss 0.594968, acc [0.89453125 0.83847656 0.81787109 0.72294922]\n",
      "2019-02-22T00:53:09.309506: step 63160, loss 0.57264, acc [0.89326172 0.84892578 0.82490234 0.73173828]\n",
      "2019-02-22T00:53:10.480033: step 63200, loss 0.599967, acc [0.89980469 0.84394531 0.82255859 0.7328125 ]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:53:14.331467: step 63200, loss 0.66041, acc [0.8973861  0.8419846  0.81530499 0.73105147] \n",
      "\n",
      "2019-02-22T00:53:15.585899: step 63240, loss 0.575396, acc [0.9015625  0.84365234 0.82363281 0.73388672]\n",
      "2019-02-22T00:53:16.747975: step 63280, loss 0.579686, acc [0.89824219 0.84853516 0.82822266 0.73964844]\n",
      "2019-02-22T00:53:17.911591: step 63320, loss 0.61072, acc [0.89521484 0.83115234 0.81640625 0.72363281]\n",
      "2019-02-22T00:53:19.082644: step 63360, loss 0.590325, acc [0.90244141 0.84511719 0.81933594 0.7359375 ]\n",
      "2019-02-22T00:53:20.243777: step 63400, loss 0.593732, acc [0.89365234 0.83457031 0.82060547 0.72587891]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:53:24.124477: step 63400, loss 0.661194, acc [0.89784661 0.8410836  0.81497462 0.72849863] \n",
      "\n",
      "2019-02-22T00:53:25.369436: step 63440, loss 0.584733, acc [0.89580078 0.85146484 0.82011719 0.73671875]\n",
      "2019-02-22T00:53:26.536025: step 63480, loss 0.59127, acc [0.90029297 0.84033203 0.82431641 0.73496094]\n",
      "2019-02-22T00:53:27.706088: step 63520, loss 0.589599, acc [0.89824219 0.84443359 0.82353516 0.73359375]\n",
      "2019-02-22T00:53:28.877638: step 63560, loss 0.586772, acc [0.89716797 0.84248047 0.81972656 0.73056641]\n",
      "2019-02-22T00:53:30.043236: step 63600, loss 0.604788, acc [0.89501953 0.84414062 0.82021484 0.72900391]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:53:33.885294: step 63600, loss 0.652036, acc [0.89807687 0.84167426 0.81826828 0.732433  ] \n",
      "\n",
      "2019-02-22T00:53:35.135663: step 63640, loss 0.605884, acc [0.90205078 0.84482422 0.81982422 0.73398438]\n",
      "2019-02-22T00:53:36.302251: step 63680, loss 0.582466, acc [0.89443359 0.84570312 0.82333984 0.73261719]\n",
      "2019-02-22T00:53:37.466361: step 63720, loss 0.610156, acc [0.89746094 0.84228516 0.81064453 0.73017578]\n",
      "2019-02-22T00:53:38.629480: step 63760, loss 0.604455, acc [0.89335937 0.84355469 0.81982422 0.73085937]\n",
      "2019-02-22T00:53:39.794086: step 63800, loss 0.593205, acc [0.90292969 0.84726563 0.82451172 0.73935547]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:53:43.624192: step 63800, loss 0.653216, acc [0.89831713 0.84077326 0.81829831 0.73070108] \n",
      "\n",
      "2019-02-22T00:53:44.862757: step 63840, loss 0.595628, acc [0.89853516 0.8375     0.8203125  0.72529297]\n",
      "2019-02-22T00:53:46.021412: step 63880, loss 0.58249, acc [0.89648438 0.84580078 0.82568359 0.73583984]\n",
      "2019-02-22T00:53:47.180066: step 63920, loss 0.600682, acc [0.89345703 0.83603516 0.81787109 0.73095703]\n",
      "2019-02-22T00:53:48.341697: step 63960, loss 0.596959, acc [0.89638672 0.84306641 0.82138672 0.73173828]\n",
      "2019-02-22T00:53:49.501887: step 64000, loss 0.602108, acc [0.89560547 0.84277344 0.81572266 0.72646484]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:53:53.395431: step 64000, loss 0.653299, acc [0.8979267  0.84194456 0.81649631 0.72938962] \n",
      "\n",
      "2019-02-22T00:53:54.757448: step 64040, loss 0.55386, acc [0.90358172 0.85098544 0.82906507 0.74374408]\n",
      "2019-02-22T00:53:55.912133: step 64080, loss 0.552733, acc [0.90009766 0.84824219 0.83076172 0.74121094]\n",
      "2019-02-22T00:53:57.067810: step 64120, loss 0.559242, acc [0.89980469 0.84423828 0.83212891 0.73740234]\n",
      "2019-02-22T00:53:58.224978: step 64160, loss 0.572608, acc [0.90019531 0.84492188 0.82832031 0.73847656]\n",
      "2019-02-22T00:53:59.380654: step 64200, loss 0.544059, acc [0.89746094 0.85048828 0.83447266 0.74023438]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:54:03.224154: step 64200, loss 0.655513, acc [0.89840723 0.84109361 0.81854859 0.73130175] \n",
      "\n",
      "2019-02-22T00:54:04.464150: step 64240, loss 0.556008, acc [0.89511719 0.84628906 0.83154297 0.73955078]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-02-22T00:54:05.621318: step 64280, loss 0.557273, acc [0.90332031 0.84130859 0.83310547 0.73886719]\n",
      "2019-02-22T00:54:06.777492: step 64320, loss 0.572705, acc [0.89814453 0.84160156 0.828125   0.73671875]\n",
      "2019-02-22T00:54:07.932674: step 64360, loss 0.587506, acc [0.89384766 0.84130859 0.82529297 0.72939453]\n",
      "2019-02-22T00:54:09.087361: step 64400, loss 0.578661, acc [0.90087891 0.83808594 0.82734375 0.73242188]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:54:12.928378: step 64400, loss 0.655751, acc [0.89782659 0.84105357 0.81716706 0.73106148] \n",
      "\n",
      "2019-02-22T00:54:14.162921: step 64440, loss 0.588473, acc [0.89589844 0.84287109 0.82373047 0.73242188]\n",
      "2019-02-22T00:54:15.323559: step 64480, loss 0.553153, acc [0.89912109 0.84785156 0.83583984 0.74355469]\n",
      "2019-02-22T00:54:16.478246: step 64520, loss 0.566901, acc [0.90087891 0.84746094 0.82363281 0.73652344]\n",
      "2019-02-22T00:54:17.629461: step 64560, loss 0.566521, acc [0.90302734 0.84814453 0.82851562 0.73740234]\n",
      "2019-02-22T00:54:18.787619: step 64600, loss 0.604781, acc [0.89472656 0.84306641 0.81679687 0.72851562]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:54:22.634093: step 64600, loss 0.660742, acc [0.89791669 0.84031275 0.81395349 0.72736738] \n",
      "\n",
      "2019-02-22T00:54:23.875084: step 64640, loss 0.578639, acc [0.90224609 0.84589844 0.82089844 0.73388672]\n",
      "2019-02-22T00:54:25.034234: step 64680, loss 0.577681, acc [0.89716797 0.84628906 0.82666016 0.73632812]\n",
      "2019-02-22T00:54:26.191400: step 64720, loss 0.587489, acc [0.89814453 0.84511719 0.82539063 0.73681641]\n",
      "2019-02-22T00:54:27.345095: step 64760, loss 0.595019, acc [0.89990234 0.84023437 0.82226562 0.73007813]\n",
      "2019-02-22T00:54:28.501765: step 64800, loss 0.588553, acc [0.89941406 0.84404297 0.82294922 0.73515625]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:54:32.352739: step 64800, loss 0.653561, acc [0.89887775 0.84103354 0.81824826 0.73050085] \n",
      "\n",
      "2019-02-22T00:54:33.601134: step 64840, loss 0.573647, acc [0.89873047 0.84550781 0.82539063 0.73710937]\n",
      "2019-02-22T00:54:34.760780: step 64880, loss 0.596527, acc [0.89453125 0.83886719 0.81982422 0.72646484]\n",
      "2019-02-22T00:54:35.916497: step 64920, loss 0.593771, acc [0.89228516 0.83886719 0.82138672 0.72675781]\n",
      "2019-02-22T00:54:37.070629: step 64960, loss 0.568161, acc [0.90009766 0.84785156 0.82900391 0.74160156]\n",
      "2019-02-22T00:54:38.231268: step 65000, loss 0.584324, acc [0.89990234 0.84511719 0.82490234 0.73525391]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:54:42.038061: step 65000, loss 0.656305, acc [0.8981069  0.8422549  0.81620599 0.73061098] \n",
      "\n",
      "2019-02-22T00:54:43.267196: step 65040, loss 0.580521, acc [0.90166016 0.84423828 0.82363281 0.74013672]\n",
      "2019-02-22T00:54:44.428779: step 65080, loss 0.588261, acc [0.89775391 0.84541016 0.82109375 0.734375  ]\n",
      "2019-02-22T00:54:45.590420: step 65120, loss 0.596736, acc [0.89462891 0.8421875  0.81992188 0.72851562]\n",
      "2019-02-22T00:54:46.745591: step 65160, loss 0.592367, acc [0.89736328 0.83691406 0.82138672 0.72910156]\n",
      "2019-02-22T00:54:47.899782: step 65200, loss 0.593754, acc [0.89833984 0.840625   0.81816406 0.72988281]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:54:51.748240: step 65200, loss 0.65615, acc [0.89807687 0.84103355 0.81752746 0.73098139] \n",
      "\n",
      "2019-02-22T00:54:53.024445: step 65240, loss 0.602059, acc [0.89697266 0.84521484 0.81484375 0.72685547]\n",
      "2019-02-22T00:54:54.187068: step 65280, loss 0.585283, acc [0.896875   0.84013672 0.81787109 0.72744141]\n",
      "2019-02-22T00:54:55.341754: step 65320, loss 0.6006, acc [0.89599609 0.83828125 0.81904297 0.72714844]\n",
      "2019-02-22T00:54:56.503385: step 65360, loss 0.589365, acc [0.89785156 0.84433594 0.82148438 0.7328125 ]\n",
      "2019-02-22T00:54:57.661048: step 65400, loss 0.600369, acc [0.90029297 0.83837891 0.82353516 0.7328125 ]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:55:01.527909: step 65400, loss 0.657319, acc [0.89807687 0.84123377 0.81753747 0.73011042] \n",
      "\n",
      "2019-02-22T00:55:02.769840: step 65440, loss 0.603103, acc [0.89853516 0.84404297 0.81962891 0.73427734]\n",
      "2019-02-22T00:55:03.929488: step 65480, loss 0.592426, acc [0.89941406 0.84423828 0.82011719 0.73408203]\n",
      "2019-02-22T00:55:05.086157: step 65520, loss 0.604865, acc [0.89511719 0.83896484 0.81464844 0.72832031]\n",
      "2019-02-22T00:55:06.243324: step 65560, loss 0.59598, acc [0.9015625  0.84101563 0.81621094 0.7328125 ]\n",
      "2019-02-22T00:55:07.535402: step 65600, loss 0.544341, acc [0.90145005 0.84490215 0.83178267 0.73866102]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:55:11.361043: step 65600, loss 0.657192, acc [0.89754628 0.84200462 0.81803803 0.73158206] \n",
      "\n",
      "2019-02-22T00:55:12.617908: step 65640, loss 0.545383, acc [0.9046875  0.84824219 0.83037109 0.74130859]\n",
      "2019-02-22T00:55:13.785985: step 65680, loss 0.560509, acc [0.89638672 0.84707031 0.83300781 0.74003906]\n",
      "2019-02-22T00:55:14.954558: step 65720, loss 0.556714, acc [0.89814453 0.84873047 0.82919922 0.73564453]\n",
      "2019-02-22T00:55:16.125118: step 65760, loss 0.555138, acc [0.8984375  0.84775391 0.83046875 0.73837891]\n",
      "2019-02-22T00:55:17.294684: step 65800, loss 0.543032, acc [0.89667969 0.84921875 0.83662109 0.74287109]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:55:21.189767: step 65800, loss 0.659514, acc [0.89815695 0.84286558 0.81714703 0.73005035] \n",
      "\n",
      "2019-02-22T00:55:22.445139: step 65840, loss 0.569169, acc [0.89824219 0.84345703 0.8265625  0.72910156]\n",
      "2019-02-22T00:55:23.638019: step 65880, loss 0.55193, acc [0.89755859 0.84824219 0.83613281 0.74277344]\n",
      "2019-02-22T00:55:24.810065: step 65920, loss 0.560813, acc [0.90166016 0.84785156 0.82861328 0.73837891]\n",
      "2019-02-22T00:55:25.979673: step 65960, loss 0.571509, acc [0.89765625 0.84355469 0.82958984 0.73935547]\n",
      "2019-02-22T00:55:27.146221: step 66000, loss 0.6092, acc [0.89765625 0.83740234 0.81445312 0.72636719]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:55:31.035352: step 66000, loss 0.660075, acc [0.89766641 0.84104356 0.81634615 0.72890909] \n",
      "\n",
      "2019-02-22T00:55:32.295687: step 66040, loss 0.565262, acc [0.89589844 0.84404297 0.82890625 0.73896484]\n",
      "2019-02-22T00:55:33.468726: step 66080, loss 0.586125, acc [0.89726562 0.84189453 0.82363281 0.73662109]\n",
      "2019-02-22T00:55:34.631843: step 66120, loss 0.569696, acc [0.90302734 0.84267578 0.8296875  0.73984375]\n",
      "2019-02-22T00:55:35.813809: step 66160, loss 0.572126, acc [0.89550781 0.83974609 0.82695312 0.73154297]\n",
      "2019-02-22T00:55:36.980399: step 66200, loss 0.592061, acc [0.89570313 0.83583984 0.82714844 0.73076172]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:55:40.802076: step 66200, loss 0.654261, acc [0.89969866 0.84240507 0.81839842 0.73251309] \n",
      "\n",
      "2019-02-22T00:55:42.051001: step 66240, loss 0.580241, acc [0.89492187 0.84658203 0.82832031 0.73652344]\n",
      "2019-02-22T00:55:43.211639: step 66280, loss 0.574831, acc [0.903125   0.84619141 0.82324219 0.73349609]\n",
      "2019-02-22T00:55:44.379222: step 66320, loss 0.574322, acc [0.90126953 0.84785156 0.82294922 0.73359375]\n",
      "2019-02-22T00:55:45.540356: step 66360, loss 0.584604, acc [0.89931641 0.84296875 0.82207031 0.72998047]\n",
      "2019-02-22T00:55:46.704961: step 66400, loss 0.591922, acc [0.89570313 0.8453125  0.82480469 0.73496094]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:55:50.542560: step 66400, loss 0.656147, acc [0.89697564 0.84255524 0.81611589 0.73031064] \n",
      "\n",
      "2019-02-22T00:55:51.787963: step 66440, loss 0.579325, acc [0.89599609 0.84208984 0.82773438 0.73398438]\n",
      "2019-02-22T00:55:52.948601: step 66480, loss 0.561201, acc [0.90615234 0.85087891 0.82832031 0.74443359]\n",
      "2019-02-22T00:55:54.128584: step 66520, loss 0.587288, acc [0.90214844 0.84326172 0.8171875  0.72861328]\n",
      "2019-02-22T00:55:55.289222: step 66560, loss 0.584707, acc [0.89960938 0.84638672 0.82705078 0.74130859]\n",
      "2019-02-22T00:55:56.465238: step 66600, loss 0.586905, acc [0.89882812 0.84052734 0.81972656 0.73193359]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:56:00.558752: step 66600, loss 0.652799, acc [0.89889778 0.84161419 0.81814814 0.73149196] \n",
      "\n",
      "2019-02-22T00:56:01.824507: step 66640, loss 0.573842, acc [0.90029297 0.84257812 0.828125   0.73779297]\n",
      "2019-02-22T00:56:02.999035: step 66680, loss 0.59236, acc [0.89667969 0.83710938 0.81591797 0.72431641]\n",
      "2019-02-22T00:56:04.173066: step 66720, loss 0.587453, acc [0.89990234 0.84238281 0.81591797 0.73134766]\n",
      "2019-02-22T00:56:05.350568: step 66760, loss 0.600705, acc [0.89335937 0.83710938 0.81816406 0.72353516]\n",
      "2019-02-22T00:56:06.525094: step 66800, loss 0.585313, acc [0.89814453 0.840625   0.82304687 0.73037109]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:56:10.411247: step 66800, loss 0.65227, acc [0.89870757 0.84179439 0.81759753 0.73074112] \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-02-22T00:56:11.657694: step 66840, loss 0.608954, acc [0.89667969 0.8453125  0.81982422 0.73212891]\n",
      "2019-02-22T00:56:12.821806: step 66880, loss 0.593953, acc [0.89433594 0.84189453 0.81962891 0.73320312]\n",
      "2019-02-22T00:56:13.989388: step 66920, loss 0.585719, acc [0.90078125 0.84433594 0.82236328 0.734375  ]\n",
      "2019-02-22T00:56:15.153993: step 66960, loss 0.592295, acc [0.896875   0.84775391 0.81972656 0.73417969]\n",
      "2019-02-22T00:56:16.316614: step 67000, loss 0.608729, acc [0.89267578 0.84140625 0.81298828 0.72382813]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:56:20.184481: step 67000, loss 0.656521, acc [0.89782659 0.84093344 0.81610588 0.72930953] \n",
      "\n",
      "2019-02-22T00:56:21.426401: step 67040, loss 0.602237, acc [0.89658203 0.83857422 0.81689453 0.72392578]\n",
      "2019-02-22T00:56:22.590015: step 67080, loss 0.611411, acc [0.89423828 0.83798828 0.81494141 0.72587891]\n",
      "2019-02-22T00:56:23.753629: step 67120, loss 0.592743, acc [0.89814453 0.84335938 0.82167969 0.73535156]\n",
      "2019-02-22T00:56:25.053194: step 67160, loss 0.563319, acc [0.89765329 0.84148418 0.83173039 0.73706301]\n",
      "2019-02-22T00:56:26.215770: step 67200, loss 0.521849, acc [0.90341797 0.85019531 0.83769531 0.74912109]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:56:30.125731: step 67200, loss 0.658956, acc [0.89709577 0.84154411 0.81718708 0.73089129] \n",
      "\n",
      "2019-02-22T00:56:31.366228: step 67240, loss 0.550423, acc [0.89736328 0.84150391 0.82988281 0.73242188]\n",
      "2019-02-22T00:56:32.530834: step 67280, loss 0.551535, acc [0.90048828 0.8421875  0.83662109 0.74257812]\n",
      "2019-02-22T00:56:33.689487: step 67320, loss 0.571801, acc [0.89824219 0.84169922 0.82666016 0.73583984]\n",
      "2019-02-22T00:56:34.850126: step 67360, loss 0.565579, acc [0.89697266 0.85009766 0.82705078 0.73818359]\n",
      "2019-02-22T00:56:36.131291: step 67400, loss 0.547631, acc [0.90136719 0.85166016 0.83085937 0.74013672]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:56:40.297685: step 67400, loss 0.659671, acc [0.89807686 0.84203466 0.81548519 0.72992021] \n",
      "\n",
      "2019-02-22T00:56:41.602662: step 67440, loss 0.570922, acc [0.89931641 0.84511719 0.8265625  0.73710937]\n",
      "2019-02-22T00:56:42.783635: step 67480, loss 0.566998, acc [0.90371094 0.84541016 0.83037109 0.74082031]\n",
      "2019-02-22T00:56:43.971554: step 67520, loss 0.577402, acc [0.89853516 0.84052734 0.82050781 0.73222656]\n",
      "2019-02-22T00:56:45.161628: step 67560, loss 0.554348, acc [0.89951172 0.84716797 0.83359375 0.74238281]\n",
      "2019-02-22T00:56:46.335518: step 67600, loss 0.56919, acc [0.89296875 0.84492188 0.82246094 0.72900391]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:56:50.376886: step 67600, loss 0.659195, acc [0.89724595 0.84139395 0.81793791 0.72950976] \n",
      "\n",
      "2019-02-22T00:56:51.641189: step 67640, loss 0.593194, acc [0.89804688 0.84013672 0.82177734 0.72763672]\n",
      "2019-02-22T00:56:52.813237: step 67680, loss 0.572994, acc [0.90029297 0.8453125  0.82685547 0.73378906]\n",
      "2019-02-22T00:56:53.978338: step 67720, loss 0.577136, acc [0.90117187 0.84882813 0.82744141 0.7421875 ]\n",
      "2019-02-22T00:56:55.146912: step 67760, loss 0.582661, acc [0.89716797 0.84482422 0.82080078 0.73642578]\n",
      "2019-02-22T00:56:56.324911: step 67800, loss 0.568341, acc [0.89580078 0.84365234 0.82626953 0.73623047]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:57:00.276095: step 67800, loss 0.652704, acc [0.89802681 0.8438767  0.81703691 0.73179229] \n",
      "\n",
      "2019-02-22T00:57:01.531415: step 67840, loss 0.576964, acc [0.89697266 0.84414062 0.82773438 0.73330078]\n",
      "2019-02-22T00:57:02.699493: step 67880, loss 0.572049, acc [0.89580078 0.8421875  0.82714844 0.73310547]\n",
      "2019-02-22T00:57:03.873524: step 67920, loss 0.587209, acc [0.90380859 0.84423828 0.82724609 0.73896484]\n",
      "2019-02-22T00:57:05.042594: step 67960, loss 0.573936, acc [0.90019531 0.84355469 0.82910156 0.73466797]\n",
      "2019-02-22T00:57:06.213649: step 68000, loss 0.57662, acc [0.90488281 0.84746094 0.82294922 0.73515625]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:57:10.079962: step 68000, loss 0.656348, acc [0.89772648 0.842345   0.81670655 0.72970998] \n",
      "\n",
      "2019-02-22T00:57:11.329386: step 68040, loss 0.586476, acc [0.89775391 0.83828125 0.82285156 0.73359375]\n",
      "2019-02-22T00:57:12.500440: step 68080, loss 0.586993, acc [0.89238281 0.83886719 0.82763672 0.7328125 ]\n",
      "2019-02-22T00:57:13.681415: step 68120, loss 0.574852, acc [0.89609375 0.84179688 0.82236328 0.7328125 ]\n",
      "2019-02-22T00:57:14.849493: step 68160, loss 0.588617, acc [0.90146484 0.84335938 0.81943359 0.73242188]\n",
      "2019-02-22T00:57:16.013106: step 68200, loss 0.584254, acc [0.89902344 0.84296875 0.82177734 0.73193359]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:57:19.845731: step 68200, loss 0.656665, acc [0.89875762 0.84082331 0.81587562 0.72872889] \n",
      "\n",
      "2019-02-22T00:57:21.093133: step 68240, loss 0.598566, acc [0.89892578 0.83916016 0.81806641 0.72597656]\n",
      "2019-02-22T00:57:22.254762: step 68280, loss 0.595897, acc [0.89199219 0.83837891 0.81992188 0.72470703]\n",
      "2019-02-22T00:57:23.419864: step 68320, loss 0.570863, acc [0.89746094 0.84628906 0.82255859 0.73388672]\n",
      "2019-02-22T00:57:24.604357: step 68360, loss 0.592544, acc [0.89697266 0.84160156 0.82011719 0.73027344]\n",
      "2019-02-22T00:57:25.765445: step 68400, loss 0.584846, acc [0.90097656 0.84638672 0.815625   0.73037109]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:57:29.679872: step 68400, loss 0.65781, acc [0.89663526 0.84232498 0.81745738 0.73110152] \n",
      "\n",
      "2019-02-22T00:57:30.918879: step 68440, loss 0.59214, acc [0.89970703 0.84082031 0.82001953 0.73369141]\n",
      "2019-02-22T00:57:32.085964: step 68480, loss 0.592564, acc [0.89785156 0.84697266 0.81962891 0.73505859]\n",
      "2019-02-22T00:57:33.253050: step 68520, loss 0.580819, acc [0.90693359 0.84667969 0.82421875 0.73994141]\n",
      "2019-02-22T00:57:34.428075: step 68560, loss 0.575292, acc [0.89365234 0.84667969 0.82333984 0.73632812]\n",
      "2019-02-22T00:57:35.604086: step 68600, loss 0.589527, acc [0.89541016 0.84267578 0.82353516 0.73251953]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:57:39.471446: step 68600, loss 0.653333, acc [0.89794672 0.84132387 0.81920932 0.73065102] \n",
      "\n",
      "2019-02-22T00:57:40.712442: step 68640, loss 0.606045, acc [0.8953125  0.84658203 0.81826172 0.72832031]\n",
      "2019-02-22T00:57:41.884927: step 68680, loss 0.585938, acc [0.89501953 0.84511719 0.82441406 0.73476562]\n",
      "2019-02-22T00:57:43.177500: step 68720, loss 0.544967, acc [0.89833688 0.84664319 0.8339242  0.7379942 ]\n",
      "2019-02-22T00:57:44.340618: step 68760, loss 0.545324, acc [0.89853516 0.84648437 0.83193359 0.73505859]\n",
      "2019-02-22T00:57:45.603432: step 68800, loss 0.583771, acc [0.90107422 0.84443359 0.82080078 0.73251953]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:57:49.592547: step 68800, loss 0.653384, acc [0.89812692 0.8418044  0.81838841 0.7310815 ] \n",
      "\n",
      "2019-02-22T00:57:50.870196: step 68840, loss 0.563185, acc [0.90185547 0.84960938 0.828125   0.74443359]\n",
      "2019-02-22T00:57:52.050676: step 68880, loss 0.55126, acc [0.89628906 0.84453125 0.83837891 0.74306641]\n",
      "2019-02-22T00:57:53.251489: step 68920, loss 0.565112, acc [0.90175781 0.84140625 0.83251953 0.73847656]\n",
      "2019-02-22T00:57:54.431472: step 68960, loss 0.553438, acc [0.89941406 0.85009766 0.83427734 0.74326172]\n",
      "2019-02-22T00:57:55.609471: step 69000, loss 0.568957, acc [0.89550781 0.84541016 0.82636719 0.73496094]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:57:59.793719: step 69000, loss 0.654293, acc [0.89872759 0.8412638  0.81992011 0.73287349] \n",
      "\n",
      "2019-02-22T00:58:01.147798: step 69040, loss 0.560626, acc [0.896875   0.84365234 0.82988281 0.73847656]\n",
      "2019-02-22T00:58:02.340181: step 69080, loss 0.559195, acc [0.90322266 0.84306641 0.8296875  0.73554688]\n",
      "2019-02-22T00:58:03.534546: step 69120, loss 0.592078, acc [0.89472656 0.84150391 0.82324219 0.73017578]\n",
      "2019-02-22T00:58:04.713537: step 69160, loss 0.573033, acc [0.89912109 0.84511719 0.82460937 0.73896484]\n",
      "2019-02-22T00:58:05.891042: step 69200, loss 0.570327, acc [0.90107422 0.84042969 0.82910156 0.73378906]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:58:09.881856: step 69200, loss 0.653366, acc [0.89938832 0.84147403 0.81896906 0.73033067] \n",
      "\n",
      "2019-02-22T00:58:11.171455: step 69240, loss 0.563984, acc [0.89873047 0.84619141 0.83330078 0.74199219]\n",
      "2019-02-22T00:58:12.429311: step 69280, loss 0.556923, acc [0.90224609 0.84785156 0.83144531 0.74042969]\n",
      "2019-02-22T00:58:13.644507: step 69320, loss 0.568007, acc [0.89746094 0.84365234 0.82109375 0.72890625]\n",
      "2019-02-22T00:58:14.870124: step 69360, loss 0.602678, acc [0.89492187 0.84189453 0.81992188 0.73144531]\n",
      "2019-02-22T00:58:16.096728: step 69400, loss 0.573856, acc [0.89414063 0.84160156 0.82705078 0.73544922]\n",
      "\n",
      "Evaluation:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-02-22T00:58:20.108370: step 69400, loss 0.657297, acc [0.89762636 0.84142398 0.81791789 0.73149195] \n",
      "\n",
      "2019-02-22T00:58:21.527423: step 69440, loss 0.589829, acc [0.89589844 0.8390625  0.821875   0.72958984]\n",
      "2019-02-22T00:58:22.833390: step 69480, loss 0.577486, acc [0.90078125 0.84736328 0.82431641 0.73515625]\n",
      "2019-02-22T00:58:24.068470: step 69520, loss 0.554125, acc [0.89853516 0.84638672 0.82978516 0.7375    ]\n",
      "2019-02-22T00:58:25.251387: step 69560, loss 0.587547, acc [0.89755859 0.84433594 0.825      0.73408203]\n",
      "2019-02-22T00:58:26.435833: step 69600, loss 0.586865, acc [0.90058594 0.84619141 0.82158203 0.73535156]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:58:30.633391: step 69600, loss 0.649833, acc [0.89753626 0.84163421 0.81813813 0.73021053] \n",
      "\n",
      "2019-02-22T00:58:31.963660: step 69640, loss 0.59135, acc [0.8921875  0.84130859 0.82216797 0.72773438]\n",
      "2019-02-22T00:58:33.187787: step 69680, loss 0.586603, acc [0.89638672 0.84101563 0.82070312 0.73134766]\n",
      "2019-02-22T00:58:34.428325: step 69720, loss 0.559744, acc [0.90439453 0.85195312 0.82988281 0.74082031]\n",
      "2019-02-22T00:58:35.643480: step 69760, loss 0.572102, acc [0.90585938 0.84648437 0.82539063 0.74111328]\n",
      "2019-02-22T00:58:36.841319: step 69800, loss 0.603193, acc [0.9        0.83837891 0.81757813 0.72773438]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:58:40.820852: step 69800, loss 0.645974, acc [0.89814694 0.84313588 0.81947962 0.7332439 ] \n",
      "\n",
      "2019-02-22T00:58:42.109911: step 69840, loss 0.583445, acc [0.89453125 0.84228516 0.82646484 0.73271484]\n",
      "2019-02-22T00:58:43.536403: step 69880, loss 0.577164, acc [0.90419922 0.84863281 0.82148438 0.73916016]\n",
      "2019-02-22T00:58:44.756066: step 69920, loss 0.59474, acc [0.89589844 0.83886719 0.82128906 0.73007813]\n",
      "2019-02-22T00:58:45.933568: step 69960, loss 0.58382, acc [0.90224609 0.84677734 0.82197266 0.73769531]\n",
      "2019-02-22T00:58:47.129422: step 70000, loss 0.592998, acc [0.89462891 0.840625   0.82128906 0.72880859]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:58:51.119771: step 70000, loss 0.651103, acc [0.89884772 0.84169428 0.81847851 0.7312617 ] \n",
      "\n",
      "2019-02-22T00:58:52.397430: step 70040, loss 0.592378, acc [0.90019531 0.84003906 0.81845703 0.73046875]\n",
      "2019-02-22T00:58:53.569973: step 70080, loss 0.594679, acc [0.90195313 0.84765625 0.81894531 0.73486328]\n",
      "2019-02-22T00:58:54.740034: step 70120, loss 0.607574, acc [0.90107422 0.83837891 0.82099609 0.73105469]\n",
      "2019-02-22T00:58:55.910098: step 70160, loss 0.597178, acc [0.89580078 0.83984375 0.81982422 0.73056641]\n",
      "2019-02-22T00:58:57.078175: step 70200, loss 0.588056, acc [0.89638672 0.84228516 0.82324219 0.73398438]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:59:01.232171: step 70200, loss 0.650025, acc [0.89751624 0.84298571 0.81978997 0.7323429 ] \n",
      "\n",
      "2019-02-22T00:59:02.554505: step 70240, loss 0.578569, acc [0.90048828 0.84453125 0.82675781 0.7359375 ]\n",
      "2019-02-22T00:59:03.935366: step 70280, loss 0.543807, acc [0.89972577 0.84437342 0.83190893 0.73651752]\n",
      "2019-02-22T00:59:05.105924: step 70320, loss 0.532411, acc [0.90556641 0.85283203 0.83613281 0.74541016]\n",
      "2019-02-22T00:59:06.278467: step 70360, loss 0.548412, acc [0.89472656 0.8390625  0.83574219 0.73720703]\n",
      "2019-02-22T00:59:07.442081: step 70400, loss 0.555015, acc [0.89902344 0.84384766 0.83574219 0.74306641]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:59:11.500347: step 70400, loss 0.651644, acc [0.89735607 0.84124378 0.81926939 0.73064101] \n",
      "\n",
      "2019-02-22T00:59:12.786969: step 70440, loss 0.563319, acc [0.89804688 0.84423828 0.82822266 0.73232422]\n",
      "2019-02-22T00:59:14.003162: step 70480, loss 0.557782, acc [0.89707031 0.84414062 0.83173828 0.73876953]\n",
      "2019-02-22T00:59:15.192567: step 70520, loss 0.544404, acc [0.90107422 0.84912109 0.83144531 0.74091797]\n",
      "2019-02-22T00:59:16.378004: step 70560, loss 0.556132, acc [0.90087891 0.84960938 0.82919922 0.74189453]\n",
      "2019-02-22T00:59:17.597667: step 70600, loss 0.575082, acc [0.89667969 0.84121094 0.82470703 0.73183594]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:59:22.590893: step 70600, loss 0.652579, acc [0.8971158  0.84146402 0.8192794  0.73023056] \n",
      "\n",
      "2019-02-22T00:59:23.922154: step 70640, loss 0.557346, acc [0.90429688 0.85       0.83144531 0.74394531]\n",
      "2019-02-22T00:59:25.171575: step 70680, loss 0.564727, acc [0.90107422 0.85058594 0.83095703 0.74238281]\n",
      "2019-02-22T00:59:26.407607: step 70720, loss 0.570378, acc [0.90429688 0.84697266 0.82929688 0.73955078]\n",
      "2019-02-22T00:59:27.637188: step 70760, loss 0.562806, acc [0.89882812 0.84462891 0.83007812 0.73564453]\n",
      "2019-02-22T00:59:28.836515: step 70800, loss 0.575239, acc [0.89384766 0.84140625 0.82236328 0.72773438]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:59:33.006878: step 70800, loss 0.652309, acc [0.8979267  0.84187448 0.81709698 0.72943968] \n",
      "\n",
      "2019-02-22T00:59:34.299451: step 70840, loss 0.578628, acc [0.89794922 0.83896484 0.82119141 0.73173828]\n",
      "2019-02-22T00:59:35.509689: step 70880, loss 0.556898, acc [0.90175781 0.84326172 0.83056641 0.73759766]\n",
      "2019-02-22T00:59:36.715463: step 70920, loss 0.571345, acc [0.89814453 0.84384766 0.82832031 0.73632812]\n",
      "2019-02-22T00:59:37.912805: step 70960, loss 0.570231, acc [0.89716797 0.84179688 0.82998047 0.73828125]\n",
      "2019-02-22T00:59:39.099733: step 71000, loss 0.581326, acc [0.89882812 0.84023437 0.82529297 0.73339844]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:59:43.252239: step 71000, loss 0.655448, acc [0.89803682 0.8421648  0.81806805 0.73058094] \n",
      "\n",
      "2019-02-22T00:59:44.533404: step 71040, loss 0.560855, acc [0.90253906 0.84599609 0.83007812 0.73701172]\n",
      "2019-02-22T00:59:45.733297: step 71080, loss 0.578796, acc [0.89697266 0.84394531 0.82851562 0.73574219]\n",
      "2019-02-22T00:59:46.917743: step 71120, loss 0.571161, acc [0.90019531 0.84355469 0.82480469 0.73544922]\n",
      "2019-02-22T00:59:48.125501: step 71160, loss 0.585345, acc [0.90068359 0.84501953 0.82324219 0.73320312]\n",
      "2019-02-22T00:59:49.304529: step 71200, loss 0.574423, acc [0.89755859 0.84189453 0.82539063 0.73222656]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T00:59:53.370744: step 71200, loss 0.653339, acc [0.89797675 0.84076325 0.81834837 0.72996026] \n",
      "\n",
      "2019-02-22T00:59:54.633508: step 71240, loss 0.57498, acc [0.89570313 0.84443359 0.82822266 0.7328125 ]\n",
      "2019-02-22T00:59:55.832685: step 71280, loss 0.594755, acc [0.89873047 0.84179688 0.81796875 0.73027344]\n",
      "2019-02-22T00:59:57.015147: step 71320, loss 0.57292, acc [0.89707031 0.846875   0.82822266 0.73701172]\n",
      "2019-02-22T00:59:58.201577: step 71360, loss 0.603626, acc [0.90136719 0.84189453 0.82050781 0.73232422]\n",
      "2019-02-22T00:59:59.383544: step 71400, loss 0.568927, acc [0.8984375  0.85078125 0.83017578 0.74296875]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T01:00:03.458675: step 71400, loss 0.651134, acc [0.8974762  0.8415341  0.81881889 0.73069107] \n",
      "\n",
      "2019-02-22T01:00:04.767616: step 71440, loss 0.599962, acc [0.89355469 0.83925781 0.8234375  0.72792969]\n",
      "2019-02-22T01:00:05.989268: step 71480, loss 0.583549, acc [0.89951172 0.84716797 0.82070312 0.73076172]\n",
      "2019-02-22T01:00:07.205950: step 71520, loss 0.585078, acc [0.90068359 0.84033203 0.82275391 0.73613281]\n",
      "2019-02-22T01:00:08.400811: step 71560, loss 0.603037, acc [0.89677734 0.8390625  0.81328125 0.72578125]\n",
      "2019-02-22T01:00:09.701857: step 71600, loss 0.580285, acc [0.89599609 0.84814453 0.82392578 0.73320312]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T01:00:14.350383: step 71600, loss 0.648352, acc [0.8983772  0.84194456 0.81950966 0.73190241] \n",
      "\n",
      "2019-02-22T01:00:16.170204: step 71640, loss 0.577363, acc [0.89892578 0.84707031 0.82207031 0.73330078]\n",
      "2019-02-22T01:00:17.413179: step 71680, loss 0.599443, acc [0.89785156 0.84091797 0.82363281 0.73242188]\n",
      "2019-02-22T01:00:18.658633: step 71720, loss 0.615342, acc [0.89199219 0.83720703 0.81787109 0.72724609]\n",
      "2019-02-22T01:00:19.907063: step 71760, loss 0.580855, acc [0.89599609 0.84335938 0.82373047 0.73007813]\n",
      "2019-02-22T01:00:21.185749: step 71800, loss 0.584548, acc [0.90107422 0.84316406 0.82314453 0.73349609]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T01:00:25.352690: step 71800, loss 0.651745, acc [0.89794672 0.84342621 0.81959976 0.73291353] \n",
      "\n",
      "2019-02-22T01:00:26.824270: step 71840, loss 0.541596, acc [0.89987571 0.8479867  0.83495305 0.73888494]\n",
      "2019-02-22T01:00:28.031532: step 71880, loss 0.544599, acc [0.89560547 0.84746094 0.83056641 0.73974609]\n",
      "2019-02-22T01:00:29.232841: step 71920, loss 0.540255, acc [0.89726562 0.84462891 0.83662109 0.74033203]\n",
      "2019-02-22T01:00:30.428204: step 71960, loss 0.53217, acc [0.90341797 0.85009766 0.84150391 0.75068359]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-02-22T01:00:31.634967: step 72000, loss 0.542314, acc [0.89892578 0.84101563 0.83964844 0.74072266]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T01:00:35.723488: step 72000, loss 0.655839, acc [0.89743616 0.84150407 0.81753747 0.72898918] \n",
      "\n",
      "2019-02-22T01:00:37.016558: step 72040, loss 0.54552, acc [0.89980469 0.84550781 0.83847656 0.74296875]\n",
      "2019-02-22T01:00:38.208445: step 72080, loss 0.544976, acc [0.90029297 0.84951172 0.83496094 0.74111328]\n",
      "2019-02-22T01:00:39.413726: step 72120, loss 0.563617, acc [0.90048828 0.84355469 0.83027344 0.73662109]\n",
      "2019-02-22T01:00:40.615529: step 72160, loss 0.576816, acc [0.89482422 0.84765625 0.82929688 0.73310547]\n",
      "2019-02-22T01:00:41.942823: step 72200, loss 0.552757, acc [0.90126953 0.8453125  0.83076172 0.73828125]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T01:00:46.084417: step 72200, loss 0.658024, acc [0.89773649 0.84188449 0.81984002 0.732433  ] \n",
      "\n",
      "2019-02-22T01:00:47.347729: step 72240, loss 0.574003, acc [0.89716797 0.83994141 0.83232422 0.73417969]\n",
      "2019-02-22T01:00:48.533662: step 72280, loss 0.580345, acc [0.89951172 0.84199219 0.82470703 0.73085937]\n",
      "2019-02-22T01:00:49.715630: step 72320, loss 0.560523, acc [0.90117187 0.84824219 0.82519531 0.73642578]\n",
      "2019-02-22T01:00:50.913467: step 72360, loss 0.561382, acc [0.90234375 0.85048828 0.82675781 0.73867187]\n",
      "2019-02-22T01:00:52.134616: step 72400, loss 0.553183, acc [0.90419922 0.84492188 0.83320313 0.74394531]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T01:00:56.196355: step 72400, loss 0.659817, acc [0.89744616 0.84134389 0.81866872 0.73066103] \n",
      "\n",
      "2019-02-22T01:00:57.495378: step 72440, loss 0.576587, acc [0.9015625  0.84365234 0.82919922 0.73779297]\n",
      "2019-02-22T01:00:58.711072: step 72480, loss 0.555202, acc [0.90263672 0.84453125 0.83193359 0.73955078]\n",
      "2019-02-22T01:00:59.917839: step 72520, loss 0.559633, acc [0.89970703 0.84414062 0.82724609 0.7375    ]\n",
      "2019-02-22T01:01:01.125100: step 72560, loss 0.57982, acc [0.89326172 0.84238281 0.82363281 0.72910156]\n",
      "2019-02-22T01:01:02.372043: step 72600, loss 0.582533, acc [0.89443359 0.83701172 0.82841797 0.73232422]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T01:01:06.646600: step 72600, loss 0.651729, acc [0.89766641 0.84260529 0.81965982 0.7307211 ] \n",
      "\n",
      "2019-02-22T01:01:07.946082: step 72640, loss 0.569497, acc [0.896875   0.84462891 0.82509766 0.73300781]\n",
      "2019-02-22T01:01:09.136977: step 72680, loss 0.600739, acc [0.89814453 0.84169922 0.81416016 0.72480469]\n",
      "2019-02-22T01:01:10.356639: step 72720, loss 0.587977, acc [0.89863281 0.84179688 0.82460937 0.72861328]\n",
      "2019-02-22T01:01:11.586221: step 72760, loss 0.591141, acc [0.89570313 0.83945313 0.82236328 0.72939453]\n",
      "2019-02-22T01:01:12.773644: step 72800, loss 0.590802, acc [0.89833984 0.83798828 0.82119141 0.72724609]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T01:01:16.686138: step 72800, loss 0.654771, acc [0.89850734 0.84203466 0.8185586  0.73160208] \n",
      "\n",
      "2019-02-22T01:01:17.988580: step 72840, loss 0.572181, acc [0.89755859 0.8453125  0.828125   0.73652344]\n",
      "2019-02-22T01:01:19.206755: step 72880, loss 0.577487, acc [0.90117187 0.84238281 0.82246094 0.73466797]\n",
      "2019-02-22T01:01:20.422944: step 72920, loss 0.587779, acc [0.89101562 0.83642578 0.82255859 0.72519531]\n",
      "2019-02-22T01:01:21.643600: step 72960, loss 0.585541, acc [0.89248047 0.83974609 0.8265625  0.72861328]\n",
      "2019-02-22T01:01:22.858302: step 73000, loss 0.579563, acc [0.89726562 0.84287109 0.82441406 0.73271484]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T01:01:26.891271: step 73000, loss 0.65016, acc [0.89784661 0.84147404 0.82051077 0.73154201] \n",
      "\n",
      "2019-02-22T01:01:28.204183: step 73040, loss 0.588265, acc [0.89941406 0.84013672 0.82041016 0.72929687]\n",
      "2019-02-22T01:01:29.416900: step 73080, loss 0.567568, acc [0.89882812 0.85107422 0.8265625  0.73964844]\n",
      "2019-02-22T01:01:30.610274: step 73120, loss 0.587095, acc [0.89785156 0.84433594 0.82460937 0.73261719]\n",
      "2019-02-22T01:01:31.827953: step 73160, loss 0.568104, acc [0.89746094 0.84257812 0.8296875  0.73173828]\n",
      "2019-02-22T01:01:33.076879: step 73200, loss 0.563636, acc [0.89648438 0.84755859 0.82695312 0.73798828]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T01:01:37.586010: step 73200, loss 0.653016, acc [0.89848732 0.84170429 0.81886895 0.73119162] \n",
      "\n",
      "2019-02-22T01:01:38.892967: step 73240, loss 0.57954, acc [0.90195313 0.84091797 0.81826172 0.72480469]\n",
      "2019-02-22T01:01:40.136936: step 73280, loss 0.585252, acc [0.89970703 0.84638672 0.82236328 0.73515625]\n",
      "2019-02-22T01:01:41.413139: step 73320, loss 0.585579, acc [0.89716797 0.84404297 0.82158203 0.73027344]\n",
      "2019-02-22T01:01:42.665537: step 73360, loss 0.595135, acc [0.89960938 0.83720703 0.8234375  0.73203125]\n",
      "2019-02-22T01:01:44.064752: step 73400, loss 0.560669, acc [0.89948509 0.84484493 0.83766375 0.74528291]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T01:01:48.104666: step 73400, loss 0.650863, acc [0.89709578 0.84211475 0.8193695  0.73123167] \n",
      "\n",
      "2019-02-22T01:01:49.382359: step 73440, loss 0.551943, acc [0.89902344 0.84443359 0.83330078 0.73701172]\n",
      "2019-02-22T01:01:50.597062: step 73480, loss 0.543663, acc [0.90185547 0.85244141 0.83681641 0.74873047]\n",
      "2019-02-22T01:01:51.862356: step 73520, loss 0.569105, acc [0.89599609 0.84707031 0.83056641 0.74072266]\n",
      "2019-02-22T01:01:53.061684: step 73560, loss 0.557335, acc [0.90146484 0.84707031 0.82841797 0.73515625]\n",
      "2019-02-22T01:01:54.265971: step 73600, loss 0.581109, acc [0.89443359 0.83759766 0.82636719 0.73144531]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T01:01:58.886698: step 73600, loss 0.652627, acc [0.89773649 0.84181441 0.81722712 0.72912933] \n",
      "\n",
      "2019-02-22T01:02:00.177288: step 73640, loss 0.538231, acc [0.90439453 0.84960938 0.83701172 0.74628906]\n",
      "2019-02-22T01:02:01.442583: step 73680, loss 0.564631, acc [0.89716797 0.84746094 0.82900391 0.73740234]\n",
      "2019-02-22T01:02:02.682085: step 73720, loss 0.577478, acc [0.89482422 0.84160156 0.82832031 0.73515625]\n",
      "2019-02-22T01:02:03.931507: step 73760, loss 0.551048, acc [0.90097656 0.84599609 0.83085937 0.74033203]\n",
      "2019-02-22T01:02:05.146209: step 73800, loss 0.554923, acc [0.90185547 0.84589844 0.83466797 0.74003906]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T01:02:09.996621: step 73800, loss 0.651543, acc [0.89872759 0.8426153  0.8192794  0.73137182] \n",
      "\n",
      "2019-02-22T01:02:11.364056: step 73840, loss 0.542494, acc [0.89990234 0.85292969 0.83476562 0.74404297]\n",
      "2019-02-22T01:02:12.658615: step 73880, loss 0.559483, acc [0.89960938 0.84296875 0.83154297 0.73652344]\n",
      "2019-02-22T01:02:13.919941: step 73920, loss 0.571319, acc [0.89746094 0.84091797 0.82587891 0.734375  ]\n",
      "2019-02-22T01:02:15.203091: step 73960, loss 0.575098, acc [0.90117187 0.84277344 0.82529297 0.73164063]\n",
      "2019-02-22T01:02:16.450033: step 74000, loss 0.561366, acc [0.90019531 0.84404297 0.82871094 0.73779297]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T01:02:20.456263: step 74000, loss 0.652522, acc [0.89834716 0.84182442 0.81925938 0.73210264] \n",
      "\n",
      "2019-02-22T01:02:21.785994: step 74040, loss 0.558498, acc [0.90107422 0.84912109 0.82587891 0.73740234]\n",
      "2019-02-22T01:02:23.044345: step 74080, loss 0.573082, acc [0.89873047 0.8453125  0.82373047 0.73417969]\n",
      "2019-02-22T01:02:24.314598: step 74120, loss 0.546622, acc [0.90166016 0.84951172 0.83154297 0.74384766]\n",
      "2019-02-22T01:02:25.588324: step 74160, loss 0.57685, acc [0.89511719 0.83789062 0.82832031 0.73232422]\n",
      "2019-02-22T01:02:26.956290: step 74200, loss 0.561071, acc [0.89746094 0.84609375 0.83369141 0.73876953]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T01:02:30.856381: step 74200, loss 0.6499, acc [0.89838721 0.84317593 0.82057083 0.73321387] \n",
      "\n",
      "2019-02-22T01:02:32.233722: step 74240, loss 0.585625, acc [0.89228516 0.840625   0.82353516 0.73115234]\n",
      "2019-02-22T01:02:33.498027: step 74280, loss 0.580356, acc [0.89257812 0.8390625  0.82597656 0.73134766]\n",
      "2019-02-22T01:02:34.750422: step 74320, loss 0.600063, acc [0.89501953 0.83867187 0.81953125 0.72480469]\n",
      "2019-02-22T01:02:36.190309: step 74360, loss 0.566718, acc [0.89931641 0.84462891 0.82529297 0.73349609]\n",
      "2019-02-22T01:02:37.581587: step 74400, loss 0.568913, acc [0.89785156 0.84941406 0.82617188 0.73818359]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T01:02:42.043598: step 74400, loss 0.653401, acc [0.89834716 0.84328605 0.81879887 0.7317122 ] \n",
      "\n",
      "2019-02-22T01:02:43.461163: step 74440, loss 0.570236, acc [0.89667969 0.84208984 0.82851562 0.73408203]\n",
      "2019-02-22T01:02:44.761177: step 74480, loss 0.58862, acc [0.90039062 0.84521484 0.82041016 0.73417969]\n",
      "2019-02-22T01:02:46.181720: step 74520, loss 0.57688, acc [0.89736328 0.84667969 0.82431641 0.73574219]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-02-22T01:02:47.522405: step 74560, loss 0.584291, acc [0.89677734 0.84150391 0.82138672 0.73056641]\n",
      "2019-02-22T01:02:48.823411: step 74600, loss 0.582727, acc [0.89746094 0.84169922 0.82470703 0.73671875]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T01:02:52.926368: step 74600, loss 0.647579, acc [0.89760634 0.84322598 0.81944959 0.73204257] \n",
      "\n",
      "2019-02-22T01:02:54.329499: step 74640, loss 0.570566, acc [0.90039062 0.84853516 0.82363281 0.73544922]\n",
      "2019-02-22T01:02:55.643401: step 74680, loss 0.583454, acc [0.90068359 0.84033203 0.8234375  0.73232422]\n",
      "2019-02-22T01:02:56.936968: step 74720, loss 0.589424, acc [0.89365234 0.83896484 0.81748047 0.72666016]\n",
      "2019-02-22T01:02:58.233547: step 74760, loss 0.558118, acc [0.89941406 0.84921875 0.82880859 0.73857422]\n",
      "2019-02-22T01:02:59.537493: step 74800, loss 0.570713, acc [0.9015625  0.84394531 0.82226562 0.73222656]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T01:03:03.412278: step 74800, loss 0.65075, acc [0.89880768 0.84280551 0.81992011 0.73272332] \n",
      "\n",
      "2019-02-22T01:03:04.739036: step 74840, loss 0.576472, acc [0.90185547 0.84707031 0.82275391 0.73701172]\n",
      "2019-02-22T01:03:05.978043: step 74880, loss 0.572905, acc [0.89375    0.84160156 0.82958984 0.73164063]\n",
      "2019-02-22T01:03:07.319721: step 74920, loss 0.583345, acc [0.89921875 0.84316406 0.82207031 0.73554688]\n",
      "2019-02-22T01:03:08.660406: step 74960, loss 0.558018, acc [0.89909446 0.83785413 0.83139205 0.73582899]\n",
      "2019-02-22T01:03:09.860726: step 75000, loss 0.539672, acc [0.90107422 0.84716797 0.83525391 0.73818359]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T01:03:13.846079: step 75000, loss 0.651175, acc [0.89823704 0.84196458 0.82021044 0.7318023 ] \n",
      "\n",
      "2019-02-22T01:03:15.118814: step 75040, loss 0.550947, acc [0.90048828 0.84716797 0.83554688 0.74179688]\n",
      "2019-02-22T01:03:16.313181: step 75080, loss 0.527798, acc [0.90341797 0.85429687 0.83632812 0.74794922]\n",
      "2019-02-22T01:03:17.515978: step 75120, loss 0.53445, acc [0.90166016 0.85595703 0.84072266 0.75009766]\n",
      "2019-02-22T01:03:18.708857: step 75160, loss 0.555385, acc [0.89716797 0.84667969 0.82929688 0.7359375 ]\n",
      "2019-02-22T01:03:19.907191: step 75200, loss 0.55645, acc [0.90146484 0.84423828 0.82958984 0.73740234]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T01:03:24.025472: step 75200, loss 0.648795, acc [0.89780657 0.84297571 0.82127161 0.73152199] \n",
      "\n",
      "2019-02-22T01:03:25.351777: step 75240, loss 0.541559, acc [0.90136719 0.85351562 0.83496094 0.74941406]\n",
      "2019-02-22T01:03:26.591278: step 75280, loss 0.562554, acc [0.8984375  0.84296875 0.83037109 0.73857422]\n",
      "2019-02-22T01:03:27.810443: step 75320, loss 0.560013, acc [0.89414063 0.84521484 0.82890625 0.73525391]\n",
      "2019-02-22T01:03:29.015722: step 75360, loss 0.567187, acc [0.89931641 0.84082031 0.82832031 0.73378906]\n",
      "2019-02-22T01:03:30.219017: step 75400, loss 0.557632, acc [0.9        0.84238281 0.8296875  0.73886719]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T01:03:34.481141: step 75400, loss 0.651572, acc [0.89829711 0.84197459 0.81986004 0.7316221 ] \n",
      "\n",
      "2019-02-22T01:03:35.837696: step 75440, loss 0.571244, acc [0.90019531 0.84394531 0.82763672 0.73818359]\n",
      "2019-02-22T01:03:37.071742: step 75480, loss 0.568379, acc [0.89755859 0.84658203 0.82685547 0.73730469]\n",
      "2019-02-22T01:03:38.299340: step 75520, loss 0.573762, acc [0.90361328 0.84658203 0.82783203 0.74160156]\n",
      "2019-02-22T01:03:39.532395: step 75560, loss 0.570894, acc [0.89609375 0.84423828 0.82626953 0.73095703]\n",
      "2019-02-22T01:03:40.783305: step 75600, loss 0.551572, acc [0.90087891 0.84716797 0.83544922 0.74033203]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T01:03:45.270115: step 75600, loss 0.651994, acc [0.89851735 0.84114367 0.81878886 0.73057094] \n",
      "\n",
      "2019-02-22T01:03:46.614770: step 75640, loss 0.567523, acc [0.89951172 0.84179688 0.82988281 0.73730469]\n",
      "2019-02-22T01:03:47.890479: step 75680, loss 0.571325, acc [0.89785156 0.84199219 0.82460937 0.73388672]\n",
      "2019-02-22T01:03:49.157757: step 75720, loss 0.559013, acc [0.90166016 0.84414062 0.83125    0.74248047]\n",
      "2019-02-22T01:03:50.421067: step 75760, loss 0.569476, acc [0.90009766 0.84794922 0.82558594 0.73759766]\n",
      "2019-02-22T01:03:52.037034: step 75800, loss 0.564219, acc [0.89873047 0.84560547 0.83291016 0.73974609]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T01:03:56.375043: step 75800, loss 0.649656, acc [0.89876763 0.84344623 0.82057083 0.73317382] \n",
      "\n",
      "2019-02-22T01:03:58.020769: step 75840, loss 0.574955, acc [0.90205078 0.84257812 0.82792969 0.7390625 ]\n",
      "2019-02-22T01:03:59.463632: step 75880, loss 0.553961, acc [0.90205078 0.84941406 0.83037109 0.74257812]\n",
      "2019-02-22T01:04:00.848956: step 75920, loss 0.576316, acc [0.89648438 0.84101563 0.8265625  0.73388672]\n",
      "2019-02-22T01:04:02.191173: step 75960, loss 0.5732, acc [0.89765625 0.84755859 0.82724609 0.73457031]\n",
      "2019-02-22T01:04:03.500613: step 76000, loss 0.567902, acc [0.89746094 0.84394531 0.83066406 0.73457031]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T01:04:07.771665: step 76000, loss 0.650484, acc [0.89784661 0.84220485 0.81967985 0.73246303] \n",
      "\n",
      "2019-02-22T01:04:09.388124: step 76040, loss 0.56878, acc [0.8984375  0.84443359 0.82275391 0.73203125]\n",
      "2019-02-22T01:04:10.822058: step 76080, loss 0.565578, acc [0.9015625  0.84814453 0.82988281 0.74228516]\n",
      "2019-02-22T01:04:12.525815: step 76120, loss 0.574059, acc [0.89550781 0.83896484 0.82675781 0.73105469]\n",
      "2019-02-22T01:04:13.830791: step 76160, loss 0.55554, acc [0.90019531 0.84667969 0.8296875  0.73789063]\n",
      "2019-02-22T01:04:15.240943: step 76200, loss 0.580607, acc [0.89755859 0.84365234 0.82207031 0.73017578]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T01:04:19.261983: step 76200, loss 0.647086, acc [0.89886774 0.84294567 0.81967985 0.73231287] \n",
      "\n",
      "2019-02-22T01:04:20.652269: step 76240, loss 0.593721, acc [0.89345703 0.834375   0.82265625 0.72548828]\n",
      "2019-02-22T01:04:22.200281: step 76280, loss 0.589804, acc [0.89492187 0.84443359 0.82333984 0.73007813]\n",
      "2019-02-22T01:04:23.541959: step 76320, loss 0.578705, acc [0.90117187 0.84511719 0.81865234 0.73232422]\n",
      "2019-02-22T01:04:24.833541: step 76360, loss 0.588628, acc [0.90117187 0.84189453 0.81865234 0.73271484]\n",
      "2019-02-22T01:04:26.093380: step 76400, loss 0.61143, acc [0.89169922 0.84199219 0.81474609 0.72636719]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T01:04:30.094159: step 76400, loss 0.648914, acc [0.89847731 0.84276547 0.81844848 0.73060097] \n",
      "\n",
      "2019-02-22T01:04:31.433309: step 76440, loss 0.573948, acc [0.90283203 0.85019531 0.82167969 0.73779297]\n",
      "2019-02-22T01:04:32.673307: step 76480, loss 0.578405, acc [0.89628906 0.84296875 0.82236328 0.72890625]\n",
      "2019-02-22T01:04:34.047720: step 76520, loss 0.565589, acc [0.89684343 0.84464962 0.828634   0.73563368]\n",
      "2019-02-22T01:04:35.251014: step 76560, loss 0.539238, acc [0.90566406 0.84804687 0.83623047 0.74521484]\n",
      "2019-02-22T01:04:36.446869: step 76600, loss 0.545033, acc [0.90205078 0.84716797 0.83720703 0.74169922]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T01:04:40.485792: step 76600, loss 0.649514, acc [0.89863749 0.84212476 0.82011032 0.73230286] \n",
      "\n",
      "2019-02-22T01:04:41.826975: step 76640, loss 0.562479, acc [0.89736328 0.84550781 0.83027344 0.7359375 ]\n",
      "2019-02-22T01:04:43.103715: step 76680, loss 0.562888, acc [0.89296875 0.84091797 0.82851562 0.73242188]\n",
      "2019-02-22T01:04:44.324825: step 76720, loss 0.545836, acc [0.9        0.84589844 0.82978516 0.73681641]\n",
      "2019-02-22T01:04:45.561850: step 76760, loss 0.556083, acc [0.89755859 0.84667969 0.828125   0.7375    ]\n",
      "2019-02-22T01:04:46.808294: step 76800, loss 0.567653, acc [0.89228516 0.84033203 0.82949219 0.73193359]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T01:04:50.872550: step 76800, loss 0.646603, acc [0.89814694 0.8417143  0.82053079 0.73115158] \n",
      "\n",
      "2019-02-22T01:04:52.192863: step 76840, loss 0.534553, acc [0.90244141 0.85283203 0.83759766 0.74414062]\n",
      "2019-02-22T01:04:53.481472: step 76880, loss 0.557823, acc [0.89960938 0.84316406 0.82919922 0.73251953]\n",
      "2019-02-22T01:04:54.717004: step 76920, loss 0.563883, acc [0.90234375 0.8515625  0.83066406 0.74121094]\n",
      "2019-02-22T01:04:55.983831: step 76960, loss 0.548727, acc [0.89951172 0.8484375  0.83447266 0.74267578]\n",
      "2019-02-22T01:04:57.211383: step 77000, loss 0.543168, acc [0.90058594 0.85058594 0.83818359 0.74746094]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T01:05:01.324751: step 77000, loss 0.65308, acc [0.89662525 0.84230496 0.82056082 0.73159207] \n",
      "\n",
      "2019-02-22T01:05:02.620257: step 77040, loss 0.586421, acc [0.89365234 0.84042969 0.82304687 0.73037109]\n",
      "2019-02-22T01:05:03.824048: step 77080, loss 0.551602, acc [0.89951172 0.83945313 0.83535156 0.73964844]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-02-22T01:05:05.041725: step 77120, loss 0.565721, acc [0.89931641 0.84267578 0.82597656 0.73105469]\n",
      "2019-02-22T01:05:06.266347: step 77160, loss 0.559454, acc [0.89892578 0.84326172 0.82890625 0.73242188]\n",
      "2019-02-22T01:05:07.518746: step 77200, loss 0.57194, acc [0.8953125  0.84140625 0.82851562 0.73193359]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-22T01:05:11.750116: step 77200, loss 0.650347, acc [0.8983772  0.84285557 0.82112144 0.7323429 ] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_train = [train_y_label1_one_hot,train_y_label2_one_hot,train_y_label3_one_hot]\n",
    "y_test = [test_y_label1_one_hot,test_y_label2_one_hot,test_y_label3_one_hot]\n",
    "train(x_train, y_train,  x_test, y_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_y_label3_one_hot.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y_label3_one_hot.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
