{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.utils import shuffle\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "特征如下：['ITEM_NAME' 'TYPE'] \n",
      "\n",
      "数据数目:  499447\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('data/train/trainx.txt', sep='\\t')\n",
    "features = data.columns.values\n",
    "print(\"特征如下：{} \\n\".format(features))\n",
    "print(\"数据数目: \", len(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据清洗\n",
    "* 替换一些字符 类似于 （ ） ‘ ’ _"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ITEM_NAME</th>\n",
       "      <th>TYPE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>腾讯QQ黄钻三个月QQ黄钻3个月季卡官方自动充值可查时间可续费</td>\n",
       "      <td>本地生活--游戏充值--QQ充值</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>腾讯q币充值30元qq币30qb30q币自动充值</td>\n",
       "      <td>本地生活--游戏充值--QQ充值</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>腾讯QQ红钻包月卡 QQ红钻一个月QQ红钻1个月</td>\n",
       "      <td>本地生活--游戏充值--QQ充值</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>腾讯QQ蓝钻贵族31个月直充</td>\n",
       "      <td>本地生活--游戏充值--QQ充值</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>腾讯QQ币148元148QQ币148个直充148Q币148个Q币148个QQB★自动充值</td>\n",
       "      <td>本地生活--游戏充值--QQ充值</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      ITEM_NAME              TYPE\n",
       "0               腾讯QQ黄钻三个月QQ黄钻3个月季卡官方自动充值可查时间可续费  本地生活--游戏充值--QQ充值\n",
       "1                      腾讯q币充值30元qq币30qb30q币自动充值  本地生活--游戏充值--QQ充值\n",
       "2                      腾讯QQ红钻包月卡 QQ红钻一个月QQ红钻1个月  本地生活--游戏充值--QQ充值\n",
       "3                                腾讯QQ蓝钻贵族31个月直充  本地生活--游戏充值--QQ充值\n",
       "4  腾讯QQ币148元148QQ币148个直充148Q币148个Q币148个QQB★自动充值  本地生活--游戏充值--QQ充值"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "from zhon.hanzi import punctuation\n",
    "from string import punctuation as english_punc\n",
    "def  rm_punc(strs):\n",
    "    return re.sub(r\"[{}]+\".format(punctuation + english_punc),\" \",strs)\n",
    "data[features[0]]  = data[features[0]].apply(rm_punc)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练数据数目:   399558\n",
      "测试数据数目:   99889\n",
      "(399558,) (99889,)\n"
     ]
    }
   ],
   "source": [
    "# 因为有的label里本身就存在空格，所以先要把空格去掉，再去掉- - ， 再以空格划分\n",
    "data = shuffle(data)\n",
    "data_y = data[features[1]]\n",
    "\n",
    "# 取训练数据和测试数据\n",
    "train = data.sample(frac= 0.8).reset_index()\n",
    "test = data.sample(frac= 0.2).reset_index()\n",
    "\n",
    "\n",
    "print(\"训练数据数目:  \", len(train))\n",
    "print(\"测试数据数目:  \", len(test))\n",
    "\n",
    "train_x  = train[features[0]]\n",
    "train_y  = train[features[1]]\n",
    "\n",
    "test_x  = test[features[0]]\n",
    "test_y  = test[features[1]]\n",
    "\n",
    "print(train_x.shape, test_x.shape)\n",
    "\n",
    "# 分出数据集 训练集 测试集 数据集用来训练词向量\n",
    "data_y = data_y.apply(lambda x: x.replace(\" \", \"\").replace(\"--\", \" \").replace(\"/\", \"\")).tolist()\n",
    "data_y= [each.split() for each in data_y]\n",
    "\n",
    "train_y = train_y.apply(lambda x: x.replace(\" \", \"\").replace(\"--\", \" \").replace(\"/\", \"\")).tolist()\n",
    "train_y = [each.split() for each in train_y]\n",
    "\n",
    "test_y = test_y.apply(lambda x: x.replace(\" \", \"\").replace(\"--\", \" \").replace(\"/\", \"\")).tolist()\n",
    "test_y = [each.split() for each in test_y]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 分词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba\n",
    "from random import shuffle\n",
    "def participle(data, doshuffle = False):\n",
    "    words = []\n",
    "    for i in range(len(data)):\n",
    "        result=[]\n",
    "        seg_list = jieba.cut(data.iloc[i])\n",
    "        for w in seg_list :#读取每一行分词\n",
    "            if w != \" \":\n",
    "                result.append(w)\n",
    "        # 打乱每条数据的分词\n",
    "        if doshuffle:\n",
    "            shuffle(result)\n",
    "        words.append(result)#将该行分词写入列表形式的总分词列表\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\Richado\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 1.716 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "# 每条数据的分词结果是个长度不等的词列表\n",
    "word_data = participle(data[features[0]], doshuffle = False)\n",
    "word_data_train = participle(train_x,doshuffle = False)\n",
    "word_data_test = participle(test_x, doshuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'腾讯QQ黄钻三个月QQ黄钻3个月季卡官方自动充值可查时间可续费'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[features[0]][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['北山',\n",
       " '狼',\n",
       " '睡袋',\n",
       " '户外',\n",
       " '成人',\n",
       " '睡袋',\n",
       " '信封',\n",
       " '式',\n",
       " '抓绒',\n",
       " '睡袋',\n",
       " '内胆',\n",
       " '春夏季',\n",
       " '睡袋',\n",
       " '旅行',\n",
       " '隔脏',\n",
       " '睡袋',\n",
       " '可',\n",
       " '拼接',\n",
       " '午休',\n",
       " '睡袋',\n",
       " '蓝']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from random import shuffle\n",
    "word_data[8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 标签分离(一个标签分成三个标签)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y = pd.DataFrame(train_y, columns=['label1', 'label2', 'label3'])\n",
    "test_y = pd.DataFrame(test_y, columns=['label1', 'label2', 'label3'])\n",
    "data_y = pd.DataFrame(data_y, columns=['label1', 'label2', 'label3'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_y[\"label1\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "191"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_y[\"label2\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1192"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_y[\"label3\"].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 标签onn_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import to_categorical\n",
    "# 先生成唯一数组\n",
    "y_label1 = data_y['label1'].unique().tolist()\n",
    "y_label2 = data_y['label2'].unique().tolist()\n",
    "y_label3 = data_y['label3'].unique().tolist()\n",
    "\n",
    "# 获取在唯一数组中的索引(训练集和测试集各有3个标签需要处理)\n",
    "train_y_label1_map = train_y['label1'].apply(lambda x: y_label1.index(x))\n",
    "train_y_label2_map = train_y['label2'].apply(lambda x: y_label2.index(x))\n",
    "train_y_label3_map = train_y['label3'].apply(lambda x: y_label3.index(x))\n",
    "test_y_label1_map = test_y['label1'].apply(lambda x: y_label1.index(x))\n",
    "test_y_label2_map = test_y['label2'].apply(lambda x: y_label2.index(x))\n",
    "test_y_label3_map = test_y['label3'].apply(lambda x: y_label3.index(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生成对应one-hot(用做训练模型的标签)\n",
    "train_y_label1_one_hot = to_categorical(train_y_label1_map, 22)\n",
    "train_y_label2_one_hot = to_categorical(train_y_label2_map, 191)\n",
    "train_y_label3_one_hot = to_categorical(train_y_label3_map, 1192)\n",
    "test_y_label1_one_hot = to_categorical(test_y_label1_map, 22)\n",
    "test_y_label2_one_hot = to_categorical(test_y_label2_map, 191)\n",
    "test_y_label3_one_hot = to_categorical(test_y_label3_map, 1192)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import *\n",
    "from keras.preprocessing import sequence\n",
    "tok = Tokenizer()\n",
    "tok.fit_on_texts(word_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'新款': 1,\n",
       " '1': 2,\n",
       " '汽车': 3,\n",
       " '时尚': 4,\n",
       " '黑色': 5,\n",
       " '鞋': 6,\n",
       " '2016': 7,\n",
       " '休闲': 8,\n",
       " '款': 9,\n",
       " '专用': 10,\n",
       " '2': 11,\n",
       " '5': 12,\n",
       " '男士': 13,\n",
       " '跟': 14,\n",
       " '手机': 15,\n",
       " '适用': 16,\n",
       " '女': 17,\n",
       " '套装': 18,\n",
       " '男': 19,\n",
       " '韩版': 20,\n",
       " '夏季': 21,\n",
       " '儿童': 22,\n",
       " '3': 23,\n",
       " '包': 24,\n",
       " '于': 25,\n",
       " '透气': 26,\n",
       " '壳': 27,\n",
       " '套': 28,\n",
       " '休闲鞋': 29,\n",
       " '米': 30,\n",
       " '户外': 31,\n",
       " '男鞋': 32,\n",
       " '真皮': 33,\n",
       " '垫': 34,\n",
       " '红色': 35,\n",
       " '6': 36,\n",
       " '4': 37,\n",
       " '脚垫': 38,\n",
       " '女鞋': 39,\n",
       " '男女': 40,\n",
       " '白色': 41,\n",
       " '全': 42,\n",
       " '蓝色': 43,\n",
       " '创意': 44,\n",
       " '宝宝': 45,\n",
       " '防水': 46,\n",
       " '简约': 47,\n",
       " '四季': 48,\n",
       " '女士': 49,\n",
       " '8': 50,\n",
       " '保护套': 51,\n",
       " '凉鞋': 52,\n",
       " '牛皮': 53,\n",
       " '四件套': 54,\n",
       " '皮鞋': 55,\n",
       " '系列': 56,\n",
       " '专车': 57,\n",
       " '情侣': 58,\n",
       " '包围': 59,\n",
       " '灯': 60,\n",
       " '通用': 61,\n",
       " '纯棉': 62,\n",
       " '加厚': 63,\n",
       " '小': 64,\n",
       " '运动': 65,\n",
       " '单鞋': 66,\n",
       " '商务': 67,\n",
       " '学生': 68,\n",
       " '大': 69,\n",
       " '2015': 70,\n",
       " '送': 71,\n",
       " '潮流': 72,\n",
       " '客厅': 73,\n",
       " '婴儿': 74,\n",
       " '带': 75,\n",
       " '新': 76,\n",
       " '卡通': 77,\n",
       " '号': 78,\n",
       " '全棉': 79,\n",
       " '英伦': 80,\n",
       " '7': 81,\n",
       " '0': 82,\n",
       " '寸': 83,\n",
       " '可': 84,\n",
       " '现代': 85,\n",
       " '鞋子': 86,\n",
       " 'led': 87,\n",
       " '单肩': 88,\n",
       " '玩具': 89,\n",
       " '黑': 90,\n",
       " '被': 91,\n",
       " '座垫': 92,\n",
       " '39': 93,\n",
       " '礼物': 94,\n",
       " '苹果': 95,\n",
       " '40': 96,\n",
       " '版': 97,\n",
       " '的': 98,\n",
       " '摆件': 99,\n",
       " '保暖': 100,\n",
       " '礼品': 101,\n",
       " '码': 102,\n",
       " '38': 103,\n",
       " '保护': 104,\n",
       " '改装': 105,\n",
       " '灰色': 106,\n",
       " '春季': 107,\n",
       " '床上用品': 108,\n",
       " '舒适': 109,\n",
       " '卧室': 110,\n",
       " '可爱': 111,\n",
       " '印花': 112,\n",
       " '拖鞋': 113,\n",
       " '装饰': 114,\n",
       " '多功能': 115,\n",
       " '高': 116,\n",
       " '坐垫': 117,\n",
       " '棕色': 118,\n",
       " '被套': 119,\n",
       " '茶具': 120,\n",
       " '进口': 121,\n",
       " '系带': 122,\n",
       " '米床': 123,\n",
       " '10': 124,\n",
       " '防': 125,\n",
       " '背包': 126,\n",
       " '金属': 127,\n",
       " '防滑': 128,\n",
       " '杯': 129,\n",
       " '复古': 130,\n",
       " '座套': 131,\n",
       " '经典': 132,\n",
       " '正版': 133,\n",
       " '秋冬': 134,\n",
       " '系': 135,\n",
       " '贴': 136,\n",
       " '春夏': 137,\n",
       " '12': 138,\n",
       " '汽车坐垫': 139,\n",
       " '奥迪': 140,\n",
       " '不锈钢': 141,\n",
       " '陶瓷': 142,\n",
       " '粉色': 143,\n",
       " '36': 144,\n",
       " '床单': 145,\n",
       " '欧美': 146,\n",
       " '床': 147,\n",
       " '定制': 148,\n",
       " '生日礼物': 149,\n",
       " '双人': 150,\n",
       " '手表': 151,\n",
       " '35': 152,\n",
       " '孕妇': 153,\n",
       " '板鞋': 154,\n",
       " '宝马': 155,\n",
       " '女款': 156,\n",
       " '车载': 157,\n",
       " '大众': 158,\n",
       " '套件': 159,\n",
       " '实木': 160,\n",
       " '年': 161,\n",
       " '女包': 162,\n",
       " '夹': 163,\n",
       " '枕': 164,\n",
       " '空调': 165,\n",
       " '37': 166,\n",
       " '办公': 167,\n",
       " '膜': 168,\n",
       " 'l': 169,\n",
       " '红': 170,\n",
       " '风': 171,\n",
       " '手机套': 172,\n",
       " '欧式': 173,\n",
       " '15': 174,\n",
       " '平底': 175,\n",
       " '车': 176,\n",
       " '尖头': 177,\n",
       " '男款': 178,\n",
       " '装': 179,\n",
       " '个性': 180,\n",
       " '手链': 181,\n",
       " '42': 182,\n",
       " '运动鞋': 183,\n",
       " '41': 184,\n",
       " '靴': 185,\n",
       " 'm': 186,\n",
       " '英寸': 187,\n",
       " '钻': 188,\n",
       " '皮革': 189,\n",
       " '色': 190,\n",
       " '防晒': 191,\n",
       " '新品': 192,\n",
       " '金': 193,\n",
       " '家用': 194,\n",
       " '水晶': 195,\n",
       " '懒人': 196,\n",
       " '吊坠': 197,\n",
       " '厚底': 198,\n",
       " '中': 199,\n",
       " '家居': 200,\n",
       " '配件': 201,\n",
       " '43': 202,\n",
       " '黄色': 203,\n",
       " '组合': 204,\n",
       " '粗': 205,\n",
       " '全包': 206,\n",
       " '书包': 207,\n",
       " '200': 208,\n",
       " '架': 209,\n",
       " '高跟鞋': 210,\n",
       " '饰品': 211,\n",
       " '三星': 212,\n",
       " '米色': 213,\n",
       " '公仔': 214,\n",
       " '硅胶': 215,\n",
       " '绿色': 216,\n",
       " '内': 217,\n",
       " '百搭': 218,\n",
       " '丝圈': 219,\n",
       " '后备箱': 220,\n",
       " '吸顶灯': 221,\n",
       " '皮套': 222,\n",
       " '透明': 223,\n",
       " '电脑': 224,\n",
       " '钱包': 225,\n",
       " 's': 226,\n",
       " '三件套': 227,\n",
       " '16': 228,\n",
       " '手工': 229,\n",
       " '蓝': 230,\n",
       " '家具': 231,\n",
       " '厘米': 232,\n",
       " '中国': 233,\n",
       " '个': 234,\n",
       " '高清': 235,\n",
       " '正品': 236,\n",
       " '毛绒玩具': 237,\n",
       " '型': 238,\n",
       " '9': 239,\n",
       " '春秋': 240,\n",
       " '台': 241,\n",
       " '冬季': 242,\n",
       " '挂件': 243,\n",
       " '奔驰': 244,\n",
       " '金色': 245,\n",
       " '跑步': 246,\n",
       " '银': 247,\n",
       " '包邮': 248,\n",
       " '智能': 249,\n",
       " '抱': 250,\n",
       " '3d': 251,\n",
       " '斜挎包': 252,\n",
       " 'a': 253,\n",
       " '双肩包': 254,\n",
       " 'xl': 255,\n",
       " '福克斯': 256,\n",
       " '纯色': 257,\n",
       " '岁': 258,\n",
       " '婚庆': 259,\n",
       " '天然': 260,\n",
       " '级': 261,\n",
       " '潮': 262,\n",
       " '礼盒': 263,\n",
       " '增高': 264,\n",
       " '白': 265,\n",
       " '20': 266,\n",
       " '灯具': 267,\n",
       " '一': 268,\n",
       " '帆布鞋': 269,\n",
       " '家纺': 270,\n",
       " '华为': 271,\n",
       " '银色': 272,\n",
       " '迷你': 273,\n",
       " '紫色': 274,\n",
       " '双层': 275,\n",
       " '丰田': 276,\n",
       " '斜': 277,\n",
       " '单': 278,\n",
       " '本田': 279,\n",
       " '单人': 280,\n",
       " '卡': 281,\n",
       " '支架': 282,\n",
       " '短袖': 283,\n",
       " '手提包': 284,\n",
       " '戒指': 285,\n",
       " '皮': 286,\n",
       " '用品': 287,\n",
       " '件套': 288,\n",
       " '钻石': 289,\n",
       " '圆头': 290,\n",
       " '14': 291,\n",
       " '片': 292,\n",
       " '双': 293,\n",
       " '项链': 294,\n",
       " '耐磨': 295,\n",
       " '车型': 296,\n",
       " '立体': 297,\n",
       " '月': 298,\n",
       " '玻璃': 299,\n",
       " '水钻': 300,\n",
       " '性感': 301,\n",
       " 't恤': 302,\n",
       " '餐厅': 303,\n",
       " '方向盘': 304,\n",
       " '边框': 305,\n",
       " '高尔夫': 306,\n",
       " '贴纸': 307,\n",
       " '贴膜': 308,\n",
       " '头层': 309,\n",
       " '外壳': 310,\n",
       " '式': 311,\n",
       " '镂空': 312,\n",
       " '原装': 313,\n",
       " '盒': 314,\n",
       " '冰丝': 315,\n",
       " '豆豆': 316,\n",
       " '100': 317,\n",
       " '益智': 318,\n",
       " '椅': 319,\n",
       " '扣': 320,\n",
       " '头': 321,\n",
       " '磨砂': 322,\n",
       " '包包': 323,\n",
       " '彩色': 324,\n",
       " '44': 325,\n",
       " '床品': 326,\n",
       " '一对': 327,\n",
       " '别克': 328,\n",
       " '摔': 329,\n",
       " '玫瑰': 330,\n",
       " '棉': 331,\n",
       " '册': 332,\n",
       " '帕萨特': 333,\n",
       " '把': 334,\n",
       " '充电': 335,\n",
       " '收纳': 336,\n",
       " '十字绣': 337,\n",
       " '旅行': 338,\n",
       " '折叠': 339,\n",
       " '公主': 340,\n",
       " '浅口': 341,\n",
       " '功夫': 342,\n",
       " '环保': 343,\n",
       " '韩国': 344,\n",
       " '人': 345,\n",
       " '小米': 346,\n",
       " '车用': 347,\n",
       " '多': 348,\n",
       " '不': 349,\n",
       " '粉': 350,\n",
       " '衣': 351,\n",
       " '吊灯': 352,\n",
       " '彩绘': 353,\n",
       " '钥匙包': 354,\n",
       " '手': 355,\n",
       " '条': 356,\n",
       " '夏': 357,\n",
       " '春夏季': 358,\n",
       " '大容量': 359,\n",
       " '凉': 360,\n",
       " '笔': 361,\n",
       " '布鞋': 362,\n",
       " '24': 363,\n",
       " '厨房': 364,\n",
       " '斜纹': 365,\n",
       " '茶杯': 366,\n",
       " '水杯': 367,\n",
       " '与': 368,\n",
       " '结婚': 369,\n",
       " '途观': 370,\n",
       " '专用汽车': 371,\n",
       " '被子': 372,\n",
       " '衣服': 373,\n",
       " '本': 374,\n",
       " '925': 375,\n",
       " '猫': 376,\n",
       " '灯饰': 377,\n",
       " '和': 378,\n",
       " '座椅': 379,\n",
       " '套餐': 380,\n",
       " '子': 381,\n",
       " '拉': 382,\n",
       " '无线': 383,\n",
       " '遥控': 384,\n",
       " '细': 385,\n",
       " '小号': 386,\n",
       " '卡罗': 387,\n",
       " '沙发': 388,\n",
       " '低帮': 389,\n",
       " '便携': 390,\n",
       " '办公室': 391,\n",
       " '花': 392,\n",
       " '无': 393,\n",
       " '手提': 394,\n",
       " '蝴蝶结': 395,\n",
       " '潮鞋': 396,\n",
       " '低': 397,\n",
       " '13': 398,\n",
       " '标准': 399,\n",
       " '纹': 400,\n",
       " '福特': 401,\n",
       " '笔记本': 402,\n",
       " '钢化': 403,\n",
       " '桌': 404,\n",
       " '全新': 405,\n",
       " '袋': 406,\n",
       " 'diy': 407,\n",
       " '坡': 408,\n",
       " 'plus': 409,\n",
       " '30': 410,\n",
       " 'iphone6': 411,\n",
       " '双肩': 412,\n",
       " '女单': 413,\n",
       " '零食': 414,\n",
       " '6s': 415,\n",
       " '北京': 416,\n",
       " '粉红色': 417,\n",
       " '短靴': 418,\n",
       " '咖啡色': 419,\n",
       " '凯美瑞': 420,\n",
       " '芯': 421,\n",
       " '套脚': 422,\n",
       " '驾车': 423,\n",
       " '起亚': 424,\n",
       " '工具': 425,\n",
       " 'usb': 426,\n",
       " '秋季': 427,\n",
       " '书籍': 428,\n",
       " '罩': 429,\n",
       " '60': 430,\n",
       " '备注': 431,\n",
       " '钥匙': 432,\n",
       " '50': 433,\n",
       " '均码': 434,\n",
       " '坠': 435,\n",
       " '后': 436,\n",
       " '老': 437,\n",
       " '器': 438,\n",
       " '颜色': 439,\n",
       " '女童': 440,\n",
       " '工艺品': 441,\n",
       " '塑料': 442,\n",
       " '模型': 443,\n",
       " '230cm': 444,\n",
       " '情人节': 445,\n",
       " '防风': 446,\n",
       " '大码': 447,\n",
       " '11': 448,\n",
       " '爱': 449,\n",
       " '线': 450,\n",
       " '徒步': 451,\n",
       " '鲜花': 452,\n",
       " '克': 453,\n",
       " '酒': 454,\n",
       " '雅阁': 455,\n",
       " '柜': 456,\n",
       " '34': 457,\n",
       " '尾箱': 458,\n",
       " '活性': 459,\n",
       " '18': 460,\n",
       " '外套': 461,\n",
       " '积木': 462,\n",
       " '标配': 463,\n",
       " '手串': 464,\n",
       " '新生儿': 465,\n",
       " '拉链': 466,\n",
       " '紫': 467,\n",
       " '花花公子': 468,\n",
       " '教材': 469,\n",
       " 'b': 470,\n",
       " '长': 471,\n",
       " '迈腾': 472,\n",
       " '中式': 473,\n",
       " '滤清器': 474,\n",
       " '绿': 475,\n",
       " 't': 476,\n",
       " '宽': 477,\n",
       " '150': 478,\n",
       " '田园': 479,\n",
       " '灰': 480,\n",
       " '孕妇装': 481,\n",
       " '甜美': 482,\n",
       " '玩偶': 483,\n",
       " '女式': 484,\n",
       " '壶': 485,\n",
       " 'c': 486,\n",
       " '现货': 487,\n",
       " '马丁': 488,\n",
       " '安全': 489,\n",
       " '亚麻': 490,\n",
       " '脚': 491,\n",
       " '遮阳': 492,\n",
       " '自动': 493,\n",
       " '圆形': 494,\n",
       " '沙滩鞋': 495,\n",
       " '长安': 496,\n",
       " '裤': 497,\n",
       " '座': 498,\n",
       " '女友': 499,\n",
       " '豪华版': 500,\n",
       " '虎': 501,\n",
       " '温馨': 502,\n",
       " '咖啡': 503,\n",
       " '女孩': 504,\n",
       " '美': 505,\n",
       " '长袖': 506,\n",
       " '帆布': 507,\n",
       " '软壳': 508,\n",
       " '女生': 509,\n",
       " '松糕': 510,\n",
       " '条纹': 511,\n",
       " '长款': 512,\n",
       " '茶盘': 513,\n",
       " '迪士尼': 514,\n",
       " '美式': 515,\n",
       " '熊': 516,\n",
       " '两用': 517,\n",
       " '用': 518,\n",
       " '马自达': 519,\n",
       " '标致': 520,\n",
       " '游戏': 521,\n",
       " '日产': 522,\n",
       " '含': 523,\n",
       " '滤': 524,\n",
       " '香水': 525,\n",
       " '板': 526,\n",
       " '美国': 527,\n",
       " '专业': 528,\n",
       " '冲锋衣': 529,\n",
       " '速腾': 530,\n",
       " '布娃娃': 531,\n",
       " '挡': 532,\n",
       " '空气': 533,\n",
       " '韩版潮': 534,\n",
       " '蓝牙': 535,\n",
       " '一字': 536,\n",
       " '白光': 537,\n",
       " '科鲁兹': 538,\n",
       " '装饰品': 539,\n",
       " 'crv': 540,\n",
       " '音乐': 541,\n",
       " '服': 542,\n",
       " '成人': 543,\n",
       " '车衣': 544,\n",
       " '双人床': 545,\n",
       " '学院': 546,\n",
       " '隔热': 547,\n",
       " '平板': 548,\n",
       " '斤': 549,\n",
       " '网面': 550,\n",
       " '凉席': 551,\n",
       " '靴子': 552,\n",
       " '优雅': 553,\n",
       " '垫子': 554,\n",
       " '键盘': 555,\n",
       " '度': 556,\n",
       " '件': 557,\n",
       " '棕': 558,\n",
       " '日本': 559,\n",
       " '盖': 560,\n",
       " '皮肤': 561,\n",
       " '配': 562,\n",
       " '黄': 563,\n",
       " '蕾丝': 564,\n",
       " '德国': 565,\n",
       " '证书': 566,\n",
       " '联想': 567,\n",
       " '翡翠': 568,\n",
       " '茶': 569,\n",
       " '速干': 570,\n",
       " '轻便': 571,\n",
       " '拖': 572,\n",
       " '三': 573,\n",
       " '捷达': 574,\n",
       " '全套': 575,\n",
       " '深蓝色': 576,\n",
       " '糖果': 577,\n",
       " '碗': 578,\n",
       " '茶壶': 579,\n",
       " '仿真': 580,\n",
       " '小包': 581,\n",
       " '英朗': 582,\n",
       " 'polo': 583,\n",
       " '内衣': 584,\n",
       " '180': 585,\n",
       " '达': 586,\n",
       " '书房': 587,\n",
       " '请': 588,\n",
       " '上衣': 589,\n",
       " '办公桌': 590,\n",
       " '橙色': 591,\n",
       " '挎': 592,\n",
       " '合金': 593,\n",
       " '小学生': 594,\n",
       " '钥匙扣': 595,\n",
       " '宝': 596,\n",
       " '逸': 597,\n",
       " '监控': 598,\n",
       " '钢化玻璃': 599,\n",
       " '儿童玩具': 600,\n",
       " '格': 601,\n",
       " '蒙迪欧': 602,\n",
       " '水壶': 603,\n",
       " '浅': 604,\n",
       " '汽车用品': 605,\n",
       " '特产': 606,\n",
       " '妈妈': 607,\n",
       " '枕套': 608,\n",
       " '婴幼儿': 609,\n",
       " '耳机': 610,\n",
       " '玫': 611,\n",
       " '箱': 612,\n",
       " '天籁': 613,\n",
       " '狗': 614,\n",
       " '鱼': 615,\n",
       " '浮雕': 616,\n",
       " '比亚迪': 617,\n",
       " '2014': 618,\n",
       " '钱': 619,\n",
       " '椅子': 620,\n",
       " '清新': 621,\n",
       " '200cm': 622,\n",
       " '适合': 623,\n",
       " '直径': 624,\n",
       " '坐套': 625,\n",
       " '插': 626,\n",
       " '登山': 627,\n",
       " '下': 628,\n",
       " '紫砂': 629,\n",
       " 'ipad': 630,\n",
       " '80': 631,\n",
       " '被罩': 632,\n",
       " '修复': 633,\n",
       " '滤芯': 634,\n",
       " '电池': 635,\n",
       " '明锐': 636,\n",
       " '娃娃': 637,\n",
       " '迷彩': 638,\n",
       " '卡其色': 639,\n",
       " '盘': 640,\n",
       " '置物架': 641,\n",
       " '绣': 642,\n",
       " '18k': 643,\n",
       " '货到付款': 644,\n",
       " '味': 645,\n",
       " '移动': 646,\n",
       " '随机': 647,\n",
       " '脚蹬': 648,\n",
       " '茶几': 649,\n",
       " '男包': 650,\n",
       " '瓶': 651,\n",
       " 'rav4': 652,\n",
       " '玉石': 653,\n",
       " 'xxl': 654,\n",
       " '开关': 655,\n",
       " '珠宝': 656,\n",
       " '博世': 657,\n",
       " '机油': 658,\n",
       " '浪漫': 659,\n",
       " '雨刮器': 660,\n",
       " '高档': 661,\n",
       " '书': 662,\n",
       " '双面': 663,\n",
       " '佛珠': 664,\n",
       " '睡袋': 665,\n",
       " '翼': 666,\n",
       " '加绒': 667,\n",
       " '拼色': 668,\n",
       " '夏装': 669,\n",
       " '电动': 670,\n",
       " '超薄': 671,\n",
       " '链': 672,\n",
       " 'u': 673,\n",
       " '景德镇': 674,\n",
       " '餐具': 675,\n",
       " '短款': 676,\n",
       " '软底': 677,\n",
       " '沙滩': 678,\n",
       " '男式': 679,\n",
       " '秋冬季': 680,\n",
       " '考试': 681,\n",
       " '长裤': 682,\n",
       " '喷漆': 683,\n",
       " '镶': 684,\n",
       " '和田玉': 685,\n",
       " '提花': 686,\n",
       " '室内': 687,\n",
       " '加大': 688,\n",
       " '花瓶': 689,\n",
       " '全国': 690,\n",
       " '探路者': 691,\n",
       " '吉普': 692,\n",
       " '精品': 693,\n",
       " '荣耀': 694,\n",
       " '阳光': 695,\n",
       " '日': 696,\n",
       " '高帮': 697,\n",
       " '雪佛兰': 698,\n",
       " '手套': 699,\n",
       " '保温杯': 700,\n",
       " 'cd': 701,\n",
       " '护板': 702,\n",
       " '电源': 703,\n",
       " '哈弗': 704,\n",
       " '有': 705,\n",
       " '飞度': 706,\n",
       " '4g': 707,\n",
       " '户外运动': 708,\n",
       " '帮': 709,\n",
       " '一体机': 710,\n",
       " '夏凉': 711,\n",
       " '留言': 712,\n",
       " '画': 713,\n",
       " '后盖': 714,\n",
       " '45': 715,\n",
       " '帐篷': 716,\n",
       " '机': 717,\n",
       " '汉兰达': 718,\n",
       " '皇冠': 719,\n",
       " '电子': 720,\n",
       " '衣柜': 721,\n",
       " '网': 722,\n",
       " 'a6l': 723,\n",
       " 'pu': 724,\n",
       " '生日': 725,\n",
       " '上': 726,\n",
       " 'q5': 727,\n",
       " '世界': 728,\n",
       " '220': 729,\n",
       " '平': 730,\n",
       " '旅游': 731,\n",
       " '拼接': 732,\n",
       " '内饰': 733,\n",
       " '手镯': 734,\n",
       " '气质': 735,\n",
       " '摆设': 736,\n",
       " '面板': 737,\n",
       " 'dvd': 738,\n",
       " '帽': 739,\n",
       " '跨': 740,\n",
       " '灯泡': 741,\n",
       " '特价': 742,\n",
       " '鳄鱼': 743,\n",
       " '餐桌': 744,\n",
       " '磨毛': 745,\n",
       " '拍': 746,\n",
       " '木': 747,\n",
       " '长城': 748,\n",
       " '雪铁龙': 749,\n",
       " '元': 750,\n",
       " '杯子': 751,\n",
       " '支': 752,\n",
       " '动物': 753,\n",
       " '春': 754,\n",
       " '靠垫': 755,\n",
       " '罗马': 756,\n",
       " '玫红': 757,\n",
       " '夏天': 758,\n",
       " '防雨': 759,\n",
       " '早教': 760,\n",
       " '拉杆箱': 761,\n",
       " '其他': 762,\n",
       " '食品': 763,\n",
       " '干': 764,\n",
       " '男孩': 765,\n",
       " '鱼嘴': 766,\n",
       " '充电器': 767,\n",
       " '日常': 768,\n",
       " '头盔': 769,\n",
       " '升级版': 770,\n",
       " '毯': 771,\n",
       " '纯': 772,\n",
       " '罐': 773,\n",
       " '豪华': 774,\n",
       " '旅行包': 775,\n",
       " '泳衣': 776,\n",
       " 'x5': 777,\n",
       " '思域': 778,\n",
       " '嘴': 779,\n",
       " '实用': 780,\n",
       " '木质': 781,\n",
       " '光盘': 782,\n",
       " '升级': 783,\n",
       " '行李箱': 784,\n",
       " '趾': 785,\n",
       " '两件套': 786,\n",
       " 'e': 787,\n",
       " '单个': 788,\n",
       " '钢笔': 789,\n",
       " '马': 790,\n",
       " '凯越': 791,\n",
       " '支装': 792,\n",
       " '米奇': 793,\n",
       " '保温': 794,\n",
       " '摩托车': 795,\n",
       " '抓': 796,\n",
       " '款式': 797,\n",
       " '男表': 798,\n",
       " 'ix35': 799,\n",
       " '乐福鞋': 800,\n",
       " '正装': 801,\n",
       " '第': 802,\n",
       " '网鞋': 803,\n",
       " '黄金': 804,\n",
       " '记录仪': 805,\n",
       " '纸': 806,\n",
       " '连衣裙': 807,\n",
       " '片装': 808,\n",
       " '毛毯': 809,\n",
       " '划痕': 810,\n",
       " '宝来': 811,\n",
       " '居家': 812,\n",
       " '简易': 813,\n",
       " '男童': 814,\n",
       " '连体': 815,\n",
       " '内胆': 816,\n",
       " '230': 817,\n",
       " '大号': 818,\n",
       " '皮带': 819,\n",
       " '拼': 820,\n",
       " '拼装': 821,\n",
       " '之': 822,\n",
       " '三合一': 823,\n",
       " '松糕鞋': 824,\n",
       " '层': 825,\n",
       " '调光': 826,\n",
       " '修身': 827,\n",
       " '儿童节': 828,\n",
       " '附': 829,\n",
       " 'oppo': 830,\n",
       " '19': 831,\n",
       " '设计': 832,\n",
       " '包头': 833,\n",
       " '我': 834,\n",
       " '阳台': 835,\n",
       " '单件': 836,\n",
       " '编织': 837,\n",
       " '音箱': 838,\n",
       " '登山鞋': 839,\n",
       " '咖色': 840,\n",
       " '贡缎': 841,\n",
       " '镜': 842,\n",
       " '上海': 843,\n",
       " '棉鞋': 844,\n",
       " '安装': 845,\n",
       " '钓鱼': 846,\n",
       " '锐志': 847,\n",
       " '漆皮': 848,\n",
       " '训练': 849,\n",
       " '桌椅': 850,\n",
       " '宠物': 851,\n",
       " '游泳': 852,\n",
       " '仕': 853,\n",
       " '硬盘': 854,\n",
       " '骑行': 855,\n",
       " '逍客': 856,\n",
       " '索纳塔': 857,\n",
       " '漆': 858,\n",
       " '英语': 859,\n",
       " '故事': 860,\n",
       " '帽子': 861,\n",
       " '路': 862,\n",
       " '选': 863,\n",
       " '兔': 864,\n",
       " '方形': 865,\n",
       " '前后': 866,\n",
       " '土豪': 867,\n",
       " '开': 868,\n",
       " 'a3': 869,\n",
       " '约': 870,\n",
       " '摄像头': 871,\n",
       " '夜光': 872,\n",
       " '25': 873,\n",
       " '导航': 874,\n",
       " 'a4l': 875,\n",
       " '120': 876,\n",
       " '铆钉': 877,\n",
       " '绣花': 878,\n",
       " '睿': 879,\n",
       " '风格': 880,\n",
       " 'cc': 881,\n",
       " '学习': 882,\n",
       " '旅行箱': 883,\n",
       " '打火机': 884,\n",
       " '自': 885,\n",
       " '8g': 886,\n",
       " '东风': 887,\n",
       " '桑塔纳': 888,\n",
       " '整套': 889,\n",
       " 'h6': 890,\n",
       " '铝合金': 891,\n",
       " '长方形': 892,\n",
       " '软': 893,\n",
       " '扳手': 894,\n",
       " '仿古': 895,\n",
       " '貔貅': 896,\n",
       " '斯柯达': 897,\n",
       " '珍珠': 898,\n",
       " '行车': 899,\n",
       " 'd': 900,\n",
       " 's6': 901,\n",
       " '张': 902,\n",
       " '底': 903,\n",
       " '球': 904,\n",
       " '水洗': 905,\n",
       " '卷': 906,\n",
       " '点': 907,\n",
       " '速递': 908,\n",
       " '鼠标': 909,\n",
       " '出版社': 910,\n",
       " '配饰': 911,\n",
       " '咖': 912,\n",
       " '免': 913,\n",
       " '5mm': 914,\n",
       " '黑白': 915,\n",
       " '裙': 916,\n",
       " '图书': 917,\n",
       " '22': 918,\n",
       " '大红': 919,\n",
       " '车垫': 920,\n",
       " '宝骏': 921,\n",
       " '雕花': 922,\n",
       " '定做': 923,\n",
       " '盒装': 924,\n",
       " '电镀': 925,\n",
       " '超高': 926,\n",
       " '好': 927,\n",
       " '翻盖': 928,\n",
       " '音响': 929,\n",
       " '手绘': 930,\n",
       " '台灯': 931,\n",
       " '骆驼': 932,\n",
       " '圆领': 933,\n",
       " 'v': 934,\n",
       " '古典': 935,\n",
       " '拉手': 936,\n",
       " '浅蓝色': 937,\n",
       " '年级': 938,\n",
       " 'pro': 939,\n",
       " '吊': 940,\n",
       " '天': 941,\n",
       " '酒杯': 942,\n",
       " '台湾': 943,\n",
       " '品': 944,\n",
       " '笔记本电脑': 945,\n",
       " '镜头': 946,\n",
       " '绒': 947,\n",
       " '世家': 948,\n",
       " '精装': 949,\n",
       " '后视镜': 950,\n",
       " '指环': 951,\n",
       " '铜': 952,\n",
       " '幼儿园': 953,\n",
       " '生肖': 954,\n",
       " '烤': 955,\n",
       " '网布': 956,\n",
       " '可折叠': 957,\n",
       " '标签': 958,\n",
       " '必备': 959,\n",
       " '朗逸': 960,\n",
       " '边': 961,\n",
       " '共': 962,\n",
       " '魅族': 963,\n",
       " '电脑包': 964,\n",
       " '补漆笔': 965,\n",
       " '小熊': 966,\n",
       " '轻薄': 967,\n",
       " '网络': 968,\n",
       " '静音': 969,\n",
       " '泰迪熊': 970,\n",
       " '艺术': 971,\n",
       " '标准版': 972,\n",
       " '卡宴': 973,\n",
       " '腰包': 974,\n",
       " '文具': 975,\n",
       " '500ml': 976,\n",
       " '电视': 977,\n",
       " '头枕': 978,\n",
       " '风衣': 979,\n",
       " '年份': 980,\n",
       " '插座': 981,\n",
       " '男装': 982,\n",
       " '90': 983,\n",
       " '电视柜': 984,\n",
       " '刹车片': 985,\n",
       " '超': 986,\n",
       " '奶瓶': 987,\n",
       " '冬': 988,\n",
       " '昂科威': 989,\n",
       " '主机': 990,\n",
       " '富贵': 991,\n",
       " '自由': 992,\n",
       " '开业': 993,\n",
       " '仪表': 994,\n",
       " '工装': 995,\n",
       " '耳钉': 996,\n",
       " 'vivo': 997,\n",
       " '羊皮': 998,\n",
       " '书桌': 999,\n",
       " '机械': 1000,\n",
       " ...}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "274420"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tok.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = tok.texts_to_sequences(word_data_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = tok.texts_to_sequences(word_data_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = sequence.pad_sequences(x_train, maxlen=18)\n",
    "x_test  = sequence.pad_sequences(x_test, maxlen=18) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[    0,     0,     0, ...,   882,  2454, 14626],\n",
       "       [ 6472,    14,    26, ...,   226,   152,   102],\n",
       "       [    0,     0,     0, ..., 45092,  2574,   133],\n",
       "       ...,\n",
       "       [    0,     0,     0, ...,   162, 26206,  1540],\n",
       "       [    0, 19138,    51, ...,    12,    12,   187],\n",
       "       [  150,   208,   444, ...,    11,    23,    30]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 词向量训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "from gensim.models.word2vec import Word2Vec \n",
    "if os.path.exists(\"w2v_model\"):\n",
    "    model = Word2Vec.load(\"w2v_model\")\n",
    "    print(1)\n",
    "else:\n",
    "    sentences = word_data\n",
    "    model= Word2Vec(size=50, window=10, min_count = 1)\n",
    "    model.build_vocab(sentences)\n",
    "    model.train(sentences,total_examples = model.corpus_count,epochs = model.iter)\n",
    "    model.save(\"w2v_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 填充词向量(构造输入输出)（让每个句子拥有同样的维度）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\richado\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\ipykernel_launcher.py:2: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \n",
      "c:\\users\\richado\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\ipykernel_launcher.py:3: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "\n",
    "x_train = [model[word] for word in word_data_train]\n",
    "x_test = [model[word] for word in word_data_test]\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=18)\n",
    "x_test  = sequence.pad_sequences(x_test, maxlen=18)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 默认参数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## TextCnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class TextCNN(object):\n",
    "    \"\"\"\n",
    "    A CNN for text classification.\n",
    "    Uses an embedding layer, followed by a convolutional, max-pooling and softmax layer.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "      self, maxlen, sequence_length, num_classes, filter_sizes, embedding_size, num_filters, l2_reg_lambda=0.0):\n",
    "\n",
    "        # Placeholders for input, output and dropout\n",
    "#         self.input_x = tf.placeholder(tf.float32, [None, sequence_length, embedding_size], name=\"input_x\")\n",
    "        self.input_x = tf.placeholder(tf.int32, [None, sequence_length], name=\"input_x\")\n",
    "        self.input_y1 = tf.placeholder(tf.float32, [None, num_classes[0]], name=\"input_y1\")\n",
    "        self.input_y2 = tf.placeholder(tf.float32, [None, num_classes[1]], name=\"input_y2\")\n",
    "        self.input_y3 = tf.placeholder(tf.float32, [None, num_classes[2]], name=\"input_y3\")\n",
    "        with tf.name_scope(\"embedding\"):\n",
    "            W = tf.Variable(\n",
    "                tf.random_uniform([274420, embedding_size], -1.0, 1.0),\n",
    "                name=\"W\")\n",
    "            self.embedded_chars = tf.nn.embedding_lookup(W, self.input_x)\n",
    "            self.embedded_chars_expanded = tf.expand_dims(self.embedded_chars, -1)\n",
    "        l2_loss = tf.constant(0.0)\n",
    "        keep_prob = 0.9\n",
    "        pooled_outputs = []\n",
    "        \n",
    "        for i, filter_size in enumerate(filter_sizes):\n",
    "            with tf.name_scope(\"conv-maxpool-%s\" % filter_size):\n",
    "                # Convolution Layer\n",
    "                filter_shape = [filter_size, embedding_size, 1, num_filters]\n",
    "                W = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.1), name=\"W\")\n",
    "                b = tf.Variable(tf.constant(0.1, shape=[num_filters]), name=\"b\")\n",
    "                # 卷积\n",
    "                conv = tf.nn.conv2d(\n",
    "                    self.embedded_chars_expanded,\n",
    "                    W,\n",
    "                    strides=[1, 1, 1, 1],\n",
    "                    padding=\"VALID\",\n",
    "                    name=\"conv\")\n",
    "        \n",
    "        \n",
    "                h = tf.nn.relu(tf.nn.bias_add(conv, b), name=\"relu\")\n",
    "            \n",
    "                # 最大池化\n",
    "                pooled = tf.nn.max_pool(\n",
    "                    h,\n",
    "                    ksize=[1, sequence_length - filter_size + 1, 1, 1],\n",
    "                    strides=[1, 1, 1, 1],\n",
    "                    padding='VALID',\n",
    "                    name=\"pool\")\n",
    "                pooled = tf.nn.dropout(pooled, keep_prob)\n",
    "                pooled_outputs.append(pooled)\n",
    "                \n",
    "\n",
    "        # Combine all the pooled features\n",
    "        num_filters_total = num_filters * len(filter_sizes)\n",
    "        self.h_pool = tf.concat(pooled_outputs, 3)\n",
    "        print(self.h_pool.shape)\n",
    "        \n",
    "        self.h_pool_flat = tf.reshape(self.h_pool, [-1, num_filters_total])\n",
    "        self.h_drop = self.h_pool_flat\n",
    "        \n",
    "        # Final (unnormalized) scores and predictions\n",
    "        \n",
    "        with tf.name_scope(\"output1\"):\n",
    "            W1 = tf.get_variable(\n",
    "                \"W11\",\n",
    "                shape=[num_filters_total, 64],\n",
    "                initializer=tf.contrib.layers.xavier_initializer())\n",
    "            b1 = tf.Variable(tf.constant(0.1, shape=[64]), name=\"b11\")\n",
    "            self.output = tf.nn.xw_plus_b(self.h_drop, W1, b1, name=\"output1\")\n",
    "            self.output = tf.layers.batch_normalization(self.output, training=is_training)\n",
    "            self.output = tf.nn.relu(self.output)\n",
    "            self.output = tf.nn.dropout(self.output, keep_prob)\n",
    "            W2 = tf.get_variable(\n",
    "                \"W12\",\n",
    "                shape=[64, num_classes[0]],\n",
    "                initializer=tf.contrib.layers.xavier_initializer())\n",
    "            b2 = tf.Variable(tf.constant(0.1, shape=[num_classes[0]]), name=\"b12\")\n",
    "            l2_loss += tf.nn.l2_loss(W1)\n",
    "            l2_loss += tf.nn.l2_loss(b1)\n",
    "            self.scores1 = tf.nn.xw_plus_b(self.output, W2, b2, name=\"scores\")\n",
    "            self.predictions1 = tf.argmax(self.scores1, 1, name=\"predictions\")\n",
    "            \n",
    "        with tf.name_scope(\"output2\"):\n",
    "            W1 = tf.get_variable(\n",
    "                \"W21\",\n",
    "                shape=[num_filters_total, 128],\n",
    "                initializer=tf.contrib.layers.xavier_initializer())\n",
    "            b1 = tf.Variable(tf.constant(0.1, shape=[128]), name=\"b21\")\n",
    "            self.output = tf.nn.xw_plus_b(self.h_drop, W1, b1, name=\"output2\")\n",
    "            self.output = tf.layers.batch_normalization(self.output, training=is_training)\n",
    "            self.output = tf.nn.relu(self.output)\n",
    "            self.output = tf.nn.dropout(self.output, keep_prob)\n",
    "            W2 = tf.get_variable(\n",
    "                \"W22\",\n",
    "                shape=[128, num_classes[1]],\n",
    "                initializer=tf.contrib.layers.xavier_initializer())\n",
    "            b2 = tf.Variable(tf.constant(0.1, shape=[num_classes[1]]), name=\"b22\")\n",
    "            l2_loss += tf.nn.l2_loss(W1)\n",
    "            l2_loss += tf.nn.l2_loss(b1)\n",
    "            self.scores2 = tf.nn.xw_plus_b(self.output, W2, b2, name=\"scores\")\n",
    "            self.predictions2 = tf.argmax(self.scores2, 1, name=\"predictions\")\n",
    "            \n",
    "        with tf.name_scope(\"output3\"):\n",
    "            W1 = tf.get_variable(\n",
    "                \"W31\",\n",
    "                shape=[num_filters_total, 256],\n",
    "                initializer=tf.contrib.layers.xavier_initializer())\n",
    "            b1 = tf.Variable(tf.constant(0.1, shape=[256]), name=\"b31\")\n",
    "            self.output = tf.nn.xw_plus_b(self.h_drop, W1, b1, name=\"output3\")\n",
    "            self.output = tf.layers.batch_normalization(self.output, training=is_training)\n",
    "            self.output = tf.nn.relu(self.output)\n",
    "            self.output = tf.nn.dropout(self.output, keep_prob)\n",
    "            W2 = tf.get_variable(\n",
    "                \"W32\",\n",
    "                shape=[256, num_classes[2]],\n",
    "                initializer=tf.contrib.layers.xavier_initializer())\n",
    "            b2 = tf.Variable(tf.constant(0.1, shape=[num_classes[2]]), name=\"b32\")\n",
    "            l2_loss += tf.nn.l2_loss(W1)\n",
    "            l2_loss += tf.nn.l2_loss(b1)\n",
    "            self.scores3 = tf.nn.xw_plus_b(self.output, W2, b2, name=\"scores\")\n",
    "            self.predictions3 = tf.argmax(self.scores3, 1, name=\"predictions\")\n",
    "        # Calculate mean cross-entropy loss\n",
    "        \n",
    "        with tf.name_scope(\"loss\"):\n",
    "            losses1 = tf.nn.softmax_cross_entropy_with_logits(logits=self.scores1, labels=self.input_y1)\n",
    "            losses2 = tf.nn.softmax_cross_entropy_with_logits(logits=self.scores2, labels=self.input_y2)\n",
    "            losses3 = tf.nn.softmax_cross_entropy_with_logits(logits=self.scores3, labels=self.input_y3)\n",
    "            self.loss = 0.1 * tf.reduce_mean(losses1) + 0.2 * tf.reduce_mean(losses2) + 0.7*tf.reduce_mean(losses3) + l2_reg_lambda * l2_loss\n",
    "            tf.summary.scalar('loss1',tf.reduce_mean(losses1))\n",
    "            tf.summary.scalar('loss2',tf.reduce_mean(losses2))\n",
    "            tf.summary.scalar('loss3',tf.reduce_mean(losses3))\n",
    "            tf.summary.scalar('loss',self.loss)\n",
    "        # Accuracy\n",
    "        with tf.name_scope(\"accuracy\"):\n",
    "            correct_predictions1 = tf.equal(self.predictions1, tf.argmax(self.input_y1, 1))\n",
    "            correct_predictions2 = tf.equal(self.predictions2, tf.argmax(self.input_y2, 1))\n",
    "            correct_predictions3 = tf.equal(self.predictions3, tf.argmax(self.input_y3, 1))\n",
    "#           单独的准确率\n",
    "            self.accuracy1 = tf.reduce_mean(tf.cast(correct_predictions1, \"float\"), name=\"accuracy1\")\n",
    "            self.accuracy2 = tf.reduce_mean(tf.cast(correct_predictions2, \"float\"), name=\"accuracy2\")\n",
    "            self.accuracy3 = tf.reduce_mean(tf.cast(correct_predictions3, \"float\"), name=\"accuracy3\")\n",
    "#           一起的准确率\n",
    "            self.acc = tf.reduce_mean(tf.cast(correct_predictions1, \"float\")*tf.cast(correct_predictions2, \"float\")*tf.cast(correct_predictions3, \"float\"))\n",
    "            tf.summary.scalar('acc1',self.accuracy1)\n",
    "            tf.summary.scalar('acc2',self.accuracy2)\n",
    "            tf.summary.scalar('acc3',self.accuracy3)\n",
    "            tf.summary.scalar('acc',self.acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def batch_iter(data, batch_size, num_epochs, shuffle=True):\n",
    "    \"\"\"\n",
    "    Generates a batch iterator for a dataset.\n",
    "    \"\"\"\n",
    "    data = np.array(data)\n",
    "    data_size = len(data)\n",
    "    num_batches_per_epoch = int((len(data)-1)/batch_size) + 1\n",
    "    for epoch in range(num_epochs):\n",
    "        # Shuffle the data at each epoch\n",
    "        if shuffle:\n",
    "            shuffle_indices = np.random.permutation(np.arange(data_size))\n",
    "            shuffled_data = data[shuffle_indices]\n",
    "        else:\n",
    "            shuffled_data = data\n",
    "  \n",
    "        for batch_num in range(num_batches_per_epoch):\n",
    "            start_index = batch_num * batch_size\n",
    "            end_index = np.min(((batch_num + 1) * batch_size, data_size))\n",
    "            yield shuffled_data[start_index:end_index]\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "from tensorflow.contrib import learn\n",
    "\n",
    "# 特征数这里是第三级标签的特征数\n",
    "max_features = 1192\n",
    "\n",
    "# 句子填充的长度\n",
    "maxlen = 16\n",
    "\n",
    "batch_size = 256 \n",
    "\n",
    "epochs = 100\n",
    "\n",
    "is_training = True\n",
    "# 词向量长度\n",
    "embedding_dims = 50\n",
    "\n",
    "filters = 64\n",
    "kernel_size = 5\n",
    "hidden_dims = 128\n",
    "epochs = 100\n",
    "\n",
    "# Parameters\n",
    "# ==================================================\n",
    "\n",
    "# Data loading params\n",
    "# tf.flags.DEFINE_float(\"dev_sample_percentage\", .1, \"Percentage of the training data to use for validation\")\n",
    "# tf.flags.DEFINE_string(\"positive_data_file\", \"./data/rt-polaritydata/rt-polarity.pos\", \"Data source for the positive data.\")\n",
    "# tf.flags.DEFINE_string(\"negative_data_file\", \"./data/rt-polaritydata/rt-polarity.neg\", \"Data source for the negative data.\")\n",
    "\n",
    "# Model Hyperparameters\n",
    "# tf.flags.DEFINE_integer(\"embedding_dim\", 128, \"Dimensionality of character embedding (default: 128)\")\n",
    "# tf.flags.DEFINE_string(\"filter_sizes\", \"3,4,5\", \"Comma-separated filter sizes (default: '3,4,5')\")\n",
    "# tf.flags.DEFINE_integer(\"num_filters\", 128, \"Number of filters per filter size (default: 128)\")\n",
    "# tf.flags.DEFINE_float(\"dropout_keep_prob\", 0.5, \"Dropout keep probability (default: 0.5)\")\n",
    "# tf.flags.DEFINE_float(\"l2_reg_lambda\", 0.0, \"L2 regularization lambda (default: 0.0)\")\n",
    "\n",
    "# Training parameters\n",
    "# tf.flags.DEFINE_integer(\"batch_size\", 64, \"Batch Size (default: 64)\")\n",
    "# tf.flags.DEFINE_integer(\"num_epochs\", 200, \"Number of training epochs (default: 200)\")\n",
    "# tf.flags.DEFINE_integer(\"evaluate_every\", 100, \"Evaluate model on dev set after this many steps (default: 100)\")\n",
    "# tf.flags.DEFINE_integer(\"checkpoint_every\", 100, \"Save model after this many steps (default: 100)\")\n",
    "# tf.flags.DEFINE_integer(\"num_checkpoints\", 5, \"Number of checkpoints to store (default: 5)\")\n",
    "# Misc Parameters\n",
    "# tf.flags.DEFINE_boolean(\"allow_soft_placement\", True, \"Allow device soft device placement\")\n",
    "# tf.flags.DEFINE_boolean(\"log_device_placement\", False, \"Log placement of ops on devices\")\n",
    "\n",
    "FLAGS = tf.flags.FLAGS\n",
    "\n",
    "def train(x_train, y_train, x_dev, y_dev):\n",
    "    # Training\n",
    "    # ==================================================\n",
    "\n",
    "    with tf.Graph().as_default():\n",
    "        session_conf = tf.ConfigProto(\n",
    "#           allow_soft_placement=FLAGS.allow_soft_placement,\n",
    "#           log_device_placement=FLAGS.log_device_placement\n",
    "        )\n",
    "        sess = tf.Session(config=session_conf)\n",
    "        with sess.as_default():\n",
    "            cnn = TextCNN(maxlen = 10, sequence_length = 18, num_classes = [22, 191, 1192], filter_sizes = [1,2, 3, 4, 5], embedding_size = 32, num_filters = 64)\n",
    "\n",
    "            # Define Training procedure\n",
    "\n",
    "            global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "#           学习率衰减\n",
    "            learning_rate = tf.train.exponential_decay(\n",
    "                0.01,\n",
    "                global_step,\n",
    "                x_train.shape[0] / batch_size,\n",
    "                0.99,\n",
    "                staircase=True,)\n",
    "            optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "            grads_and_vars = optimizer.compute_gradients(cnn.loss)\n",
    "            train_op = optimizer.apply_gradients(grads_and_vars, global_step=global_step)\n",
    "\n",
    "            # Initialize all variables\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            \n",
    "            merged = tf.summary.merge_all()\n",
    "            \n",
    "            writer = tf.summary.FileWriter(\"fwwb\",sess.graph)\n",
    "            acc_sum = np.array([0.0,0.0,0.0,0.0])\n",
    "            loss_sum = 0\n",
    "            \n",
    "            def train_step(x_batch, y_batch1, y_batch2, y_batch3, loss_sum, acc_sum, interal = 10):\n",
    "  \n",
    "                \"\"\"\n",
    "                A single training step\n",
    "                \"\"\"\n",
    "                feed_dict = {\n",
    "                  cnn.input_x: x_batch,\n",
    "                  cnn.input_y1: y_batch1,\n",
    "                  cnn.input_y2: y_batch2,\n",
    "                  cnn.input_y3: y_batch3\n",
    "                }\n",
    "#         三个标签单独的准确率， 一起的准确率共四个准确率\n",
    "                rs, _, step,  loss, accuracy1, accuracy2, accuracy3, acc = sess.run(\n",
    "                    [merged, train_op, global_step, cnn.loss, cnn.accuracy1, cnn.accuracy2, cnn.accuracy3, cnn.acc],\n",
    "                    feed_dict)\n",
    "                time_str = datetime.datetime.now().isoformat()\n",
    "                acc_sum += [accuracy1, accuracy2, accuracy3, acc]\n",
    "                loss_sum += loss\n",
    "                if step % interal == 0:\n",
    "                    print(\"{}: step {}, loss {:g}, acc {}\".format(time_str, step, loss_sum/interal, acc_sum/interal))\n",
    "                    loss_sum = 0\n",
    "                    acc_sum = np.array([0.0,0.0,0.0,0.0])\n",
    "                    writer.add_summary(rs, step)\n",
    "                return loss_sum, acc_sum\n",
    "            # 评估步骤\n",
    "            def dev_step(x_batch, y_batch):\n",
    "                \"\"\"\n",
    "                Evaluates model on a dev set\n",
    "                \"\"\"\n",
    "\n",
    "                dev_batches = batch_iter(\n",
    "                    list(zip(x_batch, y_batch[0], y_batch[1], y_batch[2])), 640, 1)\n",
    "                step = 0\n",
    "                loss = 0\n",
    "                accuracy = np.array([0.0, 0.0, 0.0,0.0])\n",
    "                for batch in dev_batches:\n",
    "                    x_batch_dev, y_batch1_dev, y_batch2_dev, y_batch3_dev = zip(*batch)\n",
    "                    feed_dict = {\n",
    "                      cnn.input_x: x_batch_dev,\n",
    "                      cnn.input_y1: y_batch1_dev,\n",
    "                      cnn.input_y2: y_batch2_dev,\n",
    "                      cnn.input_y3: y_batch3_dev\n",
    "                    }\n",
    "                    step,  temp_loss, temp_accuracy1, temp_accuracy2, temp_accuracy3 , acc= sess.run(\n",
    "                        [global_step,cnn.loss, cnn.accuracy1, cnn.accuracy2, cnn.accuracy3, cnn.acc],\n",
    "                        feed_dict)\n",
    "                    accuracy[0] += temp_accuracy1 * len(x_batch_dev)\n",
    "                    accuracy[1] += temp_accuracy2 * len(x_batch_dev)\n",
    "                    accuracy[2] += temp_accuracy3 * len(x_batch_dev)\n",
    "                    accuracy[3] += acc * len(x_batch_dev)\n",
    "                    loss += temp_loss * len(x_batch_dev)\n",
    "                accuracy /= x_batch.shape[0]\n",
    "                loss /= x_batch.shape[0]\n",
    "                time_str = datetime.datetime.now().isoformat()                    \n",
    "\n",
    "                print(\"{}: step {}, loss {:g}, acc {} \".format(time_str, step, loss, accuracy))\n",
    "\n",
    "\n",
    "            # Generate batches\n",
    "            batches = batch_iter(\n",
    "                list(zip(x_train, y_train[0],y_train[1], y_train[2] )), batch_size, 200)\n",
    "            # Training loop. For each batch...\n",
    "            for batch in batches:\n",
    "                x_batch, y_batch1, y_batch2, y_batch3 = zip(*batch)\n",
    "                loss_sum, acc_sum = train_step(x_batch, y_batch1, y_batch2, y_batch3,  loss_sum, acc_sum, 40)\n",
    "                current_step = tf.train.global_step(sess, global_step)\n",
    "                if current_step % 200 == 0:\n",
    "                    print(\"\\nEvaluation:\")\n",
    "                    dev_step(x_dev, y_dev)\n",
    "                    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 1, 1, 320)\n",
      "2019-02-23T23:19:11.158880: step 40, loss 4.08853, acc [0.40341797 0.33457031 0.24404297 0.18828125]\n",
      "2019-02-23T23:19:13.406257: step 80, loss 2.95948, acc [0.60078125 0.503125   0.41552734 0.34140625]\n",
      "2019-02-23T23:19:15.625855: step 120, loss 2.44486, acc [0.68896484 0.58642578 0.50292969 0.41601562]\n",
      "2019-02-23T23:19:17.855871: step 160, loss 2.12075, acc [0.73525391 0.63769531 0.55419922 0.47460938]\n",
      "2019-02-23T23:19:20.048686: step 200, loss 1.89136, acc [0.76757812 0.675      0.58779297 0.51259766]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-23T23:19:25.726393: step 200, loss 1.78377, acc [0.78606253 0.68534073 0.60996706 0.53240096] \n",
      "\n",
      "2019-02-23T23:19:28.112598: step 240, loss 1.73932, acc [0.79199219 0.69521484 0.61630859 0.53720703]\n",
      "2019-02-23T23:19:30.360965: step 280, loss 1.62361, acc [0.81289062 0.71347656 0.63496094 0.56484375]\n",
      "2019-02-23T23:19:32.712004: step 320, loss 1.58074, acc [0.81630859 0.71904297 0.63925781 0.56826172]\n",
      "2019-02-23T23:19:34.921188: step 360, loss 1.44661, acc [0.82949219 0.73984375 0.66982422 0.59511719]\n",
      "2019-02-23T23:19:37.094164: step 400, loss 1.41867, acc [0.83466797 0.73847656 0.67460937 0.59902344]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-23T23:19:40.757620: step 400, loss 1.32377, acc [0.84582887 0.75825166 0.69082682 0.61627407] \n",
      "\n",
      "2019-02-23T23:19:43.044673: step 440, loss 1.33332, acc [0.84355469 0.75898438 0.6921875  0.61660156]\n",
      "2019-02-23T23:19:45.212194: step 480, loss 1.31655, acc [0.84931641 0.76113281 0.69287109 0.62060547]\n",
      "2019-02-23T23:19:47.386192: step 520, loss 1.28409, acc [0.8484375  0.76679688 0.69951172 0.63046875]\n",
      "2019-02-23T23:19:49.539792: step 560, loss 1.2515, acc [0.85078125 0.77109375 0.70712891 0.63066406]\n",
      "2019-02-23T23:19:51.762902: step 600, loss 1.19195, acc [0.86132812 0.78935547 0.71523437 0.64794922]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-23T23:19:55.255199: step 600, loss 1.10053, acc [0.87066644 0.79282003 0.73590686 0.66483797] \n",
      "\n",
      "2019-02-23T23:19:57.562093: step 640, loss 1.17298, acc [0.86640625 0.78164062 0.71777344 0.646875  ]\n",
      "2019-02-23T23:19:59.700349: step 680, loss 1.15592, acc [0.86445313 0.78740234 0.72128906 0.64921875]\n",
      "2019-02-23T23:20:01.964587: step 720, loss 1.1439, acc [0.86972656 0.78984375 0.72978516 0.66044922]\n",
      "2019-02-23T23:20:04.089947: step 760, loss 1.11146, acc [0.86679688 0.79746094 0.73261719 0.66357422]\n",
      "2019-02-23T23:20:06.206874: step 800, loss 1.07164, acc [0.87226563 0.79853516 0.74072266 0.67001953]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-23T23:20:10.054890: step 800, loss 0.970505, acc [0.88420146 0.81390343 0.75897246 0.69011603] \n",
      "\n",
      "2019-02-23T23:20:12.250136: step 840, loss 1.05401, acc [0.87851563 0.80634766 0.74482422 0.6796875 ]\n",
      "2019-02-23T23:20:14.354170: step 880, loss 1.06569, acc [0.87861328 0.80625    0.74365234 0.67998047]\n",
      "2019-02-23T23:20:16.432408: step 920, loss 1.02922, acc [0.87705078 0.81113281 0.74697266 0.68583984]\n",
      "2019-02-23T23:20:18.513128: step 960, loss 1.03264, acc [0.88320312 0.80986328 0.74550781 0.68271484]\n",
      "2019-02-23T23:20:20.589879: step 1000, loss 0.974932, acc [0.88623047 0.81669922 0.75839844 0.69130859]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-23T23:20:24.581687: step 1000, loss 0.878744, acc [0.89290112 0.82653746 0.77620158 0.70949754] \n",
      "\n",
      "2019-02-23T23:20:26.777973: step 1040, loss 0.978996, acc [0.8828125  0.81923828 0.75097656 0.68984375]\n",
      "2019-02-23T23:20:28.890932: step 1080, loss 0.982392, acc [0.88642578 0.8171875  0.75224609 0.69189453]\n",
      "2019-02-23T23:20:30.966694: step 1120, loss 0.962351, acc [0.88847656 0.81894531 0.76005859 0.70009766]\n",
      "2019-02-23T23:20:33.031541: step 1160, loss 0.949403, acc [0.88916016 0.82207031 0.76318359 0.70146484]\n",
      "2019-02-23T23:20:35.092916: step 1200, loss 0.924577, acc [0.89140625 0.82236328 0.76386719 0.70146484]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-23T23:20:38.688419: step 1200, loss 0.801754, acc [0.90267197 0.83775991 0.79186897 0.72826838] \n",
      "\n",
      "2019-02-23T23:20:40.874785: step 1240, loss 0.942524, acc [0.89082031 0.82138672 0.76669922 0.70488281]\n",
      "2019-02-23T23:20:42.921776: step 1280, loss 0.914126, acc [0.89755859 0.82919922 0.76953125 0.71044922]\n",
      "2019-02-23T23:20:44.986129: step 1320, loss 0.892337, acc [0.89814453 0.82998047 0.77236328 0.70839844]\n",
      "2019-02-23T23:20:47.058415: step 1360, loss 0.949789, acc [0.88847656 0.82089844 0.76591797 0.70332031]\n",
      "2019-02-23T23:20:49.095487: step 1400, loss 0.906682, acc [0.89316406 0.82714844 0.77011719 0.70878906]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-23T23:20:52.610677: step 1400, loss 0.748488, acc [0.9106308  0.85287669 0.80181001 0.74314489] \n",
      "\n",
      "2019-02-23T23:20:54.740959: step 1440, loss 0.892173, acc [0.89580078 0.83408203 0.7734375  0.71591797]\n",
      "2019-02-23T23:20:56.796391: step 1480, loss 0.888487, acc [0.89609375 0.834375   0.77216797 0.71425781]\n",
      "2019-02-23T23:20:58.818085: step 1520, loss 0.863114, acc [0.89707031 0.83056641 0.78271484 0.72197266]\n",
      "2019-02-23T23:21:00.832828: step 1560, loss 0.888328, acc [0.89355469 0.83251953 0.77480469 0.71591797]\n",
      "2019-02-23T23:21:04.348969: step 1600, loss 0.658175, acc [0.91492168 0.86406645 0.82335069 0.76410886]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-23T23:21:07.858168: step 1600, loss 0.684682, acc [0.91464526 0.86249737 0.81584559 0.75834176] \n",
      "\n",
      "2019-02-23T23:21:09.940376: step 1640, loss 0.663067, acc [0.91474609 0.865625   0.82148438 0.76289063]\n",
      "2019-02-23T23:21:11.937273: step 1680, loss 0.660356, acc [0.91708984 0.86738281 0.81787109 0.76015625]\n",
      "2019-02-23T23:21:13.936653: step 1720, loss 0.652698, acc [0.91787109 0.8625     0.82490234 0.76796875]\n",
      "2019-02-23T23:21:15.927638: step 1760, loss 0.661885, acc [0.91435547 0.86318359 0.81777344 0.75761719]\n",
      "2019-02-23T23:21:17.915096: step 1800, loss 0.656061, acc [0.91640625 0.86015625 0.81904297 0.75820312]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-23T23:21:21.360773: step 1800, loss 0.650025, acc [0.91793891 0.86928491 0.82581666 0.7678423 ] \n",
      "\n",
      "2019-02-23T23:21:23.433062: step 1840, loss 0.656053, acc [0.91611328 0.86425781 0.82177734 0.76152344]\n",
      "2019-02-23T23:21:25.427478: step 1880, loss 0.665334, acc [0.91865234 0.86611328 0.81728516 0.76054687]\n",
      "2019-02-23T23:21:27.420404: step 1920, loss 0.662871, acc [0.91962891 0.86777344 0.81757813 0.76005859]\n",
      "2019-02-23T23:21:29.416309: step 1960, loss 0.65788, acc [0.91357422 0.86396484 0.82099609 0.76083984]\n",
      "2019-02-23T23:21:31.408245: step 2000, loss 0.6859, acc [0.91523438 0.86484375 0.82080078 0.76083984]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-23T23:21:34.852043: step 2000, loss 0.626407, acc [0.92266416 0.87303907 0.83254412 0.77466988] \n",
      "\n",
      "2019-02-23T23:21:36.929291: step 2040, loss 0.647384, acc [0.92207031 0.87216797 0.82666016 0.77197266]\n",
      "2019-02-23T23:21:38.933130: step 2080, loss 0.662811, acc [0.91767578 0.86552734 0.82128906 0.76025391]\n",
      "2019-02-23T23:21:40.933497: step 2120, loss 0.665579, acc [0.91689453 0.86416016 0.81796875 0.75800781]\n",
      "2019-02-23T23:21:42.926956: step 2160, loss 0.663648, acc [0.91650391 0.86933594 0.81757813 0.76230469]\n",
      "2019-02-23T23:21:44.918360: step 2200, loss 0.666312, acc [0.91816406 0.86875    0.82119141 0.76435547]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-23T23:21:48.380440: step 2200, loss 0.598057, acc [0.92379542 0.8760324  0.84004245 0.78052638] \n",
      "\n",
      "2019-02-23T23:21:50.544983: step 2240, loss 0.651363, acc [0.9203125  0.86865234 0.82617188 0.76660156]\n",
      "2019-02-23T23:21:52.549368: step 2280, loss 0.66451, acc [0.91904297 0.86738281 0.82099609 0.76542969]\n",
      "2019-02-23T23:21:54.551175: step 2320, loss 0.673956, acc [0.91933594 0.86669922 0.81669922 0.76279297]\n",
      "2019-02-23T23:21:56.547573: step 2360, loss 0.659272, acc [0.91757813 0.86601562 0.815625   0.75732422]\n",
      "2019-02-23T23:21:58.536572: step 2400, loss 0.676422, acc [0.91396484 0.86513672 0.81826172 0.76035156]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-23T23:22:01.971332: step 2400, loss 0.572909, acc [0.92483657 0.88144841 0.84502798 0.78721381] \n",
      "\n",
      "2019-02-23T23:22:04.078339: step 2440, loss 0.640204, acc [0.91572266 0.87216797 0.82363281 0.76630859]\n",
      "2019-02-23T23:22:06.102019: step 2480, loss 0.648675, acc [0.91806641 0.86621094 0.82109375 0.76337891]\n",
      "2019-02-23T23:22:08.118259: step 2520, loss 0.672688, acc [0.91386719 0.86416016 0.82070312 0.76191406]\n",
      "2019-02-23T23:22:10.148386: step 2560, loss 0.655677, acc [0.9171875  0.87070313 0.82158203 0.76728516]\n",
      "2019-02-23T23:22:12.184961: step 2600, loss 0.637142, acc [0.91845703 0.86933594 0.82480469 0.76396484]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-23T23:22:15.719005: step 2600, loss 0.548623, acc [0.92719919 0.8863939  0.85046401 0.79248966] \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-02-23T23:22:17.800246: step 2640, loss 0.658192, acc [0.91533203 0.86738281 0.82041016 0.76308594]\n",
      "2019-02-23T23:22:19.793173: step 2680, loss 0.631035, acc [0.92050781 0.87441406 0.82373047 0.765625  ]\n",
      "2019-02-23T23:22:21.787094: step 2720, loss 0.636812, acc [0.91992188 0.8765625  0.82460937 0.77099609]\n",
      "2019-02-23T23:22:23.784485: step 2760, loss 0.646809, acc [0.91689453 0.86923828 0.82167969 0.76396484]\n",
      "2019-02-23T23:22:25.776915: step 2800, loss 0.644426, acc [0.91357422 0.86914062 0.82744141 0.77207031]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-23T23:22:29.205873: step 2800, loss 0.513215, acc [0.93112355 0.89287109 0.85815255 0.80213037] \n",
      "\n",
      "2019-02-23T23:22:31.268239: step 2840, loss 0.636168, acc [0.91826172 0.87490234 0.82226562 0.76962891]\n",
      "2019-02-23T23:22:33.267119: step 2880, loss 0.644285, acc [0.92070312 0.87363281 0.8234375  0.76982422]\n",
      "2019-02-23T23:22:35.261537: step 2920, loss 0.628392, acc [0.91884766 0.87246094 0.82783203 0.77119141]\n",
      "2019-02-23T23:22:37.252478: step 2960, loss 0.619699, acc [0.92529297 0.87548828 0.82871094 0.77568359]\n",
      "2019-02-23T23:22:39.244454: step 3000, loss 0.620371, acc [0.92373047 0.87607422 0.83105469 0.77568359]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-23T23:22:42.685238: step 3000, loss 0.497959, acc [0.93537827 0.89644505 0.86118592 0.80854749] \n",
      "\n",
      "2019-02-23T23:22:44.766403: step 3040, loss 0.641133, acc [0.92001953 0.87158203 0.82412109 0.77158203]\n",
      "2019-02-23T23:22:46.765780: step 3080, loss 0.612989, acc [0.92529297 0.87460938 0.83085937 0.77734375]\n",
      "2019-02-23T23:22:48.767138: step 3120, loss 0.631049, acc [0.92070312 0.86748047 0.82568359 0.76777344]\n",
      "2019-02-23T23:22:50.916307: step 3160, loss 0.425289, acc [0.93966225 0.90406507 0.88075383 0.82391986]\n",
      "2019-02-23T23:22:52.903282: step 3200, loss 0.385312, acc [0.94404297 0.91191406 0.88417969 0.83017578]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-23T23:22:56.335565: step 3200, loss 0.470269, acc [0.93590886 0.90076985 0.86830382 0.81406361] \n",
      "\n",
      "2019-02-23T23:22:58.403885: step 3240, loss 0.388342, acc [0.94570312 0.91269531 0.88388672 0.82939453]\n",
      "2019-02-23T23:23:00.399787: step 3280, loss 0.439511, acc [0.93759766 0.90263672 0.87392578 0.81992188]\n",
      "2019-02-23T23:23:02.399659: step 3320, loss 0.40073, acc [0.94091797 0.91269531 0.87998047 0.825     ]\n",
      "2019-02-23T23:23:04.400524: step 3360, loss 0.426043, acc [0.93808594 0.90400391 0.86953125 0.81298828]\n",
      "2019-02-23T23:23:06.395434: step 3400, loss 0.42067, acc [0.93759766 0.90498047 0.87822266 0.82119141]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-23T23:23:09.832714: step 3400, loss 0.453814, acc [0.93763077 0.90241168 0.87519146 0.82038062] \n",
      "\n",
      "2019-02-23T23:23:11.933274: step 3440, loss 0.431816, acc [0.93867188 0.89951172 0.87587891 0.81640625]\n",
      "2019-02-23T23:23:13.949631: step 3480, loss 0.417679, acc [0.93876953 0.90683594 0.87431641 0.81914062]\n",
      "2019-02-23T23:23:15.942021: step 3520, loss 0.429374, acc [0.93769531 0.90429688 0.87080078 0.81347656]\n",
      "2019-02-23T23:23:17.936931: step 3560, loss 0.443278, acc [0.93652344 0.90029297 0.87089844 0.81289062]\n",
      "2019-02-23T23:23:20.028066: step 3600, loss 0.434206, acc [0.93466797 0.90361328 0.87197266 0.81259766]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-23T23:23:23.605259: step 3600, loss 0.44973, acc [0.93758071 0.90294226 0.8764829  0.82041065] \n",
      "\n",
      "2019-02-23T23:23:25.715202: step 3640, loss 0.451341, acc [0.93681641 0.90087891 0.86621094 0.81181641]\n",
      "2019-02-23T23:23:27.769136: step 3680, loss 0.440706, acc [0.93476563 0.90068359 0.87158203 0.8125    ]\n",
      "2019-02-23T23:23:29.768016: step 3720, loss 0.453408, acc [0.93896484 0.89824219 0.87011719 0.81376953]\n",
      "2019-02-23T23:23:31.765903: step 3760, loss 0.43657, acc [0.93720703 0.90195313 0.87304688 0.81425781]\n",
      "2019-02-23T23:23:33.766594: step 3800, loss 0.443572, acc [0.93857422 0.90253906 0.86845703 0.81542969]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-23T23:23:37.210867: step 3800, loss 0.438927, acc [0.93980318 0.90651623 0.87804463 0.82422489] \n",
      "\n",
      "2019-02-23T23:23:39.315343: step 3840, loss 0.462192, acc [0.93642578 0.89892578 0.86367187 0.80810547]\n",
      "2019-02-23T23:23:41.347951: step 3880, loss 0.467751, acc [0.93203125 0.89873047 0.86689453 0.80869141]\n",
      "2019-02-23T23:23:43.348814: step 3920, loss 0.451448, acc [0.93466797 0.90019531 0.86601562 0.80888672]\n",
      "2019-02-23T23:23:45.373006: step 3960, loss 0.454384, acc [0.93291016 0.89892578 0.86962891 0.81220703]\n",
      "2019-02-23T23:23:47.405596: step 4000, loss 0.458766, acc [0.93759766 0.90625    0.86923828 0.81728516]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-23T23:23:50.835965: step 4000, loss 0.414671, acc [0.94330707 0.91197229 0.88553294 0.83363534] \n",
      "\n",
      "2019-02-23T23:23:52.941499: step 4040, loss 0.453692, acc [0.93652344 0.90068359 0.86611328 0.80908203]\n",
      "2019-02-23T23:23:54.934380: step 4080, loss 0.473548, acc [0.93164062 0.89648438 0.86416016 0.80566406]\n",
      "2019-02-23T23:23:56.929824: step 4120, loss 0.46439, acc [0.93515625 0.90644531 0.87158203 0.81572266]\n",
      "2019-02-23T23:23:58.970870: step 4160, loss 0.474957, acc [0.93486328 0.89560547 0.86220703 0.80810547]\n",
      "2019-02-23T23:24:00.969705: step 4200, loss 0.467521, acc [0.9375     0.89716797 0.8640625  0.80947266]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-23T23:24:04.379744: step 4200, loss 0.402397, acc [0.94247615 0.91142168 0.8889167  0.83554746] \n",
      "\n",
      "2019-02-23T23:24:06.513992: step 4240, loss 0.45403, acc [0.93671875 0.90732422 0.86787109 0.81806641]\n",
      "2019-02-23T23:24:08.553064: step 4280, loss 0.447562, acc [0.94003906 0.90371094 0.86689453 0.81669922]\n",
      "2019-02-23T23:24:10.558872: step 4320, loss 0.475326, acc [0.93603516 0.89501953 0.86572266 0.80810547]\n",
      "2019-02-23T23:24:12.586553: step 4360, loss 0.45083, acc [0.93740234 0.90087891 0.86826172 0.81523437]\n",
      "2019-02-23T23:24:14.613391: step 4400, loss 0.458913, acc [0.93779297 0.89804688 0.86650391 0.81220703]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-23T23:24:18.094319: step 4400, loss 0.387635, acc [0.94641052 0.91766861 0.89162971 0.84137393] \n",
      "\n",
      "2019-02-23T23:24:20.239517: step 4440, loss 0.45438, acc [0.93710938 0.90195313 0.86572266 0.81445312]\n",
      "2019-02-23T23:24:22.254767: step 4480, loss 0.479065, acc [0.93476563 0.89726562 0.86044922 0.80722656]\n",
      "2019-02-23T23:24:24.263564: step 4520, loss 0.459909, acc [0.93916016 0.90097656 0.86816406 0.81650391]\n",
      "2019-02-23T23:24:26.286252: step 4560, loss 0.452584, acc [0.93642578 0.9        0.86669922 0.81220703]\n",
      "2019-02-23T23:24:28.281659: step 4600, loss 0.459249, acc [0.93632812 0.90664062 0.8703125  0.82011719]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-23T23:24:31.725441: step 4600, loss 0.378941, acc [0.94723143 0.91758852 0.89366197 0.84352631] \n",
      "\n",
      "2019-02-23T23:24:33.849258: step 4640, loss 0.452599, acc [0.93896484 0.90087891 0.86582031 0.81269531]\n",
      "2019-02-23T23:24:35.865497: step 4680, loss 0.481149, acc [0.93505859 0.89492187 0.86298828 0.81132812]\n",
      "2019-02-23T23:24:38.009209: step 4720, loss 0.299161, acc [0.95017361 0.93009194 0.908648   0.8593099 ]\n",
      "2019-02-23T23:24:40.034873: step 4760, loss 0.267988, acc [0.95751953 0.93691406 0.915625   0.86992187]\n",
      "2019-02-23T23:24:42.081865: step 4800, loss 0.270796, acc [0.95361328 0.93115234 0.91484375 0.8640625 ]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-23T23:24:45.568462: step 4800, loss 0.359143, acc [0.94967414 0.92201343 0.90203125 0.85218593] \n",
      "\n",
      "2019-02-23T23:24:47.679438: step 4840, loss 0.275141, acc [0.95390625 0.92861328 0.91787109 0.86503906]\n",
      "2019-02-23T23:24:49.717998: step 4880, loss 0.287851, acc [0.95380859 0.92919922 0.91289062 0.86328125]\n",
      "2019-02-23T23:24:51.721339: step 4920, loss 0.291475, acc [0.95068359 0.93212891 0.90908203 0.85859375]\n",
      "2019-02-23T23:24:53.735100: step 4960, loss 0.289972, acc [0.95380859 0.93056641 0.90947266 0.85761719]\n",
      "2019-02-23T23:24:55.759772: step 5000, loss 0.283553, acc [0.94912109 0.92675781 0.90957031 0.85478516]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-23T23:24:59.232761: step 5000, loss 0.365329, acc [0.94949394 0.92255404 0.90055962 0.85102464] \n",
      "\n",
      "2019-02-23T23:25:01.351178: step 5040, loss 0.274846, acc [0.9546875  0.93242187 0.91142578 0.85878906]\n",
      "2019-02-23T23:25:03.348074: step 5080, loss 0.285448, acc [0.95205078 0.92744141 0.91269531 0.85771484]\n",
      "2019-02-23T23:25:05.370264: step 5120, loss 0.315171, acc [0.94912109 0.92646484 0.90283203 0.84794922]\n",
      "2019-02-23T23:25:07.410809: step 5160, loss 0.305402, acc [0.94970703 0.92666016 0.90341797 0.85019531]\n",
      "2019-02-23T23:25:09.413160: step 5200, loss 0.325852, acc [0.9453125  0.92617187 0.89892578 0.84326172]\n",
      "\n",
      "Evaluation:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-02-23T23:25:12.844877: step 5200, loss 0.359414, acc [0.94832264 0.92444614 0.9027921  0.85308693] \n",
      "\n",
      "2019-02-23T23:25:14.969292: step 5240, loss 0.297976, acc [0.95419922 0.92685547 0.90732422 0.85527344]\n",
      "2019-02-23T23:25:16.976847: step 5280, loss 0.317823, acc [0.95078125 0.92246094 0.90351563 0.84785156]\n",
      "2019-02-23T23:25:18.998008: step 5320, loss 0.313422, acc [0.95214844 0.92480469 0.90410156 0.85400391]\n",
      "2019-02-23T23:25:21.020229: step 5360, loss 0.309869, acc [0.9546875  0.92685547 0.90556641 0.85556641]\n",
      "2019-02-23T23:25:23.012134: step 5400, loss 0.328198, acc [0.94794922 0.92578125 0.89824219 0.84873047]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-23T23:25:26.458340: step 5400, loss 0.3553, acc [0.94902342 0.92490665 0.90362302 0.85371763] \n",
      "\n",
      "2019-02-23T23:25:28.553941: step 5440, loss 0.324988, acc [0.94726562 0.92392578 0.90292969 0.85107422]\n",
      "2019-02-23T23:25:30.604403: step 5480, loss 0.328324, acc [0.94863281 0.92011719 0.90058594 0.84560547]\n",
      "2019-02-23T23:25:32.634036: step 5520, loss 0.328239, acc [0.95068359 0.92451172 0.90371094 0.85136719]\n",
      "2019-02-23T23:25:34.664162: step 5560, loss 0.332849, acc [0.94697266 0.92294922 0.89873047 0.84648437]\n",
      "2019-02-23T23:25:36.711155: step 5600, loss 0.329723, acc [0.94892578 0.92236328 0.90029297 0.84970703]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-23T23:25:40.221840: step 5600, loss 0.344167, acc [0.94983432 0.92681877 0.90713692 0.8573016 ] \n",
      "\n",
      "2019-02-23T23:25:42.367041: step 5640, loss 0.325126, acc [0.94941406 0.92119141 0.90283203 0.85      ]\n",
      "2019-02-23T23:25:44.449744: step 5680, loss 0.315114, acc [0.94746094 0.92509766 0.9015625  0.84638672]\n",
      "2019-02-23T23:25:46.516574: step 5720, loss 0.333247, acc [0.94970703 0.91953125 0.89970703 0.84443359]\n",
      "2019-02-23T23:25:48.549182: step 5760, loss 0.345078, acc [0.95117188 0.921875   0.89716797 0.84423828]\n",
      "2019-02-23T23:25:50.606591: step 5800, loss 0.330771, acc [0.94746094 0.91953125 0.89980469 0.846875  ]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-23T23:25:54.057756: step 5800, loss 0.337784, acc [0.9508955  0.92778984 0.90929932 0.86029493] \n",
      "\n",
      "2019-02-23T23:25:56.160796: step 5840, loss 0.325995, acc [0.94921875 0.92470703 0.90087891 0.84599609]\n",
      "2019-02-23T23:25:58.199899: step 5880, loss 0.345016, acc [0.94580078 0.92148438 0.89541016 0.84160156]\n",
      "2019-02-23T23:26:00.238906: step 5920, loss 0.343853, acc [0.94697266 0.92597656 0.89746094 0.84628906]\n",
      "2019-02-23T23:26:02.340458: step 5960, loss 0.36178, acc [0.94746094 0.92197266 0.89628906 0.84638672]\n",
      "2019-02-23T23:26:04.339835: step 6000, loss 0.353824, acc [0.94433594 0.92001953 0.89492187 0.84179688]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-23T23:26:07.756777: step 6000, loss 0.324752, acc [0.95336824 0.93109351 0.91211244 0.86514031] \n",
      "\n",
      "2019-02-23T23:26:09.883624: step 6040, loss 0.338964, acc [0.94521484 0.921875   0.89726562 0.84404297]\n",
      "2019-02-23T23:26:11.960871: step 6080, loss 0.359538, acc [0.94511719 0.92011719 0.89238281 0.83808594]\n",
      "2019-02-23T23:26:14.012824: step 6120, loss 0.343243, acc [0.94892578 0.92197266 0.89433594 0.84482422]\n",
      "2019-02-23T23:26:16.042950: step 6160, loss 0.338969, acc [0.94775391 0.92363281 0.89628906 0.84482422]\n",
      "2019-02-23T23:26:18.090440: step 6200, loss 0.345235, acc [0.94628906 0.91982422 0.89462891 0.84140625]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-23T23:26:21.526725: step 6200, loss 0.31567, acc [0.9538788  0.93202455 0.91501567 0.86775321] \n",
      "\n",
      "2019-02-23T23:26:23.636213: step 6240, loss 0.341661, acc [0.95107422 0.92138672 0.89619141 0.84453125]\n",
      "2019-02-23T23:26:25.884580: step 6280, loss 0.211482, acc [0.96254735 0.9474274  0.93495107 0.88760653]\n",
      "2019-02-23T23:26:27.932563: step 6320, loss 0.190092, acc [0.9640625  0.95029297 0.94111328 0.89785156]\n",
      "2019-02-23T23:26:29.975589: step 6360, loss 0.192684, acc [0.96337891 0.94580078 0.93984375 0.88935547]\n",
      "2019-02-23T23:26:31.993315: step 6400, loss 0.201053, acc [0.96542969 0.94550781 0.93662109 0.88955078]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-23T23:26:35.432628: step 6400, loss 0.319848, acc [0.95359849 0.93481765 0.91752846 0.87051627] \n",
      "\n",
      "2019-02-23T23:26:37.542561: step 6440, loss 0.206576, acc [0.96142578 0.94824219 0.93603516 0.88847656]\n",
      "2019-02-23T23:26:39.581616: step 6480, loss 0.209405, acc [0.96171875 0.94453125 0.93154297 0.88496094]\n",
      "2019-02-23T23:26:41.643983: step 6520, loss 0.20916, acc [0.96494141 0.94833984 0.93447266 0.89306641]\n",
      "2019-02-23T23:26:43.692510: step 6560, loss 0.208977, acc [0.96035156 0.94306641 0.934375   0.88427734]\n",
      "2019-02-23T23:26:45.715646: step 6600, loss 0.218055, acc [0.95830078 0.940625   0.93095703 0.87958984]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-23T23:26:49.155407: step 6600, loss 0.32062, acc [0.95519026 0.93390664 0.91754848 0.87164753] \n",
      "\n",
      "2019-02-23T23:26:51.281260: step 6640, loss 0.219421, acc [0.96054688 0.94208984 0.93095703 0.88173828]\n",
      "2019-02-23T23:26:53.299484: step 6680, loss 0.213952, acc [0.95947266 0.94335938 0.9328125  0.88212891]\n",
      "2019-02-23T23:26:55.377724: step 6720, loss 0.221623, acc [0.96201172 0.94521484 0.93037109 0.88398438]\n",
      "2019-02-23T23:26:57.400410: step 6760, loss 0.211527, acc [0.96171875 0.94599609 0.93271484 0.88408203]\n",
      "2019-02-23T23:26:59.452859: step 6800, loss 0.238059, acc [0.95947266 0.94199219 0.92373047 0.87714844]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-23T23:27:02.893610: step 6800, loss 0.31835, acc [0.95527035 0.93438717 0.91844948 0.87237834] \n",
      "\n",
      "2019-02-23T23:27:05.026408: step 6840, loss 0.250311, acc [0.95800781 0.93818359 0.92089844 0.87128906]\n",
      "2019-02-23T23:27:07.058762: step 6880, loss 0.216167, acc [0.95820313 0.94599609 0.93291016 0.88359375]\n",
      "2019-02-23T23:27:09.108233: step 6920, loss 0.233468, acc [0.95566406 0.93955078 0.92705078 0.87734375]\n",
      "2019-02-23T23:27:11.134887: step 6960, loss 0.2479, acc [0.95810547 0.93769531 0.92333984 0.8734375 ]\n",
      "2019-02-23T23:27:13.163527: step 7000, loss 0.239536, acc [0.95791016 0.93886719 0.92685547 0.87753906]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-23T23:27:16.614198: step 7000, loss 0.31359, acc [0.95482986 0.93622921 0.91990109 0.87451071] \n",
      "\n",
      "2019-02-23T23:27:18.728150: step 7040, loss 0.231534, acc [0.95771484 0.94238281 0.92470703 0.8765625 ]\n",
      "2019-02-23T23:27:20.757521: step 7080, loss 0.244253, acc [0.95615234 0.93769531 0.92138672 0.86796875]\n",
      "2019-02-23T23:27:22.800092: step 7120, loss 0.242691, acc [0.9578125  0.93808594 0.92587891 0.87441406]\n",
      "2019-02-23T23:27:24.827198: step 7160, loss 0.248602, acc [0.95859375 0.93837891 0.92363281 0.87470703]\n",
      "2019-02-23T23:27:26.856830: step 7200, loss 0.249543, acc [0.95605469 0.93720703 0.92148438 0.87128906]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-23T23:27:30.317396: step 7200, loss 0.307807, acc [0.95677202 0.93782098 0.92059186 0.8763928 ] \n",
      "\n",
      "2019-02-23T23:27:32.442259: step 7240, loss 0.238894, acc [0.95761719 0.93525391 0.92216797 0.86953125]\n",
      "2019-02-23T23:27:34.463456: step 7280, loss 0.25908, acc [0.96015625 0.93691406 0.91650391 0.86894531]\n",
      "2019-02-23T23:27:36.511442: step 7320, loss 0.254368, acc [0.95771484 0.93574219 0.92304688 0.87324219]\n",
      "2019-02-23T23:27:38.555501: step 7360, loss 0.24825, acc [0.96230469 0.94082031 0.92011719 0.87470703]\n",
      "2019-02-23T23:27:40.585584: step 7400, loss 0.266304, acc [0.95322266 0.93369141 0.91865234 0.86591797]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-23T23:27:44.032286: step 7400, loss 0.305561, acc [0.95675199 0.93742054 0.92116249 0.87625264] \n",
      "\n",
      "2019-02-23T23:27:46.159136: step 7440, loss 0.24844, acc [0.95693359 0.93544922 0.92226562 0.87138672]\n",
      "2019-02-23T23:27:48.193230: step 7480, loss 0.275528, acc [0.95556641 0.93457031 0.91210938 0.86357422]\n",
      "2019-02-23T23:27:50.253613: step 7520, loss 0.257815, acc [0.9578125  0.93339844 0.92041016 0.86855469]\n",
      "2019-02-23T23:27:52.250012: step 7560, loss 0.257671, acc [0.96220703 0.93574219 0.91904297 0.87539062]\n",
      "2019-02-23T23:27:54.289068: step 7600, loss 0.259484, acc [0.95751953 0.93808594 0.92128906 0.87548828]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-23T23:27:57.747674: step 7600, loss 0.290774, acc [0.95742274 0.94025368 0.92569753 0.8802671 ] \n",
      "\n",
      "2019-02-23T23:27:59.886426: step 7640, loss 0.251516, acc [0.95888672 0.93984375 0.92128906 0.87197266]\n",
      "2019-02-23T23:28:01.935401: step 7680, loss 0.263295, acc [0.95898438 0.93457031 0.91787109 0.86923828]\n",
      "2019-02-23T23:28:03.974999: step 7720, loss 0.263083, acc [0.95654297 0.9359375  0.91503906 0.86914062]\n",
      "2019-02-23T23:28:06.020953: step 7760, loss 0.273543, acc [0.95478516 0.93662109 0.91611328 0.86542969]\n",
      "2019-02-23T23:28:08.043639: step 7800, loss 0.268722, acc [0.95888672 0.93330078 0.91611328 0.86464844]\n",
      "\n",
      "Evaluation:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-02-23T23:28:11.531050: step 7800, loss 0.287837, acc [0.95833375 0.94206569 0.92605792 0.88266976] \n",
      "\n",
      "2019-02-23T23:28:13.800214: step 7840, loss 0.176457, acc [0.96725162 0.95399404 0.94486663 0.9005376 ]\n",
      "2019-02-23T23:28:15.841253: step 7880, loss 0.152352, acc [0.96728516 0.95507812 0.95048828 0.90615234]\n",
      "2019-02-23T23:28:17.886298: step 7920, loss 0.145707, acc [0.96962891 0.95820313 0.95429688 0.91132813]\n",
      "2019-02-23T23:28:19.923333: step 7960, loss 0.151341, acc [0.96933594 0.95742187 0.9515625  0.90839844]\n",
      "2019-02-23T23:28:21.973797: step 8000, loss 0.152499, acc [0.96757812 0.95761719 0.95078125 0.90664062]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-23T23:28:25.404868: step 8000, loss 0.299989, acc [0.95941495 0.94195557 0.92767972 0.88510246] \n",
      "\n",
      "2019-02-23T23:28:27.530227: step 8040, loss 0.159248, acc [0.96738281 0.95722656 0.94746094 0.90322266]\n",
      "2019-02-23T23:28:29.578212: step 8080, loss 0.160425, acc [0.96904297 0.95429688 0.94746094 0.90185547]\n",
      "2019-02-23T23:28:31.602883: step 8120, loss 0.166223, acc [0.96689453 0.95214844 0.94716797 0.90341797]\n",
      "2019-02-23T23:28:33.650865: step 8160, loss 0.176144, acc [0.96699219 0.953125   0.94443359 0.90117187]\n",
      "2019-02-23T23:28:35.678018: step 8200, loss 0.165174, acc [0.96689453 0.95175781 0.94941406 0.90361328]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-23T23:28:39.152495: step 8200, loss 0.297798, acc [0.95992552 0.94158516 0.92981209 0.8871147 ] \n",
      "\n",
      "2019-02-23T23:28:41.278848: step 8240, loss 0.159409, acc [0.96748047 0.95488281 0.94667969 0.90166016]\n",
      "2019-02-23T23:28:43.361056: step 8280, loss 0.167867, acc [0.96318359 0.94912109 0.9484375  0.89863281]\n",
      "2019-02-23T23:28:45.393662: step 8320, loss 0.171709, acc [0.96962891 0.95546875 0.94492188 0.90205078]\n",
      "2019-02-23T23:28:47.445614: step 8360, loss 0.173547, acc [0.96699219 0.95224609 0.94443359 0.90087891]\n",
      "2019-02-23T23:28:49.458912: step 8400, loss 0.181973, acc [0.96679688 0.95439453 0.94208984 0.9       ]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-23T23:28:52.902108: step 8400, loss 0.298434, acc [0.95948503 0.9409044  0.92878095 0.88508244] \n",
      "\n",
      "2019-02-23T23:28:55.018539: step 8440, loss 0.18155, acc [0.96464844 0.95234375 0.93935547 0.89501953]\n",
      "2019-02-23T23:28:57.064540: step 8480, loss 0.181925, acc [0.96210938 0.95009766 0.94375    0.896875  ]\n",
      "2019-02-23T23:28:59.086730: step 8520, loss 0.183152, acc [0.9625     0.94902344 0.94228516 0.89335937]\n",
      "2019-02-23T23:29:01.105946: step 8560, loss 0.176823, acc [0.96601563 0.95195312 0.940625   0.89550781]\n",
      "2019-02-23T23:29:03.165337: step 8600, loss 0.186841, acc [0.96132812 0.95039063 0.94121094 0.89404297]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-23T23:29:06.598203: step 8600, loss 0.291909, acc [0.96072641 0.9440579  0.93200452 0.88991781] \n",
      "\n",
      "2019-02-23T23:29:08.702184: step 8640, loss 0.194554, acc [0.96308594 0.95097656 0.93994141 0.89326172]\n",
      "2019-02-23T23:29:10.726855: step 8680, loss 0.195657, acc [0.96269531 0.94863281 0.93691406 0.88867188]\n",
      "2019-02-23T23:29:12.795671: step 8720, loss 0.192157, acc [0.96386719 0.95292969 0.93818359 0.89677734]\n",
      "2019-02-23T23:29:14.802487: step 8760, loss 0.197085, acc [0.96484375 0.94873047 0.93632812 0.89296875]\n",
      "2019-02-23T23:29:16.834598: step 8800, loss 0.185782, acc [0.96738281 0.95039063 0.94033203 0.89736328]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-23T23:29:20.299652: step 8800, loss 0.293, acc [0.96067635 0.94291664 0.92907127 0.88687443] \n",
      "\n",
      "2019-02-23T23:29:22.432451: step 8840, loss 0.19574, acc [0.965625   0.94960937 0.93808594 0.89375   ]\n",
      "2019-02-23T23:29:24.476963: step 8880, loss 0.196468, acc [0.96435547 0.94833984 0.93769531 0.89042969]\n",
      "2019-02-23T23:29:26.498659: step 8920, loss 0.208831, acc [0.96416016 0.94511719 0.93300781 0.88642578]\n",
      "2019-02-23T23:29:28.559538: step 8960, loss 0.19109, acc [0.965625   0.94921875 0.93691406 0.89248047]\n",
      "2019-02-23T23:29:30.577773: step 9000, loss 0.201175, acc [0.965625   0.94873047 0.93515625 0.88701172]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-23T23:29:34.028432: step 9000, loss 0.284333, acc [0.96172752 0.94452843 0.93123367 0.89055852] \n",
      "\n",
      "2019-02-23T23:29:36.137919: step 9040, loss 0.199182, acc [0.96337891 0.94736328 0.93798828 0.89238281]\n",
      "2019-02-23T23:29:38.175983: step 9080, loss 0.209385, acc [0.9625     0.9453125  0.9328125  0.88427734]\n",
      "2019-02-23T23:29:40.236862: step 9120, loss 0.208033, acc [0.96201172 0.94609375 0.93574219 0.8890625 ]\n",
      "2019-02-23T23:29:42.242234: step 9160, loss 0.213473, acc [0.96054688 0.94404297 0.93496094 0.88515625]\n",
      "2019-02-23T23:29:44.302575: step 9200, loss 0.214161, acc [0.96191406 0.94755859 0.93027344 0.88603516]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-23T23:29:47.765148: step 9200, loss 0.27771, acc [0.96078647 0.94556958 0.93264524 0.89172982] \n",
      "\n",
      "2019-02-23T23:29:49.894972: step 9240, loss 0.213291, acc [0.959375   0.94550781 0.93378906 0.88583984]\n",
      "2019-02-23T23:29:51.919148: step 9280, loss 0.215764, acc [0.9625     0.94570312 0.93232422 0.88789063]\n",
      "2019-02-23T23:29:53.949771: step 9320, loss 0.212618, acc [0.95986328 0.94492188 0.93408203 0.88369141]\n",
      "2019-02-23T23:29:55.991802: step 9360, loss 0.227808, acc [0.96220703 0.94775391 0.92617187 0.88349609]\n",
      "2019-02-23T23:29:58.134522: step 9400, loss 0.14058, acc [0.97272037 0.96263317 0.9550801  0.91823015]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-23T23:30:01.569816: step 9400, loss 0.276722, acc [0.96216801 0.94615023 0.93491776 0.89511358] \n",
      "\n",
      "2019-02-23T23:30:03.693733: step 9440, loss 0.116962, acc [0.9734375  0.96640625 0.96220703 0.92412109]\n",
      "2019-02-23T23:30:05.739191: step 9480, loss 0.121513, acc [0.97197266 0.96279297 0.96064453 0.9203125 ]\n",
      "2019-02-23T23:30:07.783703: step 9520, loss 0.117558, acc [0.97373047 0.96611328 0.96142578 0.92382812]\n",
      "2019-02-23T23:30:09.804939: step 9560, loss 0.126553, acc [0.97246094 0.96025391 0.95898438 0.91689453]\n",
      "2019-02-23T23:30:11.858341: step 9600, loss 0.129841, acc [0.97392578 0.96542969 0.95634766 0.91953125]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-23T23:30:15.333812: step 9600, loss 0.288543, acc [0.96286878 0.94748171 0.9354083  0.89665529] \n",
      "\n",
      "2019-02-23T23:30:17.447267: step 9640, loss 0.124666, acc [0.97451172 0.96113281 0.96035156 0.91816406]\n",
      "2019-02-23T23:30:19.511264: step 9680, loss 0.128604, acc [0.97294922 0.96308594 0.95654297 0.91748047]\n",
      "2019-02-23T23:30:21.538416: step 9720, loss 0.129393, acc [0.97060547 0.96337891 0.95839844 0.91865234]\n",
      "2019-02-23T23:30:23.590862: step 9760, loss 0.152471, acc [0.97021484 0.95947266 0.95136719 0.91142578]\n",
      "2019-02-23T23:30:25.611072: step 9800, loss 0.130288, acc [0.97470703 0.96582031 0.95878906 0.92138672]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-23T23:30:29.068773: step 9800, loss 0.285436, acc [0.9626085  0.94732153 0.93532821 0.89677542] \n",
      "\n",
      "2019-02-23T23:30:31.174790: step 9840, loss 0.129359, acc [0.9734375  0.96123047 0.959375   0.91591797]\n",
      "2019-02-23T23:30:33.238643: step 9880, loss 0.139489, acc [0.97099609 0.96582031 0.95664063 0.91972656]\n",
      "2019-02-23T23:30:35.286628: step 9920, loss 0.146887, acc [0.96865234 0.95761719 0.95224609 0.90888672]\n",
      "2019-02-23T23:30:37.327666: step 9960, loss 0.144364, acc [0.97001953 0.9609375  0.95449219 0.91396484]\n",
      "2019-02-23T23:30:39.424755: step 10000, loss 0.145637, acc [0.97138672 0.96035156 0.95195312 0.91171875]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-23T23:30:42.975360: step 10000, loss 0.292673, acc [0.96303897 0.94795223 0.93401676 0.89541391] \n",
      "\n",
      "2019-02-23T23:30:45.095216: step 10040, loss 0.15269, acc [0.97089844 0.95712891 0.95341797 0.91347656]\n",
      "2019-02-23T23:30:47.137249: step 10080, loss 0.14903, acc [0.96777344 0.95810547 0.95166016 0.90908203]\n",
      "2019-02-23T23:30:49.214993: step 10120, loss 0.158472, acc [0.96552734 0.95195312 0.95019531 0.90205078]\n",
      "2019-02-23T23:30:51.264464: step 10160, loss 0.165129, acc [0.96865234 0.95673828 0.94580078 0.90273437]\n",
      "2019-02-23T23:30:53.302030: step 10200, loss 0.158381, acc [0.97148437 0.95576172 0.94628906 0.90556641]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-23T23:30:56.732862: step 10200, loss 0.285704, acc [0.96314909 0.94837269 0.93514801 0.89715585] \n",
      "\n",
      "2019-02-23T23:30:58.893935: step 10240, loss 0.165937, acc [0.96777344 0.95517578 0.94882813 0.90566406]\n",
      "2019-02-23T23:31:00.943405: step 10280, loss 0.164467, acc [0.96669922 0.95419922 0.94589844 0.90224609]\n",
      "2019-02-23T23:31:03.031116: step 10320, loss 0.168275, acc [0.96757812 0.95703125 0.94384766 0.9015625 ]\n",
      "2019-02-23T23:31:05.070619: step 10360, loss 0.153872, acc [0.96767578 0.95625    0.95019531 0.90605469]\n",
      "2019-02-23T23:31:07.092314: step 10400, loss 0.153071, acc [0.971875   0.95927734 0.94882813 0.91201172]\n",
      "\n",
      "Evaluation:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-02-23T23:31:10.524188: step 10400, loss 0.281217, acc [0.96292885 0.94802231 0.93650952 0.89852737] \n",
      "\n",
      "2019-02-23T23:31:12.666857: step 10440, loss 0.153128, acc [0.96582031 0.95742187 0.95009766 0.90644531]\n",
      "2019-02-23T23:31:14.740634: step 10480, loss 0.151734, acc [0.97148437 0.96015625 0.94931641 0.90908203]\n",
      "2019-02-23T23:31:16.798535: step 10520, loss 0.167277, acc [0.96728516 0.95361328 0.94560547 0.90380859]\n",
      "2019-02-23T23:31:18.844534: step 10560, loss 0.158901, acc [0.96962891 0.95429688 0.94833984 0.90488281]\n",
      "2019-02-23T23:31:20.865239: step 10600, loss 0.173289, acc [0.96650391 0.95126953 0.94443359 0.89931641]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-23T23:31:24.317397: step 10600, loss 0.27919, acc [0.96412017 0.94885323 0.93771086 0.89952848] \n",
      "\n",
      "2019-02-23T23:31:26.480949: step 10640, loss 0.168931, acc [0.96660156 0.95634766 0.94335938 0.90263672]\n",
      "2019-02-23T23:31:28.528255: step 10680, loss 0.173349, acc [0.96533203 0.95566406 0.94443359 0.90107422]\n",
      "2019-02-23T23:31:30.561855: step 10720, loss 0.164026, acc [0.96669922 0.95429688 0.94521484 0.90087891]\n",
      "2019-02-23T23:31:32.607357: step 10760, loss 0.167056, acc [0.9671875  0.95644531 0.94599609 0.90195313]\n",
      "2019-02-23T23:31:34.650381: step 10800, loss 0.165703, acc [0.96845703 0.95849609 0.94423828 0.90507812]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-23T23:31:38.093161: step 10800, loss 0.276096, acc [0.96493107 0.95049505 0.93710018 0.90061969] \n",
      "\n",
      "2019-02-23T23:31:40.231869: step 10840, loss 0.186291, acc [0.96601563 0.95292969 0.93886719 0.89619141]\n",
      "2019-02-23T23:31:42.251581: step 10880, loss 0.172911, acc [0.96552734 0.95390625 0.94453125 0.90136719]\n",
      "2019-02-23T23:31:44.317419: step 10920, loss 0.180103, acc [0.96669922 0.95224609 0.94033203 0.89765625]\n",
      "2019-02-23T23:31:46.486457: step 10960, loss 0.113275, acc [0.97382319 0.96885456 0.96450047 0.92846038]\n",
      "2019-02-23T23:31:48.515571: step 11000, loss 0.0951036, acc [0.97929687 0.97255859 0.96845703 0.9359375 ]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-23T23:31:52.007895: step 11000, loss 0.280882, acc [0.96559181 0.95146613 0.94084434 0.90418364] \n",
      "\n",
      "2019-02-23T23:31:54.133268: step 11040, loss 0.0942028, acc [0.97558594 0.96982422 0.97070312 0.93457031]\n",
      "2019-02-23T23:31:56.161412: step 11080, loss 0.100628, acc [0.97568359 0.96904297 0.96816406 0.93291016]\n",
      "2019-02-23T23:31:58.183601: step 11120, loss 0.100002, acc [0.97685547 0.97089844 0.96552734 0.93212891]\n",
      "2019-02-23T23:32:00.248452: step 11160, loss 0.111018, acc [0.97607422 0.96660156 0.96445313 0.92763672]\n",
      "2019-02-23T23:32:02.275601: step 11200, loss 0.107292, acc [0.97441406 0.96757812 0.96513672 0.92714844]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-23T23:32:05.728785: step 11200, loss 0.2879, acc [0.9663026  0.95029483 0.94101453 0.90443392] \n",
      "\n",
      "2019-02-23T23:32:07.845183: step 11240, loss 0.112493, acc [0.97246094 0.96826172 0.96425781 0.92685547]\n",
      "2019-02-23T23:32:09.888702: step 11280, loss 0.108379, acc [0.97607422 0.9703125  0.96445313 0.93105469]\n",
      "2019-02-23T23:32:11.946606: step 11320, loss 0.115422, acc [0.97460938 0.96816406 0.96455078 0.92871094]\n",
      "2019-02-23T23:32:14.049646: step 11360, loss 0.117645, acc [0.97392578 0.96552734 0.96347656 0.92587891]\n",
      "2019-02-23T23:32:16.076799: step 11400, loss 0.118091, acc [0.97412109 0.96445313 0.95996094 0.92265625]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-23T23:32:19.566276: step 11400, loss 0.286134, acc [0.96636266 0.95207681 0.94059406 0.90565528] \n",
      "\n",
      "2019-02-23T23:32:21.697039: step 11440, loss 0.120337, acc [0.97167969 0.96435547 0.95966797 0.92060547]\n",
      "2019-02-23T23:32:23.744032: step 11480, loss 0.131439, acc [0.97587891 0.9625     0.95507812 0.91962891]\n",
      "2019-02-23T23:32:25.801935: step 11520, loss 0.117532, acc [0.97285156 0.96787109 0.96064453 0.92392578]\n",
      "2019-02-23T23:32:27.850909: step 11560, loss 0.123238, acc [0.97705078 0.96777344 0.95859375 0.92480469]\n",
      "2019-02-23T23:32:29.901869: step 11600, loss 0.121433, acc [0.97373047 0.96630859 0.95996094 0.92333984]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-23T23:32:33.330220: step 11600, loss 0.292077, acc [0.96546166 0.95248726 0.93863188 0.90356295] \n",
      "\n",
      "2019-02-23T23:32:35.460542: step 11640, loss 0.123338, acc [0.97265625 0.96787109 0.95732422 0.91757813]\n",
      "2019-02-23T23:32:37.514972: step 11680, loss 0.124601, acc [0.97070312 0.96269531 0.95722656 0.91513672]\n",
      "2019-02-23T23:32:39.545099: step 11720, loss 0.132246, acc [0.97382813 0.96279297 0.95800781 0.92089844]\n",
      "2019-02-23T23:32:41.623835: step 11760, loss 0.128868, acc [0.97060547 0.9625     0.95849609 0.91552734]\n",
      "2019-02-23T23:32:43.663385: step 11800, loss 0.131978, acc [0.97392578 0.9625     0.95712891 0.91914063]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-23T23:32:47.134888: step 11800, loss 0.285572, acc [0.96622251 0.9516263  0.93979317 0.9052248 ] \n",
      "\n",
      "2019-02-23T23:32:49.282594: step 11840, loss 0.138628, acc [0.97490234 0.96611328 0.95302734 0.91777344]\n",
      "2019-02-23T23:32:51.355848: step 11880, loss 0.136164, acc [0.97070312 0.96123047 0.95625    0.91621094]\n",
      "2019-02-23T23:32:53.402342: step 11920, loss 0.128514, acc [0.97353516 0.96640625 0.95771484 0.92011719]\n",
      "2019-02-23T23:32:55.435943: step 11960, loss 0.147808, acc [0.97265625 0.9640625  0.9515625  0.91630859]\n",
      "2019-02-23T23:32:57.458630: step 12000, loss 0.144097, acc [0.96914062 0.96005859 0.95146484 0.91054687]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-23T23:33:00.921205: step 12000, loss 0.284216, acc [0.96524142 0.95084544 0.93896225 0.90277208] \n",
      "\n",
      "2019-02-23T23:33:03.061940: step 12040, loss 0.138433, acc [0.97119141 0.96142578 0.95742187 0.91669922]\n",
      "2019-02-23T23:33:05.096037: step 12080, loss 0.129857, acc [0.97353516 0.96113281 0.95595703 0.91699219]\n",
      "2019-02-23T23:33:07.152453: step 12120, loss 0.148431, acc [0.97246094 0.96005859 0.95126953 0.91318359]\n",
      "2019-02-23T23:33:09.226724: step 12160, loss 0.14683, acc [0.97216797 0.96074219 0.94921875 0.91113281]\n",
      "2019-02-23T23:33:11.223620: step 12200, loss 0.144477, acc [0.97314453 0.96445313 0.95488281 0.91923828]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-23T23:33:14.704049: step 12200, loss 0.280866, acc [0.96592217 0.95198671 0.94122476 0.90587552] \n",
      "\n",
      "2019-02-23T23:33:16.844784: step 12240, loss 0.154764, acc [0.96933594 0.95517578 0.94931641 0.90546875]\n",
      "2019-02-23T23:33:18.895744: step 12280, loss 0.149605, acc [0.97207031 0.95927734 0.95214844 0.91298828]\n",
      "2019-02-23T23:33:20.925871: step 12320, loss 0.149543, acc [0.96923828 0.95966797 0.94902344 0.91054687]\n",
      "2019-02-23T23:33:22.999150: step 12360, loss 0.144865, acc [0.97060547 0.95888672 0.95087891 0.90800781]\n",
      "2019-02-23T23:33:25.054576: step 12400, loss 0.138142, acc [0.97207031 0.9625     0.95458984 0.91669922]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-23T23:33:28.515165: step 12400, loss 0.281266, acc [0.96708346 0.95320806 0.94049395 0.9067565 ] \n",
      "\n",
      "2019-02-23T23:33:30.629117: step 12440, loss 0.147027, acc [0.97001953 0.96240234 0.9515625  0.91357422]\n",
      "2019-02-23T23:33:32.692569: step 12480, loss 0.157041, acc [0.96826172 0.959375   0.94746094 0.90683594]\n",
      "2019-02-23T23:33:34.878928: step 12520, loss 0.103955, acc [0.97849886 0.97293245 0.96732067 0.93696141]\n",
      "2019-02-23T23:33:36.928896: step 12560, loss 0.0900538, acc [0.97832031 0.9734375  0.97021484 0.93759766]\n",
      "2019-02-23T23:33:38.970432: step 12600, loss 0.0915062, acc [0.97871094 0.97216797 0.97011719 0.93759766]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-23T23:33:42.417668: step 12600, loss 0.291739, acc [0.96731372 0.95292775 0.94201564 0.90766751] \n",
      "\n",
      "2019-02-23T23:33:44.573247: step 12640, loss 0.0933072, acc [0.97910156 0.97236328 0.96884766 0.93789062]\n",
      "2019-02-23T23:33:46.608332: step 12680, loss 0.0872653, acc [0.97617188 0.97626953 0.97158203 0.9390625 ]\n",
      "2019-02-23T23:33:48.662268: step 12720, loss 0.088345, acc [0.97744141 0.97314453 0.97226563 0.93945312]\n",
      "2019-02-23T23:33:50.680988: step 12760, loss 0.090705, acc [0.97607422 0.97197266 0.97197266 0.93603516]\n",
      "2019-02-23T23:33:52.731992: step 12800, loss 0.0894145, acc [0.98017578 0.97314453 0.97089844 0.93857422]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-23T23:33:56.183609: step 12800, loss 0.290469, acc [0.96690326 0.9535985  0.94293666 0.90917919] \n",
      "\n",
      "2019-02-23T23:33:58.305995: step 12840, loss 0.0988878, acc [0.97617188 0.97089844 0.96933594 0.93466797]\n",
      "2019-02-23T23:34:00.348520: step 12880, loss 0.0998769, acc [0.97705078 0.97246094 0.96708984 0.93291016]\n",
      "2019-02-23T23:34:02.405929: step 12920, loss 0.0995108, acc [0.97695312 0.96894531 0.96787109 0.93193359]\n",
      "2019-02-23T23:34:04.455399: step 12960, loss 0.0987654, acc [0.98027344 0.96689453 0.96806641 0.93320313]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-02-23T23:34:06.506855: step 13000, loss 0.102925, acc [0.97763672 0.96992188 0.965625   0.93183594]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-23T23:34:09.983319: step 13000, loss 0.291377, acc [0.96707345 0.95372864 0.94210574 0.90858853] \n",
      "\n",
      "2019-02-23T23:34:12.117111: step 13040, loss 0.0973514, acc [0.978125   0.97216797 0.96777344 0.93457031]\n",
      "2019-02-23T23:34:14.143797: step 13080, loss 0.103639, acc [0.97695312 0.96738281 0.96542969 0.93017578]\n",
      "2019-02-23T23:34:16.178361: step 13120, loss 0.107783, acc [0.98066406 0.97167969 0.96552734 0.93398437]\n",
      "2019-02-23T23:34:18.250148: step 13160, loss 0.110529, acc [0.978125   0.96835938 0.96435547 0.93125   ]\n",
      "2019-02-23T23:34:20.276307: step 13200, loss 0.103173, acc [0.97949219 0.96933594 0.96884766 0.93369141]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-23T23:34:23.767787: step 13200, loss 0.286078, acc [0.96715354 0.95344833 0.94309684 0.90956962] \n",
      "\n",
      "2019-02-23T23:34:25.888684: step 13240, loss 0.109829, acc [0.97216797 0.96855469 0.96494141 0.92705078]\n",
      "2019-02-23T23:34:27.928235: step 13280, loss 0.107056, acc [0.97558594 0.96884766 0.96523437 0.92832031]\n",
      "2019-02-23T23:34:29.983164: step 13320, loss 0.103904, acc [0.97929687 0.97109375 0.96669922 0.9359375 ]\n",
      "2019-02-23T23:34:32.007835: step 13360, loss 0.113249, acc [0.97626953 0.96503906 0.96337891 0.925     ]\n",
      "2019-02-23T23:34:34.056314: step 13400, loss 0.118771, acc [0.97451172 0.96982422 0.96044922 0.92597656]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-23T23:34:37.522856: step 13400, loss 0.288988, acc [0.96685321 0.95490995 0.9420757  0.90872869] \n",
      "\n",
      "2019-02-23T23:34:39.626390: step 13440, loss 0.108781, acc [0.97421875 0.96855469 0.9609375  0.92451172]\n",
      "2019-02-23T23:34:41.676397: step 13480, loss 0.124105, acc [0.97646484 0.96484375 0.96015625 0.92558594]\n",
      "2019-02-23T23:34:43.691110: step 13520, loss 0.122542, acc [0.97304687 0.96279297 0.95849609 0.91806641]\n",
      "2019-02-23T23:34:45.730663: step 13560, loss 0.127575, acc [0.97705078 0.965625   0.95839844 0.92216797]\n",
      "2019-02-23T23:34:47.771206: step 13600, loss 0.122554, acc [0.97382813 0.96289062 0.96035156 0.92138672]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-23T23:34:51.216420: step 13600, loss 0.284266, acc [0.96762406 0.95445945 0.94239606 0.91001011] \n",
      "\n",
      "2019-02-23T23:34:53.341781: step 13640, loss 0.127269, acc [0.97421875 0.96523437 0.95859375 0.92148438]\n",
      "2019-02-23T23:34:55.395218: step 13680, loss 0.124723, acc [0.97460938 0.96552734 0.95664063 0.92050781]\n",
      "2019-02-23T23:34:57.416418: step 13720, loss 0.118362, acc [0.97412109 0.96630859 0.95986328 0.92402344]\n",
      "2019-02-23T23:34:59.412712: step 13760, loss 0.122689, acc [0.97539062 0.96611328 0.95771484 0.92441406]\n",
      "2019-02-23T23:35:01.414073: step 13800, loss 0.132358, acc [0.97685547 0.96279297 0.95732422 0.92207031]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-23T23:35:04.876647: step 13800, loss 0.278688, acc [0.96832484 0.95500005 0.94299673 0.91113135] \n",
      "\n",
      "2019-02-23T23:35:06.945959: step 13840, loss 0.127213, acc [0.97285156 0.9640625  0.95878906 0.91806641]\n",
      "2019-02-23T23:35:08.940373: step 13880, loss 0.126242, acc [0.97363281 0.96484375 0.95830078 0.92226562]\n",
      "2019-02-23T23:35:10.936773: step 13920, loss 0.117514, acc [0.97451172 0.96289062 0.96074219 0.92167969]\n",
      "2019-02-23T23:35:12.922302: step 13960, loss 0.13357, acc [0.97451172 0.96445313 0.95644531 0.92050781]\n",
      "2019-02-23T23:35:14.917708: step 14000, loss 0.136512, acc [0.97089844 0.96269531 0.95517578 0.91689453]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-23T23:35:18.369422: step 14000, loss 0.277869, acc [0.96939603 0.95519026 0.94399784 0.91282323] \n",
      "\n",
      "2019-02-23T23:35:20.434763: step 14040, loss 0.130972, acc [0.9734375  0.96298828 0.95644531 0.91835937]\n",
      "2019-02-23T23:35:22.566523: step 14080, loss 0.0859976, acc [0.9806759  0.97443774 0.97156526 0.94183239]\n",
      "2019-02-23T23:35:24.555483: step 14120, loss 0.0674827, acc [0.98203125 0.97714844 0.97753906 0.94775391]\n",
      "2019-02-23T23:35:26.551883: step 14160, loss 0.0723455, acc [0.98095703 0.97675781 0.97617188 0.94599609]\n",
      "2019-02-23T23:35:28.539855: step 14200, loss 0.0811368, acc [0.98222656 0.97617188 0.97578125 0.946875  ]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-23T23:35:31.969722: step 14200, loss 0.291229, acc [0.96924586 0.95599115 0.9454094  0.91385438] \n",
      "\n",
      "2019-02-23T23:35:34.039990: step 14240, loss 0.0775216, acc [0.98222656 0.978125   0.97490234 0.94716797]\n",
      "2019-02-23T23:35:36.053235: step 14280, loss 0.0802074, acc [0.98095703 0.97695312 0.97382813 0.94501953]\n",
      "2019-02-23T23:35:38.038722: step 14320, loss 0.0802419, acc [0.9796875  0.97695312 0.97119141 0.94179687]\n",
      "2019-02-23T23:35:40.036611: step 14360, loss 0.0827984, acc [0.98134766 0.97431641 0.97070312 0.94140625]\n",
      "2019-02-23T23:35:42.027057: step 14400, loss 0.0728587, acc [0.98115234 0.97802734 0.97568359 0.94638672]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-23T23:35:45.446480: step 14400, loss 0.301437, acc [0.96931594 0.95650172 0.9454995  0.91512579] \n",
      "\n",
      "2019-02-23T23:35:47.511823: step 14440, loss 0.0799924, acc [0.9828125  0.97539062 0.97324219 0.94355469]\n",
      "2019-02-23T23:35:49.504750: step 14480, loss 0.0873157, acc [0.97998047 0.97412109 0.97089844 0.94101563]\n",
      "2019-02-23T23:35:51.502141: step 14520, loss 0.0900241, acc [0.98037109 0.97226563 0.96923828 0.93662109]\n",
      "2019-02-23T23:35:53.495071: step 14560, loss 0.0945134, acc [0.97861328 0.97324219 0.96962891 0.93710938]\n",
      "2019-02-23T23:35:55.480557: step 14600, loss 0.0891337, acc [0.97871094 0.97490234 0.97001953 0.93955078]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-23T23:35:58.905436: step 14600, loss 0.288791, acc [0.96823474 0.95674198 0.94707125 0.91520588] \n",
      "\n",
      "2019-02-23T23:36:00.980205: step 14640, loss 0.101433, acc [0.97851562 0.97041016 0.96679688 0.93427734]\n",
      "2019-02-23T23:36:02.972680: step 14680, loss 0.0983325, acc [0.97460938 0.97138672 0.96738281 0.93007812]\n",
      "2019-02-23T23:36:04.962586: step 14720, loss 0.0866414, acc [0.97919922 0.97246094 0.97207031 0.93798828]\n",
      "2019-02-23T23:36:06.951545: step 14760, loss 0.0979478, acc [0.97695312 0.97402344 0.96777344 0.93525391]\n",
      "2019-02-23T23:36:08.943977: step 14800, loss 0.0968308, acc [0.97744141 0.97050781 0.96738281 0.93183594]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-23T23:36:12.429368: step 14800, loss 0.293097, acc [0.96919581 0.95574087 0.94570974 0.91375427] \n",
      "\n",
      "2019-02-23T23:36:14.502532: step 14840, loss 0.0914672, acc [0.9765625  0.97226563 0.97099609 0.93515625]\n",
      "2019-02-23T23:36:16.501411: step 14880, loss 0.100779, acc [0.97802734 0.97070312 0.96699219 0.93300781]\n",
      "2019-02-23T23:36:18.503268: step 14920, loss 0.106203, acc [0.97607422 0.96972656 0.96289062 0.92958984]\n",
      "2019-02-23T23:36:20.500201: step 14960, loss 0.106003, acc [0.97871094 0.96884766 0.96591797 0.93164062]\n",
      "2019-02-23T23:36:22.491106: step 15000, loss 0.102013, acc [0.97783203 0.97177734 0.96474609 0.93144531]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-23T23:36:25.926896: step 15000, loss 0.28865, acc [0.96969636 0.95722252 0.94541942 0.91520588] \n",
      "\n",
      "2019-02-23T23:36:28.003648: step 15040, loss 0.104871, acc [0.97890625 0.96826172 0.96699219 0.93300781]\n",
      "2019-02-23T23:36:29.999552: step 15080, loss 0.105321, acc [0.97666016 0.97050781 0.96513672 0.93037109]\n",
      "2019-02-23T23:36:31.989541: step 15120, loss 0.109159, acc [0.9796875  0.97089844 0.96396484 0.93164062]\n",
      "2019-02-23T23:36:33.985904: step 15160, loss 0.111276, acc [0.97675781 0.96591797 0.96181641 0.92783203]\n",
      "2019-02-23T23:36:35.981806: step 15200, loss 0.107693, acc [0.97412109 0.97070312 0.96591797 0.93232422]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-23T23:36:39.421206: step 15200, loss 0.28825, acc [0.96905565 0.95698225 0.94460852 0.91368419] \n",
      "\n",
      "2019-02-23T23:36:41.487983: step 15240, loss 0.114106, acc [0.97578125 0.96894531 0.96210938 0.92919922]\n",
      "2019-02-23T23:36:43.479423: step 15280, loss 0.106034, acc [0.97431641 0.9703125  0.96542969 0.93017578]\n",
      "2019-02-23T23:36:45.475822: step 15320, loss 0.108654, acc [0.97509766 0.96962891 0.96396484 0.92871094]\n",
      "2019-02-23T23:36:47.466767: step 15360, loss 0.105556, acc [0.97617188 0.9671875  0.96425781 0.92910156]\n",
      "2019-02-23T23:36:49.466141: step 15400, loss 0.122072, acc [0.97539062 0.96630859 0.95996094 0.92421875]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-23T23:36:52.921278: step 15400, loss 0.284099, acc [0.97094775 0.9563916  0.94572976 0.91624703] \n",
      "\n",
      "2019-02-23T23:36:55.001003: step 15440, loss 0.116483, acc [0.97636719 0.96865234 0.9609375  0.92802734]\n",
      "2019-02-23T23:36:56.999386: step 15480, loss 0.110699, acc [0.97382813 0.96962891 0.96318359 0.92724609]\n",
      "2019-02-23T23:36:58.991836: step 15520, loss 0.116728, acc [0.97519531 0.96933594 0.96357422 0.9296875 ]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-02-23T23:37:00.982268: step 15560, loss 0.121567, acc [0.97519531 0.96660156 0.96181641 0.92617187]\n",
      "2019-02-23T23:37:02.972219: step 15600, loss 0.121214, acc [0.97421875 0.96523437 0.95947266 0.92451172]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-23T23:37:06.411978: step 15600, loss 0.277024, acc [0.96913574 0.95702229 0.94606013 0.91482546] \n",
      "\n",
      "2019-02-23T23:37:08.618680: step 15640, loss 0.0818048, acc [0.98422013 0.97743647 0.97149128 0.94527403]\n",
      "2019-02-23T23:37:10.614087: step 15680, loss 0.0668902, acc [0.98515625 0.97675781 0.97851562 0.95029297]\n",
      "2019-02-23T23:37:12.614454: step 15720, loss 0.0710661, acc [0.98330078 0.97851562 0.97666016 0.95029297]\n",
      "2019-02-23T23:37:14.600439: step 15760, loss 0.0628352, acc [0.98378906 0.98085937 0.97929687 0.95439453]\n",
      "2019-02-23T23:37:16.583445: step 15800, loss 0.0689957, acc [0.98486328 0.97851562 0.97675781 0.95068359]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-23T23:37:20.008373: step 15800, loss 0.299217, acc [0.97054731 0.9584739  0.94798226 0.91845949] \n",
      "\n",
      "2019-02-23T23:37:22.071190: step 15840, loss 0.0717295, acc [0.98261719 0.97763672 0.97470703 0.94658203]\n",
      "2019-02-23T23:37:24.068083: step 15880, loss 0.0647151, acc [0.98173828 0.97998047 0.97705078 0.95019531]\n",
      "2019-02-23T23:37:26.071428: step 15920, loss 0.0677878, acc [0.98349609 0.97910156 0.97773438 0.95078125]\n",
      "2019-02-23T23:37:28.077250: step 15960, loss 0.0671475, acc [0.98242188 0.97626953 0.97773438 0.94667969]\n",
      "2019-02-23T23:37:30.078115: step 16000, loss 0.0761091, acc [0.98398438 0.97597656 0.97490234 0.94755859]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-23T23:37:33.606703: step 16000, loss 0.300746, acc [0.96991661 0.95843386 0.94798226 0.91884992] \n",
      "\n",
      "2019-02-23T23:37:35.675472: step 16040, loss 0.0725507, acc [0.98085937 0.97714844 0.97509766 0.94677734]\n",
      "2019-02-23T23:37:37.678817: step 16080, loss 0.0763018, acc [0.9828125  0.97519531 0.97519531 0.94726562]\n",
      "2019-02-23T23:37:39.682655: step 16120, loss 0.0817523, acc [0.98105469 0.97685547 0.97255859 0.94257813]\n",
      "2019-02-23T23:37:41.714766: step 16160, loss 0.0793798, acc [0.98359375 0.9765625  0.97421875 0.94707031]\n",
      "2019-02-23T23:37:43.709181: step 16200, loss 0.0846941, acc [0.98193359 0.97480469 0.97197266 0.94248047]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-23T23:37:47.147497: step 16200, loss 0.29377, acc [0.97030704 0.95752285 0.94729149 0.91766861] \n",
      "\n",
      "2019-02-23T23:37:49.215278: step 16240, loss 0.0886516, acc [0.97724609 0.97441406 0.97285156 0.94130859]\n",
      "2019-02-23T23:37:51.207212: step 16280, loss 0.0796519, acc [0.98105469 0.97919922 0.97539062 0.94892578]\n",
      "2019-02-23T23:37:53.197658: step 16320, loss 0.0869847, acc [0.98134766 0.97636719 0.97011719 0.94072266]\n",
      "2019-02-23T23:37:55.189098: step 16360, loss 0.0966121, acc [0.97773438 0.97314453 0.97070312 0.93769531]\n",
      "2019-02-23T23:37:57.180538: step 16400, loss 0.0911468, acc [0.98037109 0.97431641 0.9671875  0.93642578]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-23T23:38:00.597526: step 16400, loss 0.295692, acc [0.96977645 0.95817357 0.9471213  0.91825927] \n",
      "\n",
      "2019-02-23T23:38:02.664313: step 16440, loss 0.0875012, acc [0.97910156 0.97304687 0.97050781 0.93945312]\n",
      "2019-02-23T23:38:04.659223: step 16480, loss 0.0927198, acc [0.98027344 0.97109375 0.97089844 0.93857422]\n",
      "2019-02-23T23:38:06.648183: step 16520, loss 0.0934434, acc [0.98017578 0.97236328 0.96796875 0.93691406]\n",
      "2019-02-23T23:38:08.646071: step 16560, loss 0.0858672, acc [0.98115234 0.97363281 0.97021484 0.940625  ]\n",
      "2019-02-23T23:38:10.638006: step 16600, loss 0.0951876, acc [0.98017578 0.97236328 0.96660156 0.93964844]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-23T23:38:14.072884: step 16600, loss 0.290067, acc [0.96962628 0.9581035  0.9473916  0.91710799] \n",
      "\n",
      "2019-02-23T23:38:16.146661: step 16640, loss 0.0954466, acc [0.98076172 0.97431641 0.96552734 0.93583984]\n",
      "2019-02-23T23:38:18.141076: step 16680, loss 0.104908, acc [0.98251953 0.97099609 0.96640625 0.93554688]\n",
      "2019-02-23T23:38:20.132056: step 16720, loss 0.10428, acc [0.97695312 0.97197266 0.96708984 0.93554688]\n",
      "2019-02-23T23:38:22.144292: step 16760, loss 0.108004, acc [0.97646484 0.97177734 0.96396484 0.93291016]\n",
      "2019-02-23T23:38:24.145153: step 16800, loss 0.0930167, acc [0.97695312 0.97324219 0.96875    0.93515625]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-23T23:38:27.864656: step 16800, loss 0.28872, acc [0.97061738 0.95914465 0.94763187 0.91866972] \n",
      "\n",
      "2019-02-23T23:38:29.970671: step 16840, loss 0.103856, acc [0.97919922 0.97001953 0.96582031 0.93535156]\n",
      "2019-02-23T23:38:32.004270: step 16880, loss 0.107552, acc [0.97705078 0.97109375 0.96396484 0.93154297]\n",
      "2019-02-23T23:38:34.048783: step 16920, loss 0.095719, acc [0.98203125 0.97451172 0.96669922 0.9390625 ]\n",
      "2019-02-23T23:38:36.129503: step 16960, loss 0.0973934, acc [0.98007813 0.96982422 0.96699219 0.93417969]\n",
      "2019-02-23T23:38:38.198813: step 17000, loss 0.107029, acc [0.97539062 0.97216797 0.96269531 0.92890625]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-23T23:38:41.690652: step 17000, loss 0.29057, acc [0.97048724 0.95861406 0.94692108 0.91796895] \n",
      "\n",
      "2019-02-23T23:38:43.802620: step 17040, loss 0.110262, acc [0.97695312 0.97207031 0.96289062 0.93144531]\n",
      "2019-02-23T23:38:45.854075: step 17080, loss 0.102262, acc [0.98017578 0.97041016 0.96552734 0.93378906]\n",
      "2019-02-23T23:38:47.905035: step 17120, loss 0.0996612, acc [0.97724609 0.97041016 0.96699219 0.93505859]\n",
      "2019-02-23T23:38:49.899452: step 17160, loss 0.10538, acc [0.98076172 0.97363281 0.96162109 0.93251953]\n",
      "2019-02-23T23:38:52.179563: step 17200, loss 0.0742767, acc [0.98212397 0.97799381 0.97496646 0.94839903]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-23T23:38:55.659497: step 17200, loss 0.286893, acc [0.97177868 0.95944498 0.94871307 0.92106238] \n",
      "\n",
      "2019-02-23T23:38:57.804206: step 17240, loss 0.0604316, acc [0.98662109 0.98007813 0.98046875 0.95683594]\n",
      "2019-02-23T23:38:59.851689: step 17280, loss 0.0560943, acc [0.984375   0.98222656 0.98125    0.95742187]\n",
      "2019-02-23T23:39:01.882808: step 17320, loss 0.058966, acc [0.98447266 0.98417969 0.98105469 0.95859375]\n",
      "2019-02-23T23:39:03.918886: step 17360, loss 0.0585552, acc [0.98466797 0.98017578 0.98056641 0.95498047]\n",
      "2019-02-23T23:39:05.952981: step 17400, loss 0.0626875, acc [0.98339844 0.97871094 0.97900391 0.95302734]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-23T23:39:09.422501: step 17400, loss 0.303769, acc [0.97149836 0.96008569 0.94927369 0.92188329] \n",
      "\n",
      "2019-02-23T23:39:11.555796: step 17440, loss 0.0604753, acc [0.98378906 0.98291016 0.97851562 0.95361328]\n",
      "2019-02-23T23:39:13.636021: step 17480, loss 0.0718135, acc [0.98398438 0.97744141 0.97607422 0.95019531]\n",
      "2019-02-23T23:39:15.635395: step 17520, loss 0.0685984, acc [0.98183594 0.98007813 0.97822266 0.95029297]\n",
      "2019-02-23T23:39:17.675442: step 17560, loss 0.068544, acc [0.98271484 0.97734375 0.97861328 0.95029297]\n",
      "2019-02-23T23:39:19.725952: step 17600, loss 0.0709823, acc [0.98417969 0.97822266 0.97519531 0.94951172]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-23T23:39:23.230640: step 17600, loss 0.302033, acc [0.97213907 0.960356   0.94953398 0.92295448] \n",
      "\n",
      "2019-02-23T23:39:25.349173: step 17640, loss 0.0700132, acc [0.98330078 0.97998047 0.97626953 0.95136719]\n",
      "2019-02-23T23:39:27.349044: step 17680, loss 0.0719656, acc [0.98359375 0.97851562 0.975      0.94833984]\n",
      "2019-02-23T23:39:29.343956: step 17720, loss 0.0699869, acc [0.98378906 0.97939453 0.978125   0.95214844]\n",
      "2019-02-23T23:39:31.369621: step 17760, loss 0.075752, acc [0.98154297 0.97392578 0.97558594 0.94501953]\n",
      "2019-02-23T23:39:33.362051: step 17800, loss 0.077304, acc [0.98447266 0.97558594 0.97363281 0.94550781]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-23T23:39:36.796355: step 17800, loss 0.3004, acc [0.97177867 0.95896445 0.94811241 0.9202715 ] \n",
      "\n",
      "2019-02-23T23:39:38.931635: step 17840, loss 0.0781256, acc [0.98144531 0.97861328 0.97431641 0.94570312]\n",
      "2019-02-23T23:39:40.942913: step 17880, loss 0.0738412, acc [0.98125    0.97460938 0.97587891 0.94462891]\n",
      "2019-02-23T23:39:42.934848: step 17920, loss 0.0762219, acc [0.98261719 0.97792969 0.97373047 0.94638672]\n",
      "2019-02-23T23:39:44.948607: step 17960, loss 0.0757706, acc [0.98183594 0.97802734 0.97412109 0.94707031]\n",
      "2019-02-23T23:39:46.939055: step 18000, loss 0.0859783, acc [0.98105469 0.97734375 0.97265625 0.94492188]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-23T23:39:50.361457: step 18000, loss 0.297355, acc [0.97187878 0.96017579 0.94852286 0.92140276] \n",
      "\n",
      "2019-02-23T23:39:52.479374: step 18040, loss 0.0805902, acc [0.98115234 0.97822266 0.9734375  0.94589844]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-02-23T23:39:54.490652: step 18080, loss 0.088226, acc [0.98359375 0.97695312 0.97119141 0.94394531]\n",
      "2019-02-23T23:39:56.532708: step 18120, loss 0.0804928, acc [0.98251953 0.97626953 0.97353516 0.94560547]\n",
      "2019-02-23T23:39:58.567276: step 18160, loss 0.0822305, acc [0.98349609 0.975      0.97353516 0.94560547]\n",
      "2019-02-23T23:40:00.565659: step 18200, loss 0.0827617, acc [0.98261719 0.97548828 0.97177734 0.94375   ]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-23T23:40:03.992059: step 18200, loss 0.296238, acc [0.97094775 0.95988547 0.94834266 0.9206319 ] \n",
      "\n",
      "2019-02-23T23:40:06.092156: step 18240, loss 0.0772054, acc [0.98134766 0.97695312 0.97285156 0.94570312]\n",
      "2019-02-23T23:40:08.107833: step 18280, loss 0.090971, acc [0.9796875  0.97519531 0.97001953 0.94101563]\n",
      "2019-02-23T23:40:10.161769: step 18320, loss 0.0869338, acc [0.98017578 0.97363281 0.97275391 0.94208984]\n",
      "2019-02-23T23:40:12.159161: step 18360, loss 0.0881733, acc [0.98144531 0.97568359 0.97060547 0.94160156]\n",
      "2019-02-23T23:40:14.194745: step 18400, loss 0.0774872, acc [0.97998047 0.97539062 0.97353516 0.94365234]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-23T23:40:17.618134: step 18400, loss 0.303996, acc [0.97144831 0.95991551 0.94800228 0.92106238] \n",
      "\n",
      "2019-02-23T23:40:19.728119: step 18440, loss 0.097856, acc [0.97695312 0.97099609 0.96777344 0.93388672]\n",
      "2019-02-23T23:40:21.752790: step 18480, loss 0.091688, acc [0.97949219 0.9765625  0.96894531 0.93994141]\n",
      "2019-02-23T23:40:23.782918: step 18520, loss 0.0827325, acc [0.97919922 0.97285156 0.97324219 0.94042969]\n",
      "2019-02-23T23:40:25.804116: step 18560, loss 0.0909847, acc [0.97988281 0.97519531 0.96845703 0.93935547]\n",
      "2019-02-23T23:40:27.835732: step 18600, loss 0.0951503, acc [0.98076172 0.97480469 0.9671875  0.93984375]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-23T23:40:31.263586: step 18600, loss 0.294722, acc [0.97234931 0.96009571 0.94767192 0.92132267] \n",
      "\n",
      "2019-02-23T23:40:33.386466: step 18640, loss 0.0974498, acc [0.97851562 0.97207031 0.96796875 0.93496094]\n",
      "2019-02-23T23:40:35.426514: step 18680, loss 0.10532, acc [0.97832031 0.97128906 0.96494141 0.93427734]\n",
      "2019-02-23T23:40:37.415472: step 18720, loss 0.104358, acc [0.97871094 0.97041016 0.96621094 0.93310547]\n",
      "2019-02-23T23:40:39.543808: step 18760, loss 0.0710388, acc [0.98500138 0.98195727 0.97837259 0.95508503]\n",
      "2019-02-23T23:40:41.574431: step 18800, loss 0.0578395, acc [0.98554688 0.98447266 0.98115234 0.959375  ]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-23T23:40:45.004315: step 18800, loss 0.296946, acc [0.97155843 0.96082652 0.95069527 0.92379541] \n",
      "\n",
      "2019-02-23T23:40:47.142526: step 18840, loss 0.0544565, acc [0.98603516 0.98017578 0.98300781 0.95869141]\n",
      "2019-02-23T23:40:49.135453: step 18880, loss 0.0508528, acc [0.98623047 0.98300781 0.98330078 0.96015625]\n",
      "2019-02-23T23:40:51.176000: step 18920, loss 0.0608778, acc [0.98564453 0.98134766 0.97763672 0.95537109]\n",
      "2019-02-23T23:40:53.170908: step 18960, loss 0.0659061, acc [0.98330078 0.98037109 0.97939453 0.95439453]\n",
      "2019-02-23T23:40:55.190622: step 19000, loss 0.0590456, acc [0.98476562 0.98144531 0.98037109 0.95615234]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-23T23:40:58.612525: step 19000, loss 0.305643, acc [0.97243941 0.96128703 0.9502748  0.92409575] \n",
      "\n",
      "2019-02-23T23:41:00.727466: step 19040, loss 0.0593828, acc [0.98564453 0.98212891 0.97939453 0.95673828]\n",
      "2019-02-23T23:41:02.767078: step 19080, loss 0.0560325, acc [0.98515625 0.98105469 0.98203125 0.95742187]\n",
      "2019-02-23T23:41:04.808553: step 19120, loss 0.0624846, acc [0.98447266 0.98095703 0.97998047 0.95478516]\n",
      "2019-02-23T23:41:06.828762: step 19160, loss 0.057881, acc [0.98251953 0.97988281 0.97939453 0.95205078]\n",
      "2019-02-23T23:41:08.877736: step 19200, loss 0.0629654, acc [0.98457031 0.97998047 0.97841797 0.95341797]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-23T23:41:12.294182: step 19200, loss 0.315821, acc [0.97254953 0.95957513 0.95011463 0.92349508] \n",
      "\n",
      "2019-02-23T23:41:14.400199: step 19240, loss 0.0646162, acc [0.98349609 0.98134766 0.97919922 0.95361328]\n",
      "2019-02-23T23:41:16.467031: step 19280, loss 0.0582608, acc [0.98242188 0.98066406 0.98154297 0.95341797]\n",
      "2019-02-23T23:41:18.482332: step 19320, loss 0.072348, acc [0.98417969 0.97880859 0.97724609 0.95175781]\n",
      "2019-02-23T23:41:20.500997: step 19360, loss 0.067441, acc [0.98330078 0.97871094 0.97900391 0.95195312]\n",
      "2019-02-23T23:41:22.540053: step 19400, loss 0.0655315, acc [0.98369141 0.97705078 0.97744141 0.94941406]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-23T23:41:26.064132: step 19400, loss 0.311231, acc [0.97326032 0.96074643 0.95022475 0.92484658] \n",
      "\n",
      "2019-02-23T23:41:28.166675: step 19440, loss 0.0702754, acc [0.98349609 0.97861328 0.97558594 0.94824219]\n",
      "2019-02-23T23:41:30.235490: step 19480, loss 0.0766417, acc [0.98613281 0.97714844 0.97304687 0.95029297]\n",
      "2019-02-23T23:41:32.254705: step 19520, loss 0.0730943, acc [0.9828125  0.97900391 0.97558594 0.94980469]\n",
      "2019-02-23T23:41:34.304722: step 19560, loss 0.0757281, acc [0.98330078 0.97910156 0.97509766 0.94921875]\n",
      "2019-02-23T23:41:36.303566: step 19600, loss 0.0742996, acc [0.98447266 0.97841797 0.97490234 0.95078125]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-23T23:41:39.727934: step 19600, loss 0.304578, acc [0.97302005 0.96051617 0.94945389 0.92332489] \n",
      "\n",
      "2019-02-23T23:41:41.830518: step 19640, loss 0.0843452, acc [0.98291016 0.97451172 0.97265625 0.94384766]\n",
      "2019-02-23T23:41:43.859938: step 19680, loss 0.0752284, acc [0.98330078 0.97822266 0.97451172 0.94804687]\n",
      "2019-02-23T23:41:45.883618: step 19720, loss 0.0774852, acc [0.98544922 0.97753906 0.9734375  0.94951172]\n",
      "2019-02-23T23:41:47.885968: step 19760, loss 0.0699038, acc [0.98193359 0.97841797 0.97646484 0.94873047]\n",
      "2019-02-23T23:41:49.929983: step 19800, loss 0.0753736, acc [0.98105469 0.97792969 0.97275391 0.94462891]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-23T23:41:53.368255: step 19800, loss 0.30355, acc [0.97308012 0.96056623 0.95023476 0.92439608] \n",
      "\n",
      "2019-02-23T23:41:55.461458: step 19840, loss 0.0744738, acc [0.98105469 0.97753906 0.975      0.94707031]\n",
      "2019-02-23T23:41:57.473231: step 19880, loss 0.0810456, acc [0.98183594 0.97783203 0.97314453 0.94550781]\n",
      "2019-02-23T23:41:59.520762: step 19920, loss 0.0817774, acc [0.98115234 0.97792969 0.97304687 0.94404297]\n",
      "2019-02-23T23:42:01.538446: step 19960, loss 0.0734453, acc [0.98427734 0.97832031 0.97460938 0.94941406]\n",
      "2019-02-23T23:42:03.587920: step 20000, loss 0.0824858, acc [0.98144531 0.97851562 0.97294922 0.94648438]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-23T23:42:07.014284: step 20000, loss 0.297186, acc [0.97249947 0.96174754 0.95082542 0.92545726] \n",
      "\n",
      "2019-02-23T23:42:09.138157: step 20040, loss 0.0827835, acc [0.98134766 0.97675781 0.97109375 0.94345703]\n",
      "2019-02-23T23:42:11.173740: step 20080, loss 0.0874787, acc [0.98085937 0.97353516 0.97158203 0.94130859]\n",
      "2019-02-23T23:42:13.240077: step 20120, loss 0.0896894, acc [0.98095703 0.97636719 0.97001953 0.94355469]\n",
      "2019-02-23T23:42:15.287067: step 20160, loss 0.0768885, acc [0.98291016 0.98037109 0.97246094 0.94560547]\n",
      "2019-02-23T23:42:17.285945: step 20200, loss 0.0909196, acc [0.98212891 0.97636719 0.96845703 0.94072266]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-23T23:42:20.710328: step 20200, loss 0.294789, acc [0.97307011 0.96140716 0.95051507 0.92478651] \n",
      "\n",
      "2019-02-23T23:42:22.842632: step 20240, loss 0.0859924, acc [0.98066406 0.97509766 0.97255859 0.94179687]\n",
      "2019-02-23T23:42:24.872760: step 20280, loss 0.0829373, acc [0.98232422 0.97480469 0.97109375 0.94267578]\n",
      "2019-02-23T23:42:27.067560: step 20320, loss 0.0647789, acc [0.98540384 0.97980883 0.97849886 0.95463719]\n",
      "2019-02-23T23:42:29.086774: step 20360, loss 0.0439252, acc [0.9859375  0.98583984 0.98408203 0.96210938]\n",
      "2019-02-23T23:42:31.136306: step 20400, loss 0.0490882, acc [0.98740234 0.98330078 0.98330078 0.96201172]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-23T23:42:34.579505: step 20400, loss 0.306782, acc [0.97331038 0.96183764 0.95292775 0.92767973] \n",
      "\n",
      "2019-02-23T23:42:36.708803: step 20440, loss 0.0465422, acc [0.98632812 0.98388672 0.98476562 0.96152344]\n",
      "2019-02-23T23:42:38.761254: step 20480, loss 0.0544224, acc [0.98486328 0.98291016 0.98154297 0.95800781]\n",
      "2019-02-23T23:42:40.803283: step 20520, loss 0.0566805, acc [0.98535156 0.98222656 0.98037109 0.95722656]\n",
      "2019-02-23T23:42:42.800178: step 20560, loss 0.0554602, acc [0.98691406 0.98359375 0.98154297 0.96025391]\n",
      "2019-02-23T23:42:44.858082: step 20600, loss 0.0599895, acc [0.98427734 0.98173828 0.97949219 0.95527344]\n",
      "\n",
      "Evaluation:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-02-23T23:42:48.269568: step 20600, loss 0.309867, acc [0.97309013 0.9610868  0.95173643 0.926128  ] \n",
      "\n",
      "2019-02-23T23:42:50.389967: step 20640, loss 0.0513171, acc [0.98701172 0.98544922 0.98291016 0.9625    ]\n",
      "2019-02-23T23:42:52.455807: step 20680, loss 0.0590192, acc [0.98603516 0.98242188 0.97939453 0.95654297]\n",
      "2019-02-23T23:42:54.473039: step 20720, loss 0.0538069, acc [0.98623047 0.98203125 0.98369141 0.96005859]\n",
      "2019-02-23T23:42:56.510609: step 20760, loss 0.0560552, acc [0.98691406 0.98574219 0.98203125 0.96171875]\n",
      "2019-02-23T23:42:58.537760: step 20800, loss 0.0621334, acc [0.98535156 0.98183594 0.97988281 0.95625   ]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-23T23:43:01.969085: step 20800, loss 0.314116, acc [0.97368079 0.9620779  0.95234711 0.92801009] \n",
      "\n",
      "2019-02-23T23:43:04.116268: step 20840, loss 0.057506, acc [0.98486328 0.98330078 0.98173828 0.95878906]\n",
      "2019-02-23T23:43:06.169712: step 20880, loss 0.0661378, acc [0.98271484 0.98017578 0.97841797 0.95292969]\n",
      "2019-02-23T23:43:08.198553: step 20920, loss 0.0654482, acc [0.98378906 0.98046875 0.97851562 0.95458984]\n",
      "2019-02-23T23:43:10.256402: step 20960, loss 0.0609007, acc [0.98535156 0.98271484 0.98076172 0.95722656]\n",
      "2019-02-23T23:43:12.276112: step 21000, loss 0.0713269, acc [0.98320312 0.97841797 0.97666016 0.95048828]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-23T23:43:15.744639: step 21000, loss 0.311076, acc [0.9732403  0.96110683 0.95061518 0.92524702] \n",
      "\n",
      "2019-02-23T23:43:17.870991: step 21040, loss 0.0665159, acc [0.98359375 0.97988281 0.98037109 0.95458984]\n",
      "2019-02-23T23:43:19.913518: step 21080, loss 0.0617475, acc [0.98544922 0.98105469 0.97832031 0.95410156]\n",
      "2019-02-23T23:43:21.969439: step 21120, loss 0.0634422, acc [0.98330078 0.97998047 0.97880859 0.95087891]\n",
      "2019-02-23T23:43:24.019406: step 21160, loss 0.0748338, acc [0.98457031 0.97783203 0.97460938 0.94912109]\n",
      "2019-02-23T23:43:26.072349: step 21200, loss 0.0636598, acc [0.9859375  0.98076172 0.97802734 0.95322266]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-23T23:43:29.515645: step 21200, loss 0.313223, acc [0.97364074 0.96171751 0.95114577 0.9265785 ] \n",
      "\n",
      "2019-02-23T23:43:31.625630: step 21240, loss 0.0711511, acc [0.98583984 0.98085937 0.97558594 0.95371094]\n",
      "2019-02-23T23:43:33.673614: step 21280, loss 0.0735057, acc [0.98408203 0.97822266 0.97587891 0.94941406]\n",
      "2019-02-23T23:43:35.701260: step 21320, loss 0.0750249, acc [0.98310547 0.97998047 0.97421875 0.94931641]\n",
      "2019-02-23T23:43:37.741803: step 21360, loss 0.0727074, acc [0.98476562 0.97890625 0.97578125 0.95078125]\n",
      "2019-02-23T23:43:39.792762: step 21400, loss 0.0787532, acc [0.98398438 0.9765625  0.97431641 0.94882813]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-23T23:43:43.269225: step 21400, loss 0.30564, acc [0.97283985 0.96176756 0.95074533 0.9257676 ] \n",
      "\n",
      "2019-02-23T23:43:45.379208: step 21440, loss 0.074859, acc [0.98398438 0.97763672 0.97509766 0.94658203]\n",
      "2019-02-23T23:43:47.445806: step 21480, loss 0.0808422, acc [0.98212891 0.97753906 0.97236328 0.94501953]\n",
      "2019-02-23T23:43:49.480937: step 21520, loss 0.0777277, acc [0.98359375 0.97949219 0.97324219 0.94824219]\n",
      "2019-02-23T23:43:51.523918: step 21560, loss 0.077398, acc [0.98369141 0.98007813 0.97333984 0.94892578]\n",
      "2019-02-23T23:43:53.531728: step 21600, loss 0.0783319, acc [0.98388672 0.97919922 0.97373047 0.94931641]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-23T23:43:56.974459: step 21600, loss 0.303347, acc [0.97356065 0.96183764 0.95088548 0.92645837] \n",
      "\n",
      "2019-02-23T23:43:59.096348: step 21640, loss 0.072802, acc [0.98339844 0.97861328 0.9734375  0.94667969]\n",
      "2019-02-23T23:44:01.131931: step 21680, loss 0.0801904, acc [0.98291016 0.97724609 0.9734375  0.94609375]\n",
      "2019-02-23T23:44:03.205743: step 21720, loss 0.0797289, acc [0.98134766 0.97763672 0.97353516 0.94736328]\n",
      "2019-02-23T23:44:05.214009: step 21760, loss 0.0788548, acc [0.98378906 0.97685547 0.97402344 0.94726562]\n",
      "2019-02-23T23:44:07.305144: step 21800, loss 0.0850301, acc [0.98251953 0.97539062 0.97001953 0.94355469]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-23T23:44:10.736471: step 21800, loss 0.300303, acc [0.97359069 0.96214798 0.95011463 0.9256775 ] \n",
      "\n",
      "2019-02-23T23:44:12.852902: step 21840, loss 0.0735461, acc [0.98232422 0.97900391 0.97480469 0.95039063]\n",
      "2019-02-23T23:44:15.018976: step 21880, loss 0.0507167, acc [0.98712121 0.9847893  0.98198587 0.96284032]\n",
      "2019-02-23T23:44:17.059481: step 21920, loss 0.0442339, acc [0.99023438 0.98417969 0.98681641 0.96689453]\n",
      "2019-02-23T23:44:19.081185: step 21960, loss 0.0451346, acc [0.98730469 0.98515625 0.98583984 0.96474609]\n",
      "2019-02-23T23:44:21.114774: step 22000, loss 0.051286, acc [0.98613281 0.98417969 0.98310547 0.96035156]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-23T23:44:24.534691: step 22000, loss 0.317133, acc [0.97331037 0.96254843 0.95243721 0.92737939] \n",
      "\n",
      "2019-02-23T23:44:26.679945: step 22040, loss 0.0499696, acc [0.98652344 0.98388672 0.98408203 0.96103516]\n",
      "2019-02-23T23:44:28.715971: step 22080, loss 0.0464353, acc [0.98701172 0.98388672 0.98212891 0.96025391]\n",
      "2019-02-23T23:44:30.769905: step 22120, loss 0.0475658, acc [0.98720703 0.98691406 0.98525391 0.96503906]\n",
      "2019-02-23T23:44:32.799043: step 22160, loss 0.0562091, acc [0.98701172 0.98408203 0.98125    0.96005859]\n",
      "2019-02-23T23:44:34.818752: step 22200, loss 0.0559428, acc [0.98759766 0.98203125 0.98251953 0.96025391]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-23T23:44:38.255535: step 22200, loss 0.321512, acc [0.97318023 0.96217802 0.95174644 0.92629819] \n",
      "\n",
      "2019-02-23T23:44:40.389824: step 22240, loss 0.0550471, acc [0.98896484 0.98398438 0.98173828 0.96162109]\n",
      "2019-02-23T23:44:42.418462: step 22280, loss 0.048398, acc [0.98701172 0.98359375 0.98388672 0.9625    ]\n",
      "2019-02-23T23:44:44.467437: step 22320, loss 0.0538256, acc [0.98730469 0.98378906 0.98251953 0.96103516]\n",
      "2019-02-23T23:44:46.507981: step 22360, loss 0.0575895, acc [0.98554688 0.98339844 0.98144531 0.95917969]\n",
      "2019-02-23T23:44:48.548525: step 22400, loss 0.0532908, acc [0.98515625 0.98349609 0.98027344 0.95683594]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-23T23:44:51.975883: step 22400, loss 0.323892, acc [0.97415131 0.96288881 0.95206679 0.92836048] \n",
      "\n",
      "2019-02-23T23:44:54.110171: step 22440, loss 0.0606803, acc [0.98466797 0.98085937 0.98017578 0.95537109]\n",
      "2019-02-23T23:44:56.195851: step 22480, loss 0.0570236, acc [0.98466797 0.98388672 0.98154297 0.95917969]\n",
      "2019-02-23T23:44:58.216554: step 22520, loss 0.0513762, acc [0.98642578 0.98349609 0.98320312 0.96113281]\n",
      "2019-02-23T23:45:00.256106: step 22560, loss 0.0602863, acc [0.98544922 0.98173828 0.97939453 0.95537109]\n",
      "2019-02-23T23:45:02.272842: step 22600, loss 0.0569251, acc [0.9859375  0.98310547 0.98085937 0.95810547]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-23T23:45:05.699207: step 22600, loss 0.323063, acc [0.97346054 0.96240827 0.95128593 0.92692889] \n",
      "\n",
      "2019-02-23T23:45:07.804231: step 22640, loss 0.0649209, acc [0.98310547 0.98037109 0.97949219 0.95498047]\n",
      "2019-02-23T23:45:09.921159: step 22680, loss 0.0677906, acc [0.98466797 0.98085937 0.97685547 0.95263672]\n",
      "2019-02-23T23:45:11.987991: step 22720, loss 0.0656156, acc [0.98496094 0.98339844 0.9765625  0.95527344]\n",
      "2019-02-23T23:45:14.123767: step 22760, loss 0.0685306, acc [0.98369141 0.98046875 0.97548828 0.95166016]\n",
      "2019-02-23T23:45:16.165797: step 22800, loss 0.0662432, acc [0.98710937 0.97988281 0.97832031 0.95498047]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-23T23:45:19.747411: step 22800, loss 0.309426, acc [0.97350059 0.96228814 0.95201674 0.92748951] \n",
      "\n",
      "2019-02-23T23:45:21.891619: step 22840, loss 0.0654301, acc [0.98544922 0.98222656 0.97763672 0.95419922]\n",
      "2019-02-23T23:45:24.020451: step 22880, loss 0.0671441, acc [0.98496094 0.9796875  0.97685547 0.95185547]\n",
      "2019-02-23T23:45:26.183010: step 22920, loss 0.0658356, acc [0.98730469 0.98242188 0.97539062 0.95478516]\n",
      "2019-02-23T23:45:28.318786: step 22960, loss 0.0687707, acc [0.98378906 0.97939453 0.97617188 0.95048828]\n",
      "2019-02-23T23:45:30.408472: step 23000, loss 0.071646, acc [0.98642578 0.98027344 0.97490234 0.95214844]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-23T23:45:33.999967: step 23000, loss 0.311966, acc [0.97383095 0.96223809 0.95111574 0.92692889] \n",
      "\n",
      "2019-02-23T23:45:36.143184: step 23040, loss 0.0751073, acc [0.98291016 0.97675781 0.97519531 0.94765625]\n",
      "2019-02-23T23:45:38.195134: step 23080, loss 0.0618837, acc [0.98574219 0.97880859 0.9796875  0.95419922]\n",
      "2019-02-23T23:45:40.281310: step 23120, loss 0.0682799, acc [0.98466797 0.97988281 0.97578125 0.95126953]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-02-23T23:45:42.348659: step 23160, loss 0.0722816, acc [0.98359375 0.98046875 0.97460938 0.94970703]\n",
      "2019-02-23T23:45:44.394636: step 23200, loss 0.077662, acc [0.98144531 0.97792969 0.97509766 0.946875  ]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-23T23:45:47.893915: step 23200, loss 0.307711, acc [0.97406121 0.96230816 0.95187658 0.92770976] \n",
      "\n",
      "2019-02-23T23:45:50.047096: step 23240, loss 0.0707747, acc [0.98300781 0.97890625 0.97451172 0.94824219]\n",
      "2019-02-23T23:45:52.107929: step 23280, loss 0.0746708, acc [0.98261719 0.97597656 0.97304687 0.94433594]\n",
      "2019-02-23T23:45:54.164345: step 23320, loss 0.0741239, acc [0.98398438 0.97949219 0.975      0.94941406]\n",
      "2019-02-23T23:45:56.199433: step 23360, loss 0.0762691, acc [0.98291016 0.97705078 0.97460938 0.94638672]\n",
      "2019-02-23T23:45:58.248906: step 23400, loss 0.0753275, acc [0.98242188 0.98085937 0.97402344 0.95019531]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-23T23:46:01.686680: step 23400, loss 0.309667, acc [0.97350059 0.96291884 0.95227703 0.92822033] \n",
      "\n",
      "2019-02-23T23:46:03.978238: step 23440, loss 0.064854, acc [0.98427241 0.98085444 0.97810823 0.95516592]\n",
      "2019-02-23T23:46:06.007831: step 23480, loss 0.0452437, acc [0.98955078 0.98662109 0.98427734 0.96630859]\n",
      "2019-02-23T23:46:08.055814: step 23520, loss 0.0449445, acc [0.98886719 0.98525391 0.98476562 0.96660156]\n",
      "2019-02-23T23:46:10.122645: step 23560, loss 0.0426074, acc [0.98818359 0.98623047 0.98574219 0.96591797]\n",
      "2019-02-23T23:46:12.184562: step 23600, loss 0.0483597, acc [0.98945313 0.98583984 0.98515625 0.96660156]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-23T23:46:15.616339: step 23600, loss 0.328307, acc [0.97439157 0.96253842 0.952247   0.92925147] \n",
      "\n",
      "2019-02-23T23:46:17.763027: step 23640, loss 0.0444013, acc [0.98701172 0.98525391 0.98486328 0.96425781]\n",
      "2019-02-23T23:46:19.817460: step 23680, loss 0.0449139, acc [0.98710937 0.98427734 0.98476562 0.96337891]\n",
      "2019-02-23T23:46:21.852049: step 23720, loss 0.0519622, acc [0.98740234 0.98662109 0.98134766 0.96191406]\n",
      "2019-02-23T23:46:23.893586: step 23760, loss 0.0510542, acc [0.98681641 0.98300781 0.98330078 0.96074219]\n",
      "2019-02-23T23:46:25.942561: step 23800, loss 0.0485388, acc [0.98769531 0.98408203 0.984375   0.96210938]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-23T23:46:29.426463: step 23800, loss 0.326527, acc [0.97443162 0.96289882 0.9530579  0.92983211] \n",
      "\n",
      "2019-02-23T23:46:31.547359: step 23840, loss 0.0429909, acc [0.98876953 0.98652344 0.98623047 0.96689453]\n",
      "2019-02-23T23:46:33.592366: step 23880, loss 0.0509993, acc [0.98691406 0.98369141 0.98291016 0.96054688]\n",
      "2019-02-23T23:46:35.612077: step 23920, loss 0.0553323, acc [0.98691406 0.98388672 0.98310547 0.96230469]\n",
      "2019-02-23T23:46:37.642209: step 23960, loss 0.0581779, acc [0.98662109 0.98212891 0.98105469 0.95908203]\n",
      "2019-02-23T23:46:39.722926: step 24000, loss 0.0519124, acc [0.98710937 0.98496094 0.98291016 0.96181641]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-23T23:46:43.185004: step 24000, loss 0.330722, acc [0.97379091 0.96276868 0.95255734 0.92900119] \n",
      "\n",
      "2019-02-23T23:46:45.313394: step 24040, loss 0.0559692, acc [0.98730469 0.98242188 0.98066406 0.95888672]\n",
      "2019-02-23T23:46:47.346939: step 24080, loss 0.0605429, acc [0.98457031 0.98173828 0.97949219 0.95585937]\n",
      "2019-02-23T23:46:49.406827: step 24120, loss 0.0584923, acc [0.98730469 0.98417969 0.97949219 0.95966797]\n",
      "2019-02-23T23:46:51.458777: step 24160, loss 0.0553113, acc [0.98681641 0.98544922 0.98144531 0.96035156]\n",
      "2019-02-23T23:46:53.511721: step 24200, loss 0.0646995, acc [0.98535156 0.98154297 0.97919922 0.95576172]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-23T23:46:56.994632: step 24200, loss 0.3224, acc [0.97418134 0.96263853 0.95168637 0.92809018] \n",
      "\n",
      "2019-02-23T23:46:59.141321: step 24240, loss 0.0567356, acc [0.9875     0.98212891 0.98154297 0.95966797]\n",
      "2019-02-23T23:47:01.268166: step 24280, loss 0.0535582, acc [0.98779297 0.98457031 0.98056641 0.9609375 ]\n",
      "2019-02-23T23:47:03.367736: step 24320, loss 0.0577616, acc [0.98671875 0.98291016 0.97919922 0.95800781]\n",
      "2019-02-23T23:47:05.415221: step 24360, loss 0.0603137, acc [0.9859375  0.97998047 0.97919922 0.95419922]\n",
      "2019-02-23T23:47:07.487016: step 24400, loss 0.0582372, acc [0.98710937 0.984375   0.97832031 0.95810547]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-23T23:47:11.095909: step 24400, loss 0.321688, acc [0.97389102 0.96321917 0.95224699 0.9285607 ] \n",
      "\n",
      "2019-02-23T23:47:13.314020: step 24440, loss 0.0619758, acc [0.984375   0.98212891 0.97919922 0.95498047]\n",
      "2019-02-23T23:47:15.391762: step 24480, loss 0.0714656, acc [0.98642578 0.97978516 0.97822266 0.95498047]\n",
      "2019-02-23T23:47:17.468018: step 24520, loss 0.062517, acc [0.98398438 0.98076172 0.97832031 0.953125  ]\n",
      "2019-02-23T23:47:19.507570: step 24560, loss 0.0679459, acc [0.98359375 0.97871094 0.97617188 0.95058594]\n",
      "2019-02-23T23:47:21.553573: step 24600, loss 0.0651008, acc [0.98349609 0.97978516 0.97763672 0.95185547]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-23T23:47:25.028544: step 24600, loss 0.313787, acc [0.97460181 0.96231817 0.95343831 0.92942166] \n",
      "\n",
      "2019-02-23T23:47:27.178803: step 24640, loss 0.0650998, acc [0.98535156 0.98320312 0.98027344 0.95849609]\n",
      "2019-02-23T23:47:29.235190: step 24680, loss 0.0619218, acc [0.98662109 0.98085937 0.97998047 0.95683594]\n",
      "2019-02-23T23:47:31.281698: step 24720, loss 0.0711967, acc [0.98339844 0.97851562 0.97792969 0.95136719]\n",
      "2019-02-23T23:47:33.327164: step 24760, loss 0.0647005, acc [0.98378906 0.98105469 0.97783203 0.95292969]\n",
      "2019-02-23T23:47:35.377656: step 24800, loss 0.0627209, acc [0.98564453 0.98251953 0.97910156 0.95537109]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-23T23:47:38.858868: step 24800, loss 0.310504, acc [0.97451171 0.9637798  0.95327814 0.93001231] \n",
      "\n",
      "2019-02-23T23:47:41.006548: step 24840, loss 0.0649385, acc [0.98388672 0.97958984 0.97841797 0.95195312]\n",
      "2019-02-23T23:47:43.042132: step 24880, loss 0.0677092, acc [0.9859375  0.98027344 0.97685547 0.95332031]\n",
      "2019-02-23T23:47:45.099043: step 24920, loss 0.0727219, acc [0.98447266 0.97929687 0.97587891 0.95058594]\n",
      "2019-02-23T23:47:47.144050: step 24960, loss 0.0721835, acc [0.98427734 0.98242188 0.97529297 0.95419922]\n",
      "2019-02-23T23:47:49.310582: step 25000, loss 0.0542067, acc [0.98844796 0.98456538 0.98332446 0.96225438]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-23T23:47:52.789520: step 25000, loss 0.31395, acc [0.97413129 0.96307902 0.95378871 0.92974201] \n",
      "\n",
      "2019-02-23T23:47:54.950095: step 25040, loss 0.0396589, acc [0.98945313 0.98662109 0.98652344 0.96865234]\n",
      "2019-02-23T23:47:57.008070: step 25080, loss 0.0424411, acc [0.9890625  0.98447266 0.98652344 0.96630859]\n",
      "2019-02-23T23:47:59.081390: step 25120, loss 0.0424629, acc [0.99003906 0.98525391 0.98554688 0.96679688]\n",
      "2019-02-23T23:48:01.126853: step 25160, loss 0.0391373, acc [0.9875     0.98769531 0.98613281 0.96816406]\n",
      "2019-02-23T23:48:03.171861: step 25200, loss 0.0454641, acc [0.98886719 0.98720703 0.98505859 0.96640625]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-23T23:48:06.637908: step 25200, loss 0.327509, acc [0.97478201 0.96453063 0.95416913 0.93202455] \n",
      "\n",
      "2019-02-23T23:48:08.800468: step 25240, loss 0.043002, acc [0.99003906 0.98632812 0.98544922 0.96748047]\n",
      "2019-02-23T23:48:10.852919: step 25280, loss 0.041996, acc [0.98837891 0.98701172 0.984375   0.96611328]\n",
      "2019-02-23T23:48:12.897922: step 25320, loss 0.0475229, acc [0.98652344 0.98623047 0.98232422 0.96230469]\n",
      "2019-02-23T23:48:14.950867: step 25360, loss 0.0496327, acc [0.98730469 0.98544922 0.98203125 0.96191406]\n",
      "2019-02-23T23:48:16.991906: step 25400, loss 0.0422429, acc [0.98896484 0.98535156 0.98642578 0.96679688]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-23T23:48:20.462724: step 25400, loss 0.335769, acc [0.97473195 0.96399003 0.95325812 0.9308132 ] \n",
      "\n",
      "2019-02-23T23:48:22.590564: step 25440, loss 0.0459614, acc [0.98876953 0.98525391 0.98398438 0.96455078]\n",
      "2019-02-23T23:48:24.636565: step 25480, loss 0.0544006, acc [0.98828125 0.98583984 0.98183594 0.9625    ]\n",
      "2019-02-23T23:48:26.659747: step 25520, loss 0.0486882, acc [0.98808594 0.98505859 0.98427734 0.96464844]\n",
      "2019-02-23T23:48:28.722612: step 25560, loss 0.0479626, acc [0.98925781 0.98701172 0.98300781 0.96621094]\n",
      "2019-02-23T23:48:30.771588: step 25600, loss 0.0472407, acc [0.98808594 0.98457031 0.98330078 0.96240234]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-23T23:48:34.256979: step 25600, loss 0.324124, acc [0.97471193 0.96404008 0.95402897 0.93152399] \n",
      "\n",
      "2019-02-23T23:48:36.380392: step 25640, loss 0.052732, acc [0.98623047 0.98408203 0.98125    0.95966797]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-02-23T23:48:38.471984: step 25680, loss 0.0497995, acc [0.98642578 0.98271484 0.98310547 0.9609375 ]\n",
      "2019-02-23T23:48:40.493679: step 25720, loss 0.049901, acc [0.98769531 0.98574219 0.98261719 0.9640625 ]\n",
      "2019-02-23T23:48:42.552574: step 25760, loss 0.0517249, acc [0.98798828 0.98496094 0.98232422 0.96298828]\n",
      "2019-02-23T23:48:44.592127: step 25800, loss 0.0564509, acc [0.98574219 0.98330078 0.98222656 0.95966797]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-23T23:48:48.085994: step 25800, loss 0.333349, acc [0.97493217 0.96431039 0.95375867 0.93181431] \n",
      "\n",
      "2019-02-23T23:48:50.268349: step 25840, loss 0.0541274, acc [0.98603516 0.98193359 0.98320312 0.95986328]\n",
      "2019-02-23T23:48:52.324764: step 25880, loss 0.0566106, acc [0.98789063 0.9828125  0.98212891 0.9609375 ]\n",
      "2019-02-23T23:48:54.373739: step 25920, loss 0.0579566, acc [0.98623047 0.98261719 0.98037109 0.95712891]\n",
      "2019-02-23T23:48:56.405356: step 25960, loss 0.0589137, acc [0.98652344 0.98251953 0.98125    0.95917969]\n",
      "2019-02-23T23:48:58.426554: step 26000, loss 0.0623699, acc [0.98652344 0.98398438 0.97929687 0.95830078]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-23T23:49:01.860362: step 26000, loss 0.331214, acc [0.97458179 0.96378981 0.95247725 0.92996226] \n",
      "\n",
      "2019-02-23T23:49:03.998617: step 26040, loss 0.0613404, acc [0.98574219 0.978125   0.98095703 0.95556641]\n",
      "2019-02-23T23:49:06.044170: step 26080, loss 0.0643582, acc [0.98613281 0.98271484 0.97890625 0.95576172]\n",
      "2019-02-23T23:49:08.068792: step 26120, loss 0.0647449, acc [0.98671875 0.98408203 0.97705078 0.95703125]\n",
      "2019-02-23T23:49:10.120744: step 26160, loss 0.0598101, acc [0.98710937 0.98222656 0.97871094 0.95742187]\n",
      "2019-02-23T23:49:12.163271: step 26200, loss 0.0581464, acc [0.98378906 0.98369141 0.98144531 0.9578125 ]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-23T23:49:15.628822: step 26200, loss 0.319736, acc [0.97490214 0.96460071 0.95396891 0.9321547 ] \n",
      "\n",
      "2019-02-23T23:49:17.753684: step 26240, loss 0.0589062, acc [0.98564453 0.98242188 0.98085937 0.95800781]\n",
      "2019-02-23T23:49:19.824486: step 26280, loss 0.0637324, acc [0.98525391 0.97841797 0.97802734 0.95117188]\n",
      "2019-02-23T23:49:21.855170: step 26320, loss 0.0612797, acc [0.98759766 0.98291016 0.97841797 0.95742187]\n",
      "2019-02-23T23:49:23.896256: step 26360, loss 0.061565, acc [0.98710937 0.98144531 0.98037109 0.95771484]\n",
      "2019-02-23T23:49:25.973455: step 26400, loss 0.0654971, acc [0.98496094 0.98027344 0.97626953 0.95361328]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-23T23:49:29.404286: step 26400, loss 0.320204, acc [0.97508234 0.96380983 0.95426924 0.93182432] \n",
      "\n",
      "2019-02-23T23:49:31.530167: step 26440, loss 0.0597943, acc [0.98564453 0.97919922 0.98085937 0.95517578]\n",
      "2019-02-23T23:49:33.593996: step 26480, loss 0.0657555, acc [0.98613281 0.98007813 0.97783203 0.95566406]\n",
      "2019-02-23T23:49:35.636028: step 26520, loss 0.068895, acc [0.98574219 0.98173828 0.97851562 0.95654297]\n",
      "2019-02-23T23:49:37.849677: step 26560, loss 0.0451226, acc [0.98721887 0.98667337 0.98318636 0.9632191 ]\n",
      "2019-02-23T23:49:39.901668: step 26600, loss 0.0346353, acc [0.990625   0.98789063 0.98945313 0.97246094]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-23T23:49:43.328986: step 26600, loss 0.331284, acc [0.97484208 0.96477089 0.95457958 0.93295558] \n",
      "\n",
      "2019-02-23T23:49:45.492042: step 26640, loss 0.0380008, acc [0.98769531 0.98730469 0.9875     0.96806641]\n",
      "2019-02-23T23:49:47.574250: step 26680, loss 0.0443678, acc [0.99082031 0.98798828 0.98505859 0.96962891]\n",
      "2019-02-23T23:49:49.628188: step 26720, loss 0.0426823, acc [0.99023438 0.9875     0.98466797 0.96787109]\n",
      "2019-02-23T23:49:51.655337: step 26760, loss 0.0390893, acc [0.98984375 0.99072266 0.98662109 0.97148437]\n",
      "2019-02-23T23:49:53.712744: step 26800, loss 0.0427278, acc [0.98925781 0.98603516 0.98603516 0.96796875]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-23T23:49:57.155986: step 26800, loss 0.344709, acc [0.97489213 0.9642303  0.95333821 0.93167416] \n",
      "\n",
      "2019-02-23T23:49:59.326438: step 26840, loss 0.0451895, acc [0.99033203 0.98759766 0.98457031 0.96767578]\n",
      "2019-02-23T23:50:01.384837: step 26880, loss 0.0441153, acc [0.98603516 0.98671875 0.98613281 0.96523437]\n",
      "2019-02-23T23:50:03.440261: step 26920, loss 0.0422666, acc [0.98769531 0.98486328 0.98671875 0.96630859]\n",
      "2019-02-23T23:50:05.482293: step 26960, loss 0.0419866, acc [0.98955078 0.98671875 0.98505859 0.9671875 ]\n",
      "2019-02-23T23:50:07.519363: step 27000, loss 0.0458878, acc [0.98779297 0.98525391 0.98525391 0.96445313]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-23T23:50:10.981939: step 27000, loss 0.341504, acc [0.97569302 0.96439047 0.9544094  0.93314579] \n",
      "\n",
      "2019-02-23T23:50:13.134578: step 27040, loss 0.0413755, acc [0.98935547 0.98828125 0.98632812 0.96923828]\n",
      "2019-02-23T23:50:15.172642: step 27080, loss 0.0386255, acc [0.98994141 0.98808594 0.98613281 0.96855469]\n",
      "2019-02-23T23:50:17.202769: step 27120, loss 0.0470707, acc [0.98798828 0.98476562 0.98574219 0.96523437]\n",
      "2019-02-23T23:50:19.261665: step 27160, loss 0.0494163, acc [0.98955078 0.98369141 0.98339844 0.9640625 ]\n",
      "2019-02-23T23:50:21.305681: step 27200, loss 0.0474962, acc [0.98789063 0.98457031 0.98349609 0.96337891]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-23T23:50:24.805457: step 27200, loss 0.337123, acc [0.97590325 0.96371973 0.95524032 0.9335162 ] \n",
      "\n",
      "2019-02-23T23:50:26.935774: step 27240, loss 0.0491926, acc [0.98945313 0.98564453 0.98408203 0.96601563]\n",
      "2019-02-23T23:50:28.968878: step 27280, loss 0.048552, acc [0.98710937 0.98583984 0.98447266 0.96474609]\n",
      "2019-02-23T23:50:31.033230: step 27320, loss 0.0497959, acc [0.98662109 0.9875     0.98339844 0.96494141]\n",
      "2019-02-23T23:50:33.075759: step 27360, loss 0.049353, acc [0.98623047 0.98564453 0.98339844 0.96259766]\n",
      "2019-02-23T23:50:35.164911: step 27400, loss 0.0542327, acc [0.98681641 0.98486328 0.98056641 0.96035156]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-23T23:50:38.669147: step 27400, loss 0.341174, acc [0.97424141 0.96399003 0.95372864 0.93091331] \n",
      "\n",
      "2019-02-23T23:50:40.838652: step 27440, loss 0.0496626, acc [0.98818359 0.98476562 0.98300781 0.96357422]\n",
      "2019-02-23T23:50:42.896058: step 27480, loss 0.0565969, acc [0.98759766 0.98291016 0.98066406 0.95947266]\n",
      "2019-02-23T23:50:44.957433: step 27520, loss 0.0507247, acc [0.98535156 0.98505859 0.98271484 0.96269531]\n",
      "2019-02-23T23:50:47.014841: step 27560, loss 0.0542037, acc [0.98759766 0.98388672 0.98134766 0.96025391]\n",
      "2019-02-23T23:50:49.061337: step 27600, loss 0.0566413, acc [0.98701172 0.98476562 0.97958984 0.95878906]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-23T23:50:52.497177: step 27600, loss 0.332447, acc [0.97544274 0.96487101 0.95229705 0.93116359] \n",
      "\n",
      "2019-02-23T23:50:54.650263: step 27640, loss 0.0546263, acc [0.98779297 0.98486328 0.98125    0.96152344]\n",
      "2019-02-23T23:50:56.697510: step 27680, loss 0.0545446, acc [0.98671875 0.98476562 0.98183594 0.96064453]\n",
      "2019-02-23T23:50:58.731607: step 27720, loss 0.0580495, acc [0.98896484 0.98330078 0.97998047 0.95927734]\n",
      "2019-02-23T23:51:00.795462: step 27760, loss 0.0563932, acc [0.98837891 0.98554688 0.97900391 0.96005859]\n",
      "2019-02-23T23:51:02.836052: step 27800, loss 0.0582613, acc [0.9859375  0.98388672 0.98037109 0.95947266]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-23T23:51:06.282212: step 27800, loss 0.331884, acc [0.97537266 0.96435043 0.95402897 0.93256515] \n",
      "\n",
      "2019-02-23T23:51:08.416004: step 27840, loss 0.0561396, acc [0.98574219 0.98300781 0.98095703 0.95830078]\n",
      "2019-02-23T23:51:10.470930: step 27880, loss 0.067022, acc [0.98544922 0.98173828 0.97773438 0.95556641]\n",
      "2019-02-23T23:51:12.518914: step 27920, loss 0.0610946, acc [0.98623047 0.98388672 0.97978516 0.9578125 ]\n",
      "2019-02-23T23:51:14.559459: step 27960, loss 0.0621785, acc [0.984375   0.98115234 0.98056641 0.95703125]\n",
      "2019-02-23T23:51:16.609922: step 28000, loss 0.0553477, acc [0.98496094 0.98193359 0.98076172 0.95625   ]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-23T23:51:20.087376: step 28000, loss 0.329203, acc [0.9749522  0.96325922 0.95362853 0.9310835 ] \n",
      "\n",
      "2019-02-23T23:51:22.249936: step 28040, loss 0.0626673, acc [0.98417969 0.98173828 0.97919922 0.95585937]\n",
      "2019-02-23T23:51:24.277639: step 28080, loss 0.0613497, acc [0.98564453 0.98076172 0.97949219 0.95644531]\n",
      "2019-02-23T23:51:26.505174: step 28120, loss 0.0516972, acc [0.98788569 0.98375552 0.98361742 0.96447187]\n",
      "2019-02-23T23:51:28.539270: step 28160, loss 0.0420969, acc [0.98935547 0.98828125 0.98662109 0.96962891]\n",
      "2019-02-23T23:51:30.584774: step 28200, loss 0.0378096, acc [0.98994141 0.98759766 0.98691406 0.96962891]\n",
      "\n",
      "Evaluation:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-02-23T23:51:34.205572: step 28200, loss 0.328858, acc [0.97602339 0.96444053 0.95567079 0.93483767] \n",
      "\n",
      "2019-02-23T23:51:36.369619: step 28240, loss 0.0303498, acc [0.99169922 0.98837891 0.98916016 0.97363281]\n",
      "2019-02-23T23:51:38.408180: step 28280, loss 0.0355297, acc [0.99287109 0.98828125 0.98623047 0.97119141]\n",
      "2019-02-23T23:51:40.473027: step 28320, loss 0.0368579, acc [0.98818359 0.98857422 0.98828125 0.96953125]\n",
      "2019-02-23T23:51:42.496211: step 28360, loss 0.0392282, acc [0.9890625  0.98701172 0.98730469 0.96943359]\n",
      "2019-02-23T23:51:44.536258: step 28400, loss 0.0317223, acc [0.98798828 0.98867187 0.98837891 0.96953125]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-23T23:51:47.985936: step 28400, loss 0.346921, acc [0.97616354 0.96475087 0.95587102 0.93475758] \n",
      "\n",
      "2019-02-23T23:51:50.146513: step 28440, loss 0.0371464, acc [0.99052734 0.98701172 0.98720703 0.97001953]\n",
      "2019-02-23T23:51:52.184575: step 28480, loss 0.0409398, acc [0.98916016 0.98525391 0.9875     0.96757812]\n",
      "2019-02-23T23:51:54.243471: step 28520, loss 0.0444538, acc [0.98857422 0.9875     0.98476562 0.96699219]\n",
      "2019-02-23T23:51:56.292943: step 28560, loss 0.0429563, acc [0.98994141 0.98574219 0.98408203 0.96513672]\n",
      "2019-02-23T23:51:58.332990: step 28600, loss 0.0425882, acc [0.98740234 0.98583984 0.98642578 0.96621094]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-23T23:52:01.805484: step 28600, loss 0.350237, acc [0.97595331 0.9646808  0.95554065 0.93446726] \n",
      "\n",
      "2019-02-23T23:52:03.920428: step 28640, loss 0.0430368, acc [0.98955078 0.98789063 0.98583984 0.96757812]\n",
      "2019-02-23T23:52:05.960476: step 28680, loss 0.0451901, acc [0.98935547 0.98535156 0.98574219 0.96650391]\n",
      "2019-02-23T23:52:07.997548: step 28720, loss 0.0437884, acc [0.99072266 0.98408203 0.98681641 0.96796875]\n",
      "2019-02-23T23:52:10.053961: step 28760, loss 0.0448751, acc [0.98818359 0.98496094 0.98554688 0.96513672]\n",
      "2019-02-23T23:52:12.111866: step 28800, loss 0.0448183, acc [0.98886719 0.98525391 0.98535156 0.96699219]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-23T23:52:15.557081: step 28800, loss 0.340044, acc [0.9759433  0.96545165 0.95442942 0.93335603] \n",
      "\n",
      "2019-02-23T23:52:17.688392: step 28840, loss 0.0445367, acc [0.98730469 0.98681641 0.98525391 0.96660156]\n",
      "2019-02-23T23:52:19.725463: step 28880, loss 0.0497607, acc [0.98779297 0.98623047 0.98310547 0.96396484]\n",
      "2019-02-23T23:52:21.775430: step 28920, loss 0.0480556, acc [0.98857422 0.9859375  0.98476562 0.96503906]\n",
      "2019-02-23T23:52:23.833830: step 28960, loss 0.0476075, acc [0.98652344 0.98603516 0.98388672 0.96201172]\n",
      "2019-02-23T23:52:25.884294: step 29000, loss 0.0520313, acc [0.99091797 0.98427734 0.98339844 0.96591797]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-23T23:52:29.331991: step 29000, loss 0.336674, acc [0.97624363 0.96460071 0.95461963 0.93370641] \n",
      "\n",
      "2019-02-23T23:52:31.471731: step 29040, loss 0.048813, acc [0.98710937 0.98447266 0.98271484 0.96162109]\n",
      "2019-02-23T23:52:33.514260: step 29080, loss 0.0536959, acc [0.98925781 0.98632812 0.98085937 0.96289062]\n",
      "2019-02-23T23:52:35.550835: step 29120, loss 0.0507047, acc [0.98779297 0.98564453 0.98310547 0.96367187]\n",
      "2019-02-23T23:52:37.595842: step 29160, loss 0.0482986, acc [0.98789063 0.98349609 0.98349609 0.96171875]\n",
      "2019-02-23T23:52:39.661681: step 29200, loss 0.0589497, acc [0.98701172 0.9828125  0.98105469 0.95830078]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-23T23:52:43.078624: step 29200, loss 0.334864, acc [0.97562294 0.96415021 0.95439938 0.93251509] \n",
      "\n",
      "2019-02-23T23:52:45.241680: step 29240, loss 0.0471506, acc [0.98818359 0.9859375  0.98310547 0.96318359]\n",
      "2019-02-23T23:52:47.265361: step 29280, loss 0.051326, acc [0.98789063 0.98457031 0.98369141 0.96308594]\n",
      "2019-02-23T23:52:49.355504: step 29320, loss 0.0552207, acc [0.98730469 0.98291016 0.98105469 0.96035156]\n",
      "2019-02-23T23:52:51.372238: step 29360, loss 0.0546354, acc [0.98818359 0.98417969 0.98212891 0.96259766]\n",
      "2019-02-23T23:52:53.420223: step 29400, loss 0.0561251, acc [0.98769531 0.98330078 0.97890625 0.95820313]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-23T23:52:56.870451: step 29400, loss 0.333383, acc [0.97512239 0.96524142 0.95407903 0.93274535] \n",
      "\n",
      "2019-02-23T23:52:58.996253: step 29440, loss 0.0479354, acc [0.98828125 0.98564453 0.98330078 0.96347656]\n",
      "2019-02-23T23:53:01.050188: step 29480, loss 0.057409, acc [0.98486328 0.9828125  0.98027344 0.95751953]\n",
      "2019-02-23T23:53:03.101146: step 29520, loss 0.055127, acc [0.98681641 0.9828125  0.98222656 0.96074219]\n",
      "2019-02-23T23:53:05.145163: step 29560, loss 0.0544664, acc [0.98769531 0.98222656 0.98212891 0.96123047]\n",
      "2019-02-23T23:53:07.179753: step 29600, loss 0.0580361, acc [0.98769531 0.98291016 0.97890625 0.95722656]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-23T23:53:10.635385: step 29600, loss 0.334003, acc [0.97584318 0.96475087 0.95420917 0.93285547] \n",
      "\n",
      "2019-02-23T23:53:12.739417: step 29640, loss 0.0580208, acc [0.98740234 0.98261719 0.97880859 0.95576172]\n",
      "2019-02-23T23:53:14.899497: step 29680, loss 0.0452224, acc [0.99057962 0.98611604 0.98296244 0.96719441]\n",
      "2019-02-23T23:53:16.914248: step 29720, loss 0.0322718, acc [0.99150391 0.98798828 0.98935547 0.97226563]\n",
      "2019-02-23T23:53:18.960247: step 29760, loss 0.0352651, acc [0.99199219 0.98964844 0.98720703 0.97285156]\n",
      "2019-02-23T23:53:21.036997: step 29800, loss 0.0398339, acc [0.98994141 0.98837891 0.98671875 0.96992188]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-23T23:53:24.466836: step 29800, loss 0.344479, acc [0.97565297 0.96607234 0.95558069 0.93498784] \n",
      "\n",
      "2019-02-23T23:53:26.586244: step 29840, loss 0.0360019, acc [0.99238281 0.9890625  0.98730469 0.97207031]\n",
      "2019-02-23T23:53:28.611909: step 29880, loss 0.040174, acc [0.98994141 0.9890625  0.98671875 0.97119141]\n",
      "2019-02-23T23:53:30.697878: step 29920, loss 0.0382362, acc [0.98828125 0.98544922 0.98740234 0.96787109]\n",
      "2019-02-23T23:53:32.759686: step 29960, loss 0.0362876, acc [0.98974609 0.9875     0.98886719 0.96982422]\n",
      "2019-02-23T23:53:34.806677: step 30000, loss 0.0375264, acc [0.99082031 0.99013672 0.98720703 0.97236328]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-23T23:53:38.215728: step 30000, loss 0.347267, acc [0.97643384 0.96656288 0.9548599  0.93556848] \n",
      "\n",
      "2019-02-23T23:53:40.345012: step 30040, loss 0.0365471, acc [0.98935547 0.99042969 0.98662109 0.97050781]\n",
      "2019-02-23T23:53:42.373650: step 30080, loss 0.0428056, acc [0.99033203 0.98613281 0.98544922 0.96738281]\n",
      "2019-02-23T23:53:44.421635: step 30120, loss 0.0372329, acc [0.98964844 0.98623047 0.98789063 0.96865234]\n",
      "2019-02-23T23:53:46.444817: step 30160, loss 0.0395855, acc [0.98994141 0.98662109 0.98808594 0.97060547]\n",
      "2019-02-23T23:53:48.493792: step 30200, loss 0.0439357, acc [0.98691406 0.98730469 0.98476562 0.96474609]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-23T23:53:51.965888: step 30200, loss 0.349632, acc [0.97601337 0.96483097 0.95469972 0.93410686] \n",
      "\n",
      "2019-02-23T23:53:54.094720: step 30240, loss 0.0392378, acc [0.98789063 0.9859375  0.98662109 0.96611328]\n",
      "2019-02-23T23:53:56.158614: step 30280, loss 0.0377796, acc [0.99082031 0.98779297 0.98720703 0.96992188]\n",
      "2019-02-23T23:53:58.207054: step 30320, loss 0.0437307, acc [0.98857422 0.98808594 0.98476562 0.96660156]\n",
      "2019-02-23T23:54:00.244623: step 30360, loss 0.04641, acc [0.98759766 0.98525391 0.98417969 0.96416016]\n",
      "2019-02-23T23:54:02.288636: step 30400, loss 0.0448493, acc [0.98916016 0.98574219 0.98398438 0.96376953]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-23T23:54:05.781469: step 30400, loss 0.346824, acc [0.97698445 0.96598224 0.95521028 0.93536826] \n",
      "\n",
      "2019-02-23T23:54:07.908856: step 30440, loss 0.04014, acc [0.98925781 0.98701172 0.9859375  0.96796875]\n",
      "2019-02-23T23:54:09.956306: step 30480, loss 0.0433771, acc [0.98759766 0.98720703 0.98583984 0.96738281]\n",
      "2019-02-23T23:54:12.037522: step 30520, loss 0.0508082, acc [0.98857422 0.98535156 0.98369141 0.96445313]\n",
      "2019-02-23T23:54:14.068145: step 30560, loss 0.0483292, acc [0.98886719 0.98701172 0.98359375 0.96640625]\n",
      "2019-02-23T23:54:16.126545: step 30600, loss 0.046132, acc [0.98769531 0.98554688 0.98369141 0.96367187]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-23T23:54:19.571262: step 30600, loss 0.346193, acc [0.97598334 0.96511127 0.95521029 0.93425703] \n",
      "\n",
      "2019-02-23T23:54:21.704062: step 30640, loss 0.0455521, acc [0.98896484 0.98417969 0.98408203 0.96367187]\n",
      "2019-02-23T23:54:23.736669: step 30680, loss 0.0477518, acc [0.98769531 0.98466797 0.98349609 0.96337891]\n",
      "2019-02-23T23:54:25.792093: step 30720, loss 0.0474858, acc [0.98710937 0.98642578 0.98408203 0.96425781]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-02-23T23:54:27.827677: step 30760, loss 0.0516077, acc [0.98720703 0.98544922 0.98320312 0.96376953]\n",
      "2019-02-23T23:54:29.868748: step 30800, loss 0.0480299, acc [0.98808594 0.98417969 0.98369141 0.96367187]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-23T23:54:33.297563: step 30800, loss 0.340953, acc [0.97651393 0.96589214 0.95499004 0.93469751] \n",
      "\n",
      "2019-02-23T23:54:35.420938: step 30840, loss 0.0546943, acc [0.98818359 0.98515625 0.98261719 0.96347656]\n",
      "2019-02-23T23:54:37.460985: step 30880, loss 0.0508407, acc [0.98662109 0.98730469 0.98164063 0.9625    ]\n",
      "2019-02-23T23:54:39.515417: step 30920, loss 0.0567945, acc [0.98623047 0.98476562 0.98066406 0.95878906]\n",
      "2019-02-23T23:54:41.561418: step 30960, loss 0.0514174, acc [0.98867187 0.98583984 0.98193359 0.96347656]\n",
      "2019-02-23T23:54:43.641640: step 31000, loss 0.0437796, acc [0.98828125 0.98642578 0.98466797 0.96542969]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-23T23:54:47.247559: step 31000, loss 0.349728, acc [0.97689435 0.96521139 0.9554005  0.93560853] \n",
      "\n",
      "2019-02-23T23:54:49.431942: step 31040, loss 0.048491, acc [0.98769531 0.98486328 0.9828125  0.96230469]\n",
      "2019-02-23T23:54:51.495798: step 31080, loss 0.056088, acc [0.98779297 0.98320312 0.98056641 0.95908203]\n",
      "2019-02-23T23:54:53.536394: step 31120, loss 0.0532528, acc [0.98720703 0.98339844 0.98222656 0.96103516]\n",
      "2019-02-23T23:54:55.573412: step 31160, loss 0.0544168, acc [0.98691406 0.98466797 0.98144531 0.96074219]\n",
      "2019-02-23T23:54:57.648677: step 31200, loss 0.0584911, acc [0.98769531 0.98144531 0.98046875 0.95859375]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-23T23:55:01.164323: step 31200, loss 0.344543, acc [0.97652394 0.96483096 0.95465967 0.93422699] \n",
      "\n",
      "2019-02-23T23:55:03.433027: step 31240, loss 0.0438324, acc [0.9899128  0.98741418 0.98571358 0.96870462]\n",
      "2019-02-23T23:55:05.491923: step 31280, loss 0.0302259, acc [0.99296875 0.98925781 0.99003906 0.97578125]\n",
      "2019-02-23T23:55:07.559249: step 31320, loss 0.0325841, acc [0.99111328 0.98876953 0.98974609 0.97353516]\n",
      "2019-02-23T23:55:09.576978: step 31360, loss 0.0281554, acc [0.99189453 0.98935547 0.98935547 0.97431641]\n",
      "2019-02-23T23:55:11.657201: step 31400, loss 0.030789, acc [0.99013672 0.98867187 0.98916016 0.97138672]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-23T23:55:15.080095: step 31400, loss 0.357543, acc [0.976574   0.96616244 0.95662185 0.93698005] \n",
      "\n",
      "2019-02-23T23:55:17.210557: step 31440, loss 0.0371771, acc [0.99179688 0.98867187 0.98828125 0.9734375 ]\n",
      "2019-02-23T23:55:19.254121: step 31480, loss 0.0384345, acc [0.99121094 0.98808594 0.98818359 0.97236328]\n",
      "2019-02-23T23:55:21.292636: step 31520, loss 0.0352736, acc [0.99150391 0.98847656 0.98837891 0.97314453]\n",
      "2019-02-23T23:55:23.329764: step 31560, loss 0.0359773, acc [0.99072266 0.98681641 0.98876953 0.97099609]\n",
      "2019-02-23T23:55:25.398027: step 31600, loss 0.0328142, acc [0.98945313 0.98886719 0.9890625  0.97226563]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-23T23:55:28.836642: step 31600, loss 0.359555, acc [0.97569302 0.96536155 0.95593108 0.93461743] \n",
      "\n",
      "2019-02-23T23:55:30.992258: step 31640, loss 0.0390406, acc [0.99042969 0.98701172 0.98808594 0.97177734]\n",
      "2019-02-23T23:55:33.059089: step 31680, loss 0.0344458, acc [0.9890625  0.98652344 0.98837891 0.97011719]\n",
      "2019-02-23T23:55:35.116994: step 31720, loss 0.0390688, acc [0.99228516 0.98769531 0.98681641 0.97128906]\n",
      "2019-02-23T23:55:37.166979: step 31760, loss 0.0405265, acc [0.98857422 0.98642578 0.98535156 0.965625  ]\n",
      "2019-02-23T23:55:39.200580: step 31800, loss 0.0399549, acc [0.98974609 0.98896484 0.98603516 0.96875   ]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-23T23:55:42.669602: step 31800, loss 0.360429, acc [0.97600336 0.96659292 0.95632152 0.93578873] \n",
      "\n",
      "2019-02-23T23:55:44.793809: step 31840, loss 0.0359208, acc [0.98955078 0.98769531 0.98730469 0.96953125]\n",
      "2019-02-23T23:55:46.821953: step 31880, loss 0.0427653, acc [0.98691406 0.98798828 0.98613281 0.96757812]\n",
      "2019-02-23T23:55:48.902174: step 31920, loss 0.0450298, acc [0.98945313 0.98789063 0.98574219 0.96845703]\n",
      "2019-02-23T23:55:50.939245: step 31960, loss 0.0417161, acc [0.98896484 0.98701172 0.98623047 0.96767578]\n",
      "2019-02-23T23:55:52.993181: step 32000, loss 0.0419524, acc [0.98896484 0.98652344 0.98681641 0.96796875]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-23T23:55:56.465182: step 32000, loss 0.364799, acc [0.97682428 0.96635265 0.95545055 0.93586881] \n",
      "\n",
      "2019-02-23T23:55:58.620917: step 32040, loss 0.045174, acc [0.99101562 0.98808594 0.98349609 0.96845703]\n",
      "2019-02-23T23:56:00.656469: step 32080, loss 0.049063, acc [0.99023438 0.98623047 0.98369141 0.96689453]\n",
      "2019-02-23T23:56:02.700483: step 32120, loss 0.0451649, acc [0.99130859 0.98603516 0.98388672 0.96699219]\n",
      "2019-02-23T23:56:04.755412: step 32160, loss 0.0481308, acc [0.98857422 0.98818359 0.98339844 0.96533203]\n",
      "2019-02-23T23:56:06.794963: step 32200, loss 0.0419849, acc [0.98857422 0.98574219 0.98671875 0.96796875]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-23T23:56:10.259523: step 32200, loss 0.355117, acc [0.9767542  0.96606233 0.95490995 0.93544835] \n",
      "\n",
      "2019-02-23T23:56:12.391824: step 32240, loss 0.0487852, acc [0.98964844 0.98720703 0.98251953 0.96484375]\n",
      "2019-02-23T23:56:14.518134: step 32280, loss 0.0376732, acc [0.99130859 0.98710937 0.98691406 0.97041016]\n",
      "2019-02-23T23:56:16.580005: step 32320, loss 0.0513172, acc [0.99023438 0.98623047 0.98320312 0.96748047]\n",
      "2019-02-23T23:56:18.629476: step 32360, loss 0.049255, acc [0.98916016 0.98535156 0.98359375 0.96386719]\n",
      "2019-02-23T23:56:20.692836: step 32400, loss 0.049503, acc [0.98769531 0.98613281 0.98447266 0.96425781]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-23T23:56:24.174294: step 32400, loss 0.353682, acc [0.97619357 0.96637268 0.95428926 0.93530819] \n",
      "\n",
      "2019-02-23T23:56:26.306070: step 32440, loss 0.0449963, acc [0.98896484 0.98623047 0.98691406 0.96757812]\n",
      "2019-02-23T23:56:28.364962: step 32480, loss 0.0435975, acc [0.99042969 0.9859375  0.98554688 0.96757812]\n",
      "2019-02-23T23:56:30.400049: step 32520, loss 0.0441434, acc [0.9890625  0.98505859 0.98408203 0.96435547]\n",
      "2019-02-23T23:56:32.475313: step 32560, loss 0.0562092, acc [0.98564453 0.98310547 0.98115234 0.95830078]\n",
      "2019-02-23T23:56:34.512386: step 32600, loss 0.0471896, acc [0.98818359 0.98417969 0.98339844 0.96337891]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-23T23:56:38.001264: step 32600, loss 0.352194, acc [0.97588323 0.96589214 0.95583097 0.93565858] \n",
      "\n",
      "2019-02-23T23:56:40.138528: step 32640, loss 0.0502191, acc [0.98837891 0.98349609 0.98398438 0.96367187]\n",
      "2019-02-23T23:56:42.188992: step 32680, loss 0.0565438, acc [0.98779297 0.98408203 0.98115234 0.96220703]\n",
      "2019-02-23T23:56:44.249872: step 32720, loss 0.0507902, acc [0.98818359 0.98554688 0.98261719 0.96435547]\n",
      "2019-02-23T23:56:46.283475: step 32760, loss 0.0526698, acc [0.98759766 0.98574219 0.98154297 0.96279297]\n",
      "2019-02-23T23:56:48.487199: step 32800, loss 0.0439388, acc [0.98727608 0.98640901 0.98494417 0.96659663]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-23T23:56:51.958249: step 32800, loss 0.352362, acc [0.9764839  0.96566188 0.9551402  0.93463745] \n",
      "\n",
      "2019-02-23T23:56:54.119276: step 32840, loss 0.032033, acc [0.99150391 0.98935547 0.99042969 0.97529297]\n",
      "2019-02-23T23:56:56.152379: step 32880, loss 0.0312862, acc [0.99208984 0.99072266 0.98828125 0.97421875]\n",
      "2019-02-23T23:56:58.227147: step 32920, loss 0.0311459, acc [0.99082031 0.99277344 0.98847656 0.97607422]\n",
      "2019-02-23T23:57:00.269180: step 32960, loss 0.0332507, acc [0.99082031 0.98964844 0.9890625  0.97460938]\n",
      "2019-02-23T23:57:02.342954: step 33000, loss 0.0316327, acc [0.99003906 0.98945313 0.99111328 0.97509766]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-23T23:57:05.795163: step 33000, loss 0.363389, acc [0.97627366 0.96645276 0.95589104 0.93562855] \n",
      "\n",
      "2019-02-23T23:57:07.936259: step 33040, loss 0.0277288, acc [0.99394531 0.99101562 0.99130859 0.97890625]\n",
      "2019-02-23T23:57:09.980770: step 33080, loss 0.0312668, acc [0.99101562 0.98964844 0.98955078 0.97480469]\n",
      "2019-02-23T23:57:12.041650: step 33120, loss 0.0303529, acc [0.98964844 0.98994141 0.99033203 0.9734375 ]\n",
      "2019-02-23T23:57:14.085665: step 33160, loss 0.0330756, acc [0.990625   0.98671875 0.98847656 0.97001953]\n",
      "2019-02-23T23:57:16.132161: step 33200, loss 0.0308631, acc [0.98984375 0.98955078 0.99013672 0.97421875]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-23T23:57:19.587346: step 33200, loss 0.371598, acc [0.9763037  0.96580204 0.95520027 0.93511798] \n",
      "\n",
      "2019-02-23T23:57:21.714637: step 33240, loss 0.0380291, acc [0.98945313 0.98818359 0.98720703 0.96904297]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-02-23T23:57:23.752244: step 33280, loss 0.0395199, acc [0.99140625 0.98759766 0.98691406 0.97021484]\n",
      "2019-02-23T23:57:25.794996: step 33320, loss 0.0341151, acc [0.99160156 0.99013672 0.9875     0.97285156]\n",
      "2019-02-23T23:57:27.838516: step 33360, loss 0.0416512, acc [0.98857422 0.98896484 0.98662109 0.96884766]\n",
      "2019-02-23T23:57:29.886498: step 33400, loss 0.0415179, acc [0.98925781 0.98779297 0.98623047 0.96826172]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-23T23:57:33.320801: step 33400, loss 0.376484, acc [0.97655397 0.96626255 0.95466968 0.93536826] \n",
      "\n",
      "2019-02-23T23:57:35.455585: step 33440, loss 0.0396945, acc [0.98828125 0.98603516 0.98603516 0.96601563]\n",
      "2019-02-23T23:57:37.502578: step 33480, loss 0.0431529, acc [0.98925781 0.98740234 0.98583984 0.96806641]\n",
      "2019-02-23T23:57:39.585279: step 33520, loss 0.043296, acc [0.98974609 0.98691406 0.98427734 0.96679688]\n",
      "2019-02-23T23:57:41.628303: step 33560, loss 0.0441452, acc [0.990625   0.98652344 0.98583984 0.96816406]\n",
      "2019-02-23T23:57:43.667358: step 33600, loss 0.039821, acc [0.99013672 0.98730469 0.98642578 0.96923828]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-23T23:57:47.150805: step 33600, loss 0.360052, acc [0.97664408 0.96652284 0.95515022 0.93596893] \n",
      "\n",
      "2019-02-23T23:57:49.306949: step 33640, loss 0.0387747, acc [0.99082031 0.98759766 0.98662109 0.96992188]\n",
      "2019-02-23T23:57:51.359396: step 33680, loss 0.0434753, acc [0.99082031 0.98876953 0.98466797 0.96933594]\n",
      "2019-02-23T23:57:53.419577: step 33720, loss 0.0453611, acc [0.98974609 0.98652344 0.98388672 0.96679688]\n",
      "2019-02-23T23:57:55.489882: step 33760, loss 0.0444234, acc [0.98994141 0.98632812 0.98515625 0.96738281]\n",
      "2019-02-23T23:57:57.535387: step 33800, loss 0.0469345, acc [0.98847656 0.98710937 0.98466797 0.96728516]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-23T23:58:01.030285: step 33800, loss 0.358732, acc [0.97732483 0.96582206 0.95523031 0.93583878] \n",
      "\n",
      "2019-02-23T23:58:03.188843: step 33840, loss 0.0462085, acc [0.98886719 0.98642578 0.98496094 0.96591797]\n",
      "2019-02-23T23:58:05.225916: step 33880, loss 0.0440221, acc [0.98828125 0.98554688 0.98554688 0.96523437]\n",
      "2019-02-23T23:58:07.289286: step 33920, loss 0.0480657, acc [0.990625   0.98544922 0.98359375 0.96513672]\n",
      "2019-02-23T23:58:09.336272: step 33960, loss 0.047081, acc [0.98994141 0.98613281 0.984375   0.96650391]\n",
      "2019-02-23T23:58:11.373338: step 34000, loss 0.0462808, acc [0.98876953 0.98652344 0.98427734 0.96542969]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-23T23:58:14.869678: step 34000, loss 0.357964, acc [0.9770245  0.96653285 0.9562114  0.93762076] \n",
      "\n",
      "2019-02-23T23:58:17.009384: step 34040, loss 0.0508682, acc [0.98955078 0.98457031 0.98339844 0.96416016]\n",
      "2019-02-23T23:58:19.078241: step 34080, loss 0.051311, acc [0.98964844 0.984375   0.98369141 0.96386719]\n",
      "2019-02-23T23:58:21.152965: step 34120, loss 0.04601, acc [0.98691406 0.98574219 0.984375   0.96416016]\n",
      "2019-02-23T23:58:23.185113: step 34160, loss 0.0489037, acc [0.98691406 0.9859375  0.98320312 0.96308594]\n",
      "2019-02-23T23:58:25.254389: step 34200, loss 0.0483961, acc [0.98935547 0.98583984 0.98339844 0.96621094]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-23T23:58:28.729906: step 34200, loss 0.355454, acc [0.97634374 0.96699336 0.95619137 0.93673978] \n",
      "\n",
      "2019-02-23T23:58:30.893411: step 34240, loss 0.053231, acc [0.98720703 0.98496094 0.98242188 0.96269531]\n",
      "2019-02-23T23:58:32.926515: step 34280, loss 0.0563693, acc [0.9875     0.98486328 0.98027344 0.96181641]\n",
      "2019-02-23T23:58:34.979953: step 34320, loss 0.0518317, acc [0.98779297 0.98486328 0.98320312 0.96396484]\n",
      "2019-02-23T23:58:37.165367: step 34360, loss 0.0388662, acc [0.98942452 0.9877762  0.98744279 0.97023852]\n",
      "2019-02-23T23:58:39.215792: step 34400, loss 0.0324262, acc [0.99082031 0.98955078 0.98808594 0.97246094]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-23T23:58:42.712848: step 34400, loss 0.358908, acc [0.9771146  0.96668302 0.95708236 0.93761075] \n",
      "\n",
      "2019-02-23T23:58:44.847135: step 34440, loss 0.0280638, acc [0.99189453 0.990625   0.99121094 0.97685547]\n",
      "2019-02-23T23:58:46.916943: step 34480, loss 0.0339547, acc [0.99091797 0.98945313 0.98779297 0.97236328]\n",
      "2019-02-23T23:58:48.980302: step 34520, loss 0.0259583, acc [0.99248047 0.99140625 0.99140625 0.97841797]\n",
      "2019-02-23T23:58:51.032255: step 34560, loss 0.0355473, acc [0.99091797 0.98847656 0.98916016 0.97246094]\n",
      "2019-02-23T23:58:53.115456: step 34600, loss 0.0331369, acc [0.99169922 0.98876953 0.98945313 0.97451172]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-23T23:58:56.611260: step 34600, loss 0.363541, acc [0.97703451 0.96688324 0.95627146 0.93755068] \n",
      "\n",
      "2019-02-23T23:58:58.734634: step 34640, loss 0.0336432, acc [0.99228516 0.98857422 0.98955078 0.975     ]\n",
      "2019-02-23T23:59:00.794522: step 34680, loss 0.0328794, acc [0.99111328 0.98798828 0.98945313 0.97392578]\n",
      "2019-02-23T23:59:02.878714: step 34720, loss 0.0332661, acc [0.98945313 0.98925781 0.99072266 0.97373047]\n",
      "2019-02-23T23:59:04.915785: step 34760, loss 0.0283019, acc [0.990625   0.99003906 0.99042969 0.97568359]\n",
      "2019-02-23T23:59:06.979144: step 34800, loss 0.0341487, acc [0.98994141 0.98867187 0.98818359 0.97080078]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-23T23:59:10.459080: step 34800, loss 0.369966, acc [0.97714463 0.96703341 0.95696223 0.93852176] \n",
      "\n",
      "2019-02-23T23:59:12.620151: step 34840, loss 0.0348568, acc [0.99140625 0.99150391 0.98808594 0.97460938]\n",
      "2019-02-23T23:59:14.670879: step 34880, loss 0.0366918, acc [0.99033203 0.98896484 0.9875     0.97255859]\n",
      "2019-02-23T23:59:16.723326: step 34920, loss 0.0378965, acc [0.9890625  0.98808594 0.98671875 0.96855469]\n",
      "2019-02-23T23:59:18.783212: step 34960, loss 0.0333534, acc [0.99082031 0.98955078 0.98828125 0.97216797]\n",
      "2019-02-23T23:59:20.854011: step 35000, loss 0.0359294, acc [0.99296875 0.98789063 0.98759766 0.97382813]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-23T23:59:24.322043: step 35000, loss 0.36417, acc [0.97670414 0.96708346 0.95594109 0.93699006] \n",
      "\n",
      "2019-02-23T23:59:26.492538: step 35040, loss 0.0328711, acc [0.99140625 0.98984375 0.98818359 0.97373047]\n",
      "2019-02-23T23:59:28.520683: step 35080, loss 0.0390171, acc [0.99003906 0.98720703 0.98574219 0.9671875 ]\n",
      "2019-02-23T23:59:30.587549: step 35120, loss 0.0376384, acc [0.98984375 0.98613281 0.98789063 0.96835938]\n",
      "2019-02-23T23:59:32.629545: step 35160, loss 0.0401867, acc [0.99150391 0.98798828 0.98554688 0.96992188]\n",
      "2019-02-23T23:59:34.683480: step 35200, loss 0.0440741, acc [0.98994141 0.98701172 0.98457031 0.96728516]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-23T23:59:38.162423: step 35200, loss 0.366157, acc [0.97687433 0.96695332 0.95584098 0.93726036] \n",
      "\n",
      "2019-02-23T23:59:40.320520: step 35240, loss 0.0429825, acc [0.98945313 0.98613281 0.98505859 0.96679688]\n",
      "2019-02-23T23:59:42.362055: step 35280, loss 0.0404038, acc [0.99003906 0.98857422 0.98632812 0.97138672]\n",
      "2019-02-23T23:59:44.424421: step 35320, loss 0.0406618, acc [0.98974609 0.98955078 0.98535156 0.96962891]\n",
      "2019-02-23T23:59:46.499685: step 35360, loss 0.038072, acc [0.98886719 0.98681641 0.98779297 0.96923828]\n",
      "2019-02-23T23:59:48.545684: step 35400, loss 0.0451019, acc [0.98994141 0.98759766 0.98447266 0.96669922]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-23T23:59:52.045496: step 35400, loss 0.362924, acc [0.97746498 0.96664297 0.95600116 0.93759073] \n",
      "\n",
      "2019-02-23T23:59:54.201074: step 35440, loss 0.0412722, acc [0.98935547 0.98867187 0.98447266 0.96914062]\n",
      "2019-02-23T23:59:56.285763: step 35480, loss 0.0447765, acc [0.98925781 0.98759766 0.98447266 0.96699219]\n",
      "2019-02-23T23:59:58.346145: step 35520, loss 0.0457997, acc [0.98769531 0.98378906 0.98603516 0.96464844]\n",
      "2019-02-24T00:00:00.407025: step 35560, loss 0.0451447, acc [0.98935547 0.98388672 0.98544922 0.965625  ]\n",
      "2019-02-24T00:00:02.539824: step 35600, loss 0.0426938, acc [0.98857422 0.98535156 0.98476562 0.96572266]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-24T00:00:06.043071: step 35600, loss 0.369092, acc [0.97638379 0.96666299 0.95488993 0.93579874] \n",
      "\n",
      "2019-02-24T00:00:08.220511: step 35640, loss 0.0437761, acc [0.99052734 0.98740234 0.98623047 0.96953125]\n",
      "2019-02-24T00:00:10.314126: step 35680, loss 0.0491275, acc [0.98808594 0.98818359 0.98388672 0.96611328]\n",
      "2019-02-24T00:00:12.389389: step 35720, loss 0.0430097, acc [0.98955078 0.98623047 0.98720703 0.96914062]\n",
      "2019-02-24T00:00:14.476556: step 35760, loss 0.0451889, acc [0.98955078 0.98574219 0.98623047 0.9671875 ]\n",
      "2019-02-24T00:00:16.552815: step 35800, loss 0.0441924, acc [0.98876953 0.98828125 0.98583984 0.96806641]\n",
      "\n",
      "Evaluation:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-02-24T00:00:20.117068: step 35800, loss 0.35872, acc [0.97647389 0.9663126  0.9560312  0.93642944] \n",
      "\n",
      "2019-02-24T00:00:22.267722: step 35840, loss 0.048652, acc [0.98779297 0.98427734 0.98359375 0.96318359]\n",
      "2019-02-24T00:00:24.342793: step 35880, loss 0.0435054, acc [0.98769531 0.98583984 0.98544922 0.96484375]\n",
      "2019-02-24T00:00:26.533623: step 35920, loss 0.0364953, acc [0.98855745 0.9863518  0.98754044 0.9685902 ]\n",
      "2019-02-24T00:00:28.580119: step 35960, loss 0.0269836, acc [0.99335938 0.99013672 0.99238281 0.97861328]\n",
      "2019-02-24T00:00:30.625128: step 36000, loss 0.0285115, acc [0.99228516 0.99101562 0.99101562 0.97783203]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-24T00:00:34.111510: step 36000, loss 0.369529, acc [0.97733484 0.96718357 0.95633153 0.93837159] \n",
      "\n",
      "2019-02-24T00:00:36.285550: step 36040, loss 0.0274469, acc [0.99169922 0.99179688 0.99121094 0.97763672]\n",
      "2019-02-24T00:00:38.379166: step 36080, loss 0.0255455, acc [0.99277344 0.99169922 0.99160156 0.97910156]\n",
      "2019-02-24T00:00:40.423678: step 36120, loss 0.0314267, acc [0.99169922 0.98945313 0.98818359 0.97392578]\n",
      "2019-02-24T00:00:42.482573: step 36160, loss 0.0284465, acc [0.99238281 0.99023438 0.98964844 0.97626953]\n",
      "2019-02-24T00:00:44.581644: step 36200, loss 0.0324567, acc [0.99101562 0.99111328 0.98984375 0.97480469]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-24T00:00:48.051208: step 36200, loss 0.383824, acc [0.97706454 0.96679314 0.95654176 0.93830152] \n",
      "\n",
      "2019-02-24T00:00:50.217196: step 36240, loss 0.0337077, acc [0.99042969 0.98925781 0.98798828 0.97089844]\n",
      "2019-02-24T00:00:52.256249: step 36280, loss 0.0297155, acc [0.99277344 0.99140625 0.98916016 0.97587891]\n",
      "2019-02-24T00:00:54.347385: step 36320, loss 0.0311752, acc [0.99160156 0.99082031 0.98935547 0.97529297]\n",
      "2019-02-24T00:00:56.407818: step 36360, loss 0.0340834, acc [0.99052734 0.99052734 0.98925781 0.97529297]\n",
      "2019-02-24T00:00:58.444344: step 36400, loss 0.0407963, acc [0.98916016 0.98681641 0.98720703 0.96796875]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-24T00:01:01.922509: step 36400, loss 0.378768, acc [0.97710459 0.96724364 0.95639159 0.93857182] \n",
      "\n",
      "2019-02-24T00:01:04.068204: step 36440, loss 0.0348895, acc [0.98994141 0.98994141 0.98837891 0.97167969]\n",
      "2019-02-24T00:01:06.141006: step 36480, loss 0.0323611, acc [0.99140625 0.98955078 0.98808594 0.97304687]\n",
      "2019-02-24T00:01:08.224253: step 36520, loss 0.0320828, acc [0.99306641 0.99013672 0.98818359 0.97441406]\n",
      "2019-02-24T00:01:10.278141: step 36560, loss 0.0343292, acc [0.99082031 0.98867187 0.98828125 0.97246094]\n",
      "2019-02-24T00:01:12.347451: step 36600, loss 0.0293608, acc [0.99072266 0.98974609 0.99003906 0.97382813]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-24T00:01:15.856701: step 36600, loss 0.383221, acc [0.97741493 0.96694331 0.95660183 0.93840163] \n",
      "\n",
      "2019-02-24T00:01:18.008298: step 36640, loss 0.0408069, acc [0.98974609 0.98662109 0.98671875 0.96787109]\n",
      "2019-02-24T00:01:20.043386: step 36680, loss 0.033725, acc [0.99023438 0.99042969 0.98916016 0.97382813]\n",
      "2019-02-24T00:01:22.101785: step 36720, loss 0.0346748, acc [0.99160156 0.98828125 0.98847656 0.97275391]\n",
      "2019-02-24T00:01:24.159192: step 36760, loss 0.0402847, acc [0.99179688 0.98818359 0.98564453 0.97089844]\n",
      "2019-02-24T00:01:26.247848: step 36800, loss 0.0442289, acc [0.98955078 0.98730469 0.98603516 0.96904297]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-24T00:01:29.745142: step 36800, loss 0.381107, acc [0.97713462 0.96636266 0.95666189 0.93758072] \n",
      "\n",
      "2019-02-24T00:01:31.868518: step 36840, loss 0.0428965, acc [0.99082031 0.98798828 0.98623047 0.97080078]\n",
      "2019-02-24T00:01:33.916006: step 36880, loss 0.0414963, acc [0.990625   0.98720703 0.98740234 0.97060547]\n",
      "2019-02-24T00:01:36.015076: step 36920, loss 0.045094, acc [0.98994141 0.98740234 0.98486328 0.96884766]\n",
      "2019-02-24T00:01:38.061573: step 36960, loss 0.0391583, acc [0.98974609 0.98525391 0.98710937 0.96757812]\n",
      "2019-02-24T00:01:40.153204: step 37000, loss 0.0359255, acc [0.98925781 0.98876953 0.98828125 0.97089844]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-24T00:01:43.699109: step 37000, loss 0.368941, acc [0.97737489 0.96681316 0.95566078 0.93750063] \n",
      "\n",
      "2019-02-24T00:01:45.848362: step 37040, loss 0.0445812, acc [0.99082031 0.98740234 0.98466797 0.96884766]\n",
      "2019-02-24T00:01:47.913210: step 37080, loss 0.0357182, acc [0.99023438 0.98789063 0.98710937 0.97001953]\n",
      "2019-02-24T00:01:49.994921: step 37120, loss 0.036218, acc [0.98935547 0.98710937 0.98808594 0.97001953]\n",
      "2019-02-24T00:01:52.065224: step 37160, loss 0.0416617, acc [0.98974609 0.98779297 0.98535156 0.96845703]\n",
      "2019-02-24T00:01:54.137016: step 37200, loss 0.0424582, acc [0.99189453 0.98730469 0.98535156 0.96943359]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-24T00:01:57.663078: step 37200, loss 0.373543, acc [0.97678423 0.9672036  0.95628147 0.93792109] \n",
      "\n",
      "2019-02-24T00:01:59.835560: step 37240, loss 0.0420609, acc [0.99082031 0.98671875 0.98632812 0.96914062]\n",
      "2019-02-24T00:02:01.890981: step 37280, loss 0.0476217, acc [0.99013672 0.98603516 0.98535156 0.96699219]\n",
      "2019-02-24T00:02:03.926068: step 37320, loss 0.0426115, acc [0.99033203 0.98730469 0.98447266 0.96767578]\n",
      "2019-02-24T00:02:05.995876: step 37360, loss 0.0428166, acc [0.98681641 0.98466797 0.98554688 0.96494141]\n",
      "2019-02-24T00:02:08.059732: step 37400, loss 0.0409142, acc [0.99121094 0.98818359 0.98564453 0.96943359]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-24T00:02:11.597202: step 37400, loss 0.367092, acc [0.97716465 0.96715354 0.95677202 0.93834156] \n",
      "\n",
      "2019-02-24T00:02:13.761746: step 37440, loss 0.0456525, acc [0.98964844 0.98544922 0.98408203 0.96601563]\n",
      "2019-02-24T00:02:15.942658: step 37480, loss 0.0407215, acc [0.99069405 0.98793107 0.98666154 0.97081262]\n",
      "2019-02-24T00:02:17.996097: step 37520, loss 0.0268103, acc [0.99208984 0.99130859 0.99121094 0.97714844]\n",
      "2019-02-24T00:02:20.063425: step 37560, loss 0.0272291, acc [0.99238281 0.99140625 0.98974609 0.97695312]\n",
      "2019-02-24T00:02:22.126286: step 37600, loss 0.0308563, acc [0.99296875 0.99042969 0.98994141 0.97724609]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-24T00:02:25.615403: step 37600, loss 0.376154, acc [0.97740492 0.96801449 0.9573827  0.93967304] \n",
      "\n",
      "2019-02-24T00:02:27.745228: step 37640, loss 0.0257822, acc [0.99140625 0.98916016 0.99140625 0.97548828]\n",
      "2019-02-24T00:02:29.834161: step 37680, loss 0.0313515, acc [0.99140625 0.98935547 0.98837891 0.97333984]\n",
      "2019-02-24T00:02:31.882144: step 37720, loss 0.0284149, acc [0.99169922 0.98837891 0.99052734 0.97402344]\n",
      "2019-02-24T00:02:33.938559: step 37760, loss 0.0287361, acc [0.99257812 0.98916016 0.99150391 0.97714844]\n",
      "2019-02-24T00:02:36.022751: step 37800, loss 0.03053, acc [0.99160156 0.990625   0.99052734 0.97509766]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-24T00:02:39.540381: step 37800, loss 0.377087, acc [0.97731482 0.96701338 0.95706234 0.9390123 ] \n",
      "\n",
      "2019-02-24T00:02:41.716358: step 37840, loss 0.0300173, acc [0.99238281 0.98808594 0.99023438 0.975     ]\n",
      "2019-02-24T00:02:43.766325: step 37880, loss 0.0305306, acc [0.99160156 0.98916016 0.99003906 0.97363281]\n",
      "2019-02-24T00:02:45.811831: step 37920, loss 0.0307219, acc [0.99257812 0.99091797 0.98837891 0.97539062]\n",
      "2019-02-24T00:02:47.861301: step 37960, loss 0.0330788, acc [0.99160156 0.99111328 0.98828125 0.97480469]\n",
      "2019-02-24T00:02:49.908293: step 38000, loss 0.0311893, acc [0.99150391 0.98935547 0.98994141 0.97470703]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-24T00:02:53.437330: step 38000, loss 0.385627, acc [0.976574   0.96746389 0.95683208 0.93884212] \n",
      "\n",
      "2019-02-24T00:02:55.566658: step 38040, loss 0.0333451, acc [0.99140625 0.99130859 0.98828125 0.97431641]\n",
      "2019-02-24T00:02:57.652835: step 38080, loss 0.03553, acc [0.99179688 0.98945313 0.98867187 0.975     ]\n",
      "2019-02-24T00:02:59.697131: step 38120, loss 0.0360333, acc [0.99140625 0.98925781 0.98701172 0.97236328]\n",
      "2019-02-24T00:03:01.776363: step 38160, loss 0.0331428, acc [0.99160156 0.98916016 0.9890625  0.97431641]\n",
      "2019-02-24T00:03:03.836745: step 38200, loss 0.0325593, acc [0.99228516 0.98925781 0.98886719 0.9734375 ]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-24T00:03:07.315193: step 38200, loss 0.379239, acc [0.97740492 0.96763407 0.95758292 0.93962298] \n",
      "\n",
      "2019-02-24T00:03:09.440189: step 38240, loss 0.034454, acc [0.99121094 0.98759766 0.98857422 0.97158203]\n",
      "2019-02-24T00:03:11.516442: step 38280, loss 0.0376415, acc [0.98789063 0.98769531 0.98779297 0.96914062]\n",
      "2019-02-24T00:03:13.583272: step 38320, loss 0.0387172, acc [0.98964844 0.98935547 0.9875     0.96933594]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-02-24T00:03:15.638697: step 38360, loss 0.0370905, acc [0.99160156 0.98974609 0.98720703 0.97275391]\n",
      "2019-02-24T00:03:17.721400: step 38400, loss 0.0331079, acc [0.99111328 0.99072266 0.98789063 0.97314453]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-24T00:03:21.206745: step 38400, loss 0.378723, acc [0.97725475 0.96735376 0.95665188 0.93878205] \n",
      "\n",
      "2019-02-24T00:03:23.345994: step 38440, loss 0.0339991, acc [0.99257812 0.990625   0.98808594 0.97529297]\n",
      "2019-02-24T00:03:25.395959: step 38480, loss 0.034627, acc [0.99121094 0.98896484 0.9875     0.97148437]\n",
      "2019-02-24T00:03:27.464280: step 38520, loss 0.035425, acc [0.98994141 0.98847656 0.9875     0.97128906]\n",
      "2019-02-24T00:03:29.562359: step 38560, loss 0.0402729, acc [0.99179688 0.99042969 0.98554688 0.97167969]\n",
      "2019-02-24T00:03:31.599430: step 38600, loss 0.0438727, acc [0.99052734 0.98818359 0.98535156 0.96943359]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-24T00:03:35.071924: step 38600, loss 0.375765, acc [0.97777533 0.96743385 0.95533042 0.93794112] \n",
      "\n",
      "2019-02-24T00:03:37.206709: step 38640, loss 0.0386561, acc [0.98974609 0.98955078 0.98632812 0.97041016]\n",
      "2019-02-24T00:03:39.252213: step 38680, loss 0.0396914, acc [0.99111328 0.99033203 0.98701172 0.97294922]\n",
      "2019-02-24T00:03:41.312595: step 38720, loss 0.0366314, acc [0.99169922 0.98945313 0.9875     0.97304687]\n",
      "2019-02-24T00:03:43.389843: step 38760, loss 0.0436644, acc [0.98935547 0.98652344 0.98525391 0.96816406]\n",
      "2019-02-24T00:03:45.446293: step 38800, loss 0.0410608, acc [0.99101562 0.98642578 0.98554688 0.96875   ]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-24T00:03:48.958433: step 38800, loss 0.373541, acc [0.97736487 0.96767412 0.95683208 0.93902232] \n",
      "\n",
      "2019-02-24T00:03:51.092223: step 38840, loss 0.0376685, acc [0.98896484 0.98828125 0.98740234 0.96943359]\n",
      "2019-02-24T00:03:53.166498: step 38880, loss 0.0455721, acc [0.98935547 0.98632812 0.9859375  0.96855469]\n",
      "2019-02-24T00:03:55.220926: step 38920, loss 0.0504277, acc [0.98925781 0.98564453 0.98466797 0.96542969]\n",
      "2019-02-24T00:03:57.276847: step 38960, loss 0.0412832, acc [0.99082031 0.98828125 0.98564453 0.96992188]\n",
      "2019-02-24T00:03:59.346195: step 39000, loss 0.0425463, acc [0.99160156 0.98632812 0.98574219 0.96884766]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-24T00:04:02.847421: step 39000, loss 0.376316, acc [0.97737489 0.96692328 0.95579093 0.93787104] \n",
      "\n",
      "2019-02-24T00:04:05.087852: step 39040, loss 0.0415643, acc [0.99203263 0.98793107 0.98594934 0.96997416]\n",
      "2019-02-24T00:04:07.207261: step 39080, loss 0.0307229, acc [0.99316406 0.99140625 0.99111328 0.97822266]\n",
      "2019-02-24T00:04:09.250778: step 39120, loss 0.0237983, acc [0.99287109 0.99140625 0.99248047 0.97929687]\n",
      "2019-02-24T00:04:11.323066: step 39160, loss 0.0278826, acc [0.99140625 0.99042969 0.99140625 0.97626953]\n",
      "2019-02-24T00:04:13.393370: step 39200, loss 0.0301071, acc [0.99003906 0.99121094 0.98964844 0.97451172]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-24T00:04:16.894136: step 39200, loss 0.388357, acc [0.97717466 0.96743385 0.95714243 0.93912242] \n",
      "\n",
      "2019-02-24T00:04:19.020247: step 39240, loss 0.0285634, acc [0.99257812 0.99238281 0.99013672 0.97802734]\n",
      "2019-02-24T00:04:21.104932: step 39280, loss 0.0293717, acc [0.99091797 0.99052734 0.99150391 0.97695312]\n",
      "2019-02-24T00:04:23.165827: step 39320, loss 0.0277154, acc [0.99238281 0.99160156 0.99023438 0.97695312]\n",
      "2019-02-24T00:04:25.235620: step 39360, loss 0.0274286, acc [0.99326172 0.99023438 0.99003906 0.97636719]\n",
      "2019-02-24T00:04:27.274674: step 39400, loss 0.0327019, acc [0.99267578 0.98886719 0.98857422 0.97470703]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-24T00:04:30.744193: step 39400, loss 0.39151, acc [0.97703451 0.96713352 0.95681206 0.93863188] \n",
      "\n",
      "2019-02-24T00:04:32.871581: step 39440, loss 0.0307341, acc [0.99082031 0.99052734 0.98964844 0.97412109]\n",
      "2019-02-24T00:04:34.956534: step 39480, loss 0.0272693, acc [0.99228516 0.99072266 0.99013672 0.97597656]\n",
      "2019-02-24T00:04:37.021878: step 39520, loss 0.0355442, acc [0.99101562 0.98955078 0.98740234 0.97275391]\n",
      "2019-02-24T00:04:39.076311: step 39560, loss 0.029276, acc [0.99013672 0.99160156 0.99052734 0.97587891]\n",
      "2019-02-24T00:04:41.145202: step 39600, loss 0.0294043, acc [0.99306641 0.98984375 0.98935547 0.97568359]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-24T00:04:44.708960: step 39600, loss 0.392992, acc [0.97739491 0.96804453 0.95710238 0.9398232 ] \n",
      "\n",
      "2019-02-24T00:04:46.859616: step 39640, loss 0.0344492, acc [0.99257812 0.99013672 0.98974609 0.97685547]\n",
      "2019-02-24T00:04:48.884287: step 39680, loss 0.033722, acc [0.99101562 0.99003906 0.98935547 0.97490234]\n",
      "2019-02-24T00:04:50.949630: step 39720, loss 0.0319371, acc [0.99101562 0.98974609 0.98818359 0.97255859]\n",
      "2019-02-24T00:04:53.015965: step 39760, loss 0.0308106, acc [0.99091797 0.99013672 0.98945313 0.97353516]\n",
      "2019-02-24T00:04:55.078333: step 39800, loss 0.0301568, acc [0.99033203 0.98974609 0.98945313 0.97285156]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-24T00:04:58.524044: step 39800, loss 0.386782, acc [0.97715464 0.96763407 0.95758292 0.93934267] \n",
      "\n",
      "2019-02-24T00:05:00.659325: step 39840, loss 0.0326612, acc [0.98964844 0.98935547 0.98896484 0.97285156]\n",
      "2019-02-24T00:05:02.705820: step 39880, loss 0.0319005, acc [0.99208984 0.98955078 0.99101562 0.9765625 ]\n",
      "2019-02-24T00:05:04.780587: step 39920, loss 0.034501, acc [0.99248047 0.98886719 0.98808594 0.97314453]\n",
      "2019-02-24T00:05:06.820137: step 39960, loss 0.0323882, acc [0.99091797 0.9890625  0.98916016 0.97255859]\n",
      "2019-02-24T00:05:08.865644: step 40000, loss 0.0368618, acc [0.99042969 0.99023438 0.98837891 0.97402344]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-24T00:05:12.496857: step 40000, loss 0.386412, acc [0.97751504 0.96744386 0.95740272 0.93988327] \n",
      "\n",
      "2019-02-24T00:05:14.685208: step 40040, loss 0.0341819, acc [0.99101562 0.98847656 0.98808594 0.97167969]\n",
      "2019-02-24T00:05:16.790728: step 40080, loss 0.0398664, acc [0.99150391 0.9890625  0.98720703 0.97304687]\n",
      "2019-02-24T00:05:18.906663: step 40120, loss 0.0370986, acc [0.99091797 0.98759766 0.98876953 0.97109375]\n",
      "2019-02-24T00:05:20.977508: step 40160, loss 0.040706, acc [0.98935547 0.98720703 0.98623047 0.96816406]\n",
      "2019-02-24T00:05:23.037889: step 40200, loss 0.0391806, acc [0.98789063 0.98779297 0.98681641 0.96933594]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-24T00:05:26.617476: step 40200, loss 0.385052, acc [0.97690436 0.96748391 0.95706234 0.93923255] \n",
      "\n",
      "2019-02-24T00:05:28.778576: step 40240, loss 0.045166, acc [0.98945313 0.98623047 0.98457031 0.96689453]\n",
      "2019-02-24T00:05:30.858808: step 40280, loss 0.0385714, acc [0.99033203 0.98925781 0.98798828 0.97158203]\n",
      "2019-02-24T00:05:32.944947: step 40320, loss 0.039349, acc [0.98808594 0.9875     0.98740234 0.96884766]\n",
      "2019-02-24T00:05:35.032610: step 40360, loss 0.0394999, acc [0.99042969 0.98828125 0.98671875 0.9703125 ]\n",
      "2019-02-24T00:05:37.154003: step 40400, loss 0.0358805, acc [0.98798828 0.98779297 0.98720703 0.96757812]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-24T00:05:41.124481: step 40400, loss 0.386877, acc [0.97725475 0.96805454 0.95717246 0.93936269] \n",
      "\n",
      "2019-02-24T00:05:43.362927: step 40440, loss 0.0395411, acc [0.98847656 0.9875     0.98671875 0.96904297]\n",
      "2019-02-24T00:05:45.534915: step 40480, loss 0.0352672, acc [0.98994141 0.98955078 0.98808594 0.97236328]\n",
      "2019-02-24T00:05:47.625054: step 40520, loss 0.0378524, acc [0.98984375 0.98886719 0.98808594 0.97158203]\n",
      "2019-02-24T00:05:49.738511: step 40560, loss 0.0405684, acc [0.99033203 0.9875     0.98583984 0.96845703]\n",
      "2019-02-24T00:05:51.954638: step 40600, loss 0.0378907, acc [0.99108468 0.98871232 0.98677103 0.97184146]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-24T00:05:55.487686: step 40600, loss 0.386298, acc [0.97719469 0.96753396 0.95665188 0.93899228] \n",
      "\n",
      "2019-02-24T00:05:57.614987: step 40640, loss 0.0277095, acc [0.99267578 0.99091797 0.98964844 0.97568359]\n",
      "2019-02-24T00:05:59.673882: step 40680, loss 0.0270193, acc [0.99375    0.99160156 0.99072266 0.97880859]\n",
      "2019-02-24T00:06:01.733274: step 40720, loss 0.0271177, acc [0.99384766 0.99042969 0.99042969 0.97832031]\n",
      "2019-02-24T00:06:03.793163: step 40760, loss 0.0262726, acc [0.99335938 0.990625   0.99111328 0.97705078]\n",
      "2019-02-24T00:06:05.843666: step 40800, loss 0.0253199, acc [0.99326172 0.99277344 0.99160156 0.98017578]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-24T00:06:09.327528: step 40800, loss 0.39678, acc [0.97757511 0.96818469 0.95795333 0.94098449] \n",
      "\n",
      "2019-02-24T00:06:11.452394: step 40840, loss 0.0225037, acc [0.99179688 0.99199219 0.99248047 0.97880859]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-02-24T00:06:13.522696: step 40880, loss 0.0229622, acc [0.99296875 0.99169922 0.99228516 0.97919922]\n",
      "2019-02-24T00:06:15.612838: step 40920, loss 0.0275542, acc [0.99277344 0.99072266 0.99023438 0.97714844]\n",
      "2019-02-24T00:06:17.666278: step 40960, loss 0.025988, acc [0.99111328 0.99130859 0.99179688 0.97783203]\n",
      "2019-02-24T00:06:19.709798: step 41000, loss 0.0271831, acc [0.99150391 0.98964844 0.99013672 0.97402344]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-24T00:06:23.190759: step 41000, loss 0.401686, acc [0.97750503 0.96807456 0.95758292 0.93993333] \n",
      "\n",
      "2019-02-24T00:06:25.331459: step 41040, loss 0.0330431, acc [0.99257812 0.99003906 0.98896484 0.97490234]\n",
      "2019-02-24T00:06:27.400809: step 41080, loss 0.028472, acc [0.99277344 0.99111328 0.99111328 0.97792969]\n",
      "2019-02-24T00:06:29.434868: step 41120, loss 0.0294887, acc [0.99306641 0.99199219 0.99052734 0.97890625]\n",
      "2019-02-24T00:06:31.491824: step 41160, loss 0.0326478, acc [0.99228516 0.99111328 0.98984375 0.97802734]\n",
      "2019-02-24T00:06:33.549224: step 41200, loss 0.0341082, acc [0.99306641 0.99013672 0.98857422 0.97490234]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-24T00:06:37.042513: step 41200, loss 0.405221, acc [0.97790547 0.96694331 0.95687213 0.93876203] \n",
      "\n",
      "2019-02-24T00:06:39.171839: step 41240, loss 0.0284675, acc [0.99208984 0.99267578 0.99013672 0.97753906]\n",
      "2019-02-24T00:06:41.214368: step 41280, loss 0.0316941, acc [0.99033203 0.99023438 0.9890625  0.97324219]\n",
      "2019-02-24T00:06:43.277727: step 41320, loss 0.0309417, acc [0.99140625 0.98994141 0.99033203 0.97421875]\n",
      "2019-02-24T00:06:45.358447: step 41360, loss 0.0347364, acc [0.99160156 0.99042969 0.98740234 0.97373047]\n",
      "2019-02-24T00:06:47.437703: step 41400, loss 0.0336603, acc [0.99130859 0.99013672 0.98945313 0.97568359]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-24T00:06:50.890332: step 41400, loss 0.396116, acc [0.97746499 0.96735376 0.95767302 0.93958294] \n",
      "\n",
      "2019-02-24T00:06:53.018172: step 41440, loss 0.0367812, acc [0.99160156 0.98886719 0.98730469 0.97138672]\n",
      "2019-02-24T00:06:55.102409: step 41480, loss 0.0374253, acc [0.99199219 0.98964844 0.98808594 0.97421875]\n",
      "2019-02-24T00:06:57.135962: step 41520, loss 0.0364926, acc [0.99257812 0.99082031 0.98730469 0.97460938]\n",
      "2019-02-24T00:06:59.185930: step 41560, loss 0.0367032, acc [0.98925781 0.98984375 0.98759766 0.97177734]\n",
      "2019-02-24T00:07:01.269625: step 41600, loss 0.0356834, acc [0.99033203 0.98798828 0.98886719 0.97324219]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-24T00:07:04.819000: step 41600, loss 0.398681, acc [0.97731482 0.96768413 0.95660183 0.93881208] \n",
      "\n",
      "2019-02-24T00:07:06.938455: step 41640, loss 0.0309565, acc [0.99130859 0.98984375 0.99003906 0.97470703]\n",
      "2019-02-24T00:07:08.987917: step 41680, loss 0.0376012, acc [0.99023438 0.99003906 0.9875     0.97255859]\n",
      "2019-02-24T00:07:11.049789: step 41720, loss 0.0380408, acc [0.99150391 0.98828125 0.98730469 0.97236328]\n",
      "2019-02-24T00:07:13.102694: step 41760, loss 0.0398018, acc [0.99072266 0.9859375  0.98710937 0.96953125]\n",
      "2019-02-24T00:07:15.208709: step 41800, loss 0.0380688, acc [0.99140625 0.9875     0.98740234 0.97099609]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-24T00:07:18.684676: step 41800, loss 0.390671, acc [0.97764519 0.96739381 0.95746278 0.94015357] \n",
      "\n",
      "2019-02-24T00:07:20.867077: step 41840, loss 0.0364255, acc [0.99052734 0.98613281 0.98857422 0.97041016]\n",
      "2019-02-24T00:07:22.923035: step 41880, loss 0.0385721, acc [0.99199219 0.98945313 0.9859375  0.97246094]\n",
      "2019-02-24T00:07:25.004210: step 41920, loss 0.04106, acc [0.99238281 0.98896484 0.9859375  0.97119141]\n",
      "2019-02-24T00:07:27.042274: step 41960, loss 0.0401615, acc [0.99052734 0.98798828 0.98603516 0.97050781]\n",
      "2019-02-24T00:07:29.090257: step 42000, loss 0.0406123, acc [0.99091797 0.98632812 0.98642578 0.97001953]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-24T00:07:32.582592: step 42000, loss 0.38255, acc [0.97717466 0.9672937  0.95612129 0.93825146] \n",
      "\n",
      "2019-02-24T00:07:34.776897: step 42040, loss 0.0413493, acc [0.99228516 0.98789063 0.98544922 0.97167969]\n",
      "2019-02-24T00:07:36.828351: step 42080, loss 0.0377033, acc [0.98945313 0.99042969 0.98623047 0.97099609]\n",
      "2019-02-24T00:07:38.864927: step 42120, loss 0.0344925, acc [0.99091797 0.98701172 0.98769531 0.97138672]\n",
      "2019-02-24T00:07:41.067166: step 42160, loss 0.0370711, acc [0.99102746 0.99041292 0.98767854 0.97372554]\n",
      "2019-02-24T00:07:43.121102: step 42200, loss 0.0246871, acc [0.99345703 0.9921875  0.99208984 0.98027344]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-24T00:07:46.697790: step 42200, loss 0.384025, acc [0.97780537 0.96728368 0.95822363 0.9404539 ] \n",
      "\n",
      "2019-02-24T00:07:48.851425: step 42240, loss 0.0252353, acc [0.99287109 0.99277344 0.99121094 0.97939453]\n",
      "2019-02-24T00:07:50.905324: step 42280, loss 0.025691, acc [0.99326172 0.99169922 0.99072266 0.97861328]\n",
      "2019-02-24T00:07:52.952316: step 42320, loss 0.0268526, acc [0.99199219 0.99052734 0.99160156 0.97734375]\n",
      "2019-02-24T00:07:54.993850: step 42360, loss 0.0256748, acc [0.99335938 0.99267578 0.99179688 0.98046875]\n",
      "2019-02-24T00:07:57.054235: step 42400, loss 0.0272762, acc [0.99160156 0.99179688 0.99121094 0.97714844]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-24T00:08:00.538632: step 42400, loss 0.396126, acc [0.97770525 0.9673838  0.95770305 0.93974312] \n",
      "\n",
      "2019-02-24T00:08:02.687304: step 42440, loss 0.0238237, acc [0.99365234 0.99277344 0.99208984 0.98095703]\n",
      "2019-02-24T00:08:04.716486: step 42480, loss 0.0294933, acc [0.99189453 0.98994141 0.98945313 0.97451172]\n",
      "2019-02-24T00:08:06.778807: step 42520, loss 0.0282311, acc [0.99248047 0.99101562 0.99101562 0.97822266]\n",
      "2019-02-24T00:08:08.832306: step 42560, loss 0.0275693, acc [0.99365234 0.990625   0.99101562 0.97783203]\n",
      "2019-02-24T00:08:10.857414: step 42600, loss 0.0243891, acc [0.99248047 0.99248047 0.9921875  0.97958984]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-24T00:08:14.401379: step 42600, loss 0.397724, acc [0.97759513 0.968465   0.95829371 0.94111464] \n",
      "\n",
      "2019-02-24T00:08:16.557494: step 42640, loss 0.0264576, acc [0.99326172 0.99169922 0.99160156 0.97929687]\n",
      "2019-02-24T00:08:18.624817: step 42680, loss 0.0307832, acc [0.99355469 0.99179688 0.98964844 0.97773438]\n",
      "2019-02-24T00:08:20.687139: step 42720, loss 0.0332814, acc [0.99179688 0.99042969 0.99042969 0.97675781]\n",
      "2019-02-24T00:08:22.791170: step 42760, loss 0.0314498, acc [0.99306641 0.98974609 0.99082031 0.97685547]\n",
      "2019-02-24T00:08:24.859045: step 42800, loss 0.0276617, acc [0.99179688 0.99042969 0.99072266 0.97773438]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-24T00:08:28.341409: step 42800, loss 0.396473, acc [0.97834596 0.96831483 0.95829371 0.94159517] \n",
      "\n",
      "2019-02-24T00:08:30.497025: step 42840, loss 0.0271183, acc [0.99248047 0.99199219 0.99042969 0.97763672]\n",
      "2019-02-24T00:08:32.530624: step 42880, loss 0.0286947, acc [0.99326172 0.99042969 0.99033203 0.97734375]\n",
      "2019-02-24T00:08:34.585551: step 42920, loss 0.0325286, acc [0.99179688 0.99101562 0.98964844 0.97578125]\n",
      "2019-02-24T00:08:36.682670: step 42960, loss 0.0319922, acc [0.99228516 0.98935547 0.98857422 0.975     ]\n",
      "2019-02-24T00:08:38.740045: step 43000, loss 0.0280191, acc [0.99296875 0.98974609 0.99052734 0.97607422]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-24T00:08:42.211591: step 43000, loss 0.407964, acc [0.97826588 0.9672937  0.95745277 0.94021364] \n",
      "\n",
      "2019-02-24T00:08:44.398412: step 43040, loss 0.0347586, acc [0.99042969 0.98896484 0.98798828 0.97197266]\n",
      "2019-02-24T00:08:46.453836: step 43080, loss 0.0284066, acc [0.99160156 0.99199219 0.98994141 0.97636719]\n",
      "2019-02-24T00:08:48.527157: step 43120, loss 0.0315129, acc [0.9921875  0.98945313 0.9890625  0.97402344]\n",
      "2019-02-24T00:08:50.606841: step 43160, loss 0.0325229, acc [0.99179688 0.99130859 0.98779297 0.97451172]\n",
      "2019-02-24T00:08:52.659289: step 43200, loss 0.0343957, acc [0.99257812 0.99052734 0.98955078 0.97744141]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-24T00:08:56.156088: step 43200, loss 0.400919, acc [0.97824585 0.96751394 0.95703231 0.93984322] \n",
      "\n",
      "2019-02-24T00:08:58.324118: step 43240, loss 0.0360952, acc [0.99023438 0.98759766 0.98847656 0.97060547]\n",
      "2019-02-24T00:09:00.375065: step 43280, loss 0.0352165, acc [0.99150391 0.98876953 0.98828125 0.97294922]\n",
      "2019-02-24T00:09:02.439414: step 43320, loss 0.0405213, acc [0.99140625 0.98740234 0.98740234 0.971875  ]\n",
      "2019-02-24T00:09:04.496326: step 43360, loss 0.0388706, acc [0.99101562 0.98857422 0.98710937 0.97158203]\n",
      "2019-02-24T00:09:06.578533: step 43400, loss 0.0375345, acc [0.99150391 0.98984375 0.9875     0.97373047]\n",
      "\n",
      "Evaluation:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-02-24T00:09:10.070869: step 43400, loss 0.399437, acc [0.97785542 0.96721361 0.95735266 0.93989328] \n",
      "\n",
      "2019-02-24T00:09:12.203173: step 43440, loss 0.0374671, acc [0.99238281 0.98740234 0.98642578 0.97148437]\n",
      "2019-02-24T00:09:14.269508: step 43480, loss 0.0357178, acc [0.990625   0.99003906 0.98798828 0.97333984]\n",
      "2019-02-24T00:09:16.334855: step 43520, loss 0.0416303, acc [0.99033203 0.98867187 0.98720703 0.97177734]\n",
      "2019-02-24T00:09:18.410115: step 43560, loss 0.0384045, acc [0.99121094 0.98974609 0.98789063 0.97275391]\n",
      "2019-02-24T00:09:20.471049: step 43600, loss 0.0343775, acc [0.99267578 0.98867187 0.98779297 0.97373047]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-24T00:09:23.942497: step 43600, loss 0.385381, acc [0.97840603 0.96787434 0.95747279 0.9408143 ] \n",
      "\n",
      "2019-02-24T00:09:26.099601: step 43640, loss 0.0400452, acc [0.99082031 0.9875     0.98583984 0.96923828]\n",
      "2019-02-24T00:09:28.180817: step 43680, loss 0.0403739, acc [0.99003906 0.98720703 0.98691406 0.96992188]\n",
      "2019-02-24T00:09:30.371646: step 43720, loss 0.03509, acc [0.9930378  0.98959122 0.98864327 0.97498323]\n",
      "2019-02-24T00:09:32.423598: step 43760, loss 0.0239177, acc [0.99414062 0.99277344 0.99179688 0.98076172]\n",
      "2019-02-24T00:09:34.499854: step 43800, loss 0.0278715, acc [0.99414062 0.99287109 0.990625   0.98027344]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-24T00:09:37.999681: step 43800, loss 0.402119, acc [0.97808568 0.96767412 0.95806345 0.94096447] \n",
      "\n",
      "2019-02-24T00:09:40.169133: step 43840, loss 0.0243609, acc [0.99267578 0.99296875 0.99091797 0.97900391]\n",
      "2019-02-24T00:09:42.248860: step 43880, loss 0.0198293, acc [0.99423828 0.99257812 0.99335938 0.98232422]\n",
      "2019-02-24T00:09:44.366283: step 43920, loss 0.023368, acc [0.99462891 0.99306641 0.99267578 0.98222656]\n",
      "2019-02-24T00:09:46.475770: step 43960, loss 0.0303059, acc [0.99257812 0.99296875 0.99042969 0.97851562]\n",
      "2019-02-24T00:09:48.529707: step 44000, loss 0.027566, acc [0.99316406 0.99179688 0.99111328 0.97871094]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-24T00:09:52.168362: step 44000, loss 0.405027, acc [0.97794552 0.96770415 0.95713241 0.93985324] \n",
      "\n",
      "2019-02-24T00:09:54.361672: step 44040, loss 0.025701, acc [0.99287109 0.99042969 0.99199219 0.97900391]\n",
      "2019-02-24T00:09:56.463720: step 44080, loss 0.0268987, acc [0.99189453 0.99042969 0.99082031 0.97714844]\n",
      "2019-02-24T00:09:58.548904: step 44120, loss 0.0231487, acc [0.99111328 0.99082031 0.99287109 0.97773438]\n",
      "2019-02-24T00:10:00.663351: step 44160, loss 0.0306799, acc [0.99355469 0.99091797 0.98916016 0.97734375]\n",
      "2019-02-24T00:10:02.748039: step 44200, loss 0.0270564, acc [0.99306641 0.99160156 0.99150391 0.97910156]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-24T00:10:06.260212: step 44200, loss 0.410799, acc [0.97803562 0.96795443 0.95768303 0.94121475] \n",
      "\n",
      "2019-02-24T00:10:08.439140: step 44240, loss 0.0259609, acc [0.99296875 0.99130859 0.99082031 0.97851562]\n",
      "2019-02-24T00:10:10.533747: step 44280, loss 0.0319067, acc [0.99179688 0.99316406 0.98867187 0.97734375]\n",
      "2019-02-24T00:10:12.622405: step 44320, loss 0.0266713, acc [0.99375    0.99267578 0.99101562 0.97998047]\n",
      "2019-02-24T00:10:14.679812: step 44360, loss 0.0306702, acc [0.99248047 0.99033203 0.98935547 0.97558594]\n",
      "2019-02-24T00:10:16.764498: step 44400, loss 0.0293755, acc [0.99316406 0.99257812 0.98964844 0.97822266]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-24T00:10:20.262785: step 44400, loss 0.417721, acc [0.97806566 0.96754397 0.95700227 0.93996336] \n",
      "\n",
      "2019-02-24T00:10:22.411953: step 44440, loss 0.0321428, acc [0.99199219 0.98769531 0.98935547 0.97353516]\n",
      "2019-02-24T00:10:24.489200: step 44480, loss 0.0298771, acc [0.99101562 0.99023438 0.99082031 0.97558594]\n",
      "2019-02-24T00:10:26.561038: step 44520, loss 0.0277373, acc [0.99208984 0.99111328 0.99121094 0.97851562]\n",
      "2019-02-24T00:10:28.625839: step 44560, loss 0.0298747, acc [0.99257812 0.99121094 0.98984375 0.97714844]\n",
      "2019-02-24T00:10:30.693686: step 44600, loss 0.0316798, acc [0.99208984 0.99003906 0.98925781 0.97509766]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-24T00:10:34.198396: step 44600, loss 0.409731, acc [0.9776552  0.96804453 0.95732263 0.94053399] \n",
      "\n",
      "2019-02-24T00:10:36.330700: step 44640, loss 0.0356454, acc [0.99257812 0.98994141 0.98740234 0.97470703]\n",
      "2019-02-24T00:10:38.429561: step 44680, loss 0.0319603, acc [0.98974609 0.98974609 0.98828125 0.97304687]\n",
      "2019-02-24T00:10:40.487959: step 44720, loss 0.0306364, acc [0.99150391 0.99052734 0.9890625  0.975     ]\n",
      "2019-02-24T00:10:42.556279: step 44760, loss 0.026073, acc [0.99316406 0.99023438 0.99052734 0.97734375]\n",
      "2019-02-24T00:10:44.611704: step 44800, loss 0.0323217, acc [0.99091797 0.99052734 0.98837891 0.9734375 ]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-24T00:10:48.112520: step 44800, loss 0.400905, acc [0.97780537 0.96768413 0.95781317 0.94023366] \n",
      "\n",
      "2019-02-24T00:10:50.266596: step 44840, loss 0.0335437, acc [0.9921875  0.98867187 0.98867187 0.97314453]\n",
      "2019-02-24T00:10:52.355252: step 44880, loss 0.0319708, acc [0.99160156 0.99033203 0.98876953 0.97421875]\n",
      "2019-02-24T00:10:54.411667: step 44920, loss 0.0335976, acc [0.99169922 0.98916016 0.99052734 0.97539062]\n",
      "2019-02-24T00:10:56.495860: step 44960, loss 0.0334965, acc [0.99160156 0.98945313 0.98974609 0.97470703]\n",
      "2019-02-24T00:10:58.540866: step 45000, loss 0.0326488, acc [0.99140625 0.99082031 0.98769531 0.97373047]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-24T00:11:02.047388: step 45000, loss 0.401228, acc [0.97808567 0.96795443 0.95781317 0.94078427] \n",
      "\n",
      "2019-02-24T00:11:04.191100: step 45040, loss 0.0333417, acc [0.99208984 0.99072266 0.98916016 0.97548828]\n",
      "2019-02-24T00:11:06.257931: step 45080, loss 0.0364088, acc [0.99267578 0.98847656 0.98769531 0.97324219]\n",
      "2019-02-24T00:11:08.325918: step 45120, loss 0.0341153, acc [0.99130859 0.98818359 0.98925781 0.97353516]\n",
      "2019-02-24T00:11:10.378310: step 45160, loss 0.0342537, acc [0.99277344 0.99169922 0.98808594 0.97578125]\n",
      "2019-02-24T00:11:12.436223: step 45200, loss 0.0382222, acc [0.99091797 0.98984375 0.98720703 0.97285156]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-24T00:11:15.928578: step 45200, loss 0.390986, acc [0.97797555 0.96813463 0.95809348 0.94110463] \n",
      "\n",
      "2019-02-24T00:11:18.122356: step 45240, loss 0.0389793, acc [0.99003906 0.9890625  0.98632812 0.9703125 ]\n",
      "2019-02-24T00:11:20.335508: step 45280, loss 0.0314374, acc [0.99229699 0.98913155 0.98864327 0.97381136]\n",
      "2019-02-24T00:11:22.407794: step 45320, loss 0.0240933, acc [0.99306641 0.99150391 0.99169922 0.97919922]\n",
      "2019-02-24T00:11:24.458754: step 45360, loss 0.0225218, acc [0.99277344 0.99335938 0.99189453 0.9796875 ]\n",
      "2019-02-24T00:11:26.549393: step 45400, loss 0.0257209, acc [0.99443359 0.99072266 0.99082031 0.97871094]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-24T00:11:30.021435: step 45400, loss 0.4021, acc [0.97881649 0.9681947  0.95841384 0.94204567] \n",
      "\n",
      "2019-02-24T00:11:32.178001: step 45440, loss 0.0230632, acc [0.99423828 0.99238281 0.99248047 0.98105469]\n",
      "2019-02-24T00:11:34.230943: step 45480, loss 0.0241919, acc [0.99326172 0.99335938 0.99267578 0.98251953]\n",
      "2019-02-24T00:11:36.290336: step 45520, loss 0.0233001, acc [0.99208984 0.99150391 0.99257812 0.97832031]\n",
      "2019-02-24T00:11:38.337823: step 45560, loss 0.0205384, acc [0.99335938 0.99257812 0.99316406 0.98115234]\n",
      "2019-02-24T00:11:40.393742: step 45600, loss 0.0307774, acc [0.99306641 0.99111328 0.99003906 0.97753906]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-24T00:11:43.946139: step 45600, loss 0.413676, acc [0.97840603 0.96865521 0.9571224  0.94114467] \n",
      "\n",
      "2019-02-24T00:11:46.104684: step 45640, loss 0.0229398, acc [0.99238281 0.99199219 0.99248047 0.97900391]\n",
      "2019-02-24T00:11:48.178945: step 45680, loss 0.0276839, acc [0.99453125 0.99169922 0.99023438 0.97919922]\n",
      "2019-02-24T00:11:50.281985: step 45720, loss 0.0250788, acc [0.99160156 0.99238281 0.99267578 0.97998047]\n",
      "2019-02-24T00:11:52.361216: step 45760, loss 0.0230762, acc [0.99433594 0.99140625 0.99248047 0.98046875]\n",
      "2019-02-24T00:11:54.423584: step 45800, loss 0.0251464, acc [0.99179688 0.99208984 0.99208984 0.97890625]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-24T00:11:57.936750: step 45800, loss 0.430531, acc [0.97802561 0.96778424 0.95732263 0.94062409] \n",
      "\n",
      "2019-02-24T00:12:00.081456: step 45840, loss 0.0270143, acc [0.99306641 0.99033203 0.99130859 0.97802734]\n",
      "2019-02-24T00:12:02.141344: step 45880, loss 0.0282287, acc [0.99101562 0.99101562 0.99169922 0.97646484]\n",
      "2019-02-24T00:12:04.226524: step 45920, loss 0.0277114, acc [0.99365234 0.99287109 0.98994141 0.97919922]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-02-24T00:12:06.289884: step 45960, loss 0.0253225, acc [0.9921875  0.99335938 0.99082031 0.97919922]\n",
      "2019-02-24T00:12:08.342331: step 46000, loss 0.0264121, acc [0.99287109 0.99208984 0.99042969 0.97744141]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-24T00:12:11.870874: step 46000, loss 0.429293, acc [0.97788545 0.96782429 0.95765299 0.9411747 ] \n",
      "\n",
      "2019-02-24T00:12:14.024505: step 46040, loss 0.0337062, acc [0.99414062 0.98925781 0.98994141 0.97666016]\n",
      "2019-02-24T00:12:16.119610: step 46080, loss 0.0321546, acc [0.99248047 0.98964844 0.99013672 0.97636719]\n",
      "2019-02-24T00:12:18.177017: step 46120, loss 0.0334172, acc [0.9921875  0.9921875  0.98896484 0.97626953]\n",
      "2019-02-24T00:12:20.259224: step 46160, loss 0.0378142, acc [0.99248047 0.98818359 0.98671875 0.97128906]\n",
      "2019-02-24T00:12:22.321591: step 46200, loss 0.0273571, acc [0.99238281 0.99248047 0.99042969 0.97763672]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-24T00:12:25.835320: step 46200, loss 0.416709, acc [0.97789546 0.96795443 0.9570223  0.94014356] \n",
      "\n",
      "2019-02-24T00:12:27.995400: step 46240, loss 0.0297351, acc [0.99101562 0.990625   0.99072266 0.97607422]\n",
      "2019-02-24T00:12:30.054791: step 46280, loss 0.031756, acc [0.99326172 0.99042969 0.98974609 0.97724609]\n",
      "2019-02-24T00:12:32.152374: step 46320, loss 0.0376627, acc [0.99013672 0.99042969 0.98837891 0.97304687]\n",
      "2019-02-24T00:12:34.211765: step 46360, loss 0.0369216, acc [0.99248047 0.99042969 0.98828125 0.97470703]\n",
      "2019-02-24T00:12:36.268677: step 46400, loss 0.0375472, acc [0.99082031 0.990625   0.98916016 0.97451172]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-24T00:12:39.774900: step 46400, loss 0.398664, acc [0.97768523 0.96826477 0.9573927  0.94034378] \n",
      "\n",
      "2019-02-24T00:12:41.965237: step 46440, loss 0.0275556, acc [0.99248047 0.990625   0.99082031 0.97705078]\n",
      "2019-02-24T00:12:44.031571: step 46480, loss 0.0360464, acc [0.99130859 0.99003906 0.98691406 0.97373047]\n",
      "2019-02-24T00:12:46.091956: step 46520, loss 0.0341895, acc [0.99189453 0.9890625  0.98896484 0.97490234]\n",
      "2019-02-24T00:12:48.193506: step 46560, loss 0.0352265, acc [0.99111328 0.98994141 0.9890625  0.97441406]\n",
      "2019-02-24T00:12:50.263809: step 46600, loss 0.0335983, acc [0.99228516 0.98945313 0.98886719 0.97441406]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-24T00:12:53.803760: step 46600, loss 0.407504, acc [0.97826588 0.96767412 0.95761295 0.94087437] \n",
      "\n",
      "2019-02-24T00:12:55.965369: step 46640, loss 0.035325, acc [0.98994141 0.98759766 0.98701172 0.96972656]\n",
      "2019-02-24T00:12:58.044559: step 46680, loss 0.0344195, acc [0.99111328 0.99042969 0.98818359 0.9734375 ]\n",
      "2019-02-24T00:13:00.102958: step 46720, loss 0.0347504, acc [0.99228516 0.98857422 0.98857422 0.97392578]\n",
      "2019-02-24T00:13:02.154909: step 46760, loss 0.0368051, acc [0.99160156 0.98896484 0.98925781 0.97490234]\n",
      "2019-02-24T00:13:04.221245: step 46800, loss 0.0356264, acc [0.99267578 0.98828125 0.98886719 0.97373047]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-24T00:13:07.752269: step 46800, loss 0.405455, acc [0.97782539 0.96810459 0.9573927  0.94012354] \n",
      "\n",
      "2019-02-24T00:13:10.068589: step 46840, loss 0.0300533, acc [0.99168245 0.99066544 0.98982698 0.9757191 ]\n",
      "2019-02-24T00:13:12.140876: step 46880, loss 0.0226797, acc [0.99433594 0.99169922 0.99296875 0.98144531]\n",
      "2019-02-24T00:13:14.245402: step 46920, loss 0.0217548, acc [0.99189453 0.99248047 0.99267578 0.97880859]\n",
      "2019-02-24T00:13:16.349433: step 46960, loss 0.0198694, acc [0.99238281 0.99345703 0.99296875 0.98173828]\n",
      "2019-02-24T00:13:18.448009: step 47000, loss 0.0237412, acc [0.99306641 0.99296875 0.99091797 0.97949219]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-24T00:13:22.258280: step 47000, loss 0.417325, acc [0.97870636 0.96887545 0.95804343 0.94175535] \n",
      "\n",
      "2019-02-24T00:13:24.519544: step 47040, loss 0.0226618, acc [0.99345703 0.99296875 0.99316406 0.98222656]\n",
      "2019-02-24T00:13:26.578935: step 47080, loss 0.0216792, acc [0.99453125 0.99287109 0.99277344 0.98203125]\n",
      "2019-02-24T00:13:28.662135: step 47120, loss 0.0253693, acc [0.99345703 0.99277344 0.99179688 0.98037109]\n",
      "2019-02-24T00:13:30.773607: step 47160, loss 0.0211406, acc [0.99296875 0.99267578 0.99257812 0.98056641]\n",
      "2019-02-24T00:13:32.844903: step 47200, loss 0.0245711, acc [0.99335938 0.99287109 0.9921875  0.98046875]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-24T00:13:36.407668: step 47200, loss 0.427141, acc [0.97857622 0.96796444 0.95839382 0.94151508] \n",
      "\n",
      "2019-02-24T00:13:38.611395: step 47240, loss 0.0259279, acc [0.99277344 0.99189453 0.99150391 0.97949219]\n",
      "2019-02-24T00:13:40.714932: step 47280, loss 0.023357, acc [0.99521484 0.99326172 0.99111328 0.98222656]\n",
      "2019-02-24T00:13:42.814499: step 47320, loss 0.0264895, acc [0.99326172 0.990625   0.98955078 0.97675781]\n",
      "2019-02-24T00:13:44.878278: step 47360, loss 0.0263848, acc [0.99248047 0.99257812 0.99150391 0.97949219]\n",
      "2019-02-24T00:13:46.954969: step 47400, loss 0.0281869, acc [0.99257812 0.99140625 0.99082031 0.97753906]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-24T00:13:50.561602: step 47400, loss 0.425798, acc [0.97876643 0.96884542 0.95833375 0.94265635] \n",
      "\n",
      "2019-02-24T00:13:52.697377: step 47440, loss 0.0273171, acc [0.99472656 0.99140625 0.98974609 0.97822266]\n",
      "2019-02-24T00:13:54.782064: step 47480, loss 0.0263806, acc [0.99345703 0.99033203 0.99150391 0.97890625]\n",
      "2019-02-24T00:13:56.834512: step 47520, loss 0.0291497, acc [0.99296875 0.99267578 0.98925781 0.97783203]\n",
      "2019-02-24T00:13:58.930607: step 47560, loss 0.02826, acc [0.99208984 0.99101562 0.990625   0.97587891]\n",
      "2019-02-24T00:14:01.022734: step 47600, loss 0.0270112, acc [0.99160156 0.99121094 0.99121094 0.9765625 ]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-24T00:14:04.533918: step 47600, loss 0.42434, acc [0.97872638 0.9685551  0.95779315 0.94191553] \n",
      "\n",
      "2019-02-24T00:14:06.674158: step 47640, loss 0.0259726, acc [0.99404297 0.99267578 0.99072266 0.97988281]\n",
      "2019-02-24T00:14:08.743966: step 47680, loss 0.030685, acc [0.99277344 0.99150391 0.98974609 0.97753906]\n",
      "2019-02-24T00:14:10.816827: step 47720, loss 0.0299342, acc [0.99238281 0.99150391 0.98916016 0.97783203]\n",
      "2019-02-24T00:14:12.914408: step 47760, loss 0.030257, acc [0.99228516 0.99003906 0.99033203 0.97646484]\n",
      "2019-02-24T00:14:14.994136: step 47800, loss 0.0316666, acc [0.99091797 0.99130859 0.98955078 0.97587891]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-24T00:14:18.541106: step 47800, loss 0.412571, acc [0.97841604 0.96793441 0.95807346 0.941445  ] \n",
      "\n",
      "2019-02-24T00:14:20.680320: step 47840, loss 0.0296059, acc [0.99316406 0.98964844 0.99033203 0.97685547]\n",
      "2019-02-24T00:14:22.757566: step 47880, loss 0.0251419, acc [0.99365234 0.99199219 0.99189453 0.97998047]\n",
      "2019-02-24T00:14:24.872509: step 47920, loss 0.0277874, acc [0.99208984 0.99033203 0.98945313 0.97519531]\n",
      "2019-02-24T00:14:26.945789: step 47960, loss 0.0321707, acc [0.99189453 0.990625   0.98789063 0.97363281]\n",
      "2019-02-24T00:14:29.032956: step 48000, loss 0.037538, acc [0.99121094 0.99238281 0.9875     0.97412109]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-24T00:14:32.591756: step 48000, loss 0.417446, acc [0.97806565 0.96920582 0.95708236 0.94122476] \n",
      "\n",
      "2019-02-24T00:14:34.732491: step 48040, loss 0.0314807, acc [0.99287109 0.99033203 0.98925781 0.97529297]\n",
      "2019-02-24T00:14:36.809241: step 48080, loss 0.0389221, acc [0.99130859 0.98945313 0.98828125 0.97382813]\n",
      "2019-02-24T00:14:38.879547: step 48120, loss 0.0305117, acc [0.99208984 0.98974609 0.98876953 0.97431641]\n",
      "2019-02-24T00:14:40.972664: step 48160, loss 0.0353981, acc [0.9921875  0.98984375 0.98789063 0.97441406]\n",
      "2019-02-24T00:14:43.033544: step 48200, loss 0.0317121, acc [0.99130859 0.99033203 0.98974609 0.97519531]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-24T00:14:46.625575: step 48200, loss 0.42201, acc [0.97872638 0.96804453 0.95687213 0.94096447] \n",
      "\n",
      "2019-02-24T00:14:48.800536: step 48240, loss 0.034591, acc [0.99179688 0.99072266 0.98798828 0.97412109]\n",
      "2019-02-24T00:14:50.881750: step 48280, loss 0.0332318, acc [0.99257812 0.99082031 0.98886719 0.97685547]\n",
      "2019-02-24T00:14:52.963503: step 48320, loss 0.0360928, acc [0.990625   0.99042969 0.98818359 0.97519531]\n",
      "2019-02-24T00:14:55.092338: step 48360, loss 0.0332897, acc [0.99160156 0.99111328 0.98857422 0.97529297]\n",
      "2019-02-24T00:14:57.302964: step 48400, loss 0.0350488, acc [0.99044152 0.98878137 0.98829309 0.97174381]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-24T00:15:00.841923: step 48400, loss 0.408837, acc [0.9782859  0.96850504 0.95703231 0.9402737 ] \n",
      "\n",
      "2019-02-24T00:15:02.995556: step 48440, loss 0.0205792, acc [0.99443359 0.99345703 0.99306641 0.98203125]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-02-24T00:15:05.073967: step 48480, loss 0.0202245, acc [0.99248047 0.99296875 0.99296875 0.98085937]\n",
      "2019-02-24T00:15:07.151167: step 48520, loss 0.0220251, acc [0.99306641 0.99404297 0.99306641 0.98251953]\n",
      "2019-02-24T00:15:09.236846: step 48560, loss 0.0264577, acc [0.99228516 0.99228516 0.99130859 0.97998047]\n",
      "2019-02-24T00:15:11.352285: step 48600, loss 0.0238325, acc [0.99482422 0.99228516 0.99189453 0.98125   ]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-24T00:15:15.091166: step 48600, loss 0.418053, acc [0.97795553 0.96869525 0.9584739  0.94204567] \n",
      "\n",
      "2019-02-24T00:15:17.271546: step 48640, loss 0.027061, acc [0.99365234 0.99228516 0.99199219 0.98076172]\n",
      "2019-02-24T00:15:19.368137: step 48680, loss 0.0287043, acc [0.99384766 0.99365234 0.99072266 0.98154297]\n",
      "2019-02-24T00:15:21.475146: step 48720, loss 0.0233847, acc [0.99384766 0.99033203 0.99199219 0.97929687]\n",
      "2019-02-24T00:15:23.561378: step 48760, loss 0.0213869, acc [0.99365234 0.99355469 0.99277344 0.98183594]\n",
      "2019-02-24T00:15:25.657912: step 48800, loss 0.0235541, acc [0.99335938 0.99228516 0.9921875  0.98027344]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-24T00:15:29.263367: step 48800, loss 0.422393, acc [0.97870636 0.96870526 0.95906456 0.9429767 ] \n",
      "\n",
      "2019-02-24T00:15:31.413494: step 48840, loss 0.0212476, acc [0.99306641 0.99355469 0.99306641 0.98183594]\n",
      "2019-02-24T00:15:33.554231: step 48880, loss 0.0226813, acc [0.99316406 0.99277344 0.99238281 0.98095703]\n",
      "2019-02-24T00:15:35.647847: step 48920, loss 0.0255089, acc [0.99199219 0.9921875  0.99140625 0.97744141]\n",
      "2019-02-24T00:15:37.700337: step 48960, loss 0.0251503, acc [0.99326172 0.99199219 0.99150391 0.98027344]\n",
      "2019-02-24T00:15:39.801845: step 49000, loss 0.022767, acc [0.99189453 0.99169922 0.99316406 0.97910156]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-24T00:15:43.312069: step 49000, loss 0.428664, acc [0.97931704 0.96847501 0.9582937  0.94259628] \n",
      "\n",
      "2019-02-24T00:15:45.468642: step 49040, loss 0.0249218, acc [0.99287109 0.99101562 0.99208984 0.97832031]\n",
      "2019-02-24T00:15:47.531509: step 49080, loss 0.0275392, acc [0.99345703 0.99150391 0.99091797 0.97949219]\n",
      "2019-02-24T00:15:49.624131: step 49120, loss 0.0256844, acc [0.99394531 0.99375    0.99082031 0.98066406]\n",
      "2019-02-24T00:15:51.712785: step 49160, loss 0.031283, acc [0.99296875 0.99121094 0.98925781 0.97626953]\n",
      "2019-02-24T00:15:53.799496: step 49200, loss 0.0307697, acc [0.99355469 0.99121094 0.99042969 0.97880859]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-24T00:15:57.333984: step 49200, loss 0.428381, acc [0.97886654 0.96856511 0.95812352 0.94213577] \n",
      "\n",
      "2019-02-24T00:15:59.500974: step 49240, loss 0.0318141, acc [0.9921875  0.99199219 0.99052734 0.97783203]\n",
      "2019-02-24T00:16:01.596079: step 49280, loss 0.0277132, acc [0.99296875 0.99111328 0.99140625 0.97958984]\n",
      "2019-02-24T00:16:03.660925: step 49320, loss 0.0265898, acc [0.99345703 0.99179688 0.99013672 0.978125  ]\n",
      "2019-02-24T00:16:05.751567: step 49360, loss 0.0290126, acc [0.99130859 0.99082031 0.99072266 0.97753906]\n",
      "2019-02-24T00:16:07.810957: step 49400, loss 0.028417, acc [0.99326172 0.99208984 0.98925781 0.97753906]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-24T00:16:11.348471: step 49400, loss 0.41854, acc [0.97867633 0.96877534 0.95795333 0.94188549] \n",
      "\n",
      "2019-02-24T00:16:13.512475: step 49440, loss 0.0274813, acc [0.99150391 0.99003906 0.99130859 0.97548828]\n",
      "2019-02-24T00:16:15.573849: step 49480, loss 0.0310086, acc [0.99277344 0.99189453 0.98955078 0.97724609]\n",
      "2019-02-24T00:16:17.644153: step 49520, loss 0.0286059, acc [0.99160156 0.99101562 0.99091797 0.97568359]\n",
      "2019-02-24T00:16:19.735290: step 49560, loss 0.0329487, acc [0.99169922 0.99169922 0.98925781 0.97646484]\n",
      "2019-02-24T00:16:21.792241: step 49600, loss 0.0310431, acc [0.99345703 0.99248047 0.98886719 0.97910156]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-24T00:16:25.378775: step 49600, loss 0.417261, acc [0.97824585 0.96851505 0.95851395 0.94239606] \n",
      "\n",
      "2019-02-24T00:16:27.513063: step 49640, loss 0.0305617, acc [0.99267578 0.9921875  0.98925781 0.97792969]\n",
      "2019-02-24T00:16:29.613127: step 49680, loss 0.0276458, acc [0.99238281 0.9921875  0.99082031 0.97783203]\n",
      "2019-02-24T00:16:31.696822: step 49720, loss 0.0277273, acc [0.99179688 0.99052734 0.99160156 0.97714844]\n",
      "2019-02-24T00:16:33.766134: step 49760, loss 0.0325468, acc [0.99335938 0.990625   0.98886719 0.97568359]\n",
      "2019-02-24T00:16:35.840406: step 49800, loss 0.0328157, acc [0.99160156 0.98964844 0.98779297 0.97382813]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-24T00:16:39.367998: step 49800, loss 0.421491, acc [0.97841604 0.9687353  0.95796334 0.94145502] \n",
      "\n",
      "2019-02-24T00:16:41.523570: step 49840, loss 0.0321465, acc [0.99257812 0.99033203 0.99003906 0.97685547]\n",
      "2019-02-24T00:16:43.596853: step 49880, loss 0.0371675, acc [0.99130859 0.99111328 0.98691406 0.97412109]\n",
      "2019-02-24T00:16:45.700070: step 49920, loss 0.0346667, acc [0.99238281 0.99052734 0.98798828 0.97480469]\n",
      "2019-02-24T00:16:47.906774: step 49960, loss 0.0301025, acc [0.99274483 0.99115372 0.99027482 0.97732698]\n",
      "2019-02-24T00:16:49.961247: step 50000, loss 0.0183361, acc [0.99404297 0.99404297 0.99453125 0.98447266]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-24T00:16:53.464947: step 50000, loss 0.413755, acc [0.97887655 0.96874531 0.95865411 0.94268638] \n",
      "\n",
      "2019-02-24T00:16:55.612625: step 50040, loss 0.0201305, acc [0.99345703 0.99384766 0.99296875 0.98242188]\n",
      "2019-02-24T00:16:57.687393: step 50080, loss 0.0156569, acc [0.99648437 0.99482422 0.99453125 0.98701172]\n",
      "2019-02-24T00:16:59.766128: step 50120, loss 0.0241005, acc [0.99365234 0.99404297 0.99208984 0.98232422]\n",
      "2019-02-24T00:17:01.852804: step 50160, loss 0.0202443, acc [0.99433594 0.99121094 0.99345703 0.98085937]\n",
      "2019-02-24T00:17:03.924592: step 50200, loss 0.0234827, acc [0.99414062 0.99179688 0.99238281 0.98046875]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-24T00:17:07.446712: step 50200, loss 0.432505, acc [0.97835598 0.96892551 0.95910461 0.94258627] \n",
      "\n",
      "2019-02-24T00:17:09.594367: step 50240, loss 0.0202699, acc [0.99326172 0.99267578 0.99355469 0.98144531]\n",
      "2019-02-24T00:17:11.650286: step 50280, loss 0.0230887, acc [0.99414062 0.99316406 0.99267578 0.98222656]\n",
      "2019-02-24T00:17:13.732492: step 50320, loss 0.0249119, acc [0.99414062 0.99228516 0.99033203 0.97919922]\n",
      "2019-02-24T00:17:15.793371: step 50360, loss 0.023216, acc [0.99384766 0.99287109 0.99296875 0.98183594]\n",
      "2019-02-24T00:17:17.848795: step 50400, loss 0.0221734, acc [0.99394531 0.99257812 0.99326172 0.98144531]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-24T00:17:21.400998: step 50400, loss 0.432623, acc [0.97888656 0.96848502 0.95832374 0.94246614] \n",
      "\n",
      "2019-02-24T00:17:23.520920: step 50440, loss 0.0294362, acc [0.99316406 0.99082031 0.99003906 0.97724609]\n",
      "2019-02-24T00:17:25.608087: step 50480, loss 0.0265512, acc [0.99287109 0.99296875 0.990625   0.97880859]\n",
      "2019-02-24T00:17:27.678885: step 50520, loss 0.0242596, acc [0.9921875  0.99277344 0.99169922 0.97958984]\n",
      "2019-02-24T00:17:29.758116: step 50560, loss 0.0234558, acc [0.99228516 0.99248047 0.99228516 0.9796875 ]\n",
      "2019-02-24T00:17:31.813540: step 50600, loss 0.027938, acc [0.99267578 0.99150391 0.99091797 0.97900391]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-24T00:17:35.443267: step 50600, loss 0.433, acc [0.97892661 0.968465   0.95867413 0.94230596] \n",
      "\n",
      "2019-02-24T00:17:37.621204: step 50640, loss 0.0235893, acc [0.99365234 0.99365234 0.99189453 0.98203125]\n",
      "2019-02-24T00:17:39.712836: step 50680, loss 0.0277775, acc [0.99296875 0.99228516 0.99140625 0.97880859]\n",
      "2019-02-24T00:17:41.809425: step 50720, loss 0.0283823, acc [0.99189453 0.99160156 0.99072266 0.97734375]\n",
      "2019-02-24T00:17:43.920401: step 50760, loss 0.0340476, acc [0.99287109 0.98876953 0.98867187 0.97460938]\n",
      "2019-02-24T00:17:46.059648: step 50800, loss 0.0249015, acc [0.99433594 0.99189453 0.99199219 0.98017578]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-24T00:17:49.686399: step 50800, loss 0.430074, acc [0.97924697 0.968465   0.95826367 0.94274645] \n",
      "\n",
      "2019-02-24T00:17:51.855406: step 50840, loss 0.0300713, acc [0.99355469 0.99160156 0.98974609 0.97880859]\n",
      "2019-02-24T00:17:53.947037: step 50880, loss 0.0278777, acc [0.99521484 0.99169922 0.990625   0.97978516]\n",
      "2019-02-24T00:17:56.041202: step 50920, loss 0.0302088, acc [0.99375    0.99091797 0.98994141 0.97734375]\n",
      "2019-02-24T00:17:58.106991: step 50960, loss 0.0286116, acc [0.99228516 0.99179688 0.98876953 0.97617188]\n",
      "2019-02-24T00:18:00.189196: step 51000, loss 0.0304488, acc [0.99316406 0.99150391 0.99082031 0.97910156]\n",
      "\n",
      "Evaluation:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-02-24T00:18:03.748046: step 51000, loss 0.425372, acc [0.97893662 0.9685551  0.95818358 0.94231597] \n",
      "\n",
      "2019-02-24T00:18:05.914026: step 51040, loss 0.02726, acc [0.99160156 0.990625   0.99003906 0.97519531]\n",
      "2019-02-24T00:18:07.965979: step 51080, loss 0.0269527, acc [0.99365234 0.99208984 0.98974609 0.97792969]\n",
      "2019-02-24T00:18:10.032316: step 51120, loss 0.021722, acc [0.99384766 0.99179688 0.99316406 0.98105469]\n",
      "2019-02-24T00:18:12.163626: step 51160, loss 0.0282983, acc [0.99335938 0.99160156 0.99121094 0.97890625]\n",
      "2019-02-24T00:18:14.235417: step 51200, loss 0.028322, acc [0.99277344 0.99130859 0.99160156 0.97929687]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-24T00:18:17.775863: step 51200, loss 0.42697, acc [0.97856621 0.96857512 0.9588243  0.94254622] \n",
      "\n",
      "2019-02-24T00:18:19.944962: step 51240, loss 0.0297882, acc [0.99296875 0.99199219 0.98974609 0.97734375]\n",
      "2019-02-24T00:18:22.025186: step 51280, loss 0.0310241, acc [0.99140625 0.99179688 0.99072266 0.97753906]\n",
      "2019-02-24T00:18:24.102931: step 51320, loss 0.0304774, acc [0.99267578 0.98925781 0.990625   0.97597656]\n",
      "2019-02-24T00:18:26.162320: step 51360, loss 0.0284172, acc [0.99394531 0.99023438 0.99121094 0.97871094]\n",
      "2019-02-24T00:18:28.252465: step 51400, loss 0.0325788, acc [0.99306641 0.99003906 0.98964844 0.97558594]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-24T00:18:31.802830: step 51400, loss 0.423144, acc [0.9793671  0.96938602 0.95870416 0.94331708] \n",
      "\n",
      "2019-02-24T00:18:33.980270: step 51440, loss 0.0325251, acc [0.99355469 0.99208984 0.98925781 0.97783203]\n",
      "2019-02-24T00:18:36.051609: step 51480, loss 0.0319242, acc [0.99199219 0.99101562 0.98818359 0.97490234]\n",
      "2019-02-24T00:18:38.272157: step 51520, loss 0.0321325, acc [0.99171106 0.99044152 0.98854561 0.97505228]\n",
      "2019-02-24T00:18:40.342460: step 51560, loss 0.0181809, acc [0.99453125 0.99306641 0.99375    0.98388672]\n",
      "2019-02-24T00:18:42.425659: step 51600, loss 0.0188928, acc [0.99404297 0.99316406 0.99423828 0.98242188]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-24T00:18:45.920970: step 51600, loss 0.436551, acc [0.9788265  0.96951616 0.95878425 0.94319695] \n",
      "\n",
      "2019-02-24T00:18:48.067660: step 51640, loss 0.0197798, acc [0.99550781 0.99501953 0.99306641 0.98486328]\n",
      "2019-02-24T00:18:50.178142: step 51680, loss 0.0208907, acc [0.99394531 0.99326172 0.99345703 0.98251953]\n",
      "2019-02-24T00:18:52.232079: step 51720, loss 0.0213473, acc [0.9953125  0.99394531 0.99277344 0.98349609]\n",
      "2019-02-24T00:18:54.312798: step 51760, loss 0.021112, acc [0.99238281 0.99238281 0.99394531 0.98134766]\n",
      "2019-02-24T00:18:56.397981: step 51800, loss 0.0211603, acc [0.99355469 0.99326172 0.99316406 0.98271484]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-24T00:18:59.890455: step 51800, loss 0.443361, acc [0.979277   0.96968635 0.95888436 0.94362743] \n",
      "\n",
      "2019-02-24T00:19:02.044583: step 51840, loss 0.0220247, acc [0.99462891 0.99326172 0.99296875 0.98300781]\n",
      "2019-02-24T00:19:04.129271: step 51880, loss 0.0232971, acc [0.99287109 0.99345703 0.99208984 0.98125   ]\n",
      "2019-02-24T00:19:06.186676: step 51920, loss 0.022446, acc [0.99306641 0.99345703 0.99208984 0.98125   ]\n",
      "2019-02-24T00:19:08.272356: step 51960, loss 0.0238282, acc [0.99296875 0.99355469 0.99228516 0.98066406]\n",
      "2019-02-24T00:19:10.334723: step 52000, loss 0.0204706, acc [0.99423828 0.99345703 0.99365234 0.98320312]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-24T00:19:13.828592: step 52000, loss 0.442832, acc [0.97847611 0.96910571 0.95904454 0.94281653] \n",
      "\n",
      "2019-02-24T00:19:15.974242: step 52040, loss 0.0229869, acc [0.99404297 0.99306641 0.99169922 0.98105469]\n",
      "2019-02-24T00:19:18.067859: step 52080, loss 0.0236386, acc [0.99345703 0.99404297 0.99228516 0.98183594]\n",
      "2019-02-24T00:19:20.131713: step 52120, loss 0.0230592, acc [0.99521484 0.99140625 0.99082031 0.98027344]\n",
      "2019-02-24T00:19:22.215905: step 52160, loss 0.0237979, acc [0.99326172 0.99316406 0.99169922 0.98046875]\n",
      "2019-02-24T00:19:24.273808: step 52200, loss 0.023211, acc [0.99375    0.99306641 0.99345703 0.98193359]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-24T00:19:27.764976: step 52200, loss 0.437454, acc [0.97885653 0.96857512 0.95853397 0.94226592] \n",
      "\n",
      "2019-02-24T00:19:29.944401: step 52240, loss 0.0264682, acc [0.99404297 0.99169922 0.99150391 0.98037109]\n",
      "2019-02-24T00:19:32.013711: step 52280, loss 0.0279391, acc [0.99384766 0.99150391 0.98994141 0.978125  ]\n",
      "2019-02-24T00:19:34.108814: step 52320, loss 0.024355, acc [0.99287109 0.99248047 0.99189453 0.98007813]\n",
      "2019-02-24T00:19:36.193503: step 52360, loss 0.0274542, acc [0.99208984 0.99121094 0.99082031 0.978125  ]\n",
      "2019-02-24T00:19:38.324814: step 52400, loss 0.030827, acc [0.99277344 0.99140625 0.98964844 0.97724609]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-24T00:19:41.897500: step 52400, loss 0.4396, acc [0.97877644 0.96850504 0.95797335 0.94162521] \n",
      "\n",
      "2019-02-24T00:19:44.062043: step 52440, loss 0.0299358, acc [0.99316406 0.99111328 0.99052734 0.97783203]\n",
      "2019-02-24T00:19:46.165581: step 52480, loss 0.0307123, acc [0.99306641 0.99257812 0.99013672 0.97822266]\n",
      "2019-02-24T00:19:48.233404: step 52520, loss 0.0313795, acc [0.99316406 0.99121094 0.98945313 0.97675781]\n",
      "2019-02-24T00:19:50.321067: step 52560, loss 0.0312749, acc [0.99121094 0.99111328 0.98945313 0.97529297]\n",
      "2019-02-24T00:19:52.423114: step 52600, loss 0.0250737, acc [0.9921875  0.99287109 0.990625   0.97832031]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-24T00:19:55.960585: step 52600, loss 0.431223, acc [0.97834596 0.96887545 0.95784321 0.94205568] \n",
      "\n",
      "2019-02-24T00:19:58.090410: step 52640, loss 0.028141, acc [0.99375    0.99130859 0.99023438 0.97861328]\n",
      "2019-02-24T00:20:00.141864: step 52680, loss 0.0311779, acc [0.99179688 0.99160156 0.99003906 0.97753906]\n",
      "2019-02-24T00:20:02.247382: step 52720, loss 0.0293117, acc [0.99365234 0.98994141 0.98974609 0.97568359]\n",
      "2019-02-24T00:20:04.308758: step 52760, loss 0.0360145, acc [0.99150391 0.98974609 0.98847656 0.97373047]\n",
      "2019-02-24T00:20:06.395467: step 52800, loss 0.0270735, acc [0.99189453 0.99072266 0.99160156 0.97666016]\n",
      "\n",
      "Evaluation:\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-93-87d756c70f63>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0my_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtrain_y_label1_one_hot\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrain_y_label2_one_hot\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrain_y_label3_one_hot\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0my_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtest_y_label1_one_hot\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtest_y_label2_one_hot\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtest_y_label3_one_hot\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-92-748ce757c3a2>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(x_train, y_train, x_dev, y_dev)\u001b[0m\n\u001b[0;32m    156\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mcurrent_step\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m200\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    157\u001b[0m                     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\\nEvaluation:\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 158\u001b[1;33m                     \u001b[0mdev_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_dev\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_dev\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    159\u001b[0m                     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-92-748ce757c3a2>\u001b[0m in \u001b[0;36mdev_step\u001b[1;34m(x_batch, y_batch)\u001b[0m\n\u001b[0;32m    133\u001b[0m                     step,  temp_loss, temp_accuracy1, temp_accuracy2, temp_accuracy3 , acc= sess.run(\n\u001b[0;32m    134\u001b[0m                         \u001b[1;33m[\u001b[0m\u001b[0mglobal_step\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maccuracy1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maccuracy2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maccuracy3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0macc\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 135\u001b[1;33m                         feed_dict)\n\u001b[0m\u001b[0;32m    136\u001b[0m                     \u001b[0maccuracy\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mtemp_accuracy1\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_batch_dev\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    137\u001b[0m                     \u001b[0maccuracy\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mtemp_accuracy2\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_batch_dev\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\richado\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    885\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    886\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 887\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    888\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    889\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\richado\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1108\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1109\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1110\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1111\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1112\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\richado\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1284\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1285\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1286\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1287\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1288\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\richado\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1290\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1291\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1292\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1293\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1294\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\richado\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1275\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1276\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m-> 1277\u001b[1;33m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1278\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1279\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\richado\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1365\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[0;32m   1366\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1367\u001b[1;33m         run_metadata)\n\u001b[0m\u001b[0;32m   1368\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1369\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "y_train = [train_y_label1_one_hot,train_y_label2_one_hot,train_y_label3_one_hot]\n",
    "y_test = [test_y_label1_one_hot,test_y_label2_one_hot,test_y_label3_one_hot]\n",
    "train(x_train, y_train,  x_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_y_label3_one_hot.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y_label3_one_hot.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
