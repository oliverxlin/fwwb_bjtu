{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.utils import shuffle\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "特征如下：['ITEM_NAME' 'TYPE'] \n",
      "\n",
      "数据数目:  499447\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('data/train/trainx.txt', sep='\\t')\n",
    "features = data.columns.values\n",
    "print(\"特征如下：{} \\n\".format(features))\n",
    "print(\"数据数目: \", len(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据清洗\n",
    "* 替换一些字符 类似于 （ ） ‘ ’ _"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ITEM_NAME</th>\n",
       "      <th>TYPE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>腾讯QQ黄钻三个月QQ黄钻3个月季卡官方自动充值可查时间可续费</td>\n",
       "      <td>本地生活--游戏充值--QQ充值</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>腾讯q币充值30元qq币30qb30q币自动充值</td>\n",
       "      <td>本地生活--游戏充值--QQ充值</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>腾讯QQ红钻包月卡 QQ红钻一个月QQ红钻1个月</td>\n",
       "      <td>本地生活--游戏充值--QQ充值</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>腾讯QQ蓝钻贵族31个月直充</td>\n",
       "      <td>本地生活--游戏充值--QQ充值</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>腾讯QQ币148元148QQ币148个直充148Q币148个Q币148个QQB★自动充值</td>\n",
       "      <td>本地生活--游戏充值--QQ充值</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      ITEM_NAME              TYPE\n",
       "0               腾讯QQ黄钻三个月QQ黄钻3个月季卡官方自动充值可查时间可续费  本地生活--游戏充值--QQ充值\n",
       "1                      腾讯q币充值30元qq币30qb30q币自动充值  本地生活--游戏充值--QQ充值\n",
       "2                      腾讯QQ红钻包月卡 QQ红钻一个月QQ红钻1个月  本地生活--游戏充值--QQ充值\n",
       "3                                腾讯QQ蓝钻贵族31个月直充  本地生活--游戏充值--QQ充值\n",
       "4  腾讯QQ币148元148QQ币148个直充148Q币148个Q币148个QQB★自动充值  本地生活--游戏充值--QQ充值"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "from zhon.hanzi import punctuation\n",
    "from string import punctuation as english_punc\n",
    "def  rm_punc(strs):\n",
    "    return re.sub(r\"[{}]+\".format(punctuation + english_punc),\" \",strs)\n",
    "data[features[0]]  = data[features[0]].apply(rm_punc)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练数据数目:   199779\n",
      "测试数据数目:   49945\n",
      "(199779,) (49945,)\n"
     ]
    }
   ],
   "source": [
    "# 因为有的label里本身就存在空格，所以先要把空格去掉，再去掉- - ， 再以空格划分\n",
    "data = shuffle(data)\n",
    "data_y = data[features[1]]\n",
    "\n",
    "# 取训练数据和测试数据\n",
    "train = data.sample(frac= 0.4).reset_index()\n",
    "test = data.sample(frac= 0.1).reset_index()\n",
    "\n",
    "\n",
    "print(\"训练数据数目:  \", len(train))\n",
    "print(\"测试数据数目:  \", len(test))\n",
    "\n",
    "train_x  = train[features[0]]\n",
    "train_y  = train[features[1]]\n",
    "\n",
    "test_x  = test[features[0]]\n",
    "test_y  = test[features[1]]\n",
    "\n",
    "print(train_x.shape, test_x.shape)\n",
    "\n",
    "# 分出数据集 训练集 测试集 数据集用来训练词向量\n",
    "data_y = data_y.apply(lambda x: x.replace(\" \", \"\").replace(\"--\", \" \").replace(\"/\", \"\")).tolist()\n",
    "data_y= [each.split() for each in data_y]\n",
    "\n",
    "train_y = train_y.apply(lambda x: x.replace(\" \", \"\").replace(\"--\", \" \").replace(\"/\", \"\")).tolist()\n",
    "train_y = [each.split() for each in train_y]\n",
    "\n",
    "test_y = test_y.apply(lambda x: x.replace(\" \", \"\").replace(\"--\", \" \").replace(\"/\", \"\")).tolist()\n",
    "test_y = [each.split() for each in test_y]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 分词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba\n",
    "def participle(data):\n",
    "    words = []\n",
    "    for i in range(len(data)):\n",
    "        result=[]\n",
    "        seg_list = jieba.cut(data.iloc[i])\n",
    "        for w in seg_list :#读取每一行分词\n",
    "            if w != \" \":\n",
    "                result.append(w)\n",
    "        words.append(result)#将该行分词写入列表形式的总分词列表\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\Richado\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.674 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "# 每条数据的分词结果是个长度不等的词列表\n",
    "word_data = participle(data[features[0]])\n",
    "word_data_train = participle(train_x)\n",
    "word_data_test = participle(test_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 标签分离(一个标签分成三个标签)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y = pd.DataFrame(train_y, columns=['label1', 'label2', 'label3'])\n",
    "test_y = pd.DataFrame(test_y, columns=['label1', 'label2', 'label3'])\n",
    "data_y = pd.DataFrame(data_y, columns=['label1', 'label2', 'label3'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_y[\"label1\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "191"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_y[\"label2\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1192"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_y[\"label3\"].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 标签onn_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "# 先生成唯一数组\n",
    "y_label1 = data_y['label1'].unique().tolist()\n",
    "y_label2 = data_y['label2'].unique().tolist()\n",
    "y_label3 = data_y['label3'].unique().tolist()\n",
    "\n",
    "# 获取在唯一数组中的索引(训练集和测试集各有3个标签需要处理)\n",
    "train_y_label1_map = train_y['label1'].apply(lambda x: y_label1.index(x))\n",
    "train_y_label2_map = train_y['label2'].apply(lambda x: y_label2.index(x))\n",
    "train_y_label3_map = train_y['label3'].apply(lambda x: y_label3.index(x))\n",
    "test_y_label1_map = test_y['label1'].apply(lambda x: y_label1.index(x))\n",
    "test_y_label2_map = test_y['label2'].apply(lambda x: y_label2.index(x))\n",
    "test_y_label3_map = test_y['label3'].apply(lambda x: y_label3.index(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生成对应one-hot\n",
    "train_y_label1_one_hot = to_categorical(train_y_label1_map)\n",
    "train_y_label2_one_hot = to_categorical(train_y_label2_map)\n",
    "train_y_label3_one_hot = to_categorical(train_y_label3_map)\n",
    "test_y_label1_one_hot = to_categorical(test_y_label1_map)\n",
    "test_y_label2_one_hot = to_categorical(test_y_label2_map)\n",
    "test_y_label3_one_hot = to_categorical(test_y_label3_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 词向量训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\richado\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\ipykernel_launcher.py:9: DeprecationWarning: Call to deprecated `iter` (Attribute will be removed in 4.0.0, use self.epochs instead).\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "from gensim.models.word2vec import Word2Vec \n",
    "if os.path.exists(\"w2v_model\"):\n",
    "    model = Word2Vec.load(\"w2v_model\")\n",
    "    print(1)\n",
    "else:\n",
    "    sentences = word_data\n",
    "    model= Word2Vec(size=50, window=10, min_count = 1)\n",
    "    model.build_vocab(sentences)\n",
    "    model.train(sentences,total_examples = model.corpus_count,epochs = model.iter)\n",
    "    model.save(\"w2v_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 填充词向量(构造输入输出)（让每个句子拥有同样的维度）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\richado\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\ipykernel_launcher.py:2: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \n",
      "c:\\users\\richado\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\ipykernel_launcher.py:3: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing import sequence\n",
    "x_train = [model[word] for word in word_data_train]\n",
    "x_test = [model[word] for word in word_data_test]\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=16)\n",
    "x_test  = sequence.pad_sequences(x_test, maxlen=16) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 默认参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen = 16 # 一句话的填充的词的个数\n",
    "batch_size = 32 \n",
    "embedding_dims = 50 # 一个词的词向量长度\n",
    "epochs = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## TextCnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class TextCNN(object):\n",
    "    \"\"\"\n",
    "    A CNN for text classification.\n",
    "    Uses an embedding layer, followed by a convolutional, max-pooling and softmax layer.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "      self, maxlen, sequence_length, num_classes, filter_sizes, embedding_size, num_filters, l2_reg_lambda=0.0):\n",
    "\n",
    "        # Placeholders for input, output and dropout\n",
    "        self.input_x = tf.placeholder(tf.float32, [None, sequence_length, embedding_size], name=\"input_x\")\n",
    "        self.input_y = tf.placeholder(tf.float32, [None, num_classes], name=\"input_y\")\n",
    "        \n",
    "#         self.dropout_keep_prob = tf.placeholder(tf.float32, name=\"dropout_keep_prob\")\n",
    "\n",
    "        # Keeping track of l2 regularization loss (optional)\n",
    "        l2_loss = tf.constant(0.0)\n",
    "\n",
    "        # Embedding layer\n",
    "#         with tf.device('/cpu:0'), tf.name_scope(\"embedding\"):\n",
    "#             self.W = tf.Variable(\n",
    "#                 tf.random_uniform([vocab_size, embedding_size], -1.0, 1.0),\n",
    "#                 name=\"W\")\n",
    "#             self.embedded_chars = tf.nn.embedding_lookup(self.W, self.input_x)\n",
    "#             self.embedded_chars_expanded = tf.expand_dims(self.embedded_chars, -1)\n",
    "\n",
    "#         # Create a convolution + maxpool layer for each filter size\n",
    "        pooled_outputs = []\n",
    "        self.input_x_expand = tf.expand_dims(self.input_x, -1)\n",
    "        print(self.input_x_expand.shape)\n",
    "        for i, filter_size in enumerate(filter_sizes):\n",
    "            with tf.name_scope(\"conv-maxpool-%s\" % filter_size):\n",
    "                # Convolution Layer\n",
    "                filter_shape = [filter_size, embedding_size, 1, num_filters]\n",
    "                W = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.1), name=\"W\")\n",
    "                b = tf.Variable(tf.constant(0.1, shape=[num_filters]), name=\"b\")\n",
    "                print(W.shape)\n",
    "                conv = tf.nn.conv2d(\n",
    "                    self.input_x_expand,\n",
    "                    W,\n",
    "                    strides=[1, 1, 1, 1],\n",
    "                    padding=\"VALID\",\n",
    "                    name=\"conv\")\n",
    "                # Apply nonlinearity\n",
    "                h = tf.nn.relu(tf.nn.bias_add(conv, b), name=\"relu\")\n",
    "                # Maxpooling over the outputs\n",
    "                pooled = tf.nn.max_pool(\n",
    "                    h,\n",
    "                    ksize=[1, sequence_length - filter_size + 1, 1, 1],\n",
    "                    strides=[1, 1, 1, 1],\n",
    "                    padding='VALID',\n",
    "                    name=\"pool\")\n",
    "                pooled_outputs.append(pooled)\n",
    "\n",
    "        # Combine all the pooled features\n",
    "        num_filters_total = num_filters * len(filter_sizes)\n",
    "        self.h_pool = tf.concat(pooled_outputs, 3)\n",
    "        self.h_pool_flat = tf.reshape(self.h_pool, [-1, num_filters_total])\n",
    "        print(self.h_pool_flat.shape)\n",
    "        # Add dropout\n",
    "#         with tf.name_scope(\"dropout\"):\n",
    "#             self.h_drop = tf.nn.dropout(self.h_pool_flat, self.input_x)\n",
    "#         print(self.h_pool_flat.shape, elf.h_drop.shape)\n",
    "        self.h_drop = self.h_pool_flat\n",
    "        # Final (unnormalized) scores and predictions\n",
    "        \n",
    "        with tf.name_scope(\"output\"):\n",
    "            W1 = tf.get_variable(\n",
    "                \"W1\",\n",
    "                shape=[num_filters_total, 64],\n",
    "                initializer=tf.contrib.layers.xavier_initializer())\n",
    "            b1 = tf.Variable(tf.constant(0.1, shape=[64]), name=\"b1\")\n",
    "            \n",
    "            self.output = tf.nn.relu(tf.nn.xw_plus_b(self.h_drop, W1, b1, name=\"output\"))\n",
    "            \n",
    "            W2 = tf.get_variable(\n",
    "                \"W2\",\n",
    "                shape=[64, num_classes],\n",
    "                initializer=tf.contrib.layers.xavier_initializer())\n",
    "            b2 = tf.Variable(tf.constant(0.1, shape=[num_classes]), name=\"b2\")\n",
    "            l2_loss += tf.nn.l2_loss(W1)\n",
    "            l2_loss += tf.nn.l2_loss(b1)\n",
    "            self.scores = tf.nn.xw_plus_b(self.output, W2, b2, name=\"scores\")\n",
    "            self.predictions = tf.argmax(self.scores, 1, name=\"predictions\")\n",
    "            print(self.predictions.shape, self.input_y.shape)\n",
    "        # Calculate mean cross-entropy loss\n",
    "        with tf.name_scope(\"loss\"):\n",
    "            losses = tf.nn.softmax_cross_entropy_with_logits(logits=self.scores, labels=self.input_y)\n",
    "            self.loss = tf.reduce_mean(losses) + l2_reg_lambda * l2_loss\n",
    "\n",
    "        # Accuracy\n",
    "        with tf.name_scope(\"accuracy\"):\n",
    "            correct_predictions = tf.equal(self.predictions, tf.argmax(self.input_y, 1))\n",
    "            self.accuracy = tf.reduce_mean(tf.cast(correct_predictions, \"float\"), name=\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def batch_iter(data, batch_size, num_epochs, shuffle=True):\n",
    "    \"\"\"\n",
    "    Generates a batch iterator for a dataset.\n",
    "    \"\"\"\n",
    "    data = np.array(data)\n",
    "    data_size = len(data)\n",
    "    num_batches_per_epoch = int((len(data)-1)/batch_size) + 1\n",
    "    for epoch in range(num_epochs):\n",
    "        # Shuffle the data at each epoch\n",
    "        if shuffle:\n",
    "            shuffle_indices = np.random.permutation(np.arange(data_size))\n",
    "            shuffled_data = data[shuffle_indices]\n",
    "        else:\n",
    "            shuffled_data = data\n",
    "  \n",
    "        for batch_num in range(num_batches_per_epoch):\n",
    "            start_index = batch_num * batch_size\n",
    "            end_index = np.min(((batch_num + 1) * batch_size, data_size))\n",
    "            yield shuffled_data[start_index:end_index]\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 16, 50, 1)\n",
      "(1, 50, 1, 64)\n",
      "(2, 50, 1, 64)\n",
      "(3, 50, 1, 64)\n",
      "(4, 50, 1, 64)\n",
      "(5, 50, 1, 64)\n",
      "(?, 320)\n",
      "(?,) (?, 191)\n",
      "2019-02-20T02:21:20.547438: step 40, loss 3.57215, acc 0.35625\n",
      "2019-02-20T02:21:20.893147: step 80, loss 2.26441, acc 0.528516\n",
      "2019-02-20T02:21:21.207565: step 120, loss 1.92542, acc 0.601562\n",
      "2019-02-20T02:21:21.519551: step 160, loss 1.82143, acc 0.610156\n",
      "2019-02-20T02:21:21.813183: step 200, loss 1.61032, acc 0.649609\n",
      "\n",
      "Evaluation:\n",
      "2019-02-20T02:21:23.072138: step 200, loss 1.5617, acc 0.65494\n",
      "\n",
      "2019-02-20T02:21:23.382634: step 240, loss 1.49786, acc 0.669922\n",
      "2019-02-20T02:21:23.673320: step 280, loss 1.4811, acc 0.6625\n",
      "2019-02-20T02:21:23.966427: step 320, loss 1.35618, acc 0.685547\n",
      "2019-02-20T02:21:24.269036: step 360, loss 1.30722, acc 0.690625\n",
      "2019-02-20T02:21:24.557717: step 400, loss 1.25799, acc 0.710547\n",
      "\n",
      "Evaluation:\n",
      "2019-02-20T02:21:25.454962: step 400, loss 1.33099, acc 0.680689\n",
      "\n",
      "2019-02-20T02:21:25.786257: step 440, loss 1.29254, acc 0.691016\n",
      "2019-02-20T02:21:26.092336: step 480, loss 1.22626, acc 0.707422\n",
      "2019-02-20T02:21:26.386908: step 520, loss 1.20794, acc 0.705469\n",
      "2019-02-20T02:21:26.679602: step 560, loss 1.14291, acc 0.721094\n",
      "2019-02-20T02:21:26.967763: step 600, loss 1.13314, acc 0.721875\n",
      "\n",
      "Evaluation:\n",
      "2019-02-20T02:21:27.874963: step 600, loss 1.14404, acc 0.722935\n",
      "\n",
      "2019-02-20T02:21:28.186398: step 640, loss 1.16267, acc 0.725391\n",
      "2019-02-20T02:21:28.476639: step 680, loss 1.15509, acc 0.726562\n",
      "2019-02-20T02:21:28.769252: step 720, loss 1.10887, acc 0.732422\n",
      "2019-02-20T02:21:29.092637: step 760, loss 1.09519, acc 0.738672\n",
      "2019-02-20T02:21:29.378287: step 800, loss 1.08025, acc 0.730859\n",
      "\n",
      "Evaluation:\n",
      "2019-02-20T02:21:30.317712: step 800, loss 1.06676, acc 0.738653\n",
      "\n",
      "2019-02-20T02:21:30.640652: step 840, loss 1.07475, acc 0.742969\n",
      "2019-02-20T02:21:30.929834: step 880, loss 1.06849, acc 0.739844\n",
      "2019-02-20T02:21:31.226419: step 920, loss 1.0715, acc 0.733594\n",
      "2019-02-20T02:21:31.518587: step 960, loss 1.03625, acc 0.7375\n",
      "2019-02-20T02:21:31.811209: step 1000, loss 0.976123, acc 0.756641\n",
      "\n",
      "Evaluation:\n",
      "2019-02-20T02:21:32.734772: step 1000, loss 1.02603, acc 0.744399\n",
      "\n",
      "2019-02-20T02:21:33.052178: step 1040, loss 1.02533, acc 0.744922\n",
      "2019-02-20T02:21:33.346813: step 1080, loss 1.00565, acc 0.749219\n",
      "2019-02-20T02:21:33.636501: step 1120, loss 0.984851, acc 0.75\n",
      "2019-02-20T02:21:33.924627: step 1160, loss 0.967581, acc 0.75625\n",
      "2019-02-20T02:21:34.221237: step 1200, loss 0.979888, acc 0.758203\n",
      "\n",
      "Evaluation:\n",
      "2019-02-20T02:21:35.132389: step 1200, loss 1.00213, acc 0.749985\n",
      "\n",
      "2019-02-20T02:21:35.446878: step 1240, loss 1.0187, acc 0.737109\n",
      "2019-02-20T02:21:35.741013: step 1280, loss 1.03388, acc 0.742578\n",
      "2019-02-20T02:21:36.032132: step 1320, loss 0.977787, acc 0.760547\n",
      "2019-02-20T02:21:36.327805: step 1360, loss 0.985921, acc 0.763281\n",
      "2019-02-20T02:21:36.619895: step 1400, loss 0.989139, acc 0.753906\n",
      "\n",
      "Evaluation:\n",
      "2019-02-20T02:21:37.533651: step 1400, loss 0.95834, acc 0.759235\n",
      "\n",
      "2019-02-20T02:21:37.844657: step 1440, loss 0.98891, acc 0.748437\n",
      "2019-02-20T02:21:38.137585: step 1480, loss 1.02817, acc 0.74375\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-35-3184a6002455>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    141\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    142\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 143\u001b[1;33m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-35-3184a6002455>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(x_train, y_train, x_dev, y_dev)\u001b[0m\n\u001b[0;32m    129\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mbatches\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m                 \u001b[0mx_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_batch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 131\u001b[1;33m                 \u001b[0mloss_sum\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0macc_sum\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[0mloss_sum\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0macc_sum\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m40\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    132\u001b[0m                 \u001b[0mcurrent_step\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mglobal_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mglobal_step\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    133\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mcurrent_step\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m200\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-35-3184a6002455>\u001b[0m in \u001b[0;36mtrain_step\u001b[1;34m(x_batch, y_batch, loss_sum, acc_sum, interal)\u001b[0m\n\u001b[0;32m     85\u001b[0m                 _, step,  loss, accuracy = sess.run(\n\u001b[0;32m     86\u001b[0m                     \u001b[1;33m[\u001b[0m\u001b[0mtrain_op\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mglobal_step\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 87\u001b[1;33m                     feed_dict)\n\u001b[0m\u001b[0;32m     88\u001b[0m                 \u001b[0mtime_str\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misoformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m                 \u001b[0macc_sum\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\richado\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    885\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    886\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 887\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    888\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    889\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\richado\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1108\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1109\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1110\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1111\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1112\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\richado\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1284\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1285\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1286\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1287\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1288\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\richado\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1290\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1291\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1292\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1293\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1294\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\richado\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1275\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1276\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m-> 1277\u001b[1;33m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1278\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1279\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\richado\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1365\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[0;32m   1366\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1367\u001b[1;33m         run_metadata)\n\u001b[0m\u001b[0;32m   1368\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1369\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "from tensorflow.contrib import learn\n",
    "\n",
    "# 特征数这里是第三级标签的特征数\n",
    "max_features = 1192\n",
    "\n",
    "# 句子填充的长度\n",
    "maxlen = 16\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "# 词向量长度\n",
    "embedding_dims = 50\n",
    "\n",
    "filters = 64\n",
    "kernel_size = 5\n",
    "hidden_dims = 128\n",
    "epochs = 100\n",
    "\n",
    "# Parameters\n",
    "# ==================================================\n",
    "\n",
    "# Data loading params\n",
    "# tf.flags.DEFINE_float(\"dev_sample_percentage\", .1, \"Percentage of the training data to use for validation\")\n",
    "# tf.flags.DEFINE_string(\"positive_data_file\", \"./data/rt-polaritydata/rt-polarity.pos\", \"Data source for the positive data.\")\n",
    "# tf.flags.DEFINE_string(\"negative_data_file\", \"./data/rt-polaritydata/rt-polarity.neg\", \"Data source for the negative data.\")\n",
    "\n",
    "# Model Hyperparameters\n",
    "# tf.flags.DEFINE_integer(\"embedding_dim\", 128, \"Dimensionality of character embedding (default: 128)\")\n",
    "# tf.flags.DEFINE_string(\"filter_sizes\", \"3,4,5\", \"Comma-separated filter sizes (default: '3,4,5')\")\n",
    "# tf.flags.DEFINE_integer(\"num_filters\", 128, \"Number of filters per filter size (default: 128)\")\n",
    "# tf.flags.DEFINE_float(\"dropout_keep_prob\", 0.5, \"Dropout keep probability (default: 0.5)\")\n",
    "# tf.flags.DEFINE_float(\"l2_reg_lambda\", 0.0, \"L2 regularization lambda (default: 0.0)\")\n",
    "\n",
    "# Training parameters\n",
    "# tf.flags.DEFINE_integer(\"batch_size\", 64, \"Batch Size (default: 64)\")\n",
    "# tf.flags.DEFINE_integer(\"num_epochs\", 200, \"Number of training epochs (default: 200)\")\n",
    "# tf.flags.DEFINE_integer(\"evaluate_every\", 100, \"Evaluate model on dev set after this many steps (default: 100)\")\n",
    "# tf.flags.DEFINE_integer(\"checkpoint_every\", 100, \"Save model after this many steps (default: 100)\")\n",
    "# tf.flags.DEFINE_integer(\"num_checkpoints\", 5, \"Number of checkpoints to store (default: 5)\")\n",
    "# Misc Parameters\n",
    "# tf.flags.DEFINE_boolean(\"allow_soft_placement\", True, \"Allow device soft device placement\")\n",
    "# tf.flags.DEFINE_boolean(\"log_device_placement\", False, \"Log placement of ops on devices\")\n",
    "\n",
    "FLAGS = tf.flags.FLAGS\n",
    "\n",
    "def train(x_train, y_train, x_dev, y_dev):\n",
    "    # Training\n",
    "    # ==================================================\n",
    "\n",
    "    with tf.Graph().as_default():\n",
    "        session_conf = tf.ConfigProto(\n",
    "#           allow_soft_placement=FLAGS.allow_soft_placement,\n",
    "#           log_device_placement=FLAGS.log_device_placement\n",
    "        )\n",
    "        sess = tf.Session(config=session_conf)\n",
    "        with sess.as_default():\n",
    "            cnn = TextCNN(maxlen = 16, sequence_length = 16, num_classes = 191, filter_sizes = [1,2, 3, 4, 5], embedding_size = 50, num_filters = 64)\n",
    "\n",
    "            # Define Training procedure\n",
    "            global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "            optimizer = tf.train.AdamOptimizer(1e-3)\n",
    "            grads_and_vars = optimizer.compute_gradients(cnn.loss)\n",
    "            train_op = optimizer.apply_gradients(grads_and_vars, global_step=global_step)\n",
    "\n",
    "            # Initialize all variables\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            writer = tf.summary.FileWriter(\"fwwb\",sess.graph)\n",
    "            acc_sum = 0\n",
    "            loss_sum = 0\n",
    "            \n",
    "            def train_step(x_batch, y_batch, loss_sum, acc_sum, interal = 10):\n",
    "  \n",
    "                \"\"\"\n",
    "                A single training step\n",
    "                \"\"\"\n",
    "                feed_dict = {\n",
    "                  cnn.input_x: x_batch,\n",
    "                  cnn.input_y: y_batch\n",
    "                }\n",
    "                _, step,  loss, accuracy = sess.run(\n",
    "                    [train_op, global_step, cnn.loss, cnn.accuracy],\n",
    "                    feed_dict)\n",
    "                time_str = datetime.datetime.now().isoformat()\n",
    "                acc_sum += accuracy\n",
    "                loss_sum += loss\n",
    "                if step % interal == 0:\n",
    "                    print(\"{}: step {}, loss {:g}, acc {:g}\".format(time_str, step, loss_sum/interal, acc_sum/interal))\n",
    "                    loss_sum = 0\n",
    "                    acc_sum = 0\n",
    "                return loss_sum, acc_sum\n",
    "            # 评估步骤\n",
    "            def dev_step(x_batch, y_batch):\n",
    "                \"\"\"\n",
    "                Evaluates model on a dev set\n",
    "                \"\"\"\n",
    "\n",
    "                dev_batches = batch_iter(\n",
    "                    list(zip(x_batch, y_batch)), 640, 1)\n",
    "                step = 0\n",
    "                loss = 0\n",
    "                accuracy = 0\n",
    "                for batch in dev_batches:\n",
    "                    x_batch_dev, y_batch_dev = zip(*batch)\n",
    "                    feed_dict = {\n",
    "                      cnn.input_x: x_batch_dev,\n",
    "                      cnn.input_y: y_batch_dev\n",
    "                    }\n",
    "                    step,  temp_loss, temp_accuracy = sess.run(\n",
    "                        [global_step,cnn.loss, cnn.accuracy],\n",
    "                        feed_dict)\n",
    "                    accuracy += temp_accuracy * len(x_batch_dev)\n",
    "                    loss += temp_loss * len(x_batch_dev)\n",
    "                accuracy /= x_batch.shape[0]\n",
    "                loss /= x_batch.shape[0]\n",
    "                time_str = datetime.datetime.now().isoformat()                    \n",
    "\n",
    "                print(\"{}: step {}, loss {:g}, acc {:g}\".format(time_str, step, loss, accuracy))\n",
    "\n",
    "\n",
    "            # Generate batches\n",
    "            batches = batch_iter(\n",
    "                list(zip(x_train, y_train)), 64, 200)\n",
    "            # Training loop. For each batch...\n",
    "            for batch in batches:\n",
    "                x_batch, y_batch = zip(*batch)\n",
    "                loss_sum, acc_sum = train_step(x_batch, y_batch,  loss_sum, acc_sum, 40)\n",
    "                current_step = tf.train.global_step(sess, global_step)\n",
    "                if current_step % 200 == 0:\n",
    "                    print(\"\\nEvaluation:\")\n",
    "                    dev_step(x_dev, y_dev)\n",
    "                    print(\"\")\n",
    "#                 if current_step % FLAGS.checkpoint_every == 0:\n",
    "#                     path = saver.save(sess, checkpoint_prefix, global_step=current_step)\n",
    "#                     print(\"Saved model checkpoint to {}\\n\".format(path))\n",
    "            \n",
    "\n",
    "\n",
    "train(x_train, y_train,  x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
