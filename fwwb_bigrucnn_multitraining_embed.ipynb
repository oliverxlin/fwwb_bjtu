{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.utils import shuffle\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "特征如下：['ITEM_NAME' 'TYPE'] \n",
      "\n",
      "数据数目:  499447\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('data/train/trainx.txt', sep='\\t')\n",
    "features = data.columns.values\n",
    "print(\"特征如下：{} \\n\".format(features))\n",
    "print(\"数据数目: \", len(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据清洗\n",
    "* 替换一些字符 类似于 （ ） ‘ ’ _"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ITEM_NAME</th>\n",
       "      <th>TYPE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>腾讯QQ黄钻三个月QQ黄钻3个月季卡官方自动充值可查时间可续费</td>\n",
       "      <td>本地生活--游戏充值--QQ充值</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>腾讯q币充值30元qq币30qb30q币自动充值</td>\n",
       "      <td>本地生活--游戏充值--QQ充值</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>腾讯QQ红钻包月卡 QQ红钻一个月QQ红钻1个月</td>\n",
       "      <td>本地生活--游戏充值--QQ充值</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>腾讯QQ蓝钻贵族31个月直充</td>\n",
       "      <td>本地生活--游戏充值--QQ充值</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>腾讯QQ币148元148QQ币148个直充148Q币148个Q币148个QQB★自动充值</td>\n",
       "      <td>本地生活--游戏充值--QQ充值</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      ITEM_NAME              TYPE\n",
       "0               腾讯QQ黄钻三个月QQ黄钻3个月季卡官方自动充值可查时间可续费  本地生活--游戏充值--QQ充值\n",
       "1                      腾讯q币充值30元qq币30qb30q币自动充值  本地生活--游戏充值--QQ充值\n",
       "2                      腾讯QQ红钻包月卡 QQ红钻一个月QQ红钻1个月  本地生活--游戏充值--QQ充值\n",
       "3                                腾讯QQ蓝钻贵族31个月直充  本地生活--游戏充值--QQ充值\n",
       "4  腾讯QQ币148元148QQ币148个直充148Q币148个Q币148个QQB★自动充值  本地生活--游戏充值--QQ充值"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "from zhon.hanzi import punctuation\n",
    "from string import punctuation as english_punc\n",
    "def  rm_punc(strs):\n",
    "    return re.sub(r\"[{}]+\".format(punctuation + english_punc),\" \",strs)\n",
    "data[features[0]]  = data[features[0]].apply(rm_punc)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练数据数目:   399558\n",
      "测试数据数目:   99889\n",
      "(399558,) (99889,)\n"
     ]
    }
   ],
   "source": [
    "# 因为有的label里本身就存在空格，所以先要把空格去掉，再去掉- - ， 再以空格划分\n",
    "data = shuffle(data)\n",
    "data_y = data[features[1]]\n",
    "\n",
    "# 取训练数据和测试数据\n",
    "train = data.sample(frac= 0.8).reset_index()\n",
    "test = data.sample(frac= 0.2).reset_index()\n",
    "\n",
    "\n",
    "print(\"训练数据数目:  \", len(train))\n",
    "print(\"测试数据数目:  \", len(test))\n",
    "\n",
    "train_x  = train[features[0]]\n",
    "train_y  = train[features[1]]\n",
    "\n",
    "test_x  = test[features[0]]\n",
    "test_y  = test[features[1]]\n",
    "\n",
    "print(train_x.shape, test_x.shape)\n",
    "\n",
    "# 分出数据集 训练集 测试集 数据集用来训练词向量\n",
    "data_y = data_y.apply(lambda x: x.replace(\" \", \"\").replace(\"--\", \" \").replace(\"/\", \"\")).tolist()\n",
    "data_y= [each.split() for each in data_y]\n",
    "\n",
    "train_y = train_y.apply(lambda x: x.replace(\" \", \"\").replace(\"--\", \" \").replace(\"/\", \"\")).tolist()\n",
    "train_y = [each.split() for each in train_y]\n",
    "\n",
    "test_y = test_y.apply(lambda x: x.replace(\" \", \"\").replace(\"--\", \" \").replace(\"/\", \"\")).tolist()\n",
    "test_y = [each.split() for each in test_y]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 分词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba\n",
    "from random import shuffle\n",
    "def participle(data, doshuffle = False):\n",
    "    words = []\n",
    "    for i in range(len(data)):\n",
    "        result=[]\n",
    "        seg_list = jieba.cut(data.iloc[i])\n",
    "        for w in seg_list :#读取每一行分词\n",
    "            if w != \" \":\n",
    "                result.append(w)\n",
    "        # 打乱每条数据的分词\n",
    "        if doshuffle:\n",
    "            shuffle(result)\n",
    "        words.append(result)#将该行分词写入列表形式的总分词列表\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\Richado\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.727 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "# 每条数据的分词结果是个长度不等的词列表\n",
    "word_data = participle(data[features[0]], doshuffle = False)\n",
    "word_data_train = participle(train_x,doshuffle = False)\n",
    "word_data_test = participle(test_x, doshuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'腾讯QQ黄钻三个月QQ黄钻3个月季卡官方自动充值可查时间可续费'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[features[0]][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['北京', '特产', '零食', '好', '亿家', '豌豆黄', '散装', '零食', '500g']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from random import shuffle\n",
    "word_data[8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 标签分离(一个标签分成三个标签)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y = pd.DataFrame(train_y, columns=['label1', 'label2', 'label3'])\n",
    "test_y = pd.DataFrame(test_y, columns=['label1', 'label2', 'label3'])\n",
    "data_y = pd.DataFrame(data_y, columns=['label1', 'label2', 'label3'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_y[\"label1\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "191"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_y[\"label2\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1192"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_y[\"label3\"].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 标签onn_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import to_categorical\n",
    "# 先生成唯一数组\n",
    "y_label1 = data_y['label1'].unique().tolist()\n",
    "y_label2 = data_y['label2'].unique().tolist()\n",
    "y_label3 = data_y['label3'].unique().tolist()\n",
    "\n",
    "# 获取在唯一数组中的索引(训练集和测试集各有3个标签需要处理)\n",
    "train_y_label1_map = train_y['label1'].apply(lambda x: y_label1.index(x))\n",
    "train_y_label2_map = train_y['label2'].apply(lambda x: y_label2.index(x))\n",
    "train_y_label3_map = train_y['label3'].apply(lambda x: y_label3.index(x))\n",
    "test_y_label1_map = test_y['label1'].apply(lambda x: y_label1.index(x))\n",
    "test_y_label2_map = test_y['label2'].apply(lambda x: y_label2.index(x))\n",
    "test_y_label3_map = test_y['label3'].apply(lambda x: y_label3.index(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生成对应one-hot(用做训练模型的标签)\n",
    "train_y_label1_one_hot = to_categorical(train_y_label1_map, 22)\n",
    "train_y_label2_one_hot = to_categorical(train_y_label2_map, 191)\n",
    "train_y_label3_one_hot = to_categorical(train_y_label3_map, 1192)\n",
    "test_y_label1_one_hot = to_categorical(test_y_label1_map, 22)\n",
    "test_y_label2_one_hot = to_categorical(test_y_label2_map, 191)\n",
    "test_y_label3_one_hot = to_categorical(test_y_label3_map, 1192)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import *\n",
    "from keras.preprocessing import sequence\n",
    "tok = Tokenizer()\n",
    "tok.fit_on_texts(word_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'新款': 1,\n",
       " '1': 2,\n",
       " '汽车': 3,\n",
       " '时尚': 4,\n",
       " '黑色': 5,\n",
       " '鞋': 6,\n",
       " '2016': 7,\n",
       " '休闲': 8,\n",
       " '款': 9,\n",
       " '专用': 10,\n",
       " '2': 11,\n",
       " '5': 12,\n",
       " '男士': 13,\n",
       " '跟': 14,\n",
       " '手机': 15,\n",
       " '适用': 16,\n",
       " '女': 17,\n",
       " '套装': 18,\n",
       " '男': 19,\n",
       " '韩版': 20,\n",
       " '夏季': 21,\n",
       " '儿童': 22,\n",
       " '3': 23,\n",
       " '包': 24,\n",
       " '于': 25,\n",
       " '透气': 26,\n",
       " '壳': 27,\n",
       " '套': 28,\n",
       " '休闲鞋': 29,\n",
       " '米': 30,\n",
       " '户外': 31,\n",
       " '男鞋': 32,\n",
       " '真皮': 33,\n",
       " '垫': 34,\n",
       " '红色': 35,\n",
       " '6': 36,\n",
       " '4': 37,\n",
       " '脚垫': 38,\n",
       " '女鞋': 39,\n",
       " '男女': 40,\n",
       " '白色': 41,\n",
       " '全': 42,\n",
       " '蓝色': 43,\n",
       " '创意': 44,\n",
       " '宝宝': 45,\n",
       " '防水': 46,\n",
       " '简约': 47,\n",
       " '四季': 48,\n",
       " '女士': 49,\n",
       " '8': 50,\n",
       " '保护套': 51,\n",
       " '凉鞋': 52,\n",
       " '牛皮': 53,\n",
       " '四件套': 54,\n",
       " '皮鞋': 55,\n",
       " '系列': 56,\n",
       " '专车': 57,\n",
       " '情侣': 58,\n",
       " '包围': 59,\n",
       " '灯': 60,\n",
       " '通用': 61,\n",
       " '纯棉': 62,\n",
       " '加厚': 63,\n",
       " '小': 64,\n",
       " '运动': 65,\n",
       " '单鞋': 66,\n",
       " '商务': 67,\n",
       " '学生': 68,\n",
       " '大': 69,\n",
       " '2015': 70,\n",
       " '送': 71,\n",
       " '潮流': 72,\n",
       " '客厅': 73,\n",
       " '婴儿': 74,\n",
       " '带': 75,\n",
       " '新': 76,\n",
       " '卡通': 77,\n",
       " '号': 78,\n",
       " '全棉': 79,\n",
       " '英伦': 80,\n",
       " '7': 81,\n",
       " '0': 82,\n",
       " '寸': 83,\n",
       " '可': 84,\n",
       " '现代': 85,\n",
       " '鞋子': 86,\n",
       " 'led': 87,\n",
       " '单肩': 88,\n",
       " '玩具': 89,\n",
       " '黑': 90,\n",
       " '被': 91,\n",
       " '座垫': 92,\n",
       " '39': 93,\n",
       " '礼物': 94,\n",
       " '苹果': 95,\n",
       " '40': 96,\n",
       " '版': 97,\n",
       " '的': 98,\n",
       " '摆件': 99,\n",
       " '保暖': 100,\n",
       " '礼品': 101,\n",
       " '码': 102,\n",
       " '38': 103,\n",
       " '保护': 104,\n",
       " '改装': 105,\n",
       " '灰色': 106,\n",
       " '春季': 107,\n",
       " '床上用品': 108,\n",
       " '舒适': 109,\n",
       " '卧室': 110,\n",
       " '可爱': 111,\n",
       " '印花': 112,\n",
       " '拖鞋': 113,\n",
       " '装饰': 114,\n",
       " '多功能': 115,\n",
       " '高': 116,\n",
       " '坐垫': 117,\n",
       " '棕色': 118,\n",
       " '被套': 119,\n",
       " '茶具': 120,\n",
       " '进口': 121,\n",
       " '系带': 122,\n",
       " '米床': 123,\n",
       " '10': 124,\n",
       " '防': 125,\n",
       " '背包': 126,\n",
       " '金属': 127,\n",
       " '防滑': 128,\n",
       " '杯': 129,\n",
       " '复古': 130,\n",
       " '座套': 131,\n",
       " '经典': 132,\n",
       " '正版': 133,\n",
       " '秋冬': 134,\n",
       " '系': 135,\n",
       " '贴': 136,\n",
       " '春夏': 137,\n",
       " '12': 138,\n",
       " '汽车坐垫': 139,\n",
       " '奥迪': 140,\n",
       " '不锈钢': 141,\n",
       " '陶瓷': 142,\n",
       " '粉色': 143,\n",
       " '36': 144,\n",
       " '床单': 145,\n",
       " '欧美': 146,\n",
       " '床': 147,\n",
       " '定制': 148,\n",
       " '生日礼物': 149,\n",
       " '双人': 150,\n",
       " '手表': 151,\n",
       " '35': 152,\n",
       " '孕妇': 153,\n",
       " '板鞋': 154,\n",
       " '宝马': 155,\n",
       " '女款': 156,\n",
       " '车载': 157,\n",
       " '大众': 158,\n",
       " '套件': 159,\n",
       " '实木': 160,\n",
       " '女包': 161,\n",
       " '年': 162,\n",
       " '夹': 163,\n",
       " '枕': 164,\n",
       " '空调': 165,\n",
       " '37': 166,\n",
       " '办公': 167,\n",
       " '膜': 168,\n",
       " 'l': 169,\n",
       " '红': 170,\n",
       " '风': 171,\n",
       " '手机套': 172,\n",
       " '欧式': 173,\n",
       " '15': 174,\n",
       " '平底': 175,\n",
       " '车': 176,\n",
       " '尖头': 177,\n",
       " '男款': 178,\n",
       " '装': 179,\n",
       " '个性': 180,\n",
       " '手链': 181,\n",
       " '42': 182,\n",
       " '运动鞋': 183,\n",
       " '41': 184,\n",
       " '靴': 185,\n",
       " 'm': 186,\n",
       " '英寸': 187,\n",
       " '钻': 188,\n",
       " '皮革': 189,\n",
       " '色': 190,\n",
       " '防晒': 191,\n",
       " '新品': 192,\n",
       " '金': 193,\n",
       " '家用': 194,\n",
       " '水晶': 195,\n",
       " '懒人': 196,\n",
       " '吊坠': 197,\n",
       " '厚底': 198,\n",
       " '中': 199,\n",
       " '家居': 200,\n",
       " '配件': 201,\n",
       " '43': 202,\n",
       " '黄色': 203,\n",
       " '组合': 204,\n",
       " '粗': 205,\n",
       " '全包': 206,\n",
       " '书包': 207,\n",
       " '200': 208,\n",
       " '架': 209,\n",
       " '高跟鞋': 210,\n",
       " '饰品': 211,\n",
       " '三星': 212,\n",
       " '米色': 213,\n",
       " '公仔': 214,\n",
       " '硅胶': 215,\n",
       " '绿色': 216,\n",
       " '内': 217,\n",
       " '百搭': 218,\n",
       " '丝圈': 219,\n",
       " '后备箱': 220,\n",
       " '吸顶灯': 221,\n",
       " '皮套': 222,\n",
       " '透明': 223,\n",
       " '电脑': 224,\n",
       " '钱包': 225,\n",
       " 's': 226,\n",
       " '三件套': 227,\n",
       " '16': 228,\n",
       " '手工': 229,\n",
       " '蓝': 230,\n",
       " '家具': 231,\n",
       " '厘米': 232,\n",
       " '中国': 233,\n",
       " '个': 234,\n",
       " '高清': 235,\n",
       " '正品': 236,\n",
       " '毛绒玩具': 237,\n",
       " '型': 238,\n",
       " '9': 239,\n",
       " '春秋': 240,\n",
       " '台': 241,\n",
       " '冬季': 242,\n",
       " '挂件': 243,\n",
       " '奔驰': 244,\n",
       " '金色': 245,\n",
       " '跑步': 246,\n",
       " '银': 247,\n",
       " '包邮': 248,\n",
       " '智能': 249,\n",
       " '抱': 250,\n",
       " '3d': 251,\n",
       " '斜挎包': 252,\n",
       " 'a': 253,\n",
       " '双肩包': 254,\n",
       " 'xl': 255,\n",
       " '福克斯': 256,\n",
       " '纯色': 257,\n",
       " '岁': 258,\n",
       " '婚庆': 259,\n",
       " '天然': 260,\n",
       " '级': 261,\n",
       " '潮': 262,\n",
       " '礼盒': 263,\n",
       " '增高': 264,\n",
       " '白': 265,\n",
       " '20': 266,\n",
       " '灯具': 267,\n",
       " '一': 268,\n",
       " '帆布鞋': 269,\n",
       " '家纺': 270,\n",
       " '华为': 271,\n",
       " '银色': 272,\n",
       " '迷你': 273,\n",
       " '紫色': 274,\n",
       " '双层': 275,\n",
       " '丰田': 276,\n",
       " '斜': 277,\n",
       " '单': 278,\n",
       " '本田': 279,\n",
       " '单人': 280,\n",
       " '卡': 281,\n",
       " '支架': 282,\n",
       " '短袖': 283,\n",
       " '手提包': 284,\n",
       " '戒指': 285,\n",
       " '皮': 286,\n",
       " '用品': 287,\n",
       " '件套': 288,\n",
       " '钻石': 289,\n",
       " '圆头': 290,\n",
       " '14': 291,\n",
       " '片': 292,\n",
       " '双': 293,\n",
       " '项链': 294,\n",
       " '耐磨': 295,\n",
       " '车型': 296,\n",
       " '立体': 297,\n",
       " '月': 298,\n",
       " '玻璃': 299,\n",
       " '水钻': 300,\n",
       " '性感': 301,\n",
       " 't恤': 302,\n",
       " '餐厅': 303,\n",
       " '方向盘': 304,\n",
       " '边框': 305,\n",
       " '高尔夫': 306,\n",
       " '贴纸': 307,\n",
       " '贴膜': 308,\n",
       " '头层': 309,\n",
       " '外壳': 310,\n",
       " '式': 311,\n",
       " '镂空': 312,\n",
       " '原装': 313,\n",
       " '盒': 314,\n",
       " '冰丝': 315,\n",
       " '豆豆': 316,\n",
       " '100': 317,\n",
       " '益智': 318,\n",
       " '椅': 319,\n",
       " '扣': 320,\n",
       " '头': 321,\n",
       " '磨砂': 322,\n",
       " '包包': 323,\n",
       " '彩色': 324,\n",
       " '44': 325,\n",
       " '床品': 326,\n",
       " '一对': 327,\n",
       " '别克': 328,\n",
       " '摔': 329,\n",
       " '玫瑰': 330,\n",
       " '棉': 331,\n",
       " '册': 332,\n",
       " '帕萨特': 333,\n",
       " '把': 334,\n",
       " '充电': 335,\n",
       " '收纳': 336,\n",
       " '十字绣': 337,\n",
       " '旅行': 338,\n",
       " '折叠': 339,\n",
       " '公主': 340,\n",
       " '浅口': 341,\n",
       " '功夫': 342,\n",
       " '环保': 343,\n",
       " '韩国': 344,\n",
       " '人': 345,\n",
       " '小米': 346,\n",
       " '车用': 347,\n",
       " '多': 348,\n",
       " '不': 349,\n",
       " '粉': 350,\n",
       " '衣': 351,\n",
       " '吊灯': 352,\n",
       " '彩绘': 353,\n",
       " '手': 354,\n",
       " '钥匙包': 355,\n",
       " '条': 356,\n",
       " '夏': 357,\n",
       " '春夏季': 358,\n",
       " '大容量': 359,\n",
       " '笔': 360,\n",
       " '凉': 361,\n",
       " '布鞋': 362,\n",
       " '24': 363,\n",
       " '厨房': 364,\n",
       " '斜纹': 365,\n",
       " '茶杯': 366,\n",
       " '水杯': 367,\n",
       " '与': 368,\n",
       " '结婚': 369,\n",
       " '途观': 370,\n",
       " '专用汽车': 371,\n",
       " '被子': 372,\n",
       " '衣服': 373,\n",
       " '本': 374,\n",
       " '925': 375,\n",
       " '猫': 376,\n",
       " '灯饰': 377,\n",
       " '和': 378,\n",
       " '座椅': 379,\n",
       " '套餐': 380,\n",
       " '子': 381,\n",
       " '拉': 382,\n",
       " '无线': 383,\n",
       " '遥控': 384,\n",
       " '小号': 385,\n",
       " '细': 386,\n",
       " '卡罗': 387,\n",
       " '沙发': 388,\n",
       " '低帮': 389,\n",
       " '便携': 390,\n",
       " '办公室': 391,\n",
       " '花': 392,\n",
       " '无': 393,\n",
       " '手提': 394,\n",
       " '蝴蝶结': 395,\n",
       " '潮鞋': 396,\n",
       " '低': 397,\n",
       " '13': 398,\n",
       " '标准': 399,\n",
       " '纹': 400,\n",
       " '福特': 401,\n",
       " '笔记本': 402,\n",
       " '钢化': 403,\n",
       " '桌': 404,\n",
       " '全新': 405,\n",
       " '袋': 406,\n",
       " 'diy': 407,\n",
       " '坡': 408,\n",
       " 'plus': 409,\n",
       " '30': 410,\n",
       " 'iphone6': 411,\n",
       " '双肩': 412,\n",
       " '女单': 413,\n",
       " '零食': 414,\n",
       " '6s': 415,\n",
       " '北京': 416,\n",
       " '粉红色': 417,\n",
       " '短靴': 418,\n",
       " '咖啡色': 419,\n",
       " '凯美瑞': 420,\n",
       " '芯': 421,\n",
       " '套脚': 422,\n",
       " '驾车': 423,\n",
       " '起亚': 424,\n",
       " '工具': 425,\n",
       " 'usb': 426,\n",
       " '秋季': 427,\n",
       " '书籍': 428,\n",
       " '罩': 429,\n",
       " '60': 430,\n",
       " '备注': 431,\n",
       " '钥匙': 432,\n",
       " '50': 433,\n",
       " '均码': 434,\n",
       " '坠': 435,\n",
       " '后': 436,\n",
       " '老': 437,\n",
       " '器': 438,\n",
       " '颜色': 439,\n",
       " '女童': 440,\n",
       " '工艺品': 441,\n",
       " '塑料': 442,\n",
       " '模型': 443,\n",
       " '230cm': 444,\n",
       " '防风': 445,\n",
       " '情人节': 446,\n",
       " '大码': 447,\n",
       " '11': 448,\n",
       " '爱': 449,\n",
       " '线': 450,\n",
       " '徒步': 451,\n",
       " '鲜花': 452,\n",
       " '克': 453,\n",
       " '雅阁': 454,\n",
       " '酒': 455,\n",
       " '34': 456,\n",
       " '柜': 457,\n",
       " '活性': 458,\n",
       " '尾箱': 459,\n",
       " '18': 460,\n",
       " '外套': 461,\n",
       " '积木': 462,\n",
       " '标配': 463,\n",
       " '手串': 464,\n",
       " '新生儿': 465,\n",
       " '拉链': 466,\n",
       " '紫': 467,\n",
       " '花花公子': 468,\n",
       " '教材': 469,\n",
       " 'b': 470,\n",
       " '长': 471,\n",
       " '迈腾': 472,\n",
       " '中式': 473,\n",
       " '绿': 474,\n",
       " '滤清器': 475,\n",
       " 't': 476,\n",
       " '宽': 477,\n",
       " '150': 478,\n",
       " '田园': 479,\n",
       " '灰': 480,\n",
       " '孕妇装': 481,\n",
       " '甜美': 482,\n",
       " '玩偶': 483,\n",
       " '女式': 484,\n",
       " '壶': 485,\n",
       " 'c': 486,\n",
       " '现货': 487,\n",
       " '马丁': 488,\n",
       " '安全': 489,\n",
       " '亚麻': 490,\n",
       " '脚': 491,\n",
       " '遮阳': 492,\n",
       " '自动': 493,\n",
       " '圆形': 494,\n",
       " '沙滩鞋': 495,\n",
       " '长安': 496,\n",
       " '裤': 497,\n",
       " '座': 498,\n",
       " '女友': 499,\n",
       " '豪华版': 500,\n",
       " '虎': 501,\n",
       " '温馨': 502,\n",
       " '咖啡': 503,\n",
       " '女孩': 504,\n",
       " '美': 505,\n",
       " '长袖': 506,\n",
       " '帆布': 507,\n",
       " '软壳': 508,\n",
       " '女生': 509,\n",
       " '松糕': 510,\n",
       " '条纹': 511,\n",
       " '茶盘': 512,\n",
       " '长款': 513,\n",
       " '迪士尼': 514,\n",
       " '美式': 515,\n",
       " '熊': 516,\n",
       " '两用': 517,\n",
       " '用': 518,\n",
       " '马自达': 519,\n",
       " '标致': 520,\n",
       " '游戏': 521,\n",
       " '日产': 522,\n",
       " '含': 523,\n",
       " '香水': 524,\n",
       " '滤': 525,\n",
       " '板': 526,\n",
       " '美国': 527,\n",
       " '冲锋衣': 528,\n",
       " '专业': 529,\n",
       " '速腾': 530,\n",
       " '布娃娃': 531,\n",
       " '挡': 532,\n",
       " '空气': 533,\n",
       " '韩版潮': 534,\n",
       " '蓝牙': 535,\n",
       " '一字': 536,\n",
       " '白光': 537,\n",
       " '科鲁兹': 538,\n",
       " '装饰品': 539,\n",
       " 'crv': 540,\n",
       " '音乐': 541,\n",
       " '服': 542,\n",
       " '成人': 543,\n",
       " '车衣': 544,\n",
       " '双人床': 545,\n",
       " '学院': 546,\n",
       " '斤': 547,\n",
       " '平板': 548,\n",
       " '隔热': 549,\n",
       " '网面': 550,\n",
       " '凉席': 551,\n",
       " '靴子': 552,\n",
       " '优雅': 553,\n",
       " '垫子': 554,\n",
       " '键盘': 555,\n",
       " '度': 556,\n",
       " '件': 557,\n",
       " '棕': 558,\n",
       " '日本': 559,\n",
       " '皮肤': 560,\n",
       " '盖': 561,\n",
       " '配': 562,\n",
       " '黄': 563,\n",
       " '蕾丝': 564,\n",
       " '证书': 565,\n",
       " '德国': 566,\n",
       " '联想': 567,\n",
       " '翡翠': 568,\n",
       " '茶': 569,\n",
       " '轻便': 570,\n",
       " '速干': 571,\n",
       " '拖': 572,\n",
       " '三': 573,\n",
       " '捷达': 574,\n",
       " '全套': 575,\n",
       " '深蓝色': 576,\n",
       " '糖果': 577,\n",
       " '碗': 578,\n",
       " '茶壶': 579,\n",
       " '仿真': 580,\n",
       " '小包': 581,\n",
       " '英朗': 582,\n",
       " 'polo': 583,\n",
       " '内衣': 584,\n",
       " '180': 585,\n",
       " '达': 586,\n",
       " '书房': 587,\n",
       " '请': 588,\n",
       " '上衣': 589,\n",
       " '办公桌': 590,\n",
       " '橙色': 591,\n",
       " '挎': 592,\n",
       " '合金': 593,\n",
       " '小学生': 594,\n",
       " '钥匙扣': 595,\n",
       " '宝': 596,\n",
       " '逸': 597,\n",
       " '监控': 598,\n",
       " '钢化玻璃': 599,\n",
       " '儿童玩具': 600,\n",
       " '格': 601,\n",
       " '蒙迪欧': 602,\n",
       " '水壶': 603,\n",
       " '浅': 604,\n",
       " '汽车用品': 605,\n",
       " '特产': 606,\n",
       " '妈妈': 607,\n",
       " '枕套': 608,\n",
       " '婴幼儿': 609,\n",
       " '耳机': 610,\n",
       " '玫': 611,\n",
       " '箱': 612,\n",
       " '天籁': 613,\n",
       " '狗': 614,\n",
       " '鱼': 615,\n",
       " '浮雕': 616,\n",
       " '2014': 617,\n",
       " '比亚迪': 618,\n",
       " '钱': 619,\n",
       " '椅子': 620,\n",
       " '清新': 621,\n",
       " '200cm': 622,\n",
       " '适合': 623,\n",
       " '直径': 624,\n",
       " '插': 625,\n",
       " '坐套': 626,\n",
       " '登山': 627,\n",
       " '下': 628,\n",
       " '紫砂': 629,\n",
       " 'ipad': 630,\n",
       " '80': 631,\n",
       " '被罩': 632,\n",
       " '修复': 633,\n",
       " '明锐': 634,\n",
       " '电池': 635,\n",
       " '滤芯': 636,\n",
       " '娃娃': 637,\n",
       " '迷彩': 638,\n",
       " '卡其色': 639,\n",
       " '盘': 640,\n",
       " '置物架': 641,\n",
       " '绣': 642,\n",
       " '18k': 643,\n",
       " '货到付款': 644,\n",
       " '味': 645,\n",
       " '移动': 646,\n",
       " '随机': 647,\n",
       " '脚蹬': 648,\n",
       " '茶几': 649,\n",
       " '男包': 650,\n",
       " '瓶': 651,\n",
       " 'rav4': 652,\n",
       " '玉石': 653,\n",
       " 'xxl': 654,\n",
       " '开关': 655,\n",
       " '珠宝': 656,\n",
       " '博世': 657,\n",
       " '机油': 658,\n",
       " '浪漫': 659,\n",
       " '高档': 660,\n",
       " '雨刮器': 661,\n",
       " '书': 662,\n",
       " '双面': 663,\n",
       " '佛珠': 664,\n",
       " '翼': 665,\n",
       " '睡袋': 666,\n",
       " '加绒': 667,\n",
       " '拼色': 668,\n",
       " '夏装': 669,\n",
       " '电动': 670,\n",
       " '链': 671,\n",
       " '超薄': 672,\n",
       " 'u': 673,\n",
       " '景德镇': 674,\n",
       " '短款': 675,\n",
       " '餐具': 676,\n",
       " '软底': 677,\n",
       " '沙滩': 678,\n",
       " '男式': 679,\n",
       " '秋冬季': 680,\n",
       " '考试': 681,\n",
       " '长裤': 682,\n",
       " '喷漆': 683,\n",
       " '镶': 684,\n",
       " '提花': 685,\n",
       " '和田玉': 686,\n",
       " '室内': 687,\n",
       " '加大': 688,\n",
       " '花瓶': 689,\n",
       " '全国': 690,\n",
       " '探路者': 691,\n",
       " '吉普': 692,\n",
       " '精品': 693,\n",
       " '荣耀': 694,\n",
       " '阳光': 695,\n",
       " '日': 696,\n",
       " '高帮': 697,\n",
       " '雪佛兰': 698,\n",
       " '保温杯': 699,\n",
       " '手套': 700,\n",
       " '护板': 701,\n",
       " 'cd': 702,\n",
       " '电源': 703,\n",
       " '哈弗': 704,\n",
       " '飞度': 705,\n",
       " '有': 706,\n",
       " '4g': 707,\n",
       " '户外运动': 708,\n",
       " '帮': 709,\n",
       " '一体机': 710,\n",
       " '夏凉': 711,\n",
       " '画': 712,\n",
       " '留言': 713,\n",
       " '后盖': 714,\n",
       " '帐篷': 715,\n",
       " '45': 716,\n",
       " '汉兰达': 717,\n",
       " '机': 718,\n",
       " '皇冠': 719,\n",
       " '电子': 720,\n",
       " '衣柜': 721,\n",
       " '网': 722,\n",
       " 'a6l': 723,\n",
       " '生日': 724,\n",
       " 'pu': 725,\n",
       " '上': 726,\n",
       " 'q5': 727,\n",
       " '世界': 728,\n",
       " '220': 729,\n",
       " '平': 730,\n",
       " '旅游': 731,\n",
       " '拼接': 732,\n",
       " '内饰': 733,\n",
       " '手镯': 734,\n",
       " '气质': 735,\n",
       " '摆设': 736,\n",
       " '面板': 737,\n",
       " '帽': 738,\n",
       " 'dvd': 739,\n",
       " '跨': 740,\n",
       " '灯泡': 741,\n",
       " '特价': 742,\n",
       " '鳄鱼': 743,\n",
       " '餐桌': 744,\n",
       " '磨毛': 745,\n",
       " '拍': 746,\n",
       " '木': 747,\n",
       " '长城': 748,\n",
       " '雪铁龙': 749,\n",
       " '元': 750,\n",
       " '杯子': 751,\n",
       " '支': 752,\n",
       " '动物': 753,\n",
       " '春': 754,\n",
       " '靠垫': 755,\n",
       " '罗马': 756,\n",
       " '玫红': 757,\n",
       " '夏天': 758,\n",
       " '早教': 759,\n",
       " '防雨': 760,\n",
       " '拉杆箱': 761,\n",
       " '其他': 762,\n",
       " '食品': 763,\n",
       " '干': 764,\n",
       " '男孩': 765,\n",
       " '鱼嘴': 766,\n",
       " '充电器': 767,\n",
       " '日常': 768,\n",
       " '头盔': 769,\n",
       " '毯': 770,\n",
       " '升级版': 771,\n",
       " '纯': 772,\n",
       " '罐': 773,\n",
       " '豪华': 774,\n",
       " '旅行包': 775,\n",
       " '泳衣': 776,\n",
       " 'x5': 777,\n",
       " '思域': 778,\n",
       " '嘴': 779,\n",
       " '实用': 780,\n",
       " '光盘': 781,\n",
       " '木质': 782,\n",
       " '升级': 783,\n",
       " '行李箱': 784,\n",
       " '趾': 785,\n",
       " '两件套': 786,\n",
       " 'e': 787,\n",
       " '单个': 788,\n",
       " '钢笔': 789,\n",
       " '马': 790,\n",
       " '凯越': 791,\n",
       " '支装': 792,\n",
       " '米奇': 793,\n",
       " '保温': 794,\n",
       " '摩托车': 795,\n",
       " '抓': 796,\n",
       " '款式': 797,\n",
       " '男表': 798,\n",
       " 'ix35': 799,\n",
       " '正装': 800,\n",
       " '乐福鞋': 801,\n",
       " '第': 802,\n",
       " '网鞋': 803,\n",
       " '记录仪': 804,\n",
       " '黄金': 805,\n",
       " '纸': 806,\n",
       " '连衣裙': 807,\n",
       " '片装': 808,\n",
       " '毛毯': 809,\n",
       " '划痕': 810,\n",
       " '宝来': 811,\n",
       " '居家': 812,\n",
       " '简易': 813,\n",
       " '男童': 814,\n",
       " '连体': 815,\n",
       " '230': 816,\n",
       " '内胆': 817,\n",
       " '大号': 818,\n",
       " '拼': 819,\n",
       " '皮带': 820,\n",
       " '之': 821,\n",
       " '拼装': 822,\n",
       " '三合一': 823,\n",
       " '松糕鞋': 824,\n",
       " '层': 825,\n",
       " '调光': 826,\n",
       " '修身': 827,\n",
       " '儿童节': 828,\n",
       " '附': 829,\n",
       " 'oppo': 830,\n",
       " '19': 831,\n",
       " '设计': 832,\n",
       " '包头': 833,\n",
       " '我': 834,\n",
       " '阳台': 835,\n",
       " '编织': 836,\n",
       " '单件': 837,\n",
       " '音箱': 838,\n",
       " '登山鞋': 839,\n",
       " '咖色': 840,\n",
       " '贡缎': 841,\n",
       " '镜': 842,\n",
       " '上海': 843,\n",
       " '棉鞋': 844,\n",
       " '安装': 845,\n",
       " '钓鱼': 846,\n",
       " '锐志': 847,\n",
       " '训练': 848,\n",
       " '桌椅': 849,\n",
       " '漆皮': 850,\n",
       " '宠物': 851,\n",
       " '游泳': 852,\n",
       " '仕': 853,\n",
       " '硬盘': 854,\n",
       " '骑行': 855,\n",
       " '逍客': 856,\n",
       " '索纳塔': 857,\n",
       " '漆': 858,\n",
       " '英语': 859,\n",
       " '故事': 860,\n",
       " '帽子': 861,\n",
       " '路': 862,\n",
       " '选': 863,\n",
       " '兔': 864,\n",
       " '方形': 865,\n",
       " '土豪': 866,\n",
       " '前后': 867,\n",
       " '开': 868,\n",
       " 'a3': 869,\n",
       " '约': 870,\n",
       " '摄像头': 871,\n",
       " '25': 872,\n",
       " '夜光': 873,\n",
       " '导航': 874,\n",
       " 'a4l': 875,\n",
       " '120': 876,\n",
       " '铆钉': 877,\n",
       " '睿': 878,\n",
       " '绣花': 879,\n",
       " '风格': 880,\n",
       " 'cc': 881,\n",
       " '学习': 882,\n",
       " '旅行箱': 883,\n",
       " '打火机': 884,\n",
       " '自': 885,\n",
       " '桑塔纳': 886,\n",
       " '东风': 887,\n",
       " '8g': 888,\n",
       " '整套': 889,\n",
       " 'h6': 890,\n",
       " '铝合金': 891,\n",
       " '软': 892,\n",
       " '长方形': 893,\n",
       " '仿古': 894,\n",
       " '扳手': 895,\n",
       " '貔貅': 896,\n",
       " '斯柯达': 897,\n",
       " '珍珠': 898,\n",
       " '行车': 899,\n",
       " 'd': 900,\n",
       " 's6': 901,\n",
       " '张': 902,\n",
       " '底': 903,\n",
       " '球': 904,\n",
       " '水洗': 905,\n",
       " '卷': 906,\n",
       " '点': 907,\n",
       " '速递': 908,\n",
       " '鼠标': 909,\n",
       " '出版社': 910,\n",
       " '咖': 911,\n",
       " '配饰': 912,\n",
       " '免': 913,\n",
       " '5mm': 914,\n",
       " '黑白': 915,\n",
       " '裙': 916,\n",
       " '图书': 917,\n",
       " '22': 918,\n",
       " '车垫': 919,\n",
       " '大红': 920,\n",
       " '宝骏': 921,\n",
       " '雕花': 922,\n",
       " '定做': 923,\n",
       " '电镀': 924,\n",
       " '盒装': 925,\n",
       " '好': 926,\n",
       " '超高': 927,\n",
       " '翻盖': 928,\n",
       " '音响': 929,\n",
       " '手绘': 930,\n",
       " '台灯': 931,\n",
       " '骆驼': 932,\n",
       " '圆领': 933,\n",
       " 'v': 934,\n",
       " '古典': 935,\n",
       " '拉手': 936,\n",
       " '浅蓝色': 937,\n",
       " '年级': 938,\n",
       " 'pro': 939,\n",
       " '台湾': 940,\n",
       " '吊': 941,\n",
       " '酒杯': 942,\n",
       " '天': 943,\n",
       " '品': 944,\n",
       " '笔记本电脑': 945,\n",
       " '镜头': 946,\n",
       " '绒': 947,\n",
       " '世家': 948,\n",
       " '精装': 949,\n",
       " '后视镜': 950,\n",
       " '指环': 951,\n",
       " '铜': 952,\n",
       " '幼儿园': 953,\n",
       " '生肖': 954,\n",
       " '烤': 955,\n",
       " '网布': 956,\n",
       " '可折叠': 957,\n",
       " '标签': 958,\n",
       " '朗逸': 959,\n",
       " '必备': 960,\n",
       " '边': 961,\n",
       " '共': 962,\n",
       " '电脑包': 963,\n",
       " '魅族': 964,\n",
       " '补漆笔': 965,\n",
       " '小熊': 966,\n",
       " '轻薄': 967,\n",
       " '静音': 968,\n",
       " '网络': 969,\n",
       " '泰迪熊': 970,\n",
       " '艺术': 971,\n",
       " '标准版': 972,\n",
       " '卡宴': 973,\n",
       " '腰包': 974,\n",
       " '文具': 975,\n",
       " '500ml': 976,\n",
       " '电视': 977,\n",
       " '头枕': 978,\n",
       " '风衣': 979,\n",
       " '年份': 980,\n",
       " '男装': 981,\n",
       " '插座': 982,\n",
       " '90': 983,\n",
       " '电视柜': 984,\n",
       " '刹车片': 985,\n",
       " '超': 986,\n",
       " '奶瓶': 987,\n",
       " '冬': 988,\n",
       " '昂科威': 989,\n",
       " '主机': 990,\n",
       " '富贵': 991,\n",
       " '自由': 992,\n",
       " '开业': 993,\n",
       " '仪表': 994,\n",
       " '工装': 995,\n",
       " 'vivo': 996,\n",
       " '耳钉': 997,\n",
       " '羊皮': 998,\n",
       " '书桌': 999,\n",
       " '机械': 1000,\n",
       " ...}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "274420"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tok.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = tok.texts_to_sequences(word_data_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = tok.texts_to_sequences(word_data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = sequence.pad_sequences(x_train, maxlen=20)\n",
    "x_test  = sequence.pad_sequences(x_test, maxlen=20) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 29967,    107,      1, ...,      5,    182,    102],\n",
       "       [     0,      0,      0, ...,   3281, 147266,    226],\n",
       "       [     3,     38,     42, ...,     57,     10,    923],\n",
       "       ...,\n",
       "       [     0,      0,   4474, ...,   4963,      9,     23],\n",
       "       [   597,    538,    155, ...,    378,   1109,   1541],\n",
       "       [     0,      0,      0, ...,    170,   3516,    999]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## GRU Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.contrib import rnn\n",
    "import tensorflow.contrib.layers as layers\n",
    "class GruModel(object):\n",
    "    \"\"\"\n",
    "    A CNN for text classification.\n",
    "    Uses an embedding layer, followed by a convolutional, max-pooling and softmax layer.\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 sequence_length, \n",
    "                 num_classes, \n",
    "                 embedding_size,  \n",
    "                 num_filters, \n",
    "                 l2_reg_lambda=0.0, \n",
    "                 n_layer = 1, \n",
    "                 hidden_size = 32, \n",
    "                 batch_size = 256, \n",
    "                 vac_size = 27440):\n",
    "        \"\"\"\n",
    "        sequence_length : 一个句子的长度（词的个数）\n",
    "        embedding_size : 词向量的长度\n",
    "        num_classes : 三个标签的类别数\n",
    "        vac_size : 词的个数\n",
    "        \"\"\"\n",
    "        # Placeholders for input, output and dropout\n",
    "        self.input_x = tf.placeholder(tf.int32, [None, sequence_length], name=\"input_x\")\n",
    "        self.input_y1 = tf.placeholder(tf.float32, [None, num_classes[0]], name=\"input_y1\")\n",
    "        self.input_y2 = tf.placeholder(tf.float32, [None, num_classes[1]], name=\"input_y2\")\n",
    "        self.input_y3 = tf.placeholder(tf.float32, [None, num_classes[2]], name=\"input_y3\")\n",
    "        self.keep_prob = tf.placeholder(tf.float32, name=\"keep_prob\")\n",
    "        self.n_layer = n_layer\n",
    "        self.batch_size = batch_size\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        \n",
    "        with tf.name_scope(\"embedding\"):\n",
    "            W = tf.Variable(\n",
    "                tf.random_uniform([274420, embedding_size], -1.0, 1.0),\n",
    "                name=\"W\")\n",
    "            self.embedded_chars = tf.nn.embedding_lookup(W, self.input_x)\n",
    "\n",
    "        l2_loss = tf.constant(0.0)\n",
    "        \n",
    "        #gru模型\n",
    "        with tf.variable_scope('bigru_text'):\n",
    "            self.gru_output = self.bigru_inference(self.embedded_chars)\n",
    "            print(\"gru shape\", self.gru_output.shape)\n",
    "            \n",
    "        self.h_drop = self.gru_output\n",
    "        num_filters_total = self.h_drop.shape[1]\n",
    "        \n",
    "        with tf.name_scope(\"output1\"):\n",
    "            W1 = tf.get_variable(\n",
    "                \"W11\",\n",
    "                shape=[num_filters_total, 64],\n",
    "                initializer=tf.contrib.layers.xavier_initializer())\n",
    "            b1 = tf.Variable(tf.constant(0.1, shape=[64]), name=\"b11\")\n",
    "            self.output = tf.nn.xw_plus_b(self.h_drop, W1, b1, name=\"output1\")\n",
    "            self.output = tf.layers.batch_normalization(self.output, training=is_training)\n",
    "            self.output = tf.nn.relu(self.output)\n",
    "            self.output = tf.nn.dropout(self.output, self.keep_prob)\n",
    "            W2 = tf.get_variable(\n",
    "                \"W12\",\n",
    "                shape=[64, num_classes[0]],\n",
    "                initializer=tf.contrib.layers.xavier_initializer())\n",
    "            b2 = tf.Variable(tf.constant(0.1, shape=[num_classes[0]]), name=\"b12\")\n",
    "            l2_loss += tf.nn.l2_loss(W1)\n",
    "            l2_loss += tf.nn.l2_loss(b1)\n",
    "            self.scores1 = tf.nn.xw_plus_b(self.output, W2, b2, name=\"scores\")\n",
    "            self.predictions1 = tf.argmax(self.scores1, 1, name=\"predictions\")\n",
    "            \n",
    "        with tf.name_scope(\"output2\"):\n",
    "            W1 = tf.get_variable(\n",
    "                \"W21\",\n",
    "                shape=[num_filters_total, 128],\n",
    "                initializer=tf.contrib.layers.xavier_initializer())\n",
    "            b1 = tf.Variable(tf.constant(0.1, shape=[128]), name=\"b21\")\n",
    "            self.output = tf.nn.xw_plus_b(self.h_drop, W1, b1, name=\"output2\")\n",
    "            self.output = tf.layers.batch_normalization(self.output, training=is_training)\n",
    "            self.output = tf.nn.relu(self.output)\n",
    "            self.output = tf.nn.dropout(self.output, self.keep_prob)\n",
    "            W2 = tf.get_variable(\n",
    "                \"W22\",\n",
    "                shape=[128, num_classes[1]],\n",
    "                initializer=tf.contrib.layers.xavier_initializer())\n",
    "            b2 = tf.Variable(tf.constant(0.1, shape=[num_classes[1]]), name=\"b22\")\n",
    "            l2_loss += tf.nn.l2_loss(W1)\n",
    "            l2_loss += tf.nn.l2_loss(b1)\n",
    "            self.scores2 = tf.nn.xw_plus_b(self.output, W2, b2, name=\"scores\")\n",
    "            self.predictions2 = tf.argmax(self.scores2, 1, name=\"predictions\")\n",
    "            \n",
    "        with tf.name_scope(\"output3\"):\n",
    "            W1 = tf.get_variable(\n",
    "                \"W31\",\n",
    "                shape=[num_filters_total, 256],\n",
    "                initializer=tf.contrib.layers.xavier_initializer())\n",
    "            b1 = tf.Variable(tf.constant(0.1, shape=[256]), name=\"b31\")\n",
    "            self.output = tf.nn.xw_plus_b(self.h_drop, W1, b1, name=\"output3\")\n",
    "            self.output = tf.layers.batch_normalization(self.output, training=is_training)\n",
    "            self.output = tf.nn.relu(self.output)\n",
    "            self.output = tf.nn.dropout(self.output, self.keep_prob)\n",
    "            W2 = tf.get_variable(\n",
    "                \"W32\",\n",
    "                shape=[256, num_classes[2]],\n",
    "                initializer=tf.contrib.layers.xavier_initializer())\n",
    "            b2 = tf.Variable(tf.constant(0.1, shape=[num_classes[2]]), name=\"b32\")\n",
    "            l2_loss += tf.nn.l2_loss(W1)\n",
    "            l2_loss += tf.nn.l2_loss(b1)\n",
    "            self.scores3 = tf.nn.xw_plus_b(self.output, W2, b2, name=\"scores\")\n",
    "            self.predictions3 = tf.argmax(self.scores3, 1, name=\"predictions\")\n",
    "        # Calculate mean cross-entropy loss\n",
    "        \n",
    "        with tf.name_scope(\"loss\"):\n",
    "            losses1 = tf.nn.softmax_cross_entropy_with_logits(logits=self.scores1, labels=self.input_y1)\n",
    "            losses2 = tf.nn.softmax_cross_entropy_with_logits(logits=self.scores2, labels=self.input_y2)\n",
    "            losses3 = tf.nn.softmax_cross_entropy_with_logits(logits=self.scores3, labels=self.input_y3)\n",
    "            self.loss = 0.1 * tf.reduce_mean(losses1) + 0.2 * tf.reduce_mean(losses2) + 0.7*tf.reduce_mean(losses3) + l2_reg_lambda * l2_loss\n",
    "            tf.summary.scalar('loss1',tf.reduce_mean(losses1))\n",
    "            tf.summary.scalar('loss2',tf.reduce_mean(losses2))\n",
    "            tf.summary.scalar('loss3',tf.reduce_mean(losses3))\n",
    "            tf.summary.scalar('loss',self.loss)\n",
    "        # Accuracy\n",
    "        with tf.name_scope(\"accuracy\"):\n",
    "            correct_predictions1 = tf.equal(self.predictions1, tf.argmax(self.input_y1, 1))\n",
    "            correct_predictions2 = tf.equal(self.predictions2, tf.argmax(self.input_y2, 1))\n",
    "            correct_predictions3 = tf.equal(self.predictions3, tf.argmax(self.input_y3, 1))\n",
    "#           单独的准确率\n",
    "            self.accuracy1 = tf.reduce_mean(tf.cast(correct_predictions1, \"float\"), name=\"accuracy1\")\n",
    "            self.accuracy2 = tf.reduce_mean(tf.cast(correct_predictions2, \"float\"), name=\"accuracy2\")\n",
    "            self.accuracy3 = tf.reduce_mean(tf.cast(correct_predictions3, \"float\"), name=\"accuracy3\")\n",
    "#           一起的准确率\n",
    "            self.acc = tf.reduce_mean(tf.cast(correct_predictions1, \"float\")*tf.cast(correct_predictions2, \"float\")*tf.cast(correct_predictions3, \"float\"))\n",
    "            tf.summary.scalar('acc1',self.accuracy1)\n",
    "            tf.summary.scalar('acc2',self.accuracy2)\n",
    "            tf.summary.scalar('acc3',self.accuracy3)\n",
    "            tf.summary.scalar('acc',self.acc)\n",
    "            \n",
    "    def gru_cell(self):\n",
    "        with tf.name_scope('gru_cell'):\n",
    "            cell = rnn.GRUCell(self.hidden_size, reuse=tf.get_variable_scope().reuse)\n",
    "        return rnn.DropoutWrapper(cell, output_keep_prob=self.keep_prob)\n",
    "\n",
    "    def bi_gru(self, inputs):\n",
    "        \"\"\"build the bi-GRU network. 返回个所有层的隐含状态。\"\"\"\n",
    "        cells_fw = [self.gru_cell() for _ in range(self.n_layer)]\n",
    "        cells_bw = [self.gru_cell() for _ in range(self.n_layer)]\n",
    "        initial_states_fw = [cell_fw.zero_state(self.batch_size, tf.float32) for cell_fw in cells_fw]\n",
    "        initial_states_bw = [cell_bw.zero_state(self.batch_size, tf.float32) for cell_bw in cells_bw]\n",
    "        outputs, _, _ = rnn.stack_bidirectional_dynamic_rnn(cells_fw, cells_bw, inputs,\n",
    "                                                            initial_states_fw=initial_states_fw,\n",
    "                                                            initial_states_bw=initial_states_bw, dtype=tf.float32)\n",
    "        return outputs\n",
    "    \n",
    "    def bigru_inference(self, X_inputs):\n",
    "#         inputs = tf.nn.embedding_lookup(self.title_embedding, X_inputs)\n",
    "        output_bigru = self.bi_gru(X_inputs)\n",
    "        output_att = self.task_specific_attention(output_bigru, self.hidden_size*2)\n",
    "        return output_att\n",
    "    \n",
    "    def task_specific_attention(self, inputs, output_size,\n",
    "                                initializer=layers.xavier_initializer(),\n",
    "                                activation_fn=tf.tanh, scope=None):\n",
    "        \"\"\"\n",
    "        Performs task-specific attention reduction, using learned\n",
    "        attention context vector (constant within task of interest).\n",
    "        Args:\n",
    "            inputs: Tensor of shape [batch_size, units, input_size]\n",
    "                `input_size` must be static (known)\n",
    "                `units` axis will be attended over (reduced from output)\n",
    "                `batch_size` will be preserved\n",
    "            output_size: Size of output's inner (feature) dimension\n",
    "        Returns:\n",
    "           outputs: Tensor of shape [batch_size, output_dim].\n",
    "        \"\"\"\n",
    "#         assert len(inputs.get_shape()) == 3 and inputs.get_shape()[-1].value is not None\n",
    "        with tf.variable_scope(scope or 'attention') as scope:\n",
    "            # u_w, attention 向量\n",
    "            attention_context_vector = tf.get_variable(name='attention_context_vector', shape=[output_size],\n",
    "                                                       initializer=initializer, dtype=tf.float32)\n",
    "            # 全连接层，把 h_i 转为 u_i ， shape= [batch_size, units, input_size] -> [batch_size, units, output_size]\n",
    "            input_projection = layers.fully_connected(inputs, output_size, activation_fn=activation_fn, scope=scope)\n",
    "            # 输出 [batch_size, units]\n",
    "            vector_attn = tf.reduce_sum(tf.multiply(input_projection, attention_context_vector), axis=2, keep_dims=True)\n",
    "            attention_weights = tf.nn.softmax(vector_attn, dim=1)\n",
    "            tf.summary.histogram('attention_weigths', attention_weights)\n",
    "            weighted_projection = tf.multiply(inputs, attention_weights)\n",
    "            outputs = tf.reduce_sum(weighted_projection, axis=1)\n",
    "            return outputs  # 输出 [batch_size, hidden_size*2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# batch 生成函数\n",
    "import math\n",
    "def batch_iter(data, batch_size, num_epochs, shuffle=True):\n",
    "    \"\"\"\n",
    "    Generates a batch iterator for a dataset.\n",
    "    \"\"\"\n",
    "    data = np.array(data)\n",
    "    data_size = len(data)\n",
    "    num_batches_per_epoch = int((len(data)-1)/batch_size) + 1\n",
    "    for epoch in range(num_epochs):\n",
    "        # Shuffle the data at each epoch\n",
    "        if shuffle:\n",
    "            shuffle_indices = np.random.permutation(np.arange(data_size))\n",
    "            shuffled_data = data[shuffle_indices]\n",
    "        else:\n",
    "            shuffled_data = data\n",
    "  \n",
    "        for batch_num in range(num_batches_per_epoch):\n",
    "            start_index = batch_num * batch_size\n",
    "            end_index = np.min(((batch_num + 1) * batch_size, data_size))\n",
    "            yield shuffled_data[start_index:end_index]\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 特征数这里是第三级标签的特征数\n",
    "max_features = 1192\n",
    "\n",
    "# 句子填充的长度\n",
    "sequence_len = 20\n",
    "\n",
    "# batch 大小\n",
    "batch_size = 256 \n",
    "\n",
    "# 迭代次数\n",
    "epochs = 100\n",
    "\n",
    "is_training = True\n",
    "\n",
    "# 词向量长度\n",
    "embedding_dims = 24\n",
    "\n",
    "# gru  的filters\n",
    "num_filters = 32\n",
    "\n",
    "# filter 的大小\n",
    "filter_size = [1, 2, 3, 4, 5]\n",
    "\n",
    "# 三个标签的类数\n",
    "num_classes = [22, 191, 1192]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "from tensorflow.contrib import learn\n",
    "\n",
    "\n",
    "\n",
    "FLAGS = tf.flags.FLAGS\n",
    "\n",
    "def train(x_train, y_train, x_dev, y_dev):\n",
    "    # Training\n",
    "    # ==================================================\n",
    "\n",
    "    with tf.Graph().as_default():\n",
    "        session_conf = tf.ConfigProto()\n",
    "        sess = tf.Session(config=session_conf)\n",
    "        with sess.as_default():\n",
    "            cnn = GruModel(sequence_length = sequence_len, \n",
    "                          num_classes = num_classes, \n",
    "                          embedding_size = embedding_dims, \n",
    "                          num_filters = num_filters)\n",
    "\n",
    "            # Define Training procedure\n",
    "\n",
    "            global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "#           学习率衰减\n",
    "            learning_rate = tf.train.exponential_decay(\n",
    "                0.01,\n",
    "                global_step,\n",
    "                x_train.shape[0] / batch_size,\n",
    "                0.99,\n",
    "                staircase=True,)\n",
    "            optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "            grads_and_vars = optimizer.compute_gradients(cnn.loss)\n",
    "            train_op = optimizer.apply_gradients(grads_and_vars, global_step=global_step)\n",
    "\n",
    "            # Initialize all variables\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            \n",
    "            merged = tf.summary.merge_all()\n",
    "            \n",
    "            writer = tf.summary.FileWriter(\"fwwb\",sess.graph)\n",
    "            acc_sum = np.array([0.0,0.0,0.0,0.0])\n",
    "            loss_sum = 0\n",
    "            \n",
    "            def train_step(x_batch, y_batch1, y_batch2, y_batch3, loss_sum, acc_sum, interal = 10):\n",
    "  \n",
    "                \"\"\"\n",
    "                A single training step\n",
    "                \"\"\"\n",
    "                feed_dict = {\n",
    "                  cnn.input_x: x_batch,\n",
    "                  cnn.input_y1: y_batch1,\n",
    "                  cnn.input_y2: y_batch2,\n",
    "                  cnn.input_y3: y_batch3,\n",
    "                  cnn.keep_prob: 0.9\n",
    "                }\n",
    "\n",
    "#         三个标签单独的准确率， 一起的准确率共四个准确率\n",
    "                rs, _, step,  loss, accuracy1, accuracy2, accuracy3, acc = sess.run(\n",
    "                    [merged, train_op, global_step, cnn.loss, cnn.accuracy1, cnn.accuracy2, cnn.accuracy3, cnn.acc],\n",
    "                    feed_dict)\n",
    "                time_str = datetime.datetime.now().isoformat()\n",
    "                acc_sum += [accuracy1, accuracy2, accuracy3, acc]\n",
    "                loss_sum += loss\n",
    "                if step % interal == 0:\n",
    "                    print(\"{}: step {}, loss {:g}, acc {}\".format(time_str, step, loss_sum/interal, acc_sum/interal))\n",
    "                    loss_sum = 0\n",
    "                    acc_sum = np.array([0.0,0.0,0.0,0.0])\n",
    "                    writer.add_summary(rs, step)\n",
    "                return loss_sum, acc_sum\n",
    "            # 评估步骤\n",
    "            def dev_step(x_batch, y_batch):\n",
    "                \"\"\"\n",
    "                Evaluates model on a dev set\n",
    "                \"\"\"\n",
    "\n",
    "                dev_batches = batch_iter(\n",
    "                    list(zip(x_batch, y_batch[0], y_batch[1], y_batch[2])), batch_size, 1)\n",
    "                step = 0\n",
    "                loss = 0\n",
    "                accuracy = np.array([0.0, 0.0, 0.0,0.0])\n",
    "                for batch in dev_batches:\n",
    "                    x_batch_dev, y_batch1_dev, y_batch2_dev, y_batch3_dev = zip(*batch)\n",
    "                    feed_dict = {\n",
    "                      cnn.input_x: x_batch_dev,\n",
    "                      cnn.input_y1: y_batch1_dev,\n",
    "                      cnn.input_y2: y_batch2_dev,\n",
    "                      cnn.input_y3: y_batch3_dev,\n",
    "                      cnn.keep_prob: 1\n",
    "                    }\n",
    "                    if len(x_batch_dev) < batch_size:\n",
    "                        continue\n",
    "                    step,  temp_loss, temp_accuracy1, temp_accuracy2, temp_accuracy3 , acc= sess.run(\n",
    "                        [global_step,cnn.loss, cnn.accuracy1, cnn.accuracy2, cnn.accuracy3, cnn.acc],\n",
    "                        feed_dict)\n",
    "                    accuracy[0] += temp_accuracy1 * len(x_batch_dev)\n",
    "                    accuracy[1] += temp_accuracy2 * len(x_batch_dev)\n",
    "                    accuracy[2] += temp_accuracy3 * len(x_batch_dev)\n",
    "                    accuracy[3] += acc * len(x_batch_dev)\n",
    "                    loss += temp_loss * len(x_batch_dev)\n",
    "                accuracy /= x_batch.shape[0]\n",
    "                loss /= x_batch.shape[0]\n",
    "                time_str = datetime.datetime.now().isoformat()                    \n",
    "\n",
    "                print(\"{}: step {}, loss {:g}, acc {} \".format(time_str, step, loss, accuracy))\n",
    "\n",
    "\n",
    "            # Generate batches\n",
    "            batches = batch_iter(\n",
    "                list(zip(x_train, y_train[0],y_train[1], y_train[2] )), batch_size, 200)\n",
    "            # Training loop. For each batch...\n",
    "            for batch in batches:\n",
    "                x_batch, y_batch1, y_batch2, y_batch3 = zip(*batch)\n",
    "                if len(x_batch) < batch_size:\n",
    "                    continue\n",
    "                loss_sum, acc_sum = train_step(x_batch, y_batch1, y_batch2, y_batch3,  loss_sum, acc_sum, 40)\n",
    "                current_step = tf.train.global_step(sess, global_step)\n",
    "                if current_step % 200 == 0:\n",
    "                    print(\"\\nEvaluation:\")\n",
    "                    dev_step(x_dev, y_dev)\n",
    "                    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-22-b2a1f149aef9>:184: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From <ipython-input-22-b2a1f149aef9>:185: calling softmax (from tensorflow.python.ops.nn_ops) with dim is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "dim is deprecated, use axis instead\n",
      "gru shape (256, 64)\n",
      "WARNING:tensorflow:From <ipython-input-22-b2a1f149aef9>:115: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n",
      "2019-02-27T22:51:24.542035: step 40, loss 3.88515, acc [0.44296875 0.36660156 0.28271484 0.21464844]\n",
      "2019-02-27T22:51:27.180222: step 80, loss 2.6546, acc [0.63574219 0.53984375 0.45703125 0.37216797]\n",
      "2019-02-27T22:51:29.735120: step 120, loss 2.15493, acc [0.7125     0.62724609 0.5515625  0.46113281]\n",
      "2019-02-27T22:51:32.423441: step 160, loss 1.84675, acc [0.76240234 0.67314453 0.6        0.51855469]\n",
      "2019-02-27T22:51:35.001651: step 200, loss 1.6767, acc [0.79023438 0.70976562 0.63398438 0.55751953]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-27T22:51:44.265991: step 200, loss 1.55321, acc [0.81129053 0.72649641 0.65275456 0.58267677] \n",
      "\n",
      "2019-02-27T22:51:47.127417: step 240, loss 1.58027, acc [0.80576172 0.72685547 0.65419922 0.57783203]\n",
      "2019-02-27T22:51:49.823137: step 280, loss 1.47167, acc [0.82431641 0.73847656 0.66943359 0.59033203]\n",
      "2019-02-27T22:51:52.483681: step 320, loss 1.34593, acc [0.84365234 0.75634766 0.68994141 0.61611328]\n",
      "2019-02-27T22:51:55.046516: step 360, loss 1.29515, acc [0.83847656 0.76298828 0.69794922 0.62412109]\n",
      "2019-02-27T22:51:57.661431: step 400, loss 1.20664, acc [0.85429687 0.78642578 0.71865234 0.65029297]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-27T22:52:08.613616: step 400, loss 1.12631, acc [0.86707245 0.7954029  0.73389462 0.67456877] \n",
      "\n",
      "2019-02-27T22:52:12.038003: step 440, loss 1.20775, acc [0.85722656 0.78427734 0.71728516 0.64882812]\n",
      "2019-02-27T22:52:15.307141: step 480, loss 1.19036, acc [0.85917969 0.78320312 0.72382813 0.65292969]\n",
      "2019-02-27T22:52:18.194359: step 520, loss 1.11549, acc [0.86347656 0.79169922 0.73535156 0.66914063]\n",
      "2019-02-27T22:52:21.005194: step 560, loss 1.08476, acc [0.87480469 0.80166016 0.73857422 0.67197266]\n",
      "2019-02-27T22:52:23.784780: step 600, loss 1.09456, acc [0.86923828 0.80244141 0.74335938 0.67509766]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-27T22:52:33.798533: step 600, loss 0.942952, acc [0.89077876 0.82579663 0.76893352 0.71474337] \n",
      "\n",
      "2019-02-27T22:52:36.728407: step 640, loss 1.05069, acc [0.87441406 0.80537109 0.74609375 0.67988281]\n",
      "2019-02-27T22:52:39.469801: step 680, loss 1.05116, acc [0.8734375  0.80537109 0.74589844 0.68095703]\n",
      "2019-02-27T22:52:42.128860: step 720, loss 0.998073, acc [0.88417969 0.80996094 0.75917969 0.69316406]\n",
      "2019-02-27T22:52:44.790894: step 760, loss 1.01542, acc [0.87773437 0.81298828 0.75341797 0.69208984]\n",
      "2019-02-27T22:52:47.409815: step 800, loss 0.966464, acc [0.88525391 0.82255859 0.76240234 0.70185547]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-27T22:52:56.679321: step 800, loss 0.827319, acc [0.90182102 0.84393677 0.79283004 0.74040185] \n",
      "\n",
      "2019-02-27T22:52:59.352267: step 840, loss 0.981737, acc [0.88369141 0.82060547 0.76455078 0.69990234]\n",
      "2019-02-27T22:53:01.960237: step 880, loss 0.941933, acc [0.88691406 0.81953125 0.76582031 0.70410156]\n",
      "2019-02-27T22:53:04.525056: step 920, loss 0.92285, acc [0.88896484 0.82509766 0.77255859 0.71025391]\n",
      "2019-02-27T22:53:07.122112: step 960, loss 0.956147, acc [0.89414063 0.82382813 0.76796875 0.70605469]\n",
      "2019-02-27T22:53:09.668581: step 1000, loss 0.895418, acc [0.89072266 0.83115234 0.77900391 0.71201172]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-27T22:53:18.842603: step 1000, loss 0.745, acc [0.91145171 0.85665088 0.80735617 0.75842185] \n",
      "\n",
      "2019-02-27T22:53:21.518030: step 1040, loss 0.899865, acc [0.89501953 0.83105469 0.77304688 0.71640625]\n",
      "2019-02-27T22:53:24.162207: step 1080, loss 0.898832, acc [0.89462891 0.83623047 0.77822266 0.71972656]\n",
      "2019-02-27T22:53:26.761251: step 1120, loss 0.87952, acc [0.89609375 0.83193359 0.78398437 0.72138672]\n",
      "2019-02-27T22:53:29.380628: step 1160, loss 0.877186, acc [0.89638672 0.83037109 0.77929688 0.72255859]\n",
      "2019-02-27T22:53:31.925113: step 1200, loss 0.876427, acc [0.89814453 0.83515625 0.78427734 0.72666016]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-27T22:53:41.119336: step 1200, loss 0.696472, acc [0.91619698 0.86696233 0.8165764  0.76820271] \n",
      "\n",
      "2019-02-27T22:53:43.723376: step 1240, loss 0.843528, acc [0.89785156 0.83896484 0.79013672 0.73447266]\n",
      "2019-02-27T22:53:46.316429: step 1280, loss 0.831429, acc [0.90273437 0.84404297 0.78408203 0.72841797]\n",
      "2019-02-27T22:53:48.800399: step 1320, loss 0.850301, acc [0.89746094 0.83798828 0.79169922 0.73251953]\n",
      "2019-02-27T22:53:51.318097: step 1360, loss 0.836723, acc [0.89931641 0.84052734 0.78789062 0.72880859]\n",
      "2019-02-27T22:53:53.801075: step 1400, loss 0.820145, acc [0.903125   0.84316406 0.79335937 0.73535156]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-27T22:54:03.492924: step 1400, loss 0.637659, acc [0.92302456 0.87362973 0.83074212 0.78395018] \n",
      "\n",
      "2019-02-27T22:54:06.214477: step 1440, loss 0.809309, acc [0.90517578 0.84453125 0.79736328 0.74052734]\n",
      "2019-02-27T22:54:08.751520: step 1480, loss 0.800665, acc [0.9046875  0.85224609 0.79384766 0.73974609]\n",
      "2019-02-27T22:54:11.374069: step 1520, loss 0.81399, acc [0.89873047 0.84326172 0.79257813 0.734375  ]\n",
      "2019-02-27T22:54:13.900696: step 1560, loss 0.805908, acc [0.90205078 0.84970703 0.79453125 0.74169922]\n",
      "2019-02-27T22:54:16.585050: step 1600, loss 0.562252, acc [0.92695313 0.88271484 0.84208984 0.78789062]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-27T22:54:25.924737: step 1600, loss 0.579905, acc [0.92821031 0.88307021 0.84395679 0.79826608] \n",
      "\n",
      "2019-02-27T22:54:28.597188: step 1640, loss 0.592204, acc [0.92490234 0.87939453 0.83623047 0.78310547]\n",
      "2019-02-27T22:54:31.120838: step 1680, loss 0.567578, acc [0.9265625  0.8828125  0.84462891 0.79248047]\n",
      "2019-02-27T22:54:33.612248: step 1720, loss 0.563627, acc [0.92128906 0.87861328 0.84384766 0.78408203]\n",
      "2019-02-27T22:54:36.050090: step 1760, loss 0.583121, acc [0.92460937 0.87734375 0.83798828 0.78251953]\n",
      "2019-02-27T22:54:38.538524: step 1800, loss 0.55945, acc [0.92509766 0.88408203 0.84755859 0.79365234]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-27T22:54:48.809206: step 1800, loss 0.557009, acc [0.92974201 0.88767532 0.84878215 0.80336173] \n",
      "\n",
      "2019-02-27T22:54:51.352200: step 1840, loss 0.583459, acc [0.92353516 0.87880859 0.83945313 0.78642578]\n",
      "2019-02-27T22:54:53.814880: step 1880, loss 0.595716, acc [0.92138672 0.87685547 0.83408203 0.77646484]\n",
      "2019-02-27T22:54:56.278509: step 1920, loss 0.583643, acc [0.92275391 0.87617188 0.83984375 0.78076172]\n",
      "2019-02-27T22:54:58.744835: step 1960, loss 0.599116, acc [0.92246094 0.87529297 0.83662109 0.77744141]\n",
      "2019-02-27T22:55:01.208965: step 2000, loss 0.572295, acc [0.92285156 0.87851563 0.84257812 0.78417969]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-27T22:55:10.387002: step 2000, loss 0.536821, acc [0.9318043  0.89113916 0.85493898 0.80848742] \n",
      "\n",
      "2019-02-27T22:55:12.915566: step 2040, loss 0.603262, acc [0.92861328 0.88066406 0.83691406 0.78574219]\n",
      "2019-02-27T22:55:15.387136: step 2080, loss 0.608133, acc [0.92089844 0.87333984 0.83583984 0.77890625]\n",
      "2019-02-27T22:55:17.851761: step 2120, loss 0.592034, acc [0.92363281 0.87939453 0.8359375  0.78232422]\n",
      "2019-02-27T22:55:20.331269: step 2160, loss 0.574965, acc [0.92128906 0.88212891 0.84423828 0.78798828]\n",
      "2019-02-27T22:55:22.773079: step 2200, loss 0.579818, acc [0.92607422 0.88271484 0.84101563 0.78603516]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-27T22:55:31.861295: step 2200, loss 0.517905, acc [0.93505791 0.8939623  0.85661084 0.81250188] \n",
      "\n",
      "2019-02-27T22:55:34.378496: step 2240, loss 0.592403, acc [0.92099609 0.88447266 0.83935547 0.78740234]\n",
      "2019-02-27T22:55:36.858003: step 2280, loss 0.578735, acc [0.92382812 0.88134766 0.84306641 0.78828125]\n",
      "2019-02-27T22:55:39.303780: step 2320, loss 0.572749, acc [0.92695313 0.88271484 0.84287109 0.78857422]\n",
      "2019-02-27T22:55:41.718807: step 2360, loss 0.580576, acc [0.92167969 0.88046875 0.84628906 0.79111328]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-02-27T22:55:44.161610: step 2400, loss 0.589818, acc [0.92324219 0.87597656 0.84082031 0.78398437]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-27T22:55:53.320753: step 2400, loss 0.482066, acc [0.93728038 0.89844728 0.86923485 0.82304358] \n",
      "\n",
      "2019-02-27T22:55:56.314115: step 2440, loss 0.584294, acc [0.92167969 0.88320312 0.84130859 0.784375  ]\n",
      "2019-02-27T22:55:58.882405: step 2480, loss 0.599293, acc [0.92177734 0.87421875 0.83779297 0.78300781]\n",
      "2019-02-27T22:56:01.974968: step 2520, loss 0.584687, acc [0.92294922 0.88300781 0.83925781 0.78740234]\n",
      "2019-02-27T22:56:04.585417: step 2560, loss 0.596656, acc [0.92236328 0.88574219 0.83730469 0.78515625]\n",
      "2019-02-27T22:56:07.128412: step 2600, loss 0.595483, acc [0.92470703 0.8796875  0.84101563 0.78730469]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-27T22:56:16.355555: step 2600, loss 0.459585, acc [0.93941275 0.90322258 0.87347956 0.82739841] \n",
      "\n",
      "2019-02-27T22:56:18.883624: step 2640, loss 0.573913, acc [0.92548828 0.88164062 0.84423828 0.78740234]\n",
      "2019-02-27T22:56:21.329402: step 2680, loss 0.563227, acc [0.92509766 0.88535156 0.84589844 0.79453125]\n",
      "2019-02-27T22:56:23.744973: step 2720, loss 0.565736, acc [0.93183594 0.88359375 0.84541016 0.79365234]\n",
      "2019-02-27T22:56:26.164441: step 2760, loss 0.568365, acc [0.92724609 0.88710937 0.84423828 0.79228516]\n",
      "2019-02-27T22:56:28.568570: step 2800, loss 0.593703, acc [0.92216797 0.88251953 0.83925781 0.78583984]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-27T22:56:37.795177: step 2800, loss 0.442162, acc [0.94256625 0.90564527 0.87919591 0.83600797] \n",
      "\n",
      "2019-02-27T22:56:40.317784: step 2840, loss 0.571296, acc [0.92148438 0.88330078 0.84023437 0.784375  ]\n",
      "2019-02-27T22:56:42.808204: step 2880, loss 0.56625, acc [0.92373047 0.8875     0.84433594 0.78994141]\n",
      "2019-02-27T22:56:45.241084: step 2920, loss 0.546699, acc [0.92822266 0.88310547 0.84619141 0.79423828]\n",
      "2019-02-27T22:56:47.649662: step 2960, loss 0.555079, acc [0.92695313 0.88349609 0.84824219 0.79423828]\n",
      "2019-02-27T22:56:50.093952: step 3000, loss 0.580934, acc [0.9234375  0.8828125  0.84023437 0.78583984]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-27T22:56:59.112232: step 3000, loss 0.422049, acc [0.94509906 0.91084103 0.88200903 0.83951186] \n",
      "\n",
      "2019-02-27T22:57:01.634892: step 3040, loss 0.552944, acc [0.92929688 0.88876953 0.84921875 0.7984375 ]\n",
      "2019-02-27T22:57:04.034542: step 3080, loss 0.577103, acc [0.93017578 0.88212891 0.84013672 0.78789062]\n",
      "2019-02-27T22:57:06.485776: step 3120, loss 0.593742, acc [0.925      0.88076172 0.83876953 0.78896484]\n",
      "2019-02-27T22:57:09.049106: step 3160, loss 0.351965, acc [0.94765625 0.91953125 0.8984375  0.84638672]\n",
      "2019-02-27T22:57:11.460660: step 3200, loss 0.357777, acc [0.94365234 0.9140625  0.89257812 0.83886719]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-27T22:57:20.546932: step 3200, loss 0.39194, acc [0.94772197 0.91597673 0.89096898 0.84864199] \n",
      "\n",
      "2019-02-27T22:57:23.063598: step 3240, loss 0.34919, acc [0.94648438 0.91748047 0.89736328 0.84277344]\n",
      "2019-02-27T22:57:25.503423: step 3280, loss 0.358703, acc [0.94277344 0.91230469 0.89248047 0.83759766]\n",
      "2019-02-27T22:57:27.937835: step 3320, loss 0.361385, acc [0.94912109 0.91611328 0.89443359 0.84423828]\n",
      "2019-02-27T22:57:30.380099: step 3360, loss 0.368263, acc [0.94550781 0.91484375 0.88886719 0.83808594]\n",
      "2019-02-27T22:57:32.842792: step 3400, loss 0.357432, acc [0.94638672 0.91728516 0.89697266 0.84560547]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-27T22:57:41.951334: step 3400, loss 0.388617, acc [0.94771196 0.91776872 0.89266085 0.85101463] \n",
      "\n",
      "2019-02-27T22:57:44.458575: step 3440, loss 0.389057, acc [0.94169922 0.91396484 0.88984375 0.83535156]\n",
      "2019-02-27T22:57:46.903857: step 3480, loss 0.36835, acc [0.9453125  0.91455078 0.89189453 0.83916016]\n",
      "2019-02-27T22:57:49.348146: step 3520, loss 0.386352, acc [0.94052734 0.91083984 0.88759766 0.83525391]\n",
      "2019-02-27T22:57:51.804343: step 3560, loss 0.385215, acc [0.94414062 0.91123047 0.88769531 0.834375  ]\n",
      "2019-02-27T22:57:54.230279: step 3600, loss 0.372003, acc [0.93964844 0.90996094 0.88535156 0.82900391]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-27T22:58:03.402858: step 3600, loss 0.389117, acc [0.94889327 0.9187398  0.89230045 0.85105467] \n",
      "\n",
      "2019-02-27T22:58:05.925475: step 3640, loss 0.393763, acc [0.94199219 0.91171875 0.88408203 0.83095703]\n",
      "2019-02-27T22:58:08.367336: step 3680, loss 0.378935, acc [0.94033203 0.90869141 0.88994141 0.83242187]\n",
      "2019-02-27T22:58:10.793222: step 3720, loss 0.400013, acc [0.94169922 0.91162109 0.88105469 0.82822266]\n",
      "2019-02-27T22:58:13.224617: step 3760, loss 0.39513, acc [0.94199219 0.90976563 0.88095703 0.82685547]\n",
      "2019-02-27T22:58:15.646089: step 3800, loss 0.400738, acc [0.94384766 0.90878906 0.88349609 0.83046875]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-27T22:58:24.773985: step 3800, loss 0.37727, acc [0.94812242 0.92127261 0.89616474 0.85410806] \n",
      "\n",
      "2019-02-27T22:58:27.277300: step 3840, loss 0.409554, acc [0.94082031 0.90771484 0.88144531 0.82734375]\n",
      "2019-02-27T22:58:29.706215: step 3880, loss 0.389652, acc [0.94111328 0.91318359 0.88564453 0.83105469]\n",
      "2019-02-27T22:58:32.228378: step 3920, loss 0.392838, acc [0.94257813 0.9125     0.88466797 0.83291016]\n",
      "2019-02-27T22:58:34.637451: step 3960, loss 0.40462, acc [0.940625   0.90576172 0.88378906 0.83125   ]\n",
      "2019-02-27T22:58:37.095132: step 4000, loss 0.397238, acc [0.94111328 0.91025391 0.88037109 0.83017578]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-27T22:58:46.260264: step 4000, loss 0.363645, acc [0.94976424 0.92206349 0.90028932 0.85799237] \n",
      "\n",
      "2019-02-27T22:58:48.762054: step 4040, loss 0.402483, acc [0.94023437 0.90927734 0.88378906 0.82646484]\n",
      "2019-02-27T22:58:51.260410: step 4080, loss 0.407548, acc [0.93857422 0.90791016 0.88134766 0.82587891]\n",
      "2019-02-27T22:58:53.858458: step 4120, loss 0.410011, acc [0.93720703 0.90898437 0.88349609 0.82763672]\n",
      "2019-02-27T22:58:56.322588: step 4160, loss 0.413092, acc [0.93886719 0.91269531 0.88359375 0.828125  ]\n",
      "2019-02-27T22:58:58.770848: step 4200, loss 0.40127, acc [0.94130859 0.90927734 0.88388672 0.82988281]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-27T22:59:08.026214: step 4200, loss 0.348522, acc [0.95216691 0.92589775 0.90363303 0.86405911] \n",
      "\n",
      "2019-02-27T22:59:10.515641: step 4240, loss 0.403049, acc [0.9375     0.90693359 0.88320312 0.82773438]\n",
      "2019-02-27T22:59:12.973322: step 4280, loss 0.408463, acc [0.94130859 0.91035156 0.87539062 0.82646484]\n",
      "2019-02-27T22:59:15.412207: step 4320, loss 0.411423, acc [0.93603516 0.90859375 0.87568359 0.82226562]\n",
      "2019-02-27T22:59:17.916960: step 4360, loss 0.418702, acc [0.93769531 0.91132813 0.87646484 0.82705078]\n",
      "2019-02-27T22:59:20.371847: step 4400, loss 0.394384, acc [0.94404297 0.90966797 0.88447266 0.83251953]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-27T22:59:29.552363: step 4400, loss 0.343681, acc [0.95347836 0.92622811 0.90496451 0.86422929] \n",
      "\n",
      "2019-02-27T22:59:32.037875: step 4440, loss 0.424814, acc [0.93613281 0.9046875  0.87958984 0.82597656]\n",
      "2019-02-27T22:59:34.481668: step 4480, loss 0.405799, acc [0.94072266 0.91337891 0.88193359 0.83066406]\n",
      "2019-02-27T22:59:36.938056: step 4520, loss 0.432137, acc [0.93779297 0.90673828 0.875      0.82216797]\n",
      "2019-02-27T22:59:39.416124: step 4560, loss 0.419681, acc [0.94042969 0.90478516 0.88076172 0.82431641]\n",
      "2019-02-27T22:59:41.849442: step 4600, loss 0.439496, acc [0.93964844 0.90693359 0.871875   0.82324219]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-27T22:59:51.175243: step 4600, loss 0.329925, acc [0.95312797 0.92835047 0.90908909 0.86807356] \n",
      "\n",
      "2019-02-27T22:59:53.711788: step 4640, loss 0.409667, acc [0.94414062 0.909375   0.87958984 0.82890625]\n",
      "2019-02-27T22:59:56.156009: step 4680, loss 0.430787, acc [0.93876953 0.90390625 0.87167969 0.81826172]\n",
      "2019-02-27T22:59:58.817814: step 4720, loss 0.260611, acc [0.95566406 0.93574219 0.92402344 0.8765625 ]\n",
      "2019-02-27T23:00:01.309720: step 4760, loss 0.235343, acc [0.95986328 0.93818359 0.928125   0.88144531]\n",
      "2019-02-27T23:00:03.765419: step 4800, loss 0.241107, acc [0.95703125 0.94042969 0.92822266 0.87919922]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-27T23:00:12.966722: step 4800, loss 0.318357, acc [0.95738269 0.93255514 0.91348397 0.8743205 ] \n",
      "\n",
      "2019-02-27T23:00:15.499797: step 4840, loss 0.240992, acc [0.95976562 0.93691406 0.92490234 0.87558594]\n",
      "2019-02-27T23:00:17.962439: step 4880, loss 0.268498, acc [0.95419922 0.93349609 0.92011719 0.86894531]\n",
      "2019-02-27T23:00:20.418633: step 4920, loss 0.246934, acc [0.95546875 0.93554688 0.92441406 0.87373047]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-02-27T23:00:22.843580: step 4960, loss 0.260325, acc [0.95380859 0.93466797 0.91904297 0.87021484]\n",
      "2019-02-27T23:00:25.288365: step 5000, loss 0.256194, acc [0.95771484 0.93457031 0.92158203 0.87324219]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-27T23:00:34.456933: step 5000, loss 0.315307, acc [0.95704232 0.93461743 0.91620699 0.87788445] \n",
      "\n",
      "2019-02-27T23:00:36.971160: step 5040, loss 0.270538, acc [0.95126953 0.93544922 0.91826172 0.86728516]\n",
      "2019-02-27T23:00:39.415450: step 5080, loss 0.263339, acc [0.9546875  0.93232422 0.91953125 0.86630859]\n",
      "2019-02-27T23:00:41.873628: step 5120, loss 0.276506, acc [0.95332031 0.93076172 0.91640625 0.86308594]\n",
      "2019-02-27T23:00:44.304029: step 5160, loss 0.265883, acc [0.95439453 0.93095703 0.91767578 0.86376953]\n",
      "2019-02-27T23:00:46.756802: step 5200, loss 0.284855, acc [0.95195312 0.92949219 0.91503906 0.86396484]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-27T23:00:55.930280: step 5200, loss 0.322517, acc [0.95660183 0.93350619 0.91327373 0.87489113] \n",
      "\n",
      "2019-02-27T23:00:58.457423: step 5240, loss 0.269959, acc [0.95351562 0.93359375 0.91582031 0.86435547]\n",
      "2019-02-27T23:01:00.894228: step 5280, loss 0.287404, acc [0.95009766 0.92958984 0.91455078 0.86074219]\n",
      "2019-02-27T23:01:03.364311: step 5320, loss 0.293516, acc [0.95214844 0.93095703 0.91142578 0.86103516]\n",
      "2019-02-27T23:01:05.775827: step 5360, loss 0.287221, acc [0.95507812 0.93125    0.91074219 0.85986328]\n",
      "2019-02-27T23:01:08.210236: step 5400, loss 0.288392, acc [0.95351562 0.92988281 0.91064453 0.86025391]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-27T23:01:17.393645: step 5400, loss 0.312115, acc [0.95776312 0.93454735 0.91769865 0.87951626] \n",
      "\n",
      "2019-02-27T23:01:19.912336: step 5440, loss 0.293391, acc [0.9484375  0.92783203 0.90966797 0.85722656]\n",
      "2019-02-27T23:01:22.382912: step 5480, loss 0.301858, acc [0.95126953 0.92929688 0.91142578 0.85888672]\n",
      "2019-02-27T23:01:24.841129: step 5520, loss 0.284436, acc [0.95361328 0.93193359 0.91064453 0.8625    ]\n",
      "2019-02-27T23:01:27.260143: step 5560, loss 0.303919, acc [0.95546875 0.92822266 0.91113281 0.859375  ]\n",
      "2019-02-27T23:01:29.712807: step 5600, loss 0.292352, acc [0.95087891 0.92919922 0.91396484 0.86005859]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-27T23:01:38.926015: step 5600, loss 0.305605, acc [0.95857402 0.9362192  0.91854959 0.88187889] \n",
      "\n",
      "2019-02-27T23:01:41.478434: step 5640, loss 0.298344, acc [0.94990234 0.92773438 0.90986328 0.85615234]\n",
      "2019-02-27T23:01:43.928180: step 5680, loss 0.29449, acc [0.94960937 0.93046875 0.91044922 0.85859375]\n",
      "2019-02-27T23:01:46.430999: step 5720, loss 0.304489, acc [0.95244141 0.92392578 0.90566406 0.85351562]\n",
      "2019-02-27T23:01:48.950184: step 5760, loss 0.312028, acc [0.95048828 0.92744141 0.90664062 0.85380859]\n",
      "2019-02-27T23:01:51.425722: step 5800, loss 0.311327, acc [0.94902344 0.92910156 0.90556641 0.85556641]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-27T23:02:00.678610: step 5800, loss 0.300928, acc [0.95810349 0.93422699 0.92062189 0.88067755] \n",
      "\n",
      "2019-02-27T23:02:03.263267: step 5840, loss 0.310866, acc [0.95078125 0.92275391 0.90742188 0.85361328]\n",
      "2019-02-27T23:02:05.757607: step 5880, loss 0.321962, acc [0.94697266 0.92431641 0.90429688 0.85126953]\n",
      "2019-02-27T23:02:08.265442: step 5920, loss 0.303983, acc [0.95488281 0.925      0.90566406 0.85634766]\n",
      "2019-02-27T23:02:10.718603: step 5960, loss 0.304893, acc [0.95087891 0.92539063 0.90732422 0.85478516]\n",
      "2019-02-27T23:02:13.202077: step 6000, loss 0.311452, acc [0.95       0.92636719 0.90429688 0.85527344]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-27T23:02:22.363700: step 6000, loss 0.289873, acc [0.96018581 0.93951286 0.9236052  0.88668422] \n",
      "\n",
      "2019-02-27T23:02:24.899752: step 6040, loss 0.30252, acc [0.95166016 0.92539063 0.90859375 0.85644531]\n",
      "2019-02-27T23:02:27.364873: step 6080, loss 0.330019, acc [0.94785156 0.92285156 0.89921875 0.84902344]\n",
      "2019-02-27T23:02:29.828506: step 6120, loss 0.302922, acc [0.95009766 0.92451172 0.90830078 0.85625   ]\n",
      "2019-02-27T23:02:32.304542: step 6160, loss 0.313925, acc [0.94589844 0.92099609 0.90458984 0.84804687]\n",
      "2019-02-27T23:02:34.735440: step 6200, loss 0.286416, acc [0.95253906 0.93144531 0.91494141 0.86601562]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-27T23:02:43.986344: step 6200, loss 0.284247, acc [0.96086656 0.94130485 0.92491666 0.88912693] \n",
      "\n",
      "2019-02-27T23:02:46.535786: step 6240, loss 0.311476, acc [0.94814453 0.92353516 0.90751953 0.85224609]\n",
      "2019-02-27T23:02:49.130882: step 6280, loss 0.192649, acc [0.965625   0.94736328 0.94101563 0.89443359]\n",
      "2019-02-27T23:02:51.600998: step 6320, loss 0.176023, acc [0.96689453 0.95039063 0.94628906 0.90068359]\n",
      "2019-02-27T23:02:54.097808: step 6360, loss 0.182745, acc [0.9640625  0.95107422 0.94101563 0.8953125 ]\n",
      "2019-02-27T23:02:56.581282: step 6400, loss 0.192288, acc [0.96611328 0.94609375 0.93808594 0.89121094]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-27T23:03:05.754393: step 6400, loss 0.285086, acc [0.96223808 0.94258627 0.92775981 0.89269089] \n",
      "\n",
      "2019-02-27T23:03:08.265496: step 6440, loss 0.198886, acc [0.95859375 0.94482422 0.93964844 0.88984375]\n",
      "2019-02-27T23:03:10.728632: step 6480, loss 0.184323, acc [0.9625     0.95263672 0.94443359 0.89736328]\n",
      "2019-02-27T23:03:13.188794: step 6520, loss 0.193971, acc [0.96425781 0.94492188 0.94169922 0.89179688]\n",
      "2019-02-27T23:03:15.634628: step 6560, loss 0.18694, acc [0.96123047 0.94892578 0.94199219 0.89511719]\n",
      "2019-02-27T23:03:18.121022: step 6600, loss 0.197161, acc [0.96259766 0.94765625 0.93818359 0.89072266]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-27T23:03:27.266774: step 6600, loss 0.284005, acc [0.9623382  0.9427965  0.92872088 0.89336163] \n",
      "\n",
      "2019-02-27T23:03:29.804314: step 6640, loss 0.191619, acc [0.96123047 0.9453125  0.94150391 0.89140625]\n",
      "2019-02-27T23:03:32.249101: step 6680, loss 0.204336, acc [0.95966797 0.94628906 0.93994141 0.890625  ]\n",
      "2019-02-27T23:03:34.704301: step 6720, loss 0.206697, acc [0.96074219 0.94150391 0.93798828 0.88671875]\n",
      "2019-02-27T23:03:37.156087: step 6760, loss 0.203103, acc [0.96464844 0.94453125 0.93515625 0.88964844]\n",
      "2019-02-27T23:03:39.627107: step 6800, loss 0.210419, acc [0.96132812 0.94404297 0.93525391 0.88632813]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-27T23:03:48.690072: step 6800, loss 0.291514, acc [0.9623382  0.94163522 0.92654847 0.89166975] \n",
      "\n",
      "2019-02-27T23:03:51.232030: step 6840, loss 0.214756, acc [0.96083984 0.94375    0.93642578 0.88896484]\n",
      "2019-02-27T23:03:53.678566: step 6880, loss 0.209871, acc [0.95820313 0.94228516 0.93457031 0.88115234]\n",
      "2019-02-27T23:03:56.147704: step 6920, loss 0.239803, acc [0.95556641 0.93955078 0.92753906 0.87451172]\n",
      "2019-02-27T23:03:58.589963: step 6960, loss 0.225574, acc [0.96015625 0.93837891 0.93046875 0.87714844]\n",
      "2019-02-27T23:04:01.096575: step 7000, loss 0.218991, acc [0.96289062 0.94179687 0.93125    0.88417969]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-27T23:04:10.256709: step 7000, loss 0.284557, acc [0.96258847 0.94403788 0.92806015 0.89383215] \n",
      "\n",
      "2019-02-27T23:04:12.789784: step 7040, loss 0.232827, acc [0.95986328 0.93955078 0.92988281 0.88095703]\n",
      "2019-02-27T23:04:15.254411: step 7080, loss 0.235453, acc [0.95771484 0.93916016 0.92792969 0.87724609]\n",
      "2019-02-27T23:04:17.698719: step 7120, loss 0.218985, acc [0.95947266 0.94335938 0.93105469 0.88056641]\n",
      "2019-02-27T23:04:20.187630: step 7160, loss 0.234939, acc [0.95527344 0.93886719 0.92822266 0.87744141]\n",
      "2019-02-27T23:04:22.657216: step 7200, loss 0.219851, acc [0.95927734 0.94287109 0.93154297 0.88359375]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-27T23:04:32.036088: step 7200, loss 0.277013, acc [0.96275866 0.94210574 0.93130375 0.89498343] \n",
      "\n",
      "2019-02-27T23:04:34.590490: step 7240, loss 0.229325, acc [0.95400391 0.93935547 0.92832031 0.87753906]\n",
      "2019-02-27T23:04:37.104220: step 7280, loss 0.23911, acc [0.95625    0.93759766 0.92304688 0.87177734]\n",
      "2019-02-27T23:04:39.597615: step 7320, loss 0.226551, acc [0.95673828 0.94189453 0.92753906 0.88085938]\n",
      "2019-02-27T23:04:42.094482: step 7360, loss 0.233821, acc [0.95996094 0.94248047 0.92744141 0.87861328]\n",
      "2019-02-27T23:04:44.573988: step 7400, loss 0.242492, acc [0.95722656 0.94121094 0.92246094 0.87421875]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-27T23:04:53.913675: step 7400, loss 0.271657, acc [0.96293886 0.94489884 0.93250508 0.89790668] \n",
      "\n",
      "2019-02-27T23:04:56.449726: step 7440, loss 0.231798, acc [0.95507812 0.93789062 0.92978516 0.87587891]\n",
      "2019-02-27T23:04:58.916159: step 7480, loss 0.253069, acc [0.9578125  0.93740234 0.92167969 0.87509766]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-02-27T23:05:01.388225: step 7520, loss 0.249936, acc [0.95742187 0.93847656 0.921875   0.87304688]\n",
      "2019-02-27T23:05:03.897492: step 7560, loss 0.245883, acc [0.95849609 0.93525391 0.92148438 0.87158203]\n",
      "2019-02-27T23:05:06.362172: step 7600, loss 0.251668, acc [0.95810547 0.93808594 0.92246094 0.87451172]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-27T23:05:15.555485: step 7600, loss 0.269286, acc [0.96394998 0.94322698 0.93228484 0.89640501] \n",
      "\n",
      "2019-02-27T23:05:18.106458: step 7640, loss 0.243339, acc [0.95634766 0.94150391 0.92070312 0.87177734]\n",
      "2019-02-27T23:05:20.582453: step 7680, loss 0.254674, acc [0.95751953 0.93671875 0.92246094 0.87275391]\n",
      "2019-02-27T23:05:23.071381: step 7720, loss 0.26102, acc [0.95439453 0.93632812 0.91914063 0.86826172]\n",
      "2019-02-27T23:05:25.533526: step 7760, loss 0.254933, acc [0.95986328 0.93535156 0.9203125  0.87138672]\n",
      "2019-02-27T23:05:27.992698: step 7800, loss 0.257805, acc [0.96064453 0.93515625 0.92041016 0.87402344]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-27T23:05:37.208934: step 7800, loss 0.263158, acc [0.96427034 0.94547948 0.93431709 0.89979878] \n",
      "\n",
      "2019-02-27T23:05:39.931427: step 7840, loss 0.152118, acc [0.96806641 0.95820313 0.95234375 0.91083984]\n",
      "2019-02-27T23:05:42.386684: step 7880, loss 0.136977, acc [0.96972656 0.95820313 0.95527344 0.9125    ]\n",
      "2019-02-27T23:05:44.849766: step 7920, loss 0.158962, acc [0.96933594 0.95488281 0.95224609 0.91005859]\n",
      "2019-02-27T23:05:47.290091: step 7960, loss 0.149708, acc [0.96933594 0.95791016 0.95429688 0.91191406]\n",
      "2019-02-27T23:05:49.759676: step 8000, loss 0.15473, acc [0.96923828 0.95800781 0.94902344 0.90615234]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-27T23:05:58.957010: step 8000, loss 0.274641, acc [0.96556177 0.94758182 0.93711019 0.90517474] \n",
      "\n",
      "2019-02-27T23:06:01.558533: step 8040, loss 0.144899, acc [0.97089844 0.96142578 0.95380859 0.91396484]\n",
      "2019-02-27T23:06:04.066358: step 8080, loss 0.148532, acc [0.96826172 0.95488281 0.9546875  0.9109375 ]\n",
      "2019-02-27T23:06:06.559705: step 8120, loss 0.157334, acc [0.96572266 0.95322266 0.94921875 0.90126953]\n",
      "2019-02-27T23:06:09.007029: step 8160, loss 0.159121, acc [0.96816406 0.95537109 0.9484375  0.90507812]\n",
      "2019-02-27T23:06:11.501359: step 8200, loss 0.170054, acc [0.96650391 0.95527344 0.94775391 0.90595703]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-27T23:06:20.719577: step 8200, loss 0.277975, acc [0.96516133 0.94831263 0.935138   0.90326262] \n",
      "\n",
      "2019-02-27T23:06:23.294263: step 8240, loss 0.169559, acc [0.96894531 0.95341797 0.94414062 0.90224609]\n",
      "2019-02-27T23:06:25.763355: step 8280, loss 0.157683, acc [0.96640625 0.95507812 0.94941406 0.90136719]\n",
      "2019-02-27T23:06:28.270637: step 8320, loss 0.16891, acc [0.96962891 0.95302734 0.94746094 0.90419922]\n",
      "2019-02-27T23:06:30.714926: step 8360, loss 0.172562, acc [0.96611328 0.95195312 0.9453125  0.90244141]\n",
      "2019-02-27T23:06:33.195920: step 8400, loss 0.172674, acc [0.96435547 0.95253906 0.94492188 0.89824219]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-27T23:06:42.380855: step 8400, loss 0.278359, acc [0.96532151 0.94645056 0.93494779 0.90266195] \n",
      "\n",
      "2019-02-27T23:06:44.894091: step 8440, loss 0.178991, acc [0.96279297 0.953125   0.94238281 0.89628906]\n",
      "2019-02-27T23:06:47.349789: step 8480, loss 0.182966, acc [0.96650391 0.94892578 0.94130859 0.89755859]\n",
      "2019-02-27T23:06:49.822631: step 8520, loss 0.183097, acc [0.96347656 0.95029297 0.94296875 0.89589844]\n",
      "2019-02-27T23:06:52.320983: step 8560, loss 0.172464, acc [0.96523437 0.94970703 0.94628906 0.89775391]\n",
      "2019-02-27T23:06:54.787097: step 8600, loss 0.182372, acc [0.96552734 0.94941406 0.94335938 0.89667969]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-27T23:07:04.084129: step 8600, loss 0.279344, acc [0.96445054 0.94813243 0.9341469  0.90200122] \n",
      "\n",
      "2019-02-27T23:07:06.624147: step 8640, loss 0.180202, acc [0.96601563 0.95       0.94355469 0.89628906]\n",
      "2019-02-27T23:07:09.076868: step 8680, loss 0.180808, acc [0.96298828 0.95097656 0.94072266 0.89443359]\n",
      "2019-02-27T23:07:11.563815: step 8720, loss 0.182256, acc [0.96357422 0.94931641 0.94287109 0.89609375]\n",
      "2019-02-27T23:07:14.027945: step 8760, loss 0.186861, acc [0.96386719 0.94902344 0.94111328 0.89326172]\n",
      "2019-02-27T23:07:16.523642: step 8800, loss 0.193793, acc [0.96425781 0.94697266 0.93857422 0.89130859]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-27T23:07:25.611363: step 8800, loss 0.27484, acc [0.96515132 0.94641052 0.93489774 0.90199121] \n",
      "\n",
      "2019-02-27T23:07:28.150883: step 8840, loss 0.199826, acc [0.96503906 0.94697266 0.93623047 0.89101562]\n",
      "2019-02-27T23:07:30.618982: step 8880, loss 0.197644, acc [0.96357422 0.94316406 0.93789062 0.89121094]\n",
      "2019-02-27T23:07:33.075671: step 8920, loss 0.20495, acc [0.96376953 0.94482422 0.93574219 0.88789063]\n",
      "2019-02-27T23:07:35.567578: step 8960, loss 0.204326, acc [0.96630859 0.94765625 0.93632812 0.89257812]\n",
      "2019-02-27T23:07:38.044166: step 9000, loss 0.221255, acc [0.95976562 0.94541016 0.93291016 0.88613281]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-27T23:07:47.272746: step 9000, loss 0.263241, acc [0.96519136 0.94915356 0.93696003 0.90482436] \n",
      "\n",
      "2019-02-27T23:07:49.815231: step 9040, loss 0.204451, acc [0.95869141 0.94775391 0.93779297 0.88789063]\n",
      "2019-02-27T23:07:52.254024: step 9080, loss 0.204697, acc [0.96103516 0.94453125 0.93691406 0.89013672]\n",
      "2019-02-27T23:07:54.733034: step 9120, loss 0.191048, acc [0.95976562 0.9484375  0.93876953 0.88886719]\n",
      "2019-02-27T23:07:57.225435: step 9160, loss 0.201337, acc [0.9640625  0.94453125 0.93867188 0.88955078]\n",
      "2019-02-27T23:07:59.711887: step 9200, loss 0.212581, acc [0.96152344 0.946875   0.93378906 0.88847656]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-27T23:08:08.910287: step 9200, loss 0.260331, acc [0.96623252 0.94947392 0.9379311  0.90676651] \n",
      "\n",
      "2019-02-27T23:08:11.453777: step 9240, loss 0.213304, acc [0.96220703 0.94228516 0.93447266 0.88632813]\n",
      "2019-02-27T23:08:13.962051: step 9280, loss 0.205215, acc [0.96230469 0.94472656 0.93349609 0.88603516]\n",
      "2019-02-27T23:08:16.410356: step 9320, loss 0.200512, acc [0.96171875 0.94785156 0.93701172 0.89130859]\n",
      "2019-02-27T23:08:18.897256: step 9360, loss 0.21878, acc [0.96201172 0.94179687 0.93007812 0.88222656]\n",
      "2019-02-27T23:08:21.529529: step 9400, loss 0.128427, acc [0.96826172 0.95888672 0.96054688 0.91328125]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-27T23:08:30.706084: step 9400, loss 0.260579, acc [0.96706344 0.95173643 0.94058405 0.91005016] \n",
      "\n",
      "2019-02-27T23:08:33.247540: step 9440, loss 0.120398, acc [0.97304687 0.96191406 0.96103516 0.92070312]\n",
      "2019-02-27T23:08:35.704725: step 9480, loss 0.112567, acc [0.97490234 0.96640625 0.96367187 0.92714844]\n",
      "2019-02-27T23:08:38.151000: step 9520, loss 0.117089, acc [0.97363281 0.96132812 0.96630859 0.92509766]\n",
      "2019-02-27T23:08:40.631994: step 9560, loss 0.127269, acc [0.97099609 0.96220703 0.95898438 0.91855469]\n",
      "2019-02-27T23:08:43.104556: step 9600, loss 0.132914, acc [0.9703125  0.95898438 0.9578125  0.91455078]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-27T23:08:52.456148: step 9600, loss 0.271914, acc [0.96769414 0.95219694 0.9409945  0.91137162] \n",
      "\n",
      "2019-02-27T23:08:55.004103: step 9640, loss 0.121656, acc [0.97265625 0.96347656 0.96113281 0.92050781]\n",
      "2019-02-27T23:08:57.488072: step 9680, loss 0.133615, acc [0.97041016 0.95878906 0.95810547 0.91669922]\n",
      "2019-02-27T23:08:59.964604: step 9720, loss 0.146261, acc [0.96650391 0.9546875  0.9515625  0.90664062]\n",
      "2019-02-27T23:09:02.468930: step 9760, loss 0.140395, acc [0.97089844 0.96181641 0.95595703 0.91416016]\n",
      "2019-02-27T23:09:04.977679: step 9800, loss 0.142788, acc [0.96787109 0.95751953 0.95605469 0.91152344]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-27T23:09:14.189942: step 9800, loss 0.272574, acc [0.96695332 0.95208682 0.93890218 0.90835828] \n",
      "\n",
      "2019-02-27T23:09:16.733880: step 9840, loss 0.14759, acc [0.96445313 0.95292969 0.95439453 0.90400391]\n",
      "2019-02-27T23:09:19.229258: step 9880, loss 0.138707, acc [0.96835938 0.96083984 0.95771484 0.91376953]\n",
      "2019-02-27T23:09:21.725129: step 9920, loss 0.155689, acc [0.97001953 0.95810547 0.94873047 0.90751953]\n",
      "2019-02-27T23:09:24.213564: step 9960, loss 0.13954, acc [0.96875    0.95830078 0.95361328 0.91181641]\n",
      "2019-02-27T23:09:26.701005: step 10000, loss 0.150753, acc [0.97099609 0.95761719 0.95351562 0.91191406]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-27T23:09:35.934549: step 10000, loss 0.272839, acc [0.96803452 0.95157625 0.93990329 0.90966973] \n",
      "\n",
      "2019-02-27T23:09:38.466075: step 10040, loss 0.146541, acc [0.97197266 0.95859375 0.953125   0.91210938]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-02-27T23:09:40.935662: step 10080, loss 0.147891, acc [0.9671875  0.95605469 0.95419922 0.90771484]\n",
      "2019-02-27T23:09:43.406239: step 10120, loss 0.156844, acc [0.97050781 0.95585937 0.95029297 0.90947266]\n",
      "2019-02-27T23:09:45.883822: step 10160, loss 0.154063, acc [0.96621094 0.95878906 0.95195312 0.90830078]\n",
      "2019-02-27T23:09:48.379635: step 10200, loss 0.153644, acc [0.96826172 0.95556641 0.95078125 0.90712891]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-27T23:09:57.639963: step 10200, loss 0.269188, acc [0.96758402 0.95177647 0.93995335 0.91004014] \n",
      "\n",
      "2019-02-27T23:10:00.177556: step 10240, loss 0.163575, acc [0.96630859 0.95458984 0.94951172 0.90654297]\n",
      "2019-02-27T23:10:02.653536: step 10280, loss 0.168967, acc [0.96787109 0.95439453 0.94648438 0.90380859]\n",
      "2019-02-27T23:10:05.116147: step 10320, loss 0.169789, acc [0.96484375 0.95488281 0.94628906 0.90068359]\n",
      "2019-02-27T23:10:07.574819: step 10360, loss 0.167355, acc [0.96708984 0.95166016 0.94755859 0.90429688]\n",
      "2019-02-27T23:10:10.068414: step 10400, loss 0.164069, acc [0.96640625 0.95488281 0.94921875 0.90615234]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-27T23:10:19.363391: step 10400, loss 0.270604, acc [0.96823474 0.95176646 0.93915246 0.91005016] \n",
      "\n",
      "2019-02-27T23:10:21.910354: step 10440, loss 0.167559, acc [0.96669922 0.95224609 0.94658203 0.90097656]\n",
      "2019-02-27T23:10:24.375207: step 10480, loss 0.169701, acc [0.96660156 0.95253906 0.94501953 0.89931641]\n",
      "2019-02-27T23:10:26.842313: step 10520, loss 0.157666, acc [0.96669922 0.95644531 0.95097656 0.90488281]\n",
      "2019-02-27T23:10:29.310413: step 10560, loss 0.174392, acc [0.96767578 0.94941406 0.94267578 0.8984375 ]\n",
      "2019-02-27T23:10:31.779500: step 10600, loss 0.169021, acc [0.96621094 0.95195312 0.94472656 0.89990234]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-27T23:10:41.049253: step 10600, loss 0.267078, acc [0.96804453 0.95061518 0.93993333 0.90964971] \n",
      "\n",
      "2019-02-27T23:10:43.601673: step 10640, loss 0.182028, acc [0.965625   0.94873047 0.94511719 0.89824219]\n",
      "2019-02-27T23:10:46.062330: step 10680, loss 0.176988, acc [0.96689453 0.95400391 0.94492188 0.90351563]\n",
      "2019-02-27T23:10:48.537411: step 10720, loss 0.177042, acc [0.96777344 0.94931641 0.94296875 0.89853516]\n",
      "2019-02-27T23:10:50.982654: step 10760, loss 0.16989, acc [0.96757812 0.95205078 0.94931641 0.90429688]\n",
      "2019-02-27T23:10:53.480510: step 10800, loss 0.173673, acc [0.96826172 0.95292969 0.94472656 0.90166016]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-27T23:11:02.606051: step 10800, loss 0.260149, acc [0.96883541 0.95315801 0.94240607 0.91371422] \n",
      "\n",
      "2019-02-27T23:11:05.152519: step 10840, loss 0.185567, acc [0.96386719 0.95009766 0.93994141 0.89785156]\n",
      "2019-02-27T23:11:07.593838: step 10880, loss 0.168631, acc [0.96835938 0.95283203 0.946875   0.90332031]\n",
      "2019-02-27T23:11:10.039612: step 10920, loss 0.182009, acc [0.96591797 0.94980469 0.94140625 0.89794922]\n",
      "2019-02-27T23:11:12.630717: step 10960, loss 0.0994756, acc [0.97421875 0.96630859 0.96738281 0.92851562]\n",
      "2019-02-27T23:11:15.076990: step 11000, loss 0.109885, acc [0.97597656 0.96494141 0.96582031 0.92832031]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-27T23:11:24.233655: step 11000, loss 0.262282, acc [0.96969636 0.95512018 0.94502898 0.91620699] \n",
      "\n",
      "2019-02-27T23:11:26.765242: step 11040, loss 0.119572, acc [0.97421875 0.96318359 0.9609375  0.92167969]\n",
      "2019-02-27T23:11:29.218459: step 11080, loss 0.110652, acc [0.97529297 0.96513672 0.96533203 0.92705078]\n",
      "2019-02-27T23:11:31.673663: step 11120, loss 0.104771, acc [0.97441406 0.96835938 0.96484375 0.92910156]\n",
      "2019-02-27T23:11:34.155648: step 11160, loss 0.110811, acc [0.97324219 0.96464844 0.965625   0.92558594]\n",
      "2019-02-27T23:11:36.594984: step 11200, loss 0.113341, acc [0.97412109 0.96708984 0.96455078 0.92734375]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-27T23:11:45.824103: step 11200, loss 0.27222, acc [0.96937601 0.95439938 0.94411797 0.915316  ] \n",
      "\n",
      "2019-02-27T23:11:48.336795: step 11240, loss 0.118371, acc [0.97294922 0.96386719 0.9609375  0.92128906]\n",
      "2019-02-27T23:11:50.800469: step 11280, loss 0.113659, acc [0.97294922 0.96171875 0.96464844 0.9234375 ]\n",
      "2019-02-27T23:11:53.246704: step 11320, loss 0.129496, acc [0.96650391 0.96210938 0.96103516 0.91806641]\n",
      "2019-02-27T23:11:55.683555: step 11360, loss 0.130691, acc [0.97480469 0.96181641 0.95791016 0.91953125]\n",
      "2019-02-27T23:11:58.131317: step 11400, loss 0.138426, acc [0.96943359 0.95917969 0.95615234 0.91328125]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-27T23:12:07.351020: step 11400, loss 0.268995, acc [0.96878535 0.95359849 0.94237604 0.91350399] \n",
      "\n",
      "2019-02-27T23:12:09.889005: step 11440, loss 0.118831, acc [0.97011719 0.96582031 0.96074219 0.91962891]\n",
      "2019-02-27T23:12:12.345717: step 11480, loss 0.139368, acc [0.97265625 0.95917969 0.95449219 0.91767578]\n",
      "2019-02-27T23:12:14.811809: step 11520, loss 0.123612, acc [0.97392578 0.96289062 0.96083984 0.92275391]\n",
      "2019-02-27T23:12:17.256658: step 11560, loss 0.131985, acc [0.97285156 0.96064453 0.959375   0.91865234]\n",
      "2019-02-27T23:12:19.716262: step 11600, loss 0.12951, acc [0.97236328 0.96113281 0.95761719 0.91767578]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-27T23:12:28.944898: step 11600, loss 0.27351, acc [0.9694561  0.95323809 0.9411747  0.91223258] \n",
      "\n",
      "2019-02-27T23:12:31.491807: step 11640, loss 0.135591, acc [0.9703125  0.96269531 0.953125   0.91279297]\n",
      "2019-02-27T23:12:33.964866: step 11680, loss 0.131986, acc [0.97353516 0.96396484 0.95849609 0.92138672]\n",
      "2019-02-27T23:12:36.441396: step 11720, loss 0.135313, acc [0.96943359 0.96162109 0.95683594 0.91484375]\n",
      "2019-02-27T23:12:38.907015: step 11760, loss 0.136754, acc [0.96923828 0.95859375 0.95605469 0.91152344]\n",
      "2019-02-27T23:12:41.375608: step 11800, loss 0.140685, acc [0.97148437 0.96005859 0.95126953 0.91123047]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-27T23:12:50.587377: step 11800, loss 0.269916, acc [0.97009681 0.95546056 0.94329706 0.91671756] \n",
      "\n",
      "2019-02-27T23:12:53.119410: step 11840, loss 0.143989, acc [0.96699219 0.95566406 0.95429688 0.91054687]\n",
      "2019-02-27T23:12:55.573124: step 11880, loss 0.151432, acc [0.971875   0.96005859 0.95136719 0.91240234]\n",
      "2019-02-27T23:12:58.017910: step 11920, loss 0.147775, acc [0.96845703 0.95673828 0.95039063 0.90556641]\n",
      "2019-02-27T23:13:00.498906: step 11960, loss 0.150792, acc [0.96630859 0.95332031 0.94970703 0.90292969]\n",
      "2019-02-27T23:13:02.932780: step 12000, loss 0.1443, acc [0.97080078 0.96162109 0.95175781 0.91337891]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-27T23:13:12.208038: step 12000, loss 0.26505, acc [0.96956622 0.95375867 0.94310685 0.9148655 ] \n",
      "\n",
      "2019-02-27T23:13:14.741559: step 12040, loss 0.158138, acc [0.96816406 0.95791016 0.95       0.90917969]\n",
      "2019-02-27T23:13:17.175982: step 12080, loss 0.145929, acc [0.9671875  0.95693359 0.95332031 0.90966797]\n",
      "2019-02-27T23:13:19.649976: step 12120, loss 0.158466, acc [0.97011719 0.95654297 0.95087891 0.90898437]\n",
      "2019-02-27T23:13:22.095260: step 12160, loss 0.154105, acc [0.97050781 0.95517578 0.94882813 0.90585938]\n",
      "2019-02-27T23:13:24.533642: step 12200, loss 0.149647, acc [0.96953125 0.95820313 0.95283203 0.90927734]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-27T23:13:33.739413: step 12200, loss 0.263137, acc [0.96995665 0.95477981 0.94314689 0.91533602] \n",
      "\n",
      "2019-02-27T23:13:36.269959: step 12240, loss 0.154634, acc [0.96982422 0.95859375 0.95019531 0.91025391]\n",
      "2019-02-27T23:13:38.712761: step 12280, loss 0.152726, acc [0.96796875 0.95664063 0.95263672 0.90878906]\n",
      "2019-02-27T23:13:41.187117: step 12320, loss 0.14577, acc [0.96796875 0.95878906 0.95332031 0.90888672]\n",
      "2019-02-27T23:13:43.652780: step 12360, loss 0.152816, acc [0.96923828 0.95478516 0.95234375 0.91103516]\n",
      "2019-02-27T23:13:46.115873: step 12400, loss 0.160679, acc [0.96542969 0.95419922 0.94921875 0.90507812]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-27T23:13:55.372233: step 12400, loss 0.253791, acc [0.96975643 0.95583097 0.94493888 0.91705793] \n",
      "\n",
      "2019-02-27T23:13:57.920234: step 12440, loss 0.154196, acc [0.96699219 0.95654297 0.95126953 0.90898437]\n",
      "2019-02-27T23:14:00.475581: step 12480, loss 0.155893, acc [0.96875    0.95634766 0.95058594 0.90830078]\n",
      "2019-02-27T23:14:03.131169: step 12520, loss 0.0913913, acc [0.97666016 0.97070312 0.97138672 0.9359375 ]\n",
      "2019-02-27T23:14:05.606211: step 12560, loss 0.092421, acc [0.97480469 0.96992188 0.96914062 0.93183594]\n",
      "2019-02-27T23:14:08.081253: step 12600, loss 0.105709, acc [0.97802734 0.96923828 0.96611328 0.93222656]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-27T23:14:17.423916: step 12600, loss 0.266353, acc [0.97104786 0.95749282 0.94831263 0.92160298] \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-02-27T23:14:19.982770: step 12640, loss 0.0901046, acc [0.97626953 0.97285156 0.97080078 0.93603516]\n",
      "2019-02-27T23:14:22.477651: step 12680, loss 0.0950556, acc [0.97539062 0.96904297 0.96699219 0.93037109]\n",
      "2019-02-27T23:14:24.958645: step 12720, loss 0.0923712, acc [0.97626953 0.97109375 0.97089844 0.93613281]\n",
      "2019-02-27T23:14:27.414342: step 12760, loss 0.0986727, acc [0.97773438 0.96982422 0.96835938 0.93486328]\n",
      "2019-02-27T23:14:30.170621: step 12800, loss 0.100193, acc [0.97597656 0.96630859 0.96572266 0.92802734]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-27T23:14:41.989362: step 12800, loss 0.275062, acc [0.97114797 0.95698225 0.94609016 0.91980098] \n",
      "\n",
      "2019-02-27T23:14:44.655813: step 12840, loss 0.109625, acc [0.97441406 0.96689453 0.96484375 0.92646484]\n",
      "2019-02-27T23:14:47.225600: step 12880, loss 0.102687, acc [0.97421875 0.96816406 0.96611328 0.93037109]\n",
      "2019-02-27T23:14:49.655497: step 12920, loss 0.111112, acc [0.97304687 0.96533203 0.96347656 0.92529297]\n",
      "2019-02-27T23:14:52.097806: step 12960, loss 0.116397, acc [0.97548828 0.96396484 0.96279297 0.92490234]\n",
      "2019-02-27T23:14:54.545568: step 13000, loss 0.111016, acc [0.97353516 0.96474609 0.96289062 0.9234375 ]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-27T23:15:03.559381: step 13000, loss 0.27358, acc [0.97109792 0.95704232 0.94681096 0.92049175] \n",
      "\n",
      "2019-02-27T23:15:06.059225: step 13040, loss 0.116058, acc [0.97441406 0.96640625 0.96367187 0.92509766]\n",
      "2019-02-27T23:15:08.508971: step 13080, loss 0.134766, acc [0.971875   0.96201172 0.95595703 0.91845703]\n",
      "2019-02-27T23:15:10.923005: step 13120, loss 0.116784, acc [0.97451172 0.96796875 0.96240234 0.92646484]\n",
      "2019-02-27T23:15:13.347455: step 13160, loss 0.1125, acc [0.97607422 0.96660156 0.96201172 0.92480469]\n",
      "2019-02-27T23:15:15.773888: step 13200, loss 0.13294, acc [0.96982422 0.96435547 0.95927734 0.92060547]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-27T23:15:24.783281: step 13200, loss 0.267235, acc [0.97048724 0.95792329 0.9452292  0.91922033] \n",
      "\n",
      "2019-02-27T23:15:27.296474: step 13240, loss 0.11915, acc [0.97431641 0.96484375 0.96220703 0.92402344]\n",
      "2019-02-27T23:15:29.731340: step 13280, loss 0.128083, acc [0.97158203 0.96074219 0.95800781 0.91699219]\n",
      "2019-02-27T23:15:32.163726: step 13320, loss 0.128117, acc [0.97412109 0.96210938 0.96074219 0.92177734]\n",
      "2019-02-27T23:15:34.586259: step 13360, loss 0.11764, acc [0.97392578 0.96728516 0.96240234 0.92548828]\n",
      "2019-02-27T23:15:36.985857: step 13400, loss 0.127054, acc [0.97080078 0.96142578 0.95693359 0.91503906]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-27T23:15:46.121224: step 13400, loss 0.269646, acc [0.97192884 0.95726256 0.94509906 0.91988107] \n",
      "\n",
      "2019-02-27T23:15:48.622013: step 13440, loss 0.130149, acc [0.97197266 0.96103516 0.95908203 0.92001953]\n",
      "2019-02-27T23:15:51.044974: step 13480, loss 0.130524, acc [0.97167969 0.96318359 0.95791016 0.91875   ]\n",
      "2019-02-27T23:15:53.468433: step 13520, loss 0.124538, acc [0.97314453 0.96669922 0.95771484 0.91865234]\n",
      "2019-02-27T23:15:55.881019: step 13560, loss 0.129851, acc [0.96933594 0.9625     0.95800781 0.91728516]\n",
      "2019-02-27T23:15:58.296005: step 13600, loss 0.134658, acc [0.97050781 0.96005859 0.95556641 0.91396484]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-27T23:16:07.488919: step 13600, loss 0.267407, acc [0.97121805 0.95641162 0.94599005 0.91943057] \n",
      "\n",
      "2019-02-27T23:16:09.979294: step 13640, loss 0.136675, acc [0.97011719 0.96210938 0.95673828 0.91660156]\n",
      "2019-02-27T23:16:12.388866: step 13680, loss 0.144237, acc [0.96796875 0.95800781 0.95302734 0.9078125 ]\n",
      "2019-02-27T23:16:14.792483: step 13720, loss 0.136693, acc [0.97119141 0.96064453 0.95380859 0.91201172]\n",
      "2019-02-27T23:16:17.215941: step 13760, loss 0.144725, acc [0.97285156 0.96083984 0.95410156 0.91669922]\n",
      "2019-02-27T23:16:19.620055: step 13800, loss 0.151178, acc [0.97089844 0.95898438 0.95224609 0.91025391]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-27T23:16:28.747001: step 13800, loss 0.263959, acc [0.97028702 0.95656178 0.9448688  0.91862968] \n",
      "\n",
      "2019-02-27T23:16:31.241345: step 13840, loss 0.141321, acc [0.97128906 0.959375   0.95400391 0.91455078]\n",
      "2019-02-27T23:16:33.647938: step 13880, loss 0.13731, acc [0.96982422 0.96025391 0.95566406 0.91484375]\n",
      "2019-02-27T23:16:36.082803: step 13920, loss 0.14364, acc [0.97060547 0.96044922 0.95380859 0.91494141]\n",
      "2019-02-27T23:16:38.488961: step 13960, loss 0.152147, acc [0.96855469 0.95800781 0.95185547 0.90966797]\n",
      "2019-02-27T23:16:40.918810: step 14000, loss 0.14286, acc [0.97226563 0.96054688 0.95302734 0.91191406]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-27T23:16:50.049231: step 14000, loss 0.262418, acc [0.97165854 0.95744276 0.94532932 0.92016138] \n",
      "\n",
      "2019-02-27T23:16:52.552539: step 14040, loss 0.149987, acc [0.97119141 0.95947266 0.95205078 0.91132813]\n",
      "2019-02-27T23:16:55.106404: step 14080, loss 0.0866996, acc [0.97958984 0.97167969 0.97324219 0.94023437]\n",
      "2019-02-27T23:16:57.508535: step 14120, loss 0.0770578, acc [0.97822266 0.97333984 0.97490234 0.94179687]\n",
      "2019-02-27T23:16:59.919594: step 14160, loss 0.081942, acc [0.97822266 0.97480469 0.97294922 0.94003906]\n",
      "2019-02-27T23:17:02.374795: step 14200, loss 0.0869467, acc [0.97734375 0.97080078 0.97167969 0.93720703]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-27T23:17:11.489340: step 14200, loss 0.270492, acc [0.97114797 0.95812352 0.94962408 0.92434602] \n",
      "\n",
      "2019-02-27T23:17:13.981205: step 14240, loss 0.0861347, acc [0.97763672 0.97021484 0.97089844 0.93574219]\n",
      "2019-02-27T23:17:16.383336: step 14280, loss 0.088496, acc [0.97734375 0.96982422 0.97207031 0.93691406]\n",
      "2019-02-27T23:17:18.782986: step 14320, loss 0.0943526, acc [0.97714844 0.97070312 0.97089844 0.93564453]\n",
      "2019-02-27T23:17:21.215776: step 14360, loss 0.0932311, acc [0.978125   0.97402344 0.96875    0.93681641]\n",
      "2019-02-27T23:17:23.621379: step 14400, loss 0.0942934, acc [0.97685547 0.9703125  0.97099609 0.93740234]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-27T23:17:32.883017: step 14400, loss 0.271711, acc [0.97123807 0.95712241 0.94867303 0.92267417] \n",
      "\n",
      "2019-02-27T23:17:35.408156: step 14440, loss 0.0926893, acc [0.9765625  0.96806641 0.97001953 0.93242187]\n",
      "2019-02-27T23:17:37.842573: step 14480, loss 0.100904, acc [0.9765625  0.96992188 0.96904297 0.93447266]\n",
      "2019-02-27T23:17:40.259039: step 14520, loss 0.0994208, acc [0.97490234 0.97011719 0.96767578 0.93388672]\n",
      "2019-02-27T23:17:42.715730: step 14560, loss 0.10692, acc [0.97529297 0.96748047 0.96835938 0.93076172]\n",
      "2019-02-27T23:17:45.129764: step 14600, loss 0.0980436, acc [0.97675781 0.97001953 0.96708984 0.93125   ]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-27T23:17:54.278492: step 14600, loss 0.27654, acc [0.97158846 0.95812352 0.94894333 0.9236052 ] \n",
      "\n",
      "2019-02-27T23:17:56.772383: step 14640, loss 0.108852, acc [0.97548828 0.96826172 0.965625   0.93027344]\n",
      "2019-02-27T23:17:59.193856: step 14680, loss 0.10387, acc [0.97587891 0.96787109 0.96884766 0.93203125]\n",
      "2019-02-27T23:18:01.663441: step 14720, loss 0.113032, acc [0.97578125 0.96689453 0.96152344 0.92695313]\n",
      "2019-02-27T23:18:04.088886: step 14760, loss 0.113186, acc [0.97363281 0.96738281 0.9625     0.92509766]\n",
      "2019-02-27T23:18:06.494486: step 14800, loss 0.110907, acc [0.975      0.96689453 0.96347656 0.92597656]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-27T23:18:15.771182: step 14800, loss 0.267946, acc [0.97039714 0.95738269 0.94875312 0.92179319] \n",
      "\n",
      "2019-02-27T23:18:18.348896: step 14840, loss 0.117564, acc [0.97626953 0.96425781 0.96240234 0.92529297]\n",
      "2019-02-27T23:18:20.762930: step 14880, loss 0.115632, acc [0.97587891 0.96552734 0.96083984 0.92431641]\n",
      "2019-02-27T23:18:23.190851: step 14920, loss 0.11838, acc [0.97265625 0.96787109 0.96171875 0.92353516]\n",
      "2019-02-27T23:18:25.619271: step 14960, loss 0.120688, acc [0.97216797 0.96279297 0.95966797 0.91933594]\n",
      "2019-02-27T23:18:28.049673: step 15000, loss 0.117434, acc [0.97275391 0.96503906 0.96416016 0.92402344]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-27T23:18:37.272801: step 15000, loss 0.267298, acc [0.97151839 0.95758292 0.94833265 0.9223438 ] \n",
      "\n",
      "2019-02-27T23:18:39.759251: step 15040, loss 0.127762, acc [0.97128906 0.96298828 0.95888672 0.91943359]\n",
      "2019-02-27T23:18:42.191141: step 15080, loss 0.117725, acc [0.97392578 0.96484375 0.96123047 0.92265625]\n",
      "2019-02-27T23:18:44.606206: step 15120, loss 0.123374, acc [0.97226563 0.96240234 0.96113281 0.92060547]\n",
      "2019-02-27T23:18:47.023584: step 15160, loss 0.124067, acc [0.97011719 0.96025391 0.95771484 0.91582031]\n",
      "2019-02-27T23:18:49.437121: step 15200, loss 0.12327, acc [0.97363281 0.96523437 0.95927734 0.92255859]\n",
      "\n",
      "Evaluation:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-02-27T23:18:58.639098: step 15200, loss 0.268355, acc [0.97073752 0.95777313 0.94765189 0.92150287] \n",
      "\n",
      "2019-02-27T23:19:01.149886: step 15240, loss 0.115604, acc [0.97236328 0.96650391 0.96152344 0.92177734]\n",
      "2019-02-27T23:19:03.580948: step 15280, loss 0.12503, acc [0.97138672 0.96582031 0.96054688 0.92138672]\n",
      "2019-02-27T23:19:05.998290: step 15320, loss 0.118315, acc [0.975      0.96640625 0.96054688 0.92324219]\n",
      "2019-02-27T23:19:08.411871: step 15360, loss 0.123646, acc [0.97021484 0.96386719 0.96025391 0.91767578]\n",
      "2019-02-27T23:19:10.837820: step 15400, loss 0.126838, acc [0.97431641 0.96259766 0.95839844 0.91923828]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-27T23:19:20.011294: step 15400, loss 0.264702, acc [0.97276977 0.95924476 0.94809238 0.92417584] \n",
      "\n",
      "2019-02-27T23:19:22.500773: step 15440, loss 0.12119, acc [0.97509766 0.96337891 0.95986328 0.92148438]\n",
      "2019-02-27T23:19:24.924228: step 15480, loss 0.133911, acc [0.97070312 0.96054688 0.95351562 0.91201172]\n",
      "2019-02-27T23:19:27.332756: step 15520, loss 0.136355, acc [0.97109375 0.96533203 0.95625    0.91865234]\n",
      "2019-02-27T23:19:29.745301: step 15560, loss 0.131166, acc [0.97294922 0.96259766 0.95595703 0.9171875 ]\n",
      "2019-02-27T23:19:32.163800: step 15600, loss 0.138308, acc [0.97109375 0.96025391 0.95361328 0.91328125]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-27T23:19:41.318974: step 15600, loss 0.263972, acc [0.97200893 0.95871417 0.9484728  0.92341499] \n",
      "\n",
      "2019-02-27T23:19:43.953233: step 15640, loss 0.0838617, acc [0.97919922 0.97539062 0.97285156 0.94179687]\n",
      "2019-02-27T23:19:46.369747: step 15680, loss 0.0801117, acc [0.97734375 0.97558594 0.97470703 0.94248047]\n",
      "2019-02-27T23:19:48.774358: step 15720, loss 0.0801436, acc [0.97714844 0.97441406 0.9734375  0.93867188]\n",
      "2019-02-27T23:19:51.178472: step 15760, loss 0.0784239, acc [0.97900391 0.97460938 0.97324219 0.94121094]\n",
      "2019-02-27T23:19:53.599450: step 15800, loss 0.0834076, acc [0.97802734 0.97226563 0.97324219 0.93925781]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-27T23:20:02.685682: step 15800, loss 0.274395, acc [0.97255954 0.95969526 0.95063521 0.92608796] \n",
      "\n",
      "2019-02-27T23:20:05.176596: step 15840, loss 0.0790416, acc [0.97958984 0.97617188 0.97402344 0.94257813]\n",
      "2019-02-27T23:20:07.593606: step 15880, loss 0.0818925, acc [0.98017578 0.97402344 0.97255859 0.94111328]\n",
      "2019-02-27T23:20:09.994744: step 15920, loss 0.0840477, acc [0.97705078 0.97060547 0.97451172 0.93779297]\n",
      "2019-02-27T23:20:12.400842: step 15960, loss 0.0845754, acc [0.97841797 0.97460938 0.97431641 0.94414062]\n",
      "2019-02-27T23:20:14.814875: step 16000, loss 0.0863448, acc [0.97744141 0.97177734 0.97167969 0.93574219]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-27T23:20:23.917970: step 16000, loss 0.281729, acc [0.97382094 0.95946501 0.95041496 0.92701899] \n",
      "\n",
      "2019-02-27T23:20:26.405576: step 16040, loss 0.0989446, acc [0.97744141 0.97167969 0.97001953 0.93730469]\n",
      "2019-02-27T23:20:28.828556: step 16080, loss 0.0870462, acc [0.97714844 0.96982422 0.97246094 0.93671875]\n",
      "2019-02-27T23:20:31.227693: step 16120, loss 0.0914264, acc [0.97900391 0.9703125  0.97011719 0.9359375 ]\n",
      "2019-02-27T23:20:33.660078: step 16160, loss 0.0936374, acc [0.97773438 0.97285156 0.97060547 0.93769531]\n",
      "2019-02-27T23:20:36.073121: step 16200, loss 0.0967441, acc [0.97636719 0.96943359 0.96826172 0.93095703]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-27T23:20:45.232265: step 16200, loss 0.282454, acc [0.97312016 0.95901451 0.9496441  0.92586771] \n",
      "\n",
      "2019-02-27T23:20:47.720699: step 16240, loss 0.0942765, acc [0.97832031 0.96884766 0.96796875 0.93359375]\n",
      "2019-02-27T23:20:50.126834: step 16280, loss 0.106365, acc [0.97412109 0.96699219 0.96513672 0.928125  ]\n",
      "2019-02-27T23:20:52.530911: step 16320, loss 0.109854, acc [0.97753906 0.96796875 0.96591797 0.93173828]\n",
      "2019-02-27T23:20:54.950401: step 16360, loss 0.100232, acc [0.97724609 0.96982422 0.96767578 0.93515625]\n",
      "2019-02-27T23:20:57.377333: step 16400, loss 0.101501, acc [0.97412109 0.96943359 0.96582031 0.92929688]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-27T23:21:06.650554: step 16400, loss 0.280467, acc [0.97308012 0.95985544 0.95058515 0.92725926] \n",
      "\n",
      "2019-02-27T23:21:09.155358: step 16440, loss 0.0979141, acc [0.97587891 0.96875    0.96953125 0.93320313]\n",
      "2019-02-27T23:21:11.567407: step 16480, loss 0.10704, acc [0.97519531 0.96708984 0.96464844 0.92910156]\n",
      "2019-02-27T23:21:13.976481: step 16520, loss 0.10466, acc [0.975      0.96904297 0.96435547 0.92783203]\n",
      "2019-02-27T23:21:16.410851: step 16560, loss 0.102405, acc [0.97529297 0.96855469 0.96845703 0.93115234]\n",
      "2019-02-27T23:21:18.834805: step 16600, loss 0.103519, acc [0.97353516 0.96904297 0.96445313 0.92724609]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-27T23:21:27.995436: step 16600, loss 0.278171, acc [0.97225921 0.959455   0.94877314 0.9238755 ] \n",
      "\n",
      "2019-02-27T23:21:30.479904: step 16640, loss 0.114867, acc [0.97753906 0.96767578 0.96484375 0.92949219]\n",
      "2019-02-27T23:21:32.882032: step 16680, loss 0.120851, acc [0.97246094 0.96455078 0.96123047 0.92207031]\n",
      "2019-02-27T23:21:35.277220: step 16720, loss 0.11128, acc [0.97421875 0.96582031 0.96445313 0.92578125]\n",
      "2019-02-27T23:21:37.706629: step 16760, loss 0.119863, acc [0.9703125  0.96435547 0.9625     0.92119141]\n",
      "2019-02-27T23:21:40.154888: step 16800, loss 0.115278, acc [0.97441406 0.96425781 0.96191406 0.92314453]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-27T23:21:49.306590: step 16800, loss 0.273788, acc [0.97234931 0.95891439 0.94915356 0.92430598] \n",
      "\n",
      "2019-02-27T23:21:51.786125: step 16840, loss 0.116491, acc [0.97519531 0.96748047 0.96220703 0.92617187]\n",
      "2019-02-27T23:21:54.193188: step 16880, loss 0.123113, acc [0.97441406 0.96484375 0.95664063 0.91982422]\n",
      "2019-02-27T23:21:56.605237: step 16920, loss 0.124821, acc [0.97011719 0.96337891 0.95771484 0.91699219]\n",
      "2019-02-27T23:21:59.031175: step 16960, loss 0.117375, acc [0.97402344 0.96660156 0.96005859 0.92363281]\n",
      "2019-02-27T23:22:01.519611: step 17000, loss 0.121284, acc [0.97324219 0.96357422 0.96035156 0.92158203]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-27T23:22:10.721408: step 17000, loss 0.270241, acc [0.97275976 0.96038603 0.94986435 0.92629819] \n",
      "\n",
      "2019-02-27T23:22:13.202899: step 17040, loss 0.120567, acc [0.97304687 0.96455078 0.96191406 0.92333984]\n",
      "2019-02-27T23:22:15.628838: step 17080, loss 0.13585, acc [0.97226563 0.96259766 0.95556641 0.91894531]\n",
      "2019-02-27T23:22:18.047335: step 17120, loss 0.119536, acc [0.97451172 0.96738281 0.95917969 0.92490234]\n",
      "2019-02-27T23:22:20.518435: step 17160, loss 0.124408, acc [0.97089844 0.96533203 0.95888672 0.91923828]\n",
      "2019-02-27T23:22:23.077275: step 17200, loss 0.07555, acc [0.97724609 0.97480469 0.97646484 0.9421875 ]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-27T23:22:32.257749: step 17200, loss 0.272384, acc [0.97252951 0.9596352  0.95125589 0.92698896] \n",
      "\n",
      "2019-02-27T23:22:34.743205: step 17240, loss 0.0726055, acc [0.97998047 0.975      0.97597656 0.94257813]\n",
      "2019-02-27T23:22:37.146328: step 17280, loss 0.0704806, acc [0.98183594 0.97509766 0.97841797 0.9484375 ]\n",
      "2019-02-27T23:22:39.592106: step 17320, loss 0.0689479, acc [0.97900391 0.97685547 0.97695312 0.94394531]\n",
      "2019-02-27T23:22:42.014571: step 17360, loss 0.0784532, acc [0.98046875 0.97402344 0.97373047 0.94238281]\n",
      "2019-02-27T23:22:44.440014: step 17400, loss 0.0734271, acc [0.97802734 0.97763672 0.97675781 0.94462891]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-27T23:22:53.614037: step 17400, loss 0.277012, acc [0.97433151 0.96121695 0.95123587 0.9291013 ] \n",
      "\n",
      "2019-02-27T23:22:56.120823: step 17440, loss 0.0786069, acc [0.97919922 0.97451172 0.97382813 0.94228516]\n",
      "2019-02-27T23:22:58.522954: step 17480, loss 0.0734048, acc [0.98027344 0.97568359 0.97519531 0.94326172]\n",
      "2019-02-27T23:23:00.946907: step 17520, loss 0.0777576, acc [0.98046875 0.97578125 0.97382813 0.94365234]\n",
      "2019-02-27T23:23:03.367389: step 17560, loss 0.07992, acc [0.97783203 0.97382813 0.97421875 0.94013672]\n",
      "2019-02-27T23:23:05.797793: step 17600, loss 0.0834544, acc [0.98203125 0.97177734 0.97226563 0.94042969]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-27T23:23:15.013479: step 17600, loss 0.281231, acc [0.97353062 0.96082652 0.95153621 0.928831  ] \n",
      "\n",
      "2019-02-27T23:23:17.493481: step 17640, loss 0.0841264, acc [0.97919922 0.97275391 0.97246094 0.93896484]\n",
      "2019-02-27T23:23:19.929341: step 17680, loss 0.086867, acc [0.97753906 0.97216797 0.97177734 0.93740234]\n",
      "2019-02-27T23:23:22.388076: step 17720, loss 0.092583, acc [0.97724609 0.96943359 0.97216797 0.93671875]\n",
      "2019-02-27T23:23:24.805024: step 17760, loss 0.087532, acc [0.97871094 0.97109375 0.97158203 0.93759766]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-02-27T23:23:27.229971: step 17800, loss 0.0928068, acc [0.97792969 0.97275391 0.96826172 0.93505859]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-27T23:23:36.367579: step 17800, loss 0.284547, acc [0.97347055 0.96024587 0.94887325 0.92568751] \n",
      "\n",
      "2019-02-27T23:23:38.856506: step 17840, loss 0.0986663, acc [0.97851562 0.97050781 0.96865234 0.93398437]\n",
      "2019-02-27T23:23:41.302285: step 17880, loss 0.104873, acc [0.9734375  0.96855469 0.96533203 0.92744141]\n",
      "2019-02-27T23:23:43.712847: step 17920, loss 0.0965265, acc [0.97734375 0.97099609 0.97060547 0.93662109]\n",
      "2019-02-27T23:23:46.136801: step 17960, loss 0.113011, acc [0.97714844 0.96826172 0.96269531 0.92792969]\n",
      "2019-02-27T23:23:48.581588: step 18000, loss 0.105248, acc [0.97441406 0.9640625  0.96679688 0.92792969]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-27T23:23:57.752682: step 18000, loss 0.272699, acc [0.9735106  0.9609867  0.95166635 0.92885102] \n",
      "\n",
      "2019-02-27T23:24:00.249502: step 18040, loss 0.0980085, acc [0.97763672 0.97119141 0.96699219 0.93466797]\n",
      "2019-02-27T23:24:02.726030: step 18080, loss 0.0968162, acc [0.97900391 0.97207031 0.96669922 0.93398437]\n",
      "2019-02-27T23:24:05.157425: step 18120, loss 0.0983291, acc [0.97626953 0.96943359 0.96904297 0.93388672]\n",
      "2019-02-27T23:24:07.575922: step 18160, loss 0.0990211, acc [0.97548828 0.96835938 0.96708984 0.93007812]\n",
      "2019-02-27T23:24:10.005830: step 18200, loss 0.110799, acc [0.97509766 0.96640625 0.96455078 0.92851562]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-27T23:24:19.161550: step 18200, loss 0.277117, acc [0.97302005 0.95993553 0.94954399 0.92556738] \n",
      "\n",
      "2019-02-27T23:24:21.643486: step 18240, loss 0.102983, acc [0.97558594 0.96943359 0.96679688 0.92871094]\n",
      "2019-02-27T23:24:24.066945: step 18280, loss 0.0984322, acc [0.97382813 0.96845703 0.97021484 0.93251953]\n",
      "2019-02-27T23:24:26.486435: step 18320, loss 0.101174, acc [0.97568359 0.96669922 0.96806641 0.92939453]\n",
      "2019-02-27T23:24:28.882159: step 18360, loss 0.10258, acc [0.97714844 0.96738281 0.96503906 0.92822266]\n",
      "2019-02-27T23:24:31.287225: step 18400, loss 0.10406, acc [0.97734375 0.96826172 0.96464844 0.92988281]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-27T23:24:40.381941: step 18400, loss 0.283, acc [0.97238935 0.9593649  0.94970417 0.92553735] \n",
      "\n",
      "2019-02-27T23:24:42.922897: step 18440, loss 0.104738, acc [0.97685547 0.96914062 0.96611328 0.93085938]\n",
      "2019-02-27T23:24:45.328499: step 18480, loss 0.107721, acc [0.97490234 0.96494141 0.96376953 0.92675781]\n",
      "2019-02-27T23:24:47.750469: step 18520, loss 0.109985, acc [0.97392578 0.96777344 0.96582031 0.92822266]\n",
      "2019-02-27T23:24:50.171448: step 18560, loss 0.113361, acc [0.97392578 0.96621094 0.96425781 0.92734375]\n",
      "2019-02-27T23:24:52.597385: step 18600, loss 0.116653, acc [0.975      0.965625   0.96142578 0.9265625 ]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-27T23:25:01.752608: step 18600, loss 0.274895, acc [0.97210904 0.96011573 0.949554   0.92506682] \n",
      "\n",
      "2019-02-27T23:25:04.248125: step 18640, loss 0.115833, acc [0.97294922 0.96533203 0.96191406 0.92373047]\n",
      "2019-02-27T23:25:06.674063: step 18680, loss 0.107765, acc [0.97705078 0.96884766 0.96396484 0.93076172]\n",
      "2019-02-27T23:25:09.118354: step 18720, loss 0.112058, acc [0.97304687 0.96835938 0.96445313 0.92675781]\n",
      "2019-02-27T23:25:11.668788: step 18760, loss 0.0678555, acc [0.98417969 0.97578125 0.978125   0.9484375 ]\n",
      "2019-02-27T23:25:14.123493: step 18800, loss 0.071483, acc [0.97851562 0.97412109 0.97724609 0.94335938]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-27T23:25:23.269742: step 18800, loss 0.279777, acc [0.97422139 0.96192774 0.95243721 0.92981209] \n",
      "\n",
      "2019-02-27T23:25:25.748256: step 18840, loss 0.0688191, acc [0.98203125 0.97695312 0.97988281 0.94921875]\n",
      "2019-02-27T23:25:28.173239: step 18880, loss 0.0630896, acc [0.98115234 0.97802734 0.97949219 0.95048828]\n",
      "2019-02-27T23:25:30.574835: step 18920, loss 0.0723109, acc [0.98125    0.97226563 0.97734375 0.94482422]\n",
      "2019-02-27T23:25:32.992330: step 18960, loss 0.0770329, acc [0.98007813 0.97539062 0.97509766 0.94287109]\n",
      "2019-02-27T23:25:35.417276: step 19000, loss 0.0737498, acc [0.97978516 0.97470703 0.97607422 0.94472656]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-27T23:25:44.619573: step 19000, loss 0.289069, acc [0.97438156 0.96119693 0.95192664 0.92932155] \n",
      "\n",
      "2019-02-27T23:25:47.109494: step 19040, loss 0.0846175, acc [0.98046875 0.97265625 0.97333984 0.94267578]\n",
      "2019-02-27T23:25:49.528985: step 19080, loss 0.072953, acc [0.98037109 0.97773438 0.97519531 0.94638672]\n",
      "2019-02-27T23:25:51.934092: step 19120, loss 0.0803549, acc [0.97714844 0.97177734 0.97412109 0.9390625 ]\n",
      "2019-02-27T23:25:54.392766: step 19160, loss 0.0741227, acc [0.98037109 0.97314453 0.97724609 0.94257813]\n",
      "2019-02-27T23:25:56.780511: step 19200, loss 0.0811547, acc [0.98095703 0.97412109 0.97207031 0.94199219]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-27T23:26:05.930773: step 19200, loss 0.291756, acc [0.97392105 0.96114687 0.95185656 0.92925147] \n",
      "\n",
      "2019-02-27T23:26:08.436024: step 19240, loss 0.0790887, acc [0.97636719 0.97431641 0.97314453 0.93828125]\n",
      "2019-02-27T23:26:10.892715: step 19280, loss 0.0944982, acc [0.97695312 0.97099609 0.97128906 0.93720703]\n",
      "2019-02-27T23:26:13.322125: step 19320, loss 0.0918597, acc [0.97460938 0.97158203 0.97070312 0.93457031]\n",
      "2019-02-27T23:26:15.763974: step 19360, loss 0.0887941, acc [0.97851562 0.97138672 0.97138672 0.93740234]\n",
      "2019-02-27T23:26:18.170088: step 19400, loss 0.0897687, acc [0.97929687 0.97294922 0.96933594 0.93671875]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-27T23:26:27.229523: step 19400, loss 0.283007, acc [0.97334041 0.96096667 0.95138604 0.92810019] \n",
      "\n",
      "2019-02-27T23:26:29.746226: step 19440, loss 0.0828464, acc [0.97519531 0.97412109 0.97333984 0.93789062]\n",
      "2019-02-27T23:26:32.161213: step 19480, loss 0.0931602, acc [0.97880859 0.97177734 0.96953125 0.93720703]\n",
      "2019-02-27T23:26:34.564335: step 19520, loss 0.0880507, acc [0.97773438 0.97285156 0.97099609 0.93769531]\n",
      "2019-02-27T23:26:36.958580: step 19560, loss 0.0866793, acc [0.98076172 0.97460938 0.97255859 0.94296875]\n",
      "2019-02-27T23:26:39.393395: step 19600, loss 0.0878463, acc [0.97861328 0.97607422 0.97265625 0.94257813]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-27T23:26:48.630450: step 19600, loss 0.282235, acc [0.97427144 0.96185766 0.95170639 0.92985214] \n",
      "\n",
      "2019-02-27T23:26:51.098509: step 19640, loss 0.0992298, acc [0.97685547 0.97138672 0.96875    0.93564453]\n",
      "2019-02-27T23:26:53.513536: step 19680, loss 0.0976393, acc [0.97666016 0.97099609 0.96767578 0.93457031]\n",
      "2019-02-27T23:26:55.928671: step 19720, loss 0.0970242, acc [0.9765625  0.97148437 0.96767578 0.934375  ]\n",
      "2019-02-27T23:26:58.350597: step 19760, loss 0.0919493, acc [0.97744141 0.97060547 0.97148437 0.93662109]\n",
      "2019-02-27T23:27:00.763077: step 19800, loss 0.0909221, acc [0.97744141 0.97167969 0.97001953 0.93613281]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-27T23:27:09.895932: step 19800, loss 0.281521, acc [0.97453173 0.96210794 0.95165634 0.93032266] \n",
      "\n",
      "2019-02-27T23:27:12.395279: step 19840, loss 0.0895496, acc [0.97734375 0.97109375 0.97001953 0.934375  ]\n",
      "2019-02-27T23:27:14.803431: step 19880, loss 0.0989859, acc [0.97666016 0.96777344 0.96816406 0.93242187]\n",
      "2019-02-27T23:27:17.258565: step 19920, loss 0.099535, acc [0.97744141 0.97294922 0.96699219 0.93466797]\n",
      "2019-02-27T23:27:19.683013: step 19960, loss 0.110439, acc [0.97617188 0.97119141 0.96357422 0.93017578]\n",
      "2019-02-27T23:27:22.100025: step 20000, loss 0.111304, acc [0.97587891 0.96552734 0.96201172 0.92480469]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-27T23:27:31.226974: step 20000, loss 0.278124, acc [0.97392105 0.96090661 0.95042497 0.92779986] \n",
      "\n",
      "2019-02-27T23:27:33.731288: step 20040, loss 0.106106, acc [0.97841797 0.96806641 0.965625   0.92910156]\n",
      "2019-02-27T23:27:36.164116: step 20080, loss 0.0994067, acc [0.97861328 0.9703125  0.96826172 0.9359375 ]\n",
      "2019-02-27T23:27:38.586582: step 20120, loss 0.102252, acc [0.97441406 0.96728516 0.96669922 0.92958984]\n",
      "2019-02-27T23:27:41.008551: step 20160, loss 0.105126, acc [0.97783203 0.97070312 0.96455078 0.93203125]\n",
      "2019-02-27T23:27:43.417666: step 20200, loss 0.11369, acc [0.97324219 0.96523437 0.96210938 0.92548828]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-27T23:27:52.628698: step 20200, loss 0.267919, acc [0.97337044 0.962158   0.95123587 0.92846059] \n",
      "\n",
      "2019-02-27T23:27:55.106716: step 20240, loss 0.105925, acc [0.97705078 0.97041016 0.96201172 0.92783203]\n",
      "2019-02-27T23:27:57.527695: step 20280, loss 0.106387, acc [0.975      0.96728516 0.965625   0.92802734]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-02-27T23:28:00.089537: step 20320, loss 0.0667753, acc [0.98085937 0.97744141 0.97900391 0.94833984]\n",
      "2019-02-27T23:28:02.553171: step 20360, loss 0.0647355, acc [0.98310547 0.97988281 0.97744141 0.95058594]\n",
      "2019-02-27T23:28:04.943397: step 20400, loss 0.0665013, acc [0.98203125 0.97568359 0.97880859 0.94873047]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-27T23:28:14.082204: step 20400, loss 0.287057, acc [0.97493217 0.96309904 0.95205678 0.93141387] \n",
      "\n",
      "2019-02-27T23:28:16.581551: step 20440, loss 0.0764492, acc [0.98330078 0.97626953 0.97519531 0.94726562]\n",
      "2019-02-27T23:28:19.039283: step 20480, loss 0.0745204, acc [0.98125    0.97714844 0.97529297 0.94658203]\n",
      "2019-02-27T23:28:21.447811: step 20520, loss 0.0669971, acc [0.98115234 0.97978516 0.97705078 0.94814453]\n",
      "2019-02-27T23:28:23.884164: step 20560, loss 0.0640473, acc [0.98291016 0.97988281 0.98037109 0.95361328]\n",
      "2019-02-27T23:28:26.277367: step 20600, loss 0.0712011, acc [0.97988281 0.97539062 0.97841797 0.94755859]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-27T23:28:35.480656: step 20600, loss 0.28192, acc [0.97444163 0.96217802 0.95355845 0.93159407] \n",
      "\n",
      "2019-02-27T23:28:37.963138: step 20640, loss 0.078406, acc [0.98085937 0.97451172 0.97509766 0.94511719]\n",
      "2019-02-27T23:28:40.389571: step 20680, loss 0.0737479, acc [0.98095703 0.97558594 0.9765625  0.94560547]\n",
      "2019-02-27T23:28:42.824438: step 20720, loss 0.0793683, acc [0.97851562 0.97265625 0.97558594 0.94111328]\n",
      "2019-02-27T23:28:45.277200: step 20760, loss 0.075822, acc [0.98359375 0.97548828 0.97490234 0.946875  ]\n",
      "2019-02-27T23:28:47.675322: step 20800, loss 0.0738576, acc [0.98212891 0.97324219 0.97587891 0.94628906]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-27T23:28:56.863233: step 20800, loss 0.288871, acc [0.97472194 0.96148725 0.95328815 0.93136381] \n",
      "\n",
      "2019-02-27T23:28:59.329399: step 20840, loss 0.0741863, acc [0.98193359 0.97421875 0.97490234 0.94394531]\n",
      "2019-02-27T23:29:01.772645: step 20880, loss 0.0783831, acc [0.97958984 0.97421875 0.97441406 0.94257813]\n",
      "2019-02-27T23:29:04.191201: step 20920, loss 0.0810811, acc [0.98066406 0.97460938 0.97382813 0.94208984]\n",
      "2019-02-27T23:29:06.604681: step 20960, loss 0.0860108, acc [0.97871094 0.97021484 0.97246094 0.93955078]\n",
      "2019-02-27T23:29:09.015373: step 21000, loss 0.0792864, acc [0.98056641 0.97304687 0.97412109 0.94228516]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-27T23:29:18.146787: step 21000, loss 0.2904, acc [0.9745918  0.96222807 0.95178648 0.93062299] \n",
      "\n",
      "2019-02-27T23:29:20.646086: step 21040, loss 0.0863461, acc [0.97949219 0.97519531 0.97119141 0.94013672]\n",
      "2019-02-27T23:29:23.040778: step 21080, loss 0.0906941, acc [0.97841797 0.97304687 0.97060547 0.93886719]\n",
      "2019-02-27T23:29:25.469692: step 21120, loss 0.0983784, acc [0.97890625 0.97050781 0.96865234 0.93564453]\n",
      "2019-02-27T23:29:27.888686: step 21160, loss 0.0864943, acc [0.98017578 0.97392578 0.97060547 0.9390625 ]\n",
      "2019-02-27T23:29:30.313569: step 21200, loss 0.0931926, acc [0.97919922 0.97324219 0.96894531 0.93935547]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-27T23:29:39.479122: step 21200, loss 0.285655, acc [0.97464185 0.9613471  0.95167636 0.92941165] \n",
      "\n",
      "2019-02-27T23:29:41.959125: step 21240, loss 0.0988818, acc [0.97871094 0.97167969 0.96777344 0.93525391]\n",
      "2019-02-27T23:29:44.390942: step 21280, loss 0.0909511, acc [0.97597656 0.97470703 0.96943359 0.93710938]\n",
      "2019-02-27T23:29:46.793570: step 21320, loss 0.0878502, acc [0.98134766 0.97431641 0.96982422 0.94003906]\n",
      "2019-02-27T23:29:49.206612: step 21360, loss 0.0862817, acc [0.975      0.97285156 0.97333984 0.93964844]\n",
      "2019-02-27T23:29:51.639493: step 21400, loss 0.0923418, acc [0.97958984 0.97421875 0.9703125  0.93828125]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-27T23:30:00.754720: step 21400, loss 0.279417, acc [0.97493217 0.9617976  0.95280762 0.93100341] \n",
      "\n",
      "2019-02-27T23:30:03.261009: step 21440, loss 0.0974527, acc [0.97646484 0.97041016 0.96904297 0.93320313]\n",
      "2019-02-27T23:30:05.674269: step 21480, loss 0.0910321, acc [0.97568359 0.971875   0.97080078 0.93447266]\n",
      "2019-02-27T23:30:08.100702: step 21520, loss 0.0983704, acc [0.97802734 0.97128906 0.97021484 0.93808594]\n",
      "2019-02-27T23:30:10.522178: step 21560, loss 0.0932377, acc [0.97890625 0.97353516 0.96914062 0.93642578]\n",
      "2019-02-27T23:30:12.924307: step 21600, loss 0.0962162, acc [0.97597656 0.97226563 0.96660156 0.93349609]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-27T23:30:22.064108: step 21600, loss 0.281434, acc [0.97364074 0.96144721 0.95274755 0.9300023 ] \n",
      "\n",
      "2019-02-27T23:30:24.546144: step 21640, loss 0.090689, acc [0.97626953 0.9703125  0.96943359 0.93378906]\n",
      "2019-02-27T23:30:26.953182: step 21680, loss 0.09946, acc [0.97607422 0.97285156 0.96630859 0.93339844]\n",
      "2019-02-27T23:30:29.368210: step 21720, loss 0.0913737, acc [0.97685547 0.97099609 0.97167969 0.93671875]\n",
      "2019-02-27T23:30:31.760478: step 21760, loss 0.0949602, acc [0.97939453 0.97167969 0.96865234 0.93642578]\n",
      "2019-02-27T23:30:34.180407: step 21800, loss 0.108225, acc [0.97548828 0.96943359 0.96220703 0.92675781]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-27T23:30:43.269163: step 21800, loss 0.283986, acc [0.97401115 0.9609867  0.95196668 0.92935158] \n",
      "\n",
      "2019-02-27T23:30:45.768512: step 21840, loss 0.110394, acc [0.97763672 0.96591797 0.96240234 0.92695313]\n",
      "2019-02-27T23:30:48.305505: step 21880, loss 0.0676548, acc [0.98320312 0.98203125 0.97724609 0.95234375]\n",
      "2019-02-27T23:30:50.750291: step 21920, loss 0.0586055, acc [0.98378906 0.98076172 0.98076172 0.95390625]\n",
      "2019-02-27T23:30:53.179702: step 21960, loss 0.0609004, acc [0.98125    0.97939453 0.98173828 0.95205078]\n",
      "2019-02-27T23:30:55.582825: step 22000, loss 0.064294, acc [0.98320312 0.978125   0.97910156 0.95058594]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-27T23:31:04.685921: step 22000, loss 0.287595, acc [0.9746819  0.96313908 0.95369861 0.93255514] \n",
      "\n",
      "2019-02-27T23:31:07.163441: step 22040, loss 0.060582, acc [0.98232422 0.98017578 0.98105469 0.95205078]\n",
      "2019-02-27T23:31:09.633524: step 22080, loss 0.0643716, acc [0.98046875 0.97910156 0.97978516 0.9515625 ]\n",
      "2019-02-27T23:31:12.180487: step 22120, loss 0.067553, acc [0.98134766 0.97861328 0.97763672 0.95048828]\n",
      "2019-02-27T23:31:14.635688: step 22160, loss 0.0703956, acc [0.98193359 0.97617188 0.978125   0.94658203]\n",
      "2019-02-27T23:31:17.062121: step 22200, loss 0.0695526, acc [0.98232422 0.97568359 0.97753906 0.94746094]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-27T23:31:26.212833: step 22200, loss 0.288457, acc [0.97506232 0.96289882 0.95346835 0.93277538] \n",
      "\n",
      "2019-02-27T23:31:28.676963: step 22240, loss 0.0651503, acc [0.98066406 0.97910156 0.978125   0.95009766]\n",
      "2019-02-27T23:31:31.087526: step 22280, loss 0.0656389, acc [0.98330078 0.97880859 0.97958984 0.95175781]\n",
      "2019-02-27T23:31:33.472295: step 22320, loss 0.0713882, acc [0.98154297 0.97890625 0.97626953 0.94804687]\n",
      "2019-02-27T23:31:35.888313: step 22360, loss 0.0768831, acc [0.98085937 0.97597656 0.97519531 0.94462891]\n",
      "2019-02-27T23:31:38.308300: step 22400, loss 0.0724104, acc [0.98261719 0.97705078 0.97578125 0.94707031]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-27T23:31:47.439667: step 22400, loss 0.290336, acc [0.97498223 0.96288881 0.95338826 0.93242499] \n",
      "\n",
      "2019-02-27T23:31:49.933557: step 22440, loss 0.076193, acc [0.97851562 0.97480469 0.97548828 0.94384766]\n",
      "2019-02-27T23:31:52.373484: step 22480, loss 0.0779613, acc [0.97880859 0.97275391 0.97519531 0.94199219]\n",
      "2019-02-27T23:31:54.796443: step 22520, loss 0.0811728, acc [0.97802734 0.97304687 0.97236328 0.93867188]\n",
      "2019-02-27T23:31:57.204030: step 22560, loss 0.0778587, acc [0.97568359 0.97275391 0.97490234 0.94072266]\n",
      "2019-02-27T23:31:59.618065: step 22600, loss 0.0812839, acc [0.97988281 0.97617188 0.97226563 0.94248047]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-27T23:32:08.726119: step 22600, loss 0.295404, acc [0.97367077 0.96248836 0.95155623 0.93019251] \n",
      "\n",
      "2019-02-27T23:32:11.223481: step 22640, loss 0.0837852, acc [0.97919922 0.97578125 0.97275391 0.94228516]\n",
      "2019-02-27T23:32:13.639687: step 22680, loss 0.0877957, acc [0.97695312 0.97197266 0.97011719 0.93583984]\n",
      "2019-02-27T23:32:16.034401: step 22720, loss 0.0812981, acc [0.97988281 0.97509766 0.97265625 0.94169922]\n",
      "2019-02-27T23:32:18.452891: step 22760, loss 0.0832699, acc [0.97851562 0.97109375 0.97255859 0.93896484]\n",
      "2019-02-27T23:32:20.878811: step 22800, loss 0.0876948, acc [0.97949219 0.97148437 0.97177734 0.9390625 ]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-27T23:32:30.062260: step 22800, loss 0.296516, acc [0.97397111 0.96120694 0.95238715 0.93006237] \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-02-27T23:32:32.564583: step 22840, loss 0.0890644, acc [0.97763672 0.97402344 0.97011719 0.93925781]\n",
      "2019-02-27T23:32:34.986588: step 22880, loss 0.095193, acc [0.97636719 0.97197266 0.9703125  0.93564453]\n",
      "2019-02-27T23:32:37.394648: step 22920, loss 0.0937053, acc [0.978125   0.97246094 0.96904297 0.93476563]\n",
      "2019-02-27T23:32:39.809660: step 22960, loss 0.0908402, acc [0.97880859 0.97041016 0.97089844 0.93623047]\n",
      "2019-02-27T23:32:42.219232: step 23000, loss 0.0901006, acc [0.97880859 0.97255859 0.97119141 0.93789062]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-27T23:32:51.393750: step 23000, loss 0.286199, acc [0.97494219 0.96322918 0.95322808 0.93224479] \n",
      "\n",
      "2019-02-27T23:32:53.901528: step 23040, loss 0.0902961, acc [0.9765625  0.97197266 0.97001953 0.93427734]\n",
      "2019-02-27T23:32:56.324988: step 23080, loss 0.0941721, acc [0.97685547 0.97041016 0.96962891 0.93349609]\n",
      "2019-02-27T23:32:58.735549: step 23120, loss 0.0891249, acc [0.97783203 0.97158203 0.96982422 0.93505859]\n",
      "2019-02-27T23:33:01.161353: step 23160, loss 0.1005, acc [0.97763672 0.96865234 0.96835938 0.93486328]\n",
      "2019-02-27T23:33:03.563979: step 23200, loss 0.107772, acc [0.97705078 0.96826172 0.96464844 0.92958984]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-27T23:33:12.729570: step 23200, loss 0.28281, acc [0.97383095 0.96181762 0.95211685 0.93008239] \n",
      "\n",
      "2019-02-27T23:33:15.241318: step 23240, loss 0.11085, acc [0.97470703 0.96699219 0.96298828 0.92744141]\n",
      "2019-02-27T23:33:17.653367: step 23280, loss 0.0821453, acc [0.97861328 0.975      0.97050781 0.93808594]\n",
      "2019-02-27T23:33:20.061449: step 23320, loss 0.0919183, acc [0.97714844 0.96884766 0.96914062 0.93300781]\n",
      "2019-02-27T23:33:22.466060: step 23360, loss 0.095868, acc [0.97646484 0.97275391 0.96953125 0.93613281]\n",
      "2019-02-27T23:33:24.917790: step 23400, loss 0.0998839, acc [0.97705078 0.97265625 0.96728516 0.93515625]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-27T23:33:34.102229: step 23400, loss 0.277153, acc [0.97409124 0.96136712 0.95236713 0.92975202] \n",
      "\n",
      "2019-02-27T23:33:36.768230: step 23440, loss 0.0601721, acc [0.98330078 0.98056641 0.98027344 0.95292969]\n",
      "2019-02-27T23:33:39.191193: step 23480, loss 0.054861, acc [0.98232422 0.97880859 0.98378906 0.95576172]\n",
      "2019-02-27T23:33:41.626555: step 23520, loss 0.0546517, acc [0.984375   0.97939453 0.98271484 0.95585937]\n",
      "2019-02-27T23:33:44.055514: step 23560, loss 0.0605486, acc [0.98183594 0.97880859 0.98046875 0.9515625 ]\n",
      "2019-02-27T23:33:46.471985: step 23600, loss 0.0641426, acc [0.98271484 0.978125   0.97910156 0.95214844]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-27T23:33:55.675767: step 23600, loss 0.290511, acc [0.97560292 0.96389993 0.95373865 0.93375647] \n",
      "\n",
      "2019-02-27T23:33:58.178597: step 23640, loss 0.0570819, acc [0.98300781 0.97949219 0.97998047 0.95146484]\n",
      "2019-02-27T23:34:00.591666: step 23680, loss 0.0681336, acc [0.98251953 0.978125   0.97890625 0.95214844]\n",
      "2019-02-27T23:34:03.064636: step 23720, loss 0.0649941, acc [0.98154297 0.97724609 0.97900391 0.94882813]\n",
      "2019-02-27T23:34:05.478671: step 23760, loss 0.0653063, acc [0.97958984 0.97900391 0.98027344 0.95126953]\n",
      "2019-02-27T23:34:07.901694: step 23800, loss 0.0613522, acc [0.98212891 0.97871094 0.98017578 0.95068359]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-27T23:34:17.079129: step 23800, loss 0.296542, acc [0.9759433  0.96425032 0.95389883 0.93393667] \n",
      "\n",
      "2019-02-27T23:34:19.567562: step 23840, loss 0.0639911, acc [0.98183594 0.97744141 0.97958984 0.94931641]\n",
      "2019-02-27T23:34:21.995486: step 23880, loss 0.0649624, acc [0.9828125  0.97773438 0.97734375 0.94951172]\n",
      "2019-02-27T23:34:24.424896: step 23920, loss 0.0594935, acc [0.98125    0.97724609 0.98066406 0.95107422]\n",
      "2019-02-27T23:34:26.853809: step 23960, loss 0.0707816, acc [0.9828125  0.97617188 0.97724609 0.94794922]\n",
      "2019-02-27T23:34:29.276771: step 24000, loss 0.0740503, acc [0.98105469 0.97734375 0.975      0.94501953]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-27T23:34:38.436411: step 24000, loss 0.301028, acc [0.97575309 0.96293886 0.95277758 0.93238495] \n",
      "\n",
      "2019-02-27T23:34:40.916545: step 24040, loss 0.0769223, acc [0.98027344 0.97617188 0.97421875 0.94482422]\n",
      "2019-02-27T23:34:43.330509: step 24080, loss 0.068068, acc [0.98056641 0.97548828 0.97724609 0.94589844]\n",
      "2019-02-27T23:34:45.747002: step 24120, loss 0.080217, acc [0.98076172 0.97636719 0.97314453 0.94541016]\n",
      "2019-02-27T23:34:48.160995: step 24160, loss 0.0765857, acc [0.97929687 0.97519531 0.975      0.94462891]\n",
      "2019-02-27T23:34:50.603302: step 24200, loss 0.0734549, acc [0.98027344 0.97519531 0.97558594 0.94375   ]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-27T23:34:59.836846: step 24200, loss 0.302556, acc [0.97530259 0.96230816 0.95337825 0.93223478] \n",
      "\n",
      "2019-02-27T23:35:02.345119: step 24240, loss 0.078696, acc [0.98007813 0.97607422 0.971875   0.94121094]\n",
      "2019-02-27T23:35:04.756177: step 24280, loss 0.0776554, acc [0.98066406 0.97373047 0.97333984 0.94296875]\n",
      "2019-02-27T23:35:07.153348: step 24320, loss 0.0897006, acc [0.97900391 0.97421875 0.97099609 0.93935547]\n",
      "2019-02-27T23:35:09.568374: step 24360, loss 0.0892945, acc [0.97832031 0.97304687 0.97060547 0.93671875]\n",
      "2019-02-27T23:35:11.981913: step 24400, loss 0.0818992, acc [0.97890625 0.97324219 0.97294922 0.94042969]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-27T23:35:21.276464: step 24400, loss 0.301569, acc [0.97503229 0.96235822 0.95225701 0.93121365] \n",
      "\n",
      "2019-02-27T23:35:23.785234: step 24440, loss 0.0861592, acc [0.97871094 0.97392578 0.97089844 0.93984375]\n",
      "2019-02-27T23:35:26.187385: step 24480, loss 0.0883546, acc [0.98007813 0.97304687 0.97119141 0.93935547]\n",
      "2019-02-27T23:35:28.638103: step 24520, loss 0.083307, acc [0.97802734 0.97753906 0.97285156 0.94365234]\n",
      "2019-02-27T23:35:31.056154: step 24560, loss 0.082306, acc [0.98085937 0.97519531 0.97333984 0.94248047]\n",
      "2019-02-27T23:35:33.464683: step 24600, loss 0.0885334, acc [0.97929687 0.97314453 0.96826172 0.93808594]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-27T23:35:42.635234: step 24600, loss 0.292016, acc [0.97528256 0.96295888 0.95257736 0.93214468] \n",
      "\n",
      "2019-02-27T23:35:45.146980: step 24640, loss 0.0868364, acc [0.97802734 0.97568359 0.97216797 0.94091797]\n",
      "2019-02-27T23:35:47.575399: step 24680, loss 0.0810335, acc [0.97783203 0.97109375 0.97451172 0.93984375]\n",
      "2019-02-27T23:35:49.987993: step 24720, loss 0.0894865, acc [0.98095703 0.97246094 0.97167969 0.94091797]\n",
      "2019-02-27T23:35:52.392049: step 24760, loss 0.0838631, acc [0.97998047 0.97216797 0.97099609 0.9390625 ]\n",
      "2019-02-27T23:35:54.780291: step 24800, loss 0.0867491, acc [0.97861328 0.97363281 0.97275391 0.94160156]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-27T23:36:04.017804: step 24800, loss 0.296618, acc [0.97527255 0.96277868 0.95321807 0.93245503] \n",
      "\n",
      "2019-02-27T23:36:06.515176: step 24840, loss 0.0873527, acc [0.98066406 0.97382813 0.97138672 0.9421875 ]\n",
      "2019-02-27T23:36:08.938623: step 24880, loss 0.0915698, acc [0.97841797 0.97294922 0.96757812 0.93564453]\n",
      "2019-02-27T23:36:11.347200: step 24920, loss 0.0899954, acc [0.97744141 0.97304687 0.96992188 0.93730469]\n",
      "2019-02-27T23:36:13.778595: step 24960, loss 0.0946914, acc [0.97822266 0.97285156 0.96923828 0.93603516]\n",
      "2019-02-27T23:36:16.310677: step 25000, loss 0.0544107, acc [0.98242188 0.97978516 0.98212891 0.95400391]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-27T23:36:25.421708: step 25000, loss 0.28974, acc [0.97582316 0.96255844 0.95513019 0.93420697] \n",
      "\n",
      "2019-02-27T23:36:27.914645: step 25040, loss 0.0533063, acc [0.98261719 0.98125    0.98232422 0.95605469]\n",
      "2019-02-27T23:36:30.376256: step 25080, loss 0.0541612, acc [0.98417969 0.98242188 0.98300781 0.95830078]\n",
      "2019-02-27T23:36:32.803187: step 25120, loss 0.0616063, acc [0.98310547 0.98115234 0.98095703 0.95400391]\n",
      "2019-02-27T23:36:35.229124: step 25160, loss 0.0588159, acc [0.98486328 0.98242188 0.98115234 0.95644531]\n",
      "2019-02-27T23:36:37.661015: step 25200, loss 0.056928, acc [0.98056641 0.98046875 0.98193359 0.95302734]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-27T23:36:46.762126: step 25200, loss 0.303851, acc [0.97582316 0.96416022 0.95492997 0.93500786] \n",
      "\n",
      "2019-02-27T23:36:49.267424: step 25240, loss 0.0588816, acc [0.98408203 0.98164063 0.98105469 0.95556641]\n",
      "2019-02-27T23:36:51.687239: step 25280, loss 0.0587473, acc [0.98271484 0.98085937 0.98095703 0.9546875 ]\n",
      "2019-02-27T23:36:54.126072: step 25320, loss 0.0627464, acc [0.98271484 0.97949219 0.97988281 0.95136719]\n",
      "2019-02-27T23:36:56.532706: step 25360, loss 0.0605809, acc [0.984375   0.97910156 0.97880859 0.95097656]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-02-27T23:36:58.956645: step 25400, loss 0.0598993, acc [0.98212891 0.97900391 0.97958984 0.95      ]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-27T23:37:08.163524: step 25400, loss 0.312038, acc [0.97554285 0.96372974 0.95360851 0.93370641] \n",
      "\n",
      "2019-02-27T23:37:10.664853: step 25440, loss 0.0752818, acc [0.98134766 0.97832031 0.97490234 0.94736328]\n",
      "2019-02-27T23:37:13.101207: step 25480, loss 0.0735017, acc [0.9828125  0.978125   0.97460938 0.94648438]\n",
      "2019-02-27T23:37:15.520200: step 25520, loss 0.066892, acc [0.98066406 0.97617188 0.97763672 0.94736328]\n",
      "2019-02-27T23:37:17.934732: step 25560, loss 0.0668307, acc [0.98291016 0.9796875  0.97705078 0.94951172]\n",
      "2019-02-27T23:37:20.353827: step 25600, loss 0.0724016, acc [0.98105469 0.97666016 0.97724609 0.94736328]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-27T23:37:29.482263: step 25600, loss 0.303143, acc [0.97554285 0.96369971 0.95301785 0.93308573] \n",
      "\n",
      "2019-02-27T23:37:31.998980: step 25640, loss 0.0677953, acc [0.98222656 0.97695312 0.97929687 0.94960937]\n",
      "2019-02-27T23:37:34.398575: step 25680, loss 0.0696042, acc [0.98095703 0.97783203 0.97539062 0.946875  ]\n",
      "2019-02-27T23:37:36.824017: step 25720, loss 0.07646, acc [0.98222656 0.97666016 0.97421875 0.94511719]\n",
      "2019-02-27T23:37:39.237060: step 25760, loss 0.071935, acc [0.98095703 0.97861328 0.97568359 0.94785156]\n",
      "2019-02-27T23:37:41.649109: step 25800, loss 0.0779701, acc [0.98046875 0.97783203 0.97246094 0.94335938]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-27T23:37:50.852933: step 25800, loss 0.30665, acc [0.97548279 0.96169748 0.95243721 0.93152399] \n",
      "\n",
      "2019-02-27T23:37:53.328430: step 25840, loss 0.0818613, acc [0.98330078 0.97568359 0.97431641 0.94580078]\n",
      "2019-02-27T23:37:55.749410: step 25880, loss 0.077317, acc [0.97998047 0.97402344 0.97431641 0.94140625]\n",
      "2019-02-27T23:37:58.140131: step 25920, loss 0.0759351, acc [0.98027344 0.97519531 0.97597656 0.94208984]\n",
      "2019-02-27T23:38:00.563591: step 25960, loss 0.0771509, acc [0.98291016 0.9765625  0.97451172 0.94785156]\n",
      "2019-02-27T23:38:03.045576: step 26000, loss 0.0799851, acc [0.97978516 0.97783203 0.9734375  0.94521484]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-27T23:38:12.170048: step 26000, loss 0.301366, acc [0.97544274 0.96316912 0.95410906 0.93382655] \n",
      "\n",
      "2019-02-27T23:38:14.667858: step 26040, loss 0.0760743, acc [0.98261719 0.97597656 0.97294922 0.94335938]\n",
      "2019-02-27T23:38:17.080043: step 26080, loss 0.0874171, acc [0.978125   0.97236328 0.97070312 0.93779297]\n",
      "2019-02-27T23:38:19.513915: step 26120, loss 0.0740182, acc [0.97998047 0.97753906 0.97480469 0.94511719]\n",
      "2019-02-27T23:38:21.928444: step 26160, loss 0.0803878, acc [0.97832031 0.97441406 0.97412109 0.94335938]\n",
      "2019-02-27T23:38:24.338016: step 26200, loss 0.084748, acc [0.98046875 0.97460938 0.97314453 0.94326172]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-27T23:38:33.506586: step 26200, loss 0.295179, acc [0.97541271 0.96321917 0.95318804 0.93302566] \n",
      "\n",
      "2019-02-27T23:38:35.988569: step 26240, loss 0.0853219, acc [0.98007813 0.97285156 0.97236328 0.94042969]\n",
      "2019-02-27T23:38:38.389211: step 26280, loss 0.0883015, acc [0.978125   0.97412109 0.96972656 0.93642578]\n",
      "2019-02-27T23:38:40.769021: step 26320, loss 0.0815006, acc [0.97880859 0.97294922 0.97460938 0.94208984]\n",
      "2019-02-27T23:38:43.185537: step 26360, loss 0.0858274, acc [0.97900391 0.97314453 0.97392578 0.94072266]\n",
      "2019-02-27T23:38:45.643262: step 26400, loss 0.0864943, acc [0.97958984 0.97460938 0.97109375 0.93984375]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-27T23:38:55.142660: step 26400, loss 0.291065, acc [0.97516243 0.96316912 0.95270751 0.93229485] \n",
      "\n",
      "2019-02-27T23:38:57.654364: step 26440, loss 0.0890219, acc [0.97900391 0.97333984 0.97080078 0.94003906]\n",
      "2019-02-27T23:39:00.087742: step 26480, loss 0.0883125, acc [0.97939453 0.97509766 0.96992188 0.94052734]\n",
      "2019-02-27T23:39:02.528067: step 26520, loss 0.0940266, acc [0.97587891 0.97021484 0.96689453 0.93105469]\n",
      "2019-02-27T23:39:05.078001: step 26560, loss 0.0582191, acc [0.98691406 0.97900391 0.98037109 0.95585937]\n",
      "2019-02-27T23:39:07.480150: step 26600, loss 0.0618743, acc [0.98261719 0.97783203 0.97988281 0.94921875]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-27T23:39:16.658170: step 26600, loss 0.30328, acc [0.97521249 0.96357957 0.95475978 0.93426704] \n",
      "\n",
      "2019-02-27T23:39:19.153007: step 26640, loss 0.0569373, acc [0.98378906 0.98251953 0.98242188 0.95791016]\n",
      "2019-02-27T23:39:21.569560: step 26680, loss 0.0530621, acc [0.98457031 0.98378906 0.98203125 0.95761719]\n",
      "2019-02-27T23:39:23.978593: step 26720, loss 0.0607601, acc [0.98515625 0.98046875 0.97958984 0.95488281]\n",
      "2019-02-27T23:39:26.380229: step 26760, loss 0.0644717, acc [0.98154297 0.97861328 0.97929687 0.95146484]\n",
      "2019-02-27T23:39:28.793329: step 26800, loss 0.0596903, acc [0.98095703 0.97978516 0.98066406 0.95146484]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-27T23:39:37.916253: step 26800, loss 0.309968, acc [0.97564296 0.96403007 0.95520027 0.93566859] \n",
      "\n",
      "2019-02-27T23:39:40.400176: step 26840, loss 0.0622533, acc [0.98251953 0.98046875 0.97988281 0.95351562]\n",
      "2019-02-27T23:39:42.789409: step 26880, loss 0.0625561, acc [0.98515625 0.97919922 0.97861328 0.95224609]\n",
      "2019-02-27T23:39:45.242132: step 26920, loss 0.0634891, acc [0.98183594 0.97792969 0.98115234 0.95214844]\n",
      "2019-02-27T23:39:47.667575: step 26960, loss 0.0704105, acc [0.98066406 0.97568359 0.97548828 0.94570312]\n",
      "2019-02-27T23:39:50.086568: step 27000, loss 0.0608472, acc [0.98134766 0.97763672 0.98066406 0.95048828]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-27T23:39:59.215006: step 27000, loss 0.308136, acc [0.97626365 0.96398002 0.95541051 0.9357687 ] \n",
      "\n",
      "2019-02-27T23:40:01.774322: step 27040, loss 0.0664054, acc [0.98037109 0.97929687 0.97841797 0.94785156]\n",
      "2019-02-27T23:40:04.183893: step 27080, loss 0.0587679, acc [0.98320312 0.98027344 0.98056641 0.95322266]\n",
      "2019-02-27T23:40:06.591524: step 27120, loss 0.0642307, acc [0.98115234 0.97861328 0.98046875 0.95214844]\n",
      "2019-02-27T23:40:09.013448: step 27160, loss 0.065568, acc [0.98505859 0.97871094 0.978125   0.95244141]\n",
      "2019-02-27T23:40:11.426026: step 27200, loss 0.0804493, acc [0.97978516 0.97402344 0.97226563 0.94101563]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-27T23:40:20.575218: step 27200, loss 0.303629, acc [0.97607344 0.9641402  0.95401896 0.93483767] \n",
      "\n",
      "2019-02-27T23:40:23.086966: step 27240, loss 0.0673308, acc [0.98261719 0.97529297 0.97646484 0.94638672]\n",
      "2019-02-27T23:40:25.514887: step 27280, loss 0.0678159, acc [0.98193359 0.97939453 0.97646484 0.94892578]\n",
      "2019-02-27T23:40:27.928426: step 27320, loss 0.0719876, acc [0.98164063 0.97763672 0.97685547 0.94736328]\n",
      "2019-02-27T23:40:30.357340: step 27360, loss 0.0781266, acc [0.98076172 0.97548828 0.97333984 0.94404297]\n",
      "2019-02-27T23:40:32.779309: step 27400, loss 0.0771294, acc [0.97832031 0.97353516 0.97558594 0.94277344]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-27T23:40:41.851156: step 27400, loss 0.297918, acc [0.97604341 0.96376978 0.95418915 0.93491776] \n",
      "\n",
      "2019-02-27T23:40:44.347526: step 27440, loss 0.0749726, acc [0.97626953 0.97626953 0.97529297 0.94208984]\n",
      "2019-02-27T23:40:46.774953: step 27480, loss 0.0713508, acc [0.97939453 0.97539062 0.97675781 0.94521484]\n",
      "2019-02-27T23:40:49.198909: step 27520, loss 0.0756189, acc [0.97929687 0.97470703 0.97431641 0.94169922]\n",
      "2019-02-27T23:40:51.625341: step 27560, loss 0.0832545, acc [0.98212891 0.97558594 0.97314453 0.94414062]\n",
      "2019-02-27T23:40:54.036399: step 27600, loss 0.0720482, acc [0.97841797 0.97626953 0.97519531 0.94306641]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-27T23:41:03.115732: step 27600, loss 0.305424, acc [0.97631371 0.96344943 0.95409905 0.93436715] \n",
      "\n",
      "2019-02-27T23:41:05.652234: step 27640, loss 0.0789242, acc [0.98300781 0.97929687 0.97402344 0.94794922]\n",
      "2019-02-27T23:41:08.074699: step 27680, loss 0.079063, acc [0.98183594 0.97167969 0.97255859 0.94082031]\n",
      "2019-02-27T23:41:10.497662: step 27720, loss 0.0784429, acc [0.97929687 0.97509766 0.97539062 0.94365234]\n",
      "2019-02-27T23:41:12.900830: step 27760, loss 0.0774516, acc [0.98037109 0.97705078 0.97607422 0.94511719]\n",
      "2019-02-27T23:41:15.317298: step 27800, loss 0.0756888, acc [0.97939453 0.97919922 0.97646484 0.94746094]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-27T23:41:24.507672: step 27800, loss 0.304548, acc [0.97572305 0.96426033 0.95443943 0.93488773] \n",
      "\n",
      "2019-02-27T23:41:27.016938: step 27840, loss 0.0846008, acc [0.97763672 0.97587891 0.97363281 0.94091797]\n",
      "2019-02-27T23:41:29.449324: step 27880, loss 0.0910391, acc [0.9765625  0.9765625  0.9703125  0.93896484]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-02-27T23:41:31.870236: step 27920, loss 0.0916511, acc [0.97861328 0.97177734 0.97275391 0.93828125]\n",
      "2019-02-27T23:41:34.297659: step 27960, loss 0.0809425, acc [0.97949219 0.97236328 0.97392578 0.93955078]\n",
      "2019-02-27T23:41:36.742497: step 28000, loss 0.0876374, acc [0.97949219 0.97324219 0.96914062 0.93613281]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-27T23:41:45.898117: step 28000, loss 0.297008, acc [0.97625364 0.96412017 0.95383876 0.93473756] \n",
      "\n",
      "2019-02-27T23:41:48.405894: step 28040, loss 0.0911461, acc [0.97900391 0.97128906 0.9703125  0.93642578]\n",
      "2019-02-27T23:41:50.834312: step 28080, loss 0.0820273, acc [0.97939453 0.97490234 0.971875   0.94091797]\n",
      "2019-02-27T23:41:53.381277: step 28120, loss 0.0566547, acc [0.98369141 0.98134766 0.98232422 0.95742187]\n",
      "2019-02-27T23:41:55.802750: step 28160, loss 0.0564224, acc [0.98242188 0.98017578 0.98154297 0.95439453]\n",
      "2019-02-27T23:41:58.205375: step 28200, loss 0.0551958, acc [0.9828125  0.97919922 0.98134766 0.95371094]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-27T23:42:07.398743: step 28200, loss 0.304049, acc [0.97587322 0.9642303  0.95518025 0.93547838] \n",
      "\n",
      "2019-02-27T23:42:09.886682: step 28240, loss 0.0558877, acc [0.98505859 0.97949219 0.98232422 0.95654297]\n",
      "2019-02-27T23:42:12.307680: step 28280, loss 0.0546969, acc [0.98349609 0.98076172 0.98242188 0.95517578]\n",
      "2019-02-27T23:42:14.712271: step 28320, loss 0.0586683, acc [0.98466797 0.98134766 0.98037109 0.95517578]\n",
      "2019-02-27T23:42:17.113408: step 28360, loss 0.0587885, acc [0.98339844 0.97861328 0.98066406 0.95292969]\n",
      "2019-02-27T23:42:19.542819: step 28400, loss 0.0645558, acc [0.98017578 0.97919922 0.97871094 0.94980469]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-27T23:42:28.553161: step 28400, loss 0.305314, acc [0.9763938  0.96416022 0.95528036 0.93623923] \n",
      "\n",
      "2019-02-27T23:42:31.051516: step 28440, loss 0.0653831, acc [0.98261719 0.98115234 0.97832031 0.95292969]\n",
      "2019-02-27T23:42:33.485389: step 28480, loss 0.0597197, acc [0.98300781 0.98125    0.98066406 0.95371094]\n",
      "2019-02-27T23:42:35.945553: step 28520, loss 0.0632697, acc [0.98261719 0.97919922 0.97890625 0.95146484]\n",
      "2019-02-27T23:42:38.363554: step 28560, loss 0.0628284, acc [0.98330078 0.98017578 0.97900391 0.95332031]\n",
      "2019-02-27T23:42:40.794451: step 28600, loss 0.0631622, acc [0.98232422 0.978125   0.98076172 0.95166016]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-27T23:42:49.948166: step 28600, loss 0.31329, acc [0.97685431 0.96474086 0.95475978 0.93614913] \n",
      "\n",
      "2019-02-27T23:42:52.445527: step 28640, loss 0.0593251, acc [0.98349609 0.97939453 0.9796875  0.95351562]\n",
      "2019-02-27T23:42:54.876423: step 28680, loss 0.0677033, acc [0.98330078 0.98017578 0.97792969 0.95263672]\n",
      "2019-02-27T23:42:57.313815: step 28720, loss 0.0684346, acc [0.98242188 0.97773438 0.97910156 0.94960937]\n",
      "2019-02-27T23:42:59.735740: step 28760, loss 0.0636376, acc [0.98398438 0.98105469 0.978125   0.95341797]\n",
      "2019-02-27T23:43:02.170111: step 28800, loss 0.0740409, acc [0.98359375 0.97558594 0.97587891 0.94804687]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-27T23:43:11.336198: step 28800, loss 0.310454, acc [0.97642383 0.96415021 0.95437936 0.93534824] \n",
      "\n",
      "2019-02-27T23:43:13.852904: step 28840, loss 0.0645039, acc [0.98378906 0.97871094 0.97792969 0.94990234]\n",
      "2019-02-27T23:43:16.285326: step 28880, loss 0.0578903, acc [0.98427734 0.97871094 0.9796875  0.95185547]\n",
      "2019-02-27T23:43:18.708751: step 28920, loss 0.0724713, acc [0.97900391 0.97763672 0.97744141 0.94736328]\n",
      "2019-02-27T23:43:21.114364: step 28960, loss 0.0709231, acc [0.98056641 0.97773438 0.97753906 0.94794922]\n",
      "2019-02-27T23:43:23.531858: step 29000, loss 0.0711909, acc [0.984375   0.97646484 0.97587891 0.94853516]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-27T23:43:32.682367: step 29000, loss 0.310566, acc [0.97607344 0.96393997 0.95410906 0.93475758] \n",
      "\n",
      "2019-02-27T23:43:35.178259: step 29040, loss 0.0733213, acc [0.98076172 0.97714844 0.97568359 0.94599609]\n",
      "2019-02-27T23:43:37.584853: step 29080, loss 0.0698287, acc [0.98300781 0.97519531 0.97763672 0.94912109]\n",
      "2019-02-27T23:43:40.027176: step 29120, loss 0.073996, acc [0.98378906 0.97558594 0.97470703 0.94472656]\n",
      "2019-02-27T23:43:42.451611: step 29160, loss 0.0685272, acc [0.98339844 0.97871094 0.97724609 0.94990234]\n",
      "2019-02-27T23:43:44.890940: step 29200, loss 0.0779053, acc [0.98261719 0.97529297 0.97431641 0.94580078]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-27T23:43:54.052610: step 29200, loss 0.303934, acc [0.97653395 0.96434042 0.95426924 0.93580875] \n",
      "\n",
      "2019-02-27T23:43:56.546951: step 29240, loss 0.077066, acc [0.98144531 0.97919922 0.97597656 0.94785156]\n",
      "2019-02-27T23:43:58.987769: step 29280, loss 0.0854251, acc [0.98076172 0.975      0.97197266 0.94384766]\n",
      "2019-02-27T23:44:01.458843: step 29320, loss 0.0786978, acc [0.98251953 0.9765625  0.97460938 0.94599609]\n",
      "2019-02-27T23:44:03.875853: step 29360, loss 0.0769978, acc [0.97861328 0.97353516 0.97646484 0.94462891]\n",
      "2019-02-27T23:44:06.296273: step 29400, loss 0.0877495, acc [0.97919922 0.97197266 0.97050781 0.93681641]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-27T23:44:15.461369: step 29400, loss 0.298341, acc [0.9757631  0.96335933 0.95434933 0.93439718] \n",
      "\n",
      "2019-02-27T23:44:17.967163: step 29440, loss 0.0839173, acc [0.9828125  0.97285156 0.97148437 0.9421875 ]\n",
      "2019-02-27T23:44:20.388142: step 29480, loss 0.0746202, acc [0.98085937 0.97675781 0.97519531 0.94501953]\n",
      "2019-02-27T23:44:22.815572: step 29520, loss 0.0763784, acc [0.98115234 0.97900391 0.97226563 0.94658203]\n",
      "2019-02-27T23:44:25.244481: step 29560, loss 0.0709652, acc [0.98291016 0.97539062 0.97783203 0.94736328]\n",
      "2019-02-27T23:44:27.682819: step 29600, loss 0.0794948, acc [0.98027344 0.97441406 0.97382813 0.94326172]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-27T23:44:37.147500: step 29600, loss 0.298413, acc [0.97599335 0.96472084 0.95554065 0.93676981] \n",
      "\n",
      "2019-02-27T23:44:39.760927: step 29640, loss 0.0781435, acc [0.98349609 0.978125   0.97392578 0.94853516]\n",
      "2019-02-27T23:44:42.330208: step 29680, loss 0.0468727, acc [0.98535156 0.98427734 0.98466797 0.9609375 ]\n",
      "2019-02-27T23:44:44.751240: step 29720, loss 0.0497586, acc [0.98613281 0.9828125  0.98359375 0.96103516]\n",
      "2019-02-27T23:44:47.176133: step 29760, loss 0.0465524, acc [0.98652344 0.98339844 0.98349609 0.96181641]\n",
      "2019-02-27T23:44:49.610502: step 29800, loss 0.0514851, acc [0.98476562 0.98173828 0.98505859 0.95957031]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-27T23:44:58.751293: step 29800, loss 0.31867, acc [0.97571304 0.96521138 0.95651173 0.93756069] \n",
      "\n",
      "2019-02-27T23:45:01.247460: step 29840, loss 0.0459798, acc [0.98457031 0.98398438 0.98388672 0.959375  ]\n",
      "2019-02-27T23:45:03.656521: step 29880, loss 0.0497153, acc [0.98623047 0.98271484 0.98476562 0.96064453]\n",
      "2019-02-27T23:45:06.068075: step 29920, loss 0.055305, acc [0.98339844 0.97998047 0.98251953 0.95683594]\n",
      "2019-02-27T23:45:08.482118: step 29960, loss 0.0615171, acc [0.98466797 0.98154297 0.97900391 0.95390625]\n",
      "2019-02-27T23:45:10.931856: step 30000, loss 0.0552485, acc [0.98300781 0.9796875  0.98164063 0.95400391]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-27T23:45:19.971008: step 30000, loss 0.312138, acc [0.97667411 0.96520137 0.95594109 0.93764078] \n",
      "\n",
      "2019-02-27T23:45:22.486983: step 30040, loss 0.058924, acc [0.98300781 0.98330078 0.98105469 0.95595703]\n",
      "2019-02-27T23:45:24.912367: step 30080, loss 0.0584056, acc [0.98427734 0.98115234 0.98095703 0.95585937]\n",
      "2019-02-27T23:45:27.345249: step 30120, loss 0.0606668, acc [0.98574219 0.98193359 0.98085937 0.95722656]\n",
      "2019-02-27T23:45:29.777139: step 30160, loss 0.0543497, acc [0.98525391 0.98134766 0.98300781 0.95673828]\n",
      "2019-02-27T23:45:32.210021: step 30200, loss 0.0551096, acc [0.98632812 0.98085937 0.98203125 0.95771484]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-27T23:45:41.379126: step 30200, loss 0.31644, acc [0.9763037  0.96502117 0.95567079 0.93717026] \n",
      "\n",
      "2019-02-27T23:45:43.912165: step 30240, loss 0.0556692, acc [0.98408203 0.98066406 0.98076172 0.95556641]\n",
      "2019-02-27T23:45:46.330663: step 30280, loss 0.0528985, acc [0.98271484 0.98105469 0.98310547 0.95498047]\n",
      "2019-02-27T23:45:48.732792: step 30320, loss 0.059239, acc [0.98330078 0.98095703 0.9796875  0.95302734]\n",
      "2019-02-27T23:45:51.135917: step 30360, loss 0.067328, acc [0.97939453 0.97832031 0.97841797 0.94882813]\n",
      "2019-02-27T23:45:53.540525: step 30400, loss 0.0640268, acc [0.98427734 0.98027344 0.98095703 0.95566406]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-27T23:46:02.790871: step 30400, loss 0.318339, acc [0.97617355 0.96502117 0.95587102 0.93669974] \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-02-27T23:46:05.307081: step 30440, loss 0.0692909, acc [0.98271484 0.97558594 0.97773438 0.94882813]\n",
      "2019-02-27T23:46:07.726123: step 30480, loss 0.0628358, acc [0.98330078 0.9796875  0.97978516 0.95263672]\n",
      "2019-02-27T23:46:10.159453: step 30520, loss 0.0639458, acc [0.98417969 0.98046875 0.97744141 0.95185547]\n",
      "2019-02-27T23:46:12.627551: step 30560, loss 0.0759303, acc [0.97978516 0.97519531 0.97695312 0.94501953]\n",
      "2019-02-27T23:46:15.057456: step 30600, loss 0.0639005, acc [0.98183594 0.97871094 0.97763672 0.94824219]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-27T23:46:24.247848: step 30600, loss 0.323103, acc [0.97553284 0.96387991 0.95413909 0.934237  ] \n",
      "\n",
      "2019-02-27T23:46:26.744219: step 30640, loss 0.0820513, acc [0.98183594 0.97685547 0.97382813 0.94570312]\n",
      "2019-02-27T23:46:29.181069: step 30680, loss 0.0706101, acc [0.97851562 0.97792969 0.97695312 0.94619141]\n",
      "2019-02-27T23:46:31.609538: step 30720, loss 0.0687417, acc [0.98076172 0.97871094 0.97753906 0.94980469]\n",
      "2019-02-27T23:46:34.043857: step 30760, loss 0.0741566, acc [0.97871094 0.97636719 0.97607422 0.94462891]\n",
      "2019-02-27T23:46:36.459876: step 30800, loss 0.0750592, acc [0.97998047 0.97587891 0.97578125 0.94599609]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-27T23:46:45.701852: step 30800, loss 0.312863, acc [0.97600336 0.96465076 0.95437936 0.93547838] \n",
      "\n",
      "2019-02-27T23:46:48.191773: step 30840, loss 0.0739208, acc [0.98095703 0.97675781 0.97324219 0.94375   ]\n",
      "2019-02-27T23:46:50.645038: step 30880, loss 0.0790255, acc [0.98076172 0.97519531 0.97412109 0.94462891]\n",
      "2019-02-27T23:46:53.062994: step 30920, loss 0.0789435, acc [0.98066406 0.97587891 0.97412109 0.94433594]\n",
      "2019-02-27T23:46:55.475573: step 30960, loss 0.0766874, acc [0.98203125 0.97636719 0.975      0.94609375]\n",
      "2019-02-27T23:46:57.910457: step 31000, loss 0.0884555, acc [0.98076172 0.97607422 0.9703125  0.94345703]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-27T23:47:07.114686: step 31000, loss 0.302074, acc [0.97631371 0.96433041 0.95471974 0.93518806] \n",
      "\n",
      "2019-02-27T23:47:09.610063: step 31040, loss 0.0760088, acc [0.98095703 0.97861328 0.97402344 0.94619141]\n",
      "2019-02-27T23:47:12.036994: step 31080, loss 0.0766884, acc [0.98154297 0.97675781 0.97490234 0.94697266]\n",
      "2019-02-27T23:47:14.481284: step 31120, loss 0.0751534, acc [0.98271484 0.97587891 0.97324219 0.94404297]\n",
      "2019-02-27T23:47:16.893334: step 31160, loss 0.0738163, acc [0.98046875 0.97675781 0.97578125 0.94541016]\n",
      "2019-02-27T23:47:19.295960: step 31200, loss 0.0832053, acc [0.97832031 0.97519531 0.97304687 0.94316406]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-27T23:47:28.487343: step 31200, loss 0.303599, acc [0.9757631  0.96431038 0.95504009 0.9352281 ] \n",
      "\n",
      "2019-02-27T23:47:31.127058: step 31240, loss 0.0489015, acc [0.98623047 0.98310547 0.98466797 0.96064453]\n",
      "2019-02-27T23:47:33.544563: step 31280, loss 0.046863, acc [0.98574219 0.98330078 0.98505859 0.96191406]\n",
      "2019-02-27T23:47:35.979430: step 31320, loss 0.0488701, acc [0.98515625 0.98134766 0.98408203 0.95908203]\n",
      "2019-02-27T23:47:38.394455: step 31360, loss 0.049635, acc [0.98798828 0.98125    0.98300781 0.96083984]\n",
      "2019-02-27T23:47:40.805017: step 31400, loss 0.0476817, acc [0.98554688 0.984375   0.98525391 0.9625    ]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-27T23:47:50.036082: step 31400, loss 0.317869, acc [0.97685431 0.96456066 0.95689215 0.93780096] \n",
      "\n",
      "2019-02-27T23:47:52.533486: step 31440, loss 0.0518769, acc [0.98632812 0.98027344 0.98291016 0.95800781]\n",
      "2019-02-27T23:47:54.943510: step 31480, loss 0.0518947, acc [0.98417969 0.98300781 0.98271484 0.9578125 ]\n",
      "2019-02-27T23:47:57.349113: step 31520, loss 0.0521387, acc [0.98398438 0.98134766 0.98349609 0.95644531]\n",
      "2019-02-27T23:47:59.765130: step 31560, loss 0.0482579, acc [0.98525391 0.98193359 0.98466797 0.95771484]\n",
      "2019-02-27T23:48:02.227276: step 31600, loss 0.0509843, acc [0.98623047 0.98261719 0.98125    0.95761719]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-27T23:48:11.385924: step 31600, loss 0.327095, acc [0.97677422 0.96521138 0.95650172 0.93866191] \n",
      "\n",
      "2019-02-27T23:48:13.871780: step 31640, loss 0.0578941, acc [0.98349609 0.97978516 0.98369141 0.95683594]\n",
      "2019-02-27T23:48:16.272474: step 31680, loss 0.0505378, acc [0.98613281 0.98134766 0.98300781 0.95771484]\n",
      "2019-02-27T23:48:18.685465: step 31720, loss 0.0575737, acc [0.98681641 0.98222656 0.98095703 0.95664063]\n",
      "2019-02-27T23:48:21.079657: step 31760, loss 0.0623147, acc [0.98417969 0.98056641 0.98095703 0.95410156]\n",
      "2019-02-27T23:48:23.474348: step 31800, loss 0.0622252, acc [0.98427734 0.98017578 0.97919922 0.953125  ]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-27T23:48:32.600802: step 31800, loss 0.328406, acc [0.97755509 0.9652214  0.95552063 0.93765079] \n",
      "\n",
      "2019-02-27T23:48:35.096134: step 31840, loss 0.0622074, acc [0.98271484 0.98105469 0.98076172 0.95244141]\n",
      "2019-02-27T23:48:37.486857: step 31880, loss 0.0551308, acc [0.984375   0.97939453 0.98115234 0.95478516]\n",
      "2019-02-27T23:48:39.910810: step 31920, loss 0.0629657, acc [0.98330078 0.98222656 0.97998047 0.95576172]\n",
      "2019-02-27T23:48:42.332284: step 31960, loss 0.0645159, acc [0.98447266 0.978125   0.97851562 0.95263672]\n",
      "2019-02-27T23:48:44.737391: step 32000, loss 0.0638758, acc [0.98398438 0.978125   0.97978516 0.95048828]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-27T23:48:54.005698: step 32000, loss 0.321267, acc [0.97643384 0.96451061 0.95526034 0.93636937] \n",
      "\n",
      "2019-02-27T23:48:56.488632: step 32040, loss 0.061881, acc [0.984375   0.97890625 0.97929687 0.95292969]\n",
      "2019-02-27T23:48:58.883818: step 32080, loss 0.0675996, acc [0.98173828 0.97802734 0.97841797 0.94853516]\n",
      "2019-02-27T23:49:01.300829: step 32120, loss 0.0620214, acc [0.98359375 0.97841797 0.97939453 0.95273438]\n",
      "2019-02-27T23:49:03.727337: step 32160, loss 0.0694032, acc [0.98261719 0.97783203 0.97851562 0.94960937]\n",
      "2019-02-27T23:49:06.135841: step 32200, loss 0.062052, acc [0.98203125 0.97900391 0.98027344 0.95126953]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-27T23:49:15.232036: step 32200, loss 0.32431, acc [0.97687433 0.96420026 0.95532041 0.9365796 ] \n",
      "\n",
      "2019-02-27T23:49:17.760108: step 32240, loss 0.0690107, acc [0.98232422 0.97822266 0.97861328 0.94980469]\n",
      "2019-02-27T23:49:20.178107: step 32280, loss 0.0708655, acc [0.98203125 0.97939453 0.97714844 0.95107422]\n",
      "2019-02-27T23:49:22.567661: step 32320, loss 0.0687991, acc [0.98095703 0.97617188 0.97705078 0.94814453]\n",
      "2019-02-27T23:49:25.008930: step 32360, loss 0.0769011, acc [0.98232422 0.97587891 0.97539062 0.94648438]\n",
      "2019-02-27T23:49:27.429907: step 32400, loss 0.0647364, acc [0.98349609 0.97636719 0.97822266 0.94716797]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-27T23:49:36.598302: step 32400, loss 0.311449, acc [0.97622361 0.96415021 0.95519026 0.93606904] \n",
      "\n",
      "2019-02-27T23:49:39.093138: step 32440, loss 0.0725477, acc [0.98134766 0.97509766 0.97548828 0.94414062]\n",
      "2019-02-27T23:49:41.517587: step 32480, loss 0.0737383, acc [0.98066406 0.97607422 0.97460938 0.94384766]\n",
      "2019-02-27T23:49:43.921205: step 32520, loss 0.0640127, acc [0.98320312 0.97998047 0.97861328 0.95205078]\n",
      "2019-02-27T23:49:46.331821: step 32560, loss 0.0741844, acc [0.98222656 0.97695312 0.97646484 0.94589844]\n",
      "2019-02-27T23:49:48.772586: step 32600, loss 0.072699, acc [0.98183594 0.97666016 0.97675781 0.94785156]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-27T23:49:57.870723: step 32600, loss 0.308656, acc [0.97691437 0.96545165 0.95627146 0.9381113 ] \n",
      "\n",
      "2019-02-27T23:50:00.357668: step 32640, loss 0.0718171, acc [0.98183594 0.97597656 0.97744141 0.94726562]\n",
      "2019-02-27T23:50:02.813421: step 32680, loss 0.0755792, acc [0.97919922 0.97880859 0.97480469 0.94589844]\n",
      "2019-02-27T23:50:05.233849: step 32720, loss 0.0731981, acc [0.97998047 0.97666016 0.97460938 0.94355469]\n",
      "2019-02-27T23:50:07.640442: step 32760, loss 0.08058, acc [0.98095703 0.978125   0.97324219 0.94560547]\n",
      "2019-02-27T23:50:10.188893: step 32800, loss 0.0555576, acc [0.98388672 0.98164063 0.98125    0.95576172]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-27T23:50:19.347045: step 32800, loss 0.304281, acc [0.97704452 0.9656719  0.95653175 0.93840163] \n",
      "\n",
      "2019-02-27T23:50:21.821146: step 32840, loss 0.0485484, acc [0.98271484 0.98232422 0.98515625 0.95771484]\n",
      "2019-02-27T23:50:24.225747: step 32880, loss 0.0501356, acc [0.98476562 0.98193359 0.98427734 0.95898438]\n",
      "2019-02-27T23:50:26.628826: step 32920, loss 0.0472409, acc [0.98710937 0.98349609 0.98476562 0.9625    ]\n",
      "2019-02-27T23:50:29.030461: step 32960, loss 0.0585825, acc [0.98203125 0.98125    0.98095703 0.95322266]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-02-27T23:50:31.441023: step 33000, loss 0.0496808, acc [0.98623047 0.98212891 0.98349609 0.959375  ]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-27T23:50:40.619070: step 33000, loss 0.323782, acc [0.97749502 0.96586211 0.95637157 0.93900229] \n",
      "\n",
      "2019-02-27T23:50:43.119904: step 33040, loss 0.0499937, acc [0.98691406 0.98378906 0.98359375 0.96025391]\n",
      "2019-02-27T23:50:45.531457: step 33080, loss 0.0586996, acc [0.98476562 0.98164063 0.98193359 0.95664063]\n",
      "2019-02-27T23:50:47.948964: step 33120, loss 0.0555299, acc [0.98642578 0.98193359 0.98222656 0.95761719]\n",
      "2019-02-27T23:50:50.407639: step 33160, loss 0.0569566, acc [0.9828125  0.98144531 0.98125    0.95644531]\n",
      "2019-02-27T23:50:52.843001: step 33200, loss 0.0536696, acc [0.98535156 0.98330078 0.98261719 0.95996094]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-27T23:51:02.009088: step 33200, loss 0.322639, acc [0.97646387 0.96589214 0.95676201 0.93873199] \n",
      "\n",
      "2019-02-27T23:51:04.509921: step 33240, loss 0.0513304, acc [0.98378906 0.98349609 0.98271484 0.95820313]\n",
      "2019-02-27T23:51:06.948796: step 33280, loss 0.0549867, acc [0.98417969 0.98310547 0.98242188 0.95917969]\n",
      "2019-02-27T23:51:09.382134: step 33320, loss 0.0585748, acc [0.98408203 0.98144531 0.98125    0.95712891]\n",
      "2019-02-27T23:51:11.788728: step 33360, loss 0.0615804, acc [0.98417969 0.98017578 0.97861328 0.95185547]\n",
      "2019-02-27T23:51:14.219625: step 33400, loss 0.0593345, acc [0.98378906 0.98251953 0.97910156 0.95458984]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-27T23:51:23.482434: step 33400, loss 0.329206, acc [0.97667411 0.96543163 0.95513019 0.93723033] \n",
      "\n",
      "2019-02-27T23:51:25.997263: step 33440, loss 0.0608552, acc [0.98310547 0.98085937 0.97929687 0.953125  ]\n",
      "2019-02-27T23:51:28.396456: step 33480, loss 0.0543505, acc [0.98339844 0.98007813 0.98291016 0.95498047]\n",
      "2019-02-27T23:51:30.810450: step 33520, loss 0.059248, acc [0.98349609 0.98105469 0.97988281 0.95273438]\n",
      "2019-02-27T23:51:33.221012: step 33560, loss 0.0641236, acc [0.98486328 0.97939453 0.97802734 0.95175781]\n",
      "2019-02-27T23:51:35.613265: step 33600, loss 0.0610191, acc [0.98574219 0.98085937 0.97890625 0.95439453]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-27T23:51:44.765422: step 33600, loss 0.319399, acc [0.97680425 0.96480093 0.95654176 0.93788105] \n",
      "\n",
      "2019-02-27T23:51:47.289074: step 33640, loss 0.0566896, acc [0.98447266 0.98095703 0.98134766 0.95546875]\n",
      "2019-02-27T23:51:49.703107: step 33680, loss 0.0660179, acc [0.98310547 0.98105469 0.97773438 0.95195312]\n",
      "2019-02-27T23:51:52.131029: step 33720, loss 0.0687889, acc [0.98242188 0.97753906 0.97861328 0.94892578]\n",
      "2019-02-27T23:51:54.544567: step 33760, loss 0.0659235, acc [0.98310547 0.97958984 0.97900391 0.95332031]\n",
      "2019-02-27T23:51:56.957608: step 33800, loss 0.0574939, acc [0.98388672 0.97714844 0.98027344 0.95263672]\n",
      "\n",
      "Evaluation:\n"
     ]
    }
   ],
   "source": [
    "y_train = [train_y_label1_one_hot,train_y_label2_one_hot,train_y_label3_one_hot]\n",
    "y_test = [test_y_label1_one_hot,test_y_label2_one_hot,test_y_label3_one_hot]\n",
    "train(x_train, y_train,  x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_y_label3_one_hot.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y_label3_one_hot.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
