{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.utils import shuffle\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "特征如下：['ITEM_NAME' 'TYPE'] \n",
      "\n",
      "数据数目:  499447\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('data/train/trainx.txt', sep='\\t')\n",
    "features = data.columns.values\n",
    "print(\"特征如下：{} \\n\".format(features))\n",
    "print(\"数据数目: \", len(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据清洗\n",
    "* 替换一些字符 类似于 （ ） ‘ ’ _"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ITEM_NAME</th>\n",
       "      <th>TYPE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>腾讯QQ黄钻三个月QQ黄钻3个月季卡官方自动充值可查时间可续费</td>\n",
       "      <td>本地生活--游戏充值--QQ充值</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>腾讯q币充值30元qq币30qb30q币自动充值</td>\n",
       "      <td>本地生活--游戏充值--QQ充值</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>腾讯QQ红钻包月卡 QQ红钻一个月QQ红钻1个月</td>\n",
       "      <td>本地生活--游戏充值--QQ充值</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>腾讯QQ蓝钻贵族31个月直充</td>\n",
       "      <td>本地生活--游戏充值--QQ充值</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>腾讯QQ币148元148QQ币148个直充148Q币148个Q币148个QQB★自动充值</td>\n",
       "      <td>本地生活--游戏充值--QQ充值</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      ITEM_NAME              TYPE\n",
       "0               腾讯QQ黄钻三个月QQ黄钻3个月季卡官方自动充值可查时间可续费  本地生活--游戏充值--QQ充值\n",
       "1                      腾讯q币充值30元qq币30qb30q币自动充值  本地生活--游戏充值--QQ充值\n",
       "2                      腾讯QQ红钻包月卡 QQ红钻一个月QQ红钻1个月  本地生活--游戏充值--QQ充值\n",
       "3                                腾讯QQ蓝钻贵族31个月直充  本地生活--游戏充值--QQ充值\n",
       "4  腾讯QQ币148元148QQ币148个直充148Q币148个Q币148个QQB★自动充值  本地生活--游戏充值--QQ充值"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "from zhon.hanzi import punctuation\n",
    "from string import punctuation as english_punc\n",
    "def  rm_punc(strs):\n",
    "    return re.sub(r\"[{}]+\".format(punctuation + english_punc),\" \",strs)\n",
    "data[features[0]]  = data[features[0]].apply(rm_punc)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练数据数目:   399558\n",
      "测试数据数目:   99889\n",
      "(399558,) (99889,)\n"
     ]
    }
   ],
   "source": [
    "# 因为有的label里本身就存在空格，所以先要把空格去掉，再去掉- - ， 再以空格划分\n",
    "data = shuffle(data)\n",
    "data_y = data[features[1]]\n",
    "\n",
    "# 取训练数据和测试数据\n",
    "train = data.sample(frac= 0.8).reset_index()\n",
    "test = data.sample(frac= 0.2).reset_index()\n",
    "\n",
    "\n",
    "print(\"训练数据数目:  \", len(train))\n",
    "print(\"测试数据数目:  \", len(test))\n",
    "\n",
    "train_x  = train[features[0]]\n",
    "train_y  = train[features[1]]\n",
    "\n",
    "test_x  = test[features[0]]\n",
    "test_y  = test[features[1]]\n",
    "\n",
    "print(train_x.shape, test_x.shape)\n",
    "\n",
    "# 分出数据集 训练集 测试集 数据集用来训练词向量\n",
    "data_y = data_y.apply(lambda x: x.replace(\" \", \"\").replace(\"--\", \" \").replace(\"/\", \"\")).tolist()\n",
    "data_y= [each.split() for each in data_y]\n",
    "\n",
    "train_y = train_y.apply(lambda x: x.replace(\" \", \"\").replace(\"--\", \" \").replace(\"/\", \"\")).tolist()\n",
    "train_y = [each.split() for each in train_y]\n",
    "\n",
    "test_y = test_y.apply(lambda x: x.replace(\" \", \"\").replace(\"--\", \" \").replace(\"/\", \"\")).tolist()\n",
    "test_y = [each.split() for each in test_y]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 分词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba\n",
    "from random import shuffle\n",
    "def participle(data, doshuffle = False):\n",
    "    words = []\n",
    "    for i in range(len(data)):\n",
    "        result=[]\n",
    "        seg_list = jieba.cut(data.iloc[i])\n",
    "        for w in seg_list :#读取每一行分词\n",
    "            if w != \" \":\n",
    "                result.append(w)\n",
    "        # 打乱每条数据的分词\n",
    "        if doshuffle:\n",
    "            shuffle(result)\n",
    "        words.append(result)#将该行分词写入列表形式的总分词列表\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\Richado\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 1.653 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "# 每条数据的分词结果是个长度不等的词列表\n",
    "word_data = participle(data[features[0]], doshuffle = False)\n",
    "word_data_train = participle(train_x,doshuffle = False)\n",
    "word_data_test = participle(test_x, doshuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'腾讯QQ黄钻三个月QQ黄钻3个月季卡官方自动充值可查时间可续费'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[features[0]][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['致橡树',\n",
       " '圣诞树',\n",
       " '商店',\n",
       " '橱窗',\n",
       " '玻璃',\n",
       " '贴',\n",
       " '节日',\n",
       " '圣诞老人',\n",
       " '可',\n",
       " '移除',\n",
       " '墙',\n",
       " '贴纸',\n",
       " '卧室',\n",
       " '墙壁',\n",
       " '装饰',\n",
       " '贴',\n",
       " '圣诞老人',\n",
       " 'B']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from random import shuffle\n",
    "word_data[8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 标签分离(一个标签分成三个标签)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y = pd.DataFrame(train_y, columns=['label1', 'label2', 'label3'])\n",
    "test_y = pd.DataFrame(test_y, columns=['label1', 'label2', 'label3'])\n",
    "data_y = pd.DataFrame(data_y, columns=['label1', 'label2', 'label3'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_y[\"label1\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "191"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_y[\"label2\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1192"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_y[\"label3\"].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 标签onn_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import to_categorical\n",
    "# 先生成唯一数组\n",
    "y_label1 = data_y['label1'].unique().tolist()\n",
    "y_label2 = data_y['label2'].unique().tolist()\n",
    "y_label3 = data_y['label3'].unique().tolist()\n",
    "\n",
    "# 获取在唯一数组中的索引(训练集和测试集各有3个标签需要处理)\n",
    "train_y_label1_map = train_y['label1'].apply(lambda x: y_label1.index(x))\n",
    "train_y_label2_map = train_y['label2'].apply(lambda x: y_label2.index(x))\n",
    "train_y_label3_map = train_y['label3'].apply(lambda x: y_label3.index(x))\n",
    "test_y_label1_map = test_y['label1'].apply(lambda x: y_label1.index(x))\n",
    "test_y_label2_map = test_y['label2'].apply(lambda x: y_label2.index(x))\n",
    "test_y_label3_map = test_y['label3'].apply(lambda x: y_label3.index(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生成对应one-hot(用做训练模型的标签)\n",
    "train_y_label1_one_hot = to_categorical(train_y_label1_map, 22)\n",
    "train_y_label2_one_hot = to_categorical(train_y_label2_map, 191)\n",
    "train_y_label3_one_hot = to_categorical(train_y_label3_map, 1192)\n",
    "test_y_label1_one_hot = to_categorical(test_y_label1_map, 22)\n",
    "test_y_label2_one_hot = to_categorical(test_y_label2_map, 191)\n",
    "test_y_label3_one_hot = to_categorical(test_y_label3_map, 1192)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import *\n",
    "from keras.preprocessing import sequence\n",
    "tok = Tokenizer()\n",
    "tok.fit_on_texts(word_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'新款': 1,\n",
       " '1': 2,\n",
       " '汽车': 3,\n",
       " '时尚': 4,\n",
       " '黑色': 5,\n",
       " '鞋': 6,\n",
       " '2016': 7,\n",
       " '休闲': 8,\n",
       " '款': 9,\n",
       " '专用': 10,\n",
       " '2': 11,\n",
       " '5': 12,\n",
       " '男士': 13,\n",
       " '跟': 14,\n",
       " '手机': 15,\n",
       " '适用': 16,\n",
       " '女': 17,\n",
       " '套装': 18,\n",
       " '男': 19,\n",
       " '韩版': 20,\n",
       " '夏季': 21,\n",
       " '儿童': 22,\n",
       " '3': 23,\n",
       " '包': 24,\n",
       " '于': 25,\n",
       " '透气': 26,\n",
       " '壳': 27,\n",
       " '套': 28,\n",
       " '休闲鞋': 29,\n",
       " '米': 30,\n",
       " '户外': 31,\n",
       " '男鞋': 32,\n",
       " '真皮': 33,\n",
       " '垫': 34,\n",
       " '红色': 35,\n",
       " '6': 36,\n",
       " '4': 37,\n",
       " '脚垫': 38,\n",
       " '女鞋': 39,\n",
       " '男女': 40,\n",
       " '白色': 41,\n",
       " '全': 42,\n",
       " '蓝色': 43,\n",
       " '创意': 44,\n",
       " '宝宝': 45,\n",
       " '防水': 46,\n",
       " '简约': 47,\n",
       " '四季': 48,\n",
       " '女士': 49,\n",
       " '8': 50,\n",
       " '保护套': 51,\n",
       " '凉鞋': 52,\n",
       " '牛皮': 53,\n",
       " '四件套': 54,\n",
       " '皮鞋': 55,\n",
       " '系列': 56,\n",
       " '专车': 57,\n",
       " '情侣': 58,\n",
       " '包围': 59,\n",
       " '灯': 60,\n",
       " '通用': 61,\n",
       " '纯棉': 62,\n",
       " '加厚': 63,\n",
       " '小': 64,\n",
       " '运动': 65,\n",
       " '单鞋': 66,\n",
       " '商务': 67,\n",
       " '学生': 68,\n",
       " '大': 69,\n",
       " '2015': 70,\n",
       " '送': 71,\n",
       " '潮流': 72,\n",
       " '客厅': 73,\n",
       " '婴儿': 74,\n",
       " '带': 75,\n",
       " '新': 76,\n",
       " '卡通': 77,\n",
       " '号': 78,\n",
       " '全棉': 79,\n",
       " '英伦': 80,\n",
       " '7': 81,\n",
       " '0': 82,\n",
       " '寸': 83,\n",
       " '可': 84,\n",
       " '现代': 85,\n",
       " '鞋子': 86,\n",
       " 'led': 87,\n",
       " '单肩': 88,\n",
       " '玩具': 89,\n",
       " '黑': 90,\n",
       " '被': 91,\n",
       " '座垫': 92,\n",
       " '39': 93,\n",
       " '礼物': 94,\n",
       " '苹果': 95,\n",
       " '40': 96,\n",
       " '版': 97,\n",
       " '的': 98,\n",
       " '摆件': 99,\n",
       " '保暖': 100,\n",
       " '礼品': 101,\n",
       " '码': 102,\n",
       " '38': 103,\n",
       " '保护': 104,\n",
       " '改装': 105,\n",
       " '灰色': 106,\n",
       " '春季': 107,\n",
       " '床上用品': 108,\n",
       " '舒适': 109,\n",
       " '卧室': 110,\n",
       " '可爱': 111,\n",
       " '印花': 112,\n",
       " '拖鞋': 113,\n",
       " '装饰': 114,\n",
       " '多功能': 115,\n",
       " '高': 116,\n",
       " '坐垫': 117,\n",
       " '棕色': 118,\n",
       " '被套': 119,\n",
       " '茶具': 120,\n",
       " '进口': 121,\n",
       " '系带': 122,\n",
       " '米床': 123,\n",
       " '10': 124,\n",
       " '防': 125,\n",
       " '背包': 126,\n",
       " '金属': 127,\n",
       " '防滑': 128,\n",
       " '杯': 129,\n",
       " '复古': 130,\n",
       " '座套': 131,\n",
       " '经典': 132,\n",
       " '正版': 133,\n",
       " '秋冬': 134,\n",
       " '系': 135,\n",
       " '贴': 136,\n",
       " '春夏': 137,\n",
       " '12': 138,\n",
       " '汽车坐垫': 139,\n",
       " '奥迪': 140,\n",
       " '不锈钢': 141,\n",
       " '陶瓷': 142,\n",
       " '粉色': 143,\n",
       " '36': 144,\n",
       " '床单': 145,\n",
       " '欧美': 146,\n",
       " '床': 147,\n",
       " '定制': 148,\n",
       " '双人': 149,\n",
       " '生日礼物': 150,\n",
       " '手表': 151,\n",
       " '35': 152,\n",
       " '孕妇': 153,\n",
       " '板鞋': 154,\n",
       " '宝马': 155,\n",
       " '女款': 156,\n",
       " '车载': 157,\n",
       " '大众': 158,\n",
       " '套件': 159,\n",
       " '实木': 160,\n",
       " '年': 161,\n",
       " '女包': 162,\n",
       " '夹': 163,\n",
       " '枕': 164,\n",
       " '空调': 165,\n",
       " '37': 166,\n",
       " '办公': 167,\n",
       " '膜': 168,\n",
       " 'l': 169,\n",
       " '红': 170,\n",
       " '风': 171,\n",
       " '手机套': 172,\n",
       " '欧式': 173,\n",
       " '15': 174,\n",
       " '平底': 175,\n",
       " '车': 176,\n",
       " '尖头': 177,\n",
       " '男款': 178,\n",
       " '装': 179,\n",
       " '个性': 180,\n",
       " '手链': 181,\n",
       " '42': 182,\n",
       " '运动鞋': 183,\n",
       " '靴': 184,\n",
       " '41': 185,\n",
       " 'm': 186,\n",
       " '英寸': 187,\n",
       " '钻': 188,\n",
       " '皮革': 189,\n",
       " '色': 190,\n",
       " '防晒': 191,\n",
       " '新品': 192,\n",
       " '金': 193,\n",
       " '家用': 194,\n",
       " '水晶': 195,\n",
       " '懒人': 196,\n",
       " '吊坠': 197,\n",
       " '厚底': 198,\n",
       " '中': 199,\n",
       " '家居': 200,\n",
       " '配件': 201,\n",
       " '43': 202,\n",
       " '黄色': 203,\n",
       " '组合': 204,\n",
       " '粗': 205,\n",
       " '全包': 206,\n",
       " '书包': 207,\n",
       " '200': 208,\n",
       " '架': 209,\n",
       " '高跟鞋': 210,\n",
       " '饰品': 211,\n",
       " '三星': 212,\n",
       " '米色': 213,\n",
       " '公仔': 214,\n",
       " '硅胶': 215,\n",
       " '绿色': 216,\n",
       " '内': 217,\n",
       " '百搭': 218,\n",
       " '丝圈': 219,\n",
       " '后备箱': 220,\n",
       " '吸顶灯': 221,\n",
       " '皮套': 222,\n",
       " '透明': 223,\n",
       " '电脑': 224,\n",
       " '钱包': 225,\n",
       " 's': 226,\n",
       " '三件套': 227,\n",
       " '16': 228,\n",
       " '手工': 229,\n",
       " '蓝': 230,\n",
       " '家具': 231,\n",
       " '厘米': 232,\n",
       " '中国': 233,\n",
       " '个': 234,\n",
       " '高清': 235,\n",
       " '正品': 236,\n",
       " '型': 237,\n",
       " '毛绒玩具': 238,\n",
       " '9': 239,\n",
       " '春秋': 240,\n",
       " '台': 241,\n",
       " '冬季': 242,\n",
       " '挂件': 243,\n",
       " '奔驰': 244,\n",
       " '金色': 245,\n",
       " '跑步': 246,\n",
       " '银': 247,\n",
       " '包邮': 248,\n",
       " '智能': 249,\n",
       " '抱': 250,\n",
       " '3d': 251,\n",
       " '斜挎包': 252,\n",
       " 'a': 253,\n",
       " '双肩包': 254,\n",
       " 'xl': 255,\n",
       " '福克斯': 256,\n",
       " '纯色': 257,\n",
       " '岁': 258,\n",
       " '婚庆': 259,\n",
       " '天然': 260,\n",
       " '级': 261,\n",
       " '潮': 262,\n",
       " '礼盒': 263,\n",
       " '增高': 264,\n",
       " '白': 265,\n",
       " '20': 266,\n",
       " '灯具': 267,\n",
       " '一': 268,\n",
       " '帆布鞋': 269,\n",
       " '家纺': 270,\n",
       " '华为': 271,\n",
       " '银色': 272,\n",
       " '紫色': 273,\n",
       " '迷你': 274,\n",
       " '双层': 275,\n",
       " '丰田': 276,\n",
       " '斜': 277,\n",
       " '单': 278,\n",
       " '本田': 279,\n",
       " '单人': 280,\n",
       " '卡': 281,\n",
       " '支架': 282,\n",
       " '短袖': 283,\n",
       " '手提包': 284,\n",
       " '戒指': 285,\n",
       " '皮': 286,\n",
       " '用品': 287,\n",
       " '件套': 288,\n",
       " '钻石': 289,\n",
       " '圆头': 290,\n",
       " '片': 291,\n",
       " '14': 292,\n",
       " '双': 293,\n",
       " '项链': 294,\n",
       " '耐磨': 295,\n",
       " '车型': 296,\n",
       " '立体': 297,\n",
       " '月': 298,\n",
       " '玻璃': 299,\n",
       " '水钻': 300,\n",
       " '性感': 301,\n",
       " 't恤': 302,\n",
       " '餐厅': 303,\n",
       " '方向盘': 304,\n",
       " '边框': 305,\n",
       " '高尔夫': 306,\n",
       " '贴纸': 307,\n",
       " '贴膜': 308,\n",
       " '头层': 309,\n",
       " '外壳': 310,\n",
       " '式': 311,\n",
       " '镂空': 312,\n",
       " '原装': 313,\n",
       " '盒': 314,\n",
       " '冰丝': 315,\n",
       " '豆豆': 316,\n",
       " '100': 317,\n",
       " '益智': 318,\n",
       " '椅': 319,\n",
       " '扣': 320,\n",
       " '头': 321,\n",
       " '磨砂': 322,\n",
       " '包包': 323,\n",
       " '彩色': 324,\n",
       " '44': 325,\n",
       " '床品': 326,\n",
       " '一对': 327,\n",
       " '别克': 328,\n",
       " '摔': 329,\n",
       " '玫瑰': 330,\n",
       " '棉': 331,\n",
       " '册': 332,\n",
       " '帕萨特': 333,\n",
       " '把': 334,\n",
       " '充电': 335,\n",
       " '收纳': 336,\n",
       " '十字绣': 337,\n",
       " '旅行': 338,\n",
       " '折叠': 339,\n",
       " '公主': 340,\n",
       " '浅口': 341,\n",
       " '功夫': 342,\n",
       " '环保': 343,\n",
       " '韩国': 344,\n",
       " '人': 345,\n",
       " '小米': 346,\n",
       " '车用': 347,\n",
       " '多': 348,\n",
       " '不': 349,\n",
       " '粉': 350,\n",
       " '衣': 351,\n",
       " '吊灯': 352,\n",
       " '彩绘': 353,\n",
       " '手': 354,\n",
       " '钥匙包': 355,\n",
       " '条': 356,\n",
       " '夏': 357,\n",
       " '春夏季': 358,\n",
       " '大容量': 359,\n",
       " '笔': 360,\n",
       " '凉': 361,\n",
       " '布鞋': 362,\n",
       " '24': 363,\n",
       " '厨房': 364,\n",
       " '斜纹': 365,\n",
       " '茶杯': 366,\n",
       " '水杯': 367,\n",
       " '与': 368,\n",
       " '结婚': 369,\n",
       " '途观': 370,\n",
       " '专用汽车': 371,\n",
       " '被子': 372,\n",
       " '衣服': 373,\n",
       " '本': 374,\n",
       " '925': 375,\n",
       " '猫': 376,\n",
       " '灯饰': 377,\n",
       " '和': 378,\n",
       " '座椅': 379,\n",
       " '套餐': 380,\n",
       " '子': 381,\n",
       " '拉': 382,\n",
       " '无线': 383,\n",
       " '遥控': 384,\n",
       " '细': 385,\n",
       " '小号': 386,\n",
       " '卡罗': 387,\n",
       " '沙发': 388,\n",
       " '低帮': 389,\n",
       " '便携': 390,\n",
       " '办公室': 391,\n",
       " '花': 392,\n",
       " '无': 393,\n",
       " '手提': 394,\n",
       " '蝴蝶结': 395,\n",
       " '潮鞋': 396,\n",
       " '低': 397,\n",
       " '13': 398,\n",
       " '标准': 399,\n",
       " '纹': 400,\n",
       " '福特': 401,\n",
       " '笔记本': 402,\n",
       " '钢化': 403,\n",
       " '桌': 404,\n",
       " '全新': 405,\n",
       " '袋': 406,\n",
       " 'diy': 407,\n",
       " '坡': 408,\n",
       " 'plus': 409,\n",
       " '30': 410,\n",
       " 'iphone6': 411,\n",
       " '双肩': 412,\n",
       " '女单': 413,\n",
       " '零食': 414,\n",
       " '6s': 415,\n",
       " '北京': 416,\n",
       " '粉红色': 417,\n",
       " '短靴': 418,\n",
       " '咖啡色': 419,\n",
       " '凯美瑞': 420,\n",
       " '芯': 421,\n",
       " '套脚': 422,\n",
       " '驾车': 423,\n",
       " '起亚': 424,\n",
       " '工具': 425,\n",
       " 'usb': 426,\n",
       " '秋季': 427,\n",
       " '书籍': 428,\n",
       " '罩': 429,\n",
       " '60': 430,\n",
       " '备注': 431,\n",
       " '钥匙': 432,\n",
       " '50': 433,\n",
       " '均码': 434,\n",
       " '坠': 435,\n",
       " '后': 436,\n",
       " '老': 437,\n",
       " '器': 438,\n",
       " '颜色': 439,\n",
       " '女童': 440,\n",
       " '工艺品': 441,\n",
       " '塑料': 442,\n",
       " '模型': 443,\n",
       " '230cm': 444,\n",
       " '情人节': 445,\n",
       " '防风': 446,\n",
       " '大码': 447,\n",
       " '11': 448,\n",
       " '爱': 449,\n",
       " '线': 450,\n",
       " '徒步': 451,\n",
       " '鲜花': 452,\n",
       " '克': 453,\n",
       " '雅阁': 454,\n",
       " '酒': 455,\n",
       " '34': 456,\n",
       " '柜': 457,\n",
       " '活性': 458,\n",
       " '尾箱': 459,\n",
       " '18': 460,\n",
       " '外套': 461,\n",
       " '积木': 462,\n",
       " '标配': 463,\n",
       " '手串': 464,\n",
       " '新生儿': 465,\n",
       " '拉链': 466,\n",
       " '紫': 467,\n",
       " '花花公子': 468,\n",
       " '教材': 469,\n",
       " 'b': 470,\n",
       " '长': 471,\n",
       " '迈腾': 472,\n",
       " '中式': 473,\n",
       " '滤清器': 474,\n",
       " '绿': 475,\n",
       " 't': 476,\n",
       " '宽': 477,\n",
       " '150': 478,\n",
       " '田园': 479,\n",
       " '灰': 480,\n",
       " '孕妇装': 481,\n",
       " '甜美': 482,\n",
       " '玩偶': 483,\n",
       " '女式': 484,\n",
       " '壶': 485,\n",
       " 'c': 486,\n",
       " '现货': 487,\n",
       " '马丁': 488,\n",
       " '安全': 489,\n",
       " '亚麻': 490,\n",
       " '遮阳': 491,\n",
       " '脚': 492,\n",
       " '自动': 493,\n",
       " '圆形': 494,\n",
       " '沙滩鞋': 495,\n",
       " '长安': 496,\n",
       " '裤': 497,\n",
       " '座': 498,\n",
       " '女友': 499,\n",
       " '豪华版': 500,\n",
       " '虎': 501,\n",
       " '温馨': 502,\n",
       " '咖啡': 503,\n",
       " '女孩': 504,\n",
       " '美': 505,\n",
       " '长袖': 506,\n",
       " '帆布': 507,\n",
       " '软壳': 508,\n",
       " '女生': 509,\n",
       " '松糕': 510,\n",
       " '条纹': 511,\n",
       " '长款': 512,\n",
       " '茶盘': 513,\n",
       " '迪士尼': 514,\n",
       " '美式': 515,\n",
       " '熊': 516,\n",
       " '两用': 517,\n",
       " '用': 518,\n",
       " '马自达': 519,\n",
       " '标致': 520,\n",
       " '游戏': 521,\n",
       " '日产': 522,\n",
       " '含': 523,\n",
       " '香水': 524,\n",
       " '滤': 525,\n",
       " '美国': 526,\n",
       " '板': 527,\n",
       " '冲锋衣': 528,\n",
       " '专业': 529,\n",
       " '速腾': 530,\n",
       " '布娃娃': 531,\n",
       " '挡': 532,\n",
       " '空气': 533,\n",
       " '韩版潮': 534,\n",
       " '蓝牙': 535,\n",
       " '一字': 536,\n",
       " '科鲁兹': 537,\n",
       " '白光': 538,\n",
       " '装饰品': 539,\n",
       " 'crv': 540,\n",
       " '音乐': 541,\n",
       " '服': 542,\n",
       " '成人': 543,\n",
       " '车衣': 544,\n",
       " '双人床': 545,\n",
       " '学院': 546,\n",
       " '平板': 547,\n",
       " '隔热': 548,\n",
       " '斤': 549,\n",
       " '网面': 550,\n",
       " '凉席': 551,\n",
       " '靴子': 552,\n",
       " '优雅': 553,\n",
       " '垫子': 554,\n",
       " '键盘': 555,\n",
       " '度': 556,\n",
       " '件': 557,\n",
       " '棕': 558,\n",
       " '日本': 559,\n",
       " '皮肤': 560,\n",
       " '盖': 561,\n",
       " '配': 562,\n",
       " '黄': 563,\n",
       " '蕾丝': 564,\n",
       " '证书': 565,\n",
       " '德国': 566,\n",
       " '翡翠': 567,\n",
       " '联想': 568,\n",
       " '茶': 569,\n",
       " '轻便': 570,\n",
       " '速干': 571,\n",
       " '拖': 572,\n",
       " '三': 573,\n",
       " '捷达': 574,\n",
       " '全套': 575,\n",
       " '深蓝色': 576,\n",
       " '糖果': 577,\n",
       " '碗': 578,\n",
       " '茶壶': 579,\n",
       " '仿真': 580,\n",
       " '小包': 581,\n",
       " '英朗': 582,\n",
       " 'polo': 583,\n",
       " '内衣': 584,\n",
       " '180': 585,\n",
       " '达': 586,\n",
       " '书房': 587,\n",
       " '请': 588,\n",
       " '上衣': 589,\n",
       " '办公桌': 590,\n",
       " '橙色': 591,\n",
       " '挎': 592,\n",
       " '合金': 593,\n",
       " '小学生': 594,\n",
       " '钥匙扣': 595,\n",
       " '宝': 596,\n",
       " '逸': 597,\n",
       " '监控': 598,\n",
       " '钢化玻璃': 599,\n",
       " '儿童玩具': 600,\n",
       " '格': 601,\n",
       " '蒙迪欧': 602,\n",
       " '水壶': 603,\n",
       " '浅': 604,\n",
       " '汽车用品': 605,\n",
       " '特产': 606,\n",
       " '妈妈': 607,\n",
       " '婴幼儿': 608,\n",
       " '枕套': 609,\n",
       " '耳机': 610,\n",
       " '玫': 611,\n",
       " '箱': 612,\n",
       " '狗': 613,\n",
       " '天籁': 614,\n",
       " '鱼': 615,\n",
       " '浮雕': 616,\n",
       " '比亚迪': 617,\n",
       " '2014': 618,\n",
       " '钱': 619,\n",
       " '椅子': 620,\n",
       " '清新': 621,\n",
       " '200cm': 622,\n",
       " '适合': 623,\n",
       " '直径': 624,\n",
       " '坐套': 625,\n",
       " '插': 626,\n",
       " '登山': 627,\n",
       " '下': 628,\n",
       " '紫砂': 629,\n",
       " 'ipad': 630,\n",
       " '80': 631,\n",
       " '被罩': 632,\n",
       " '修复': 633,\n",
       " '滤芯': 634,\n",
       " '电池': 635,\n",
       " '明锐': 636,\n",
       " '娃娃': 637,\n",
       " '迷彩': 638,\n",
       " '卡其色': 639,\n",
       " '盘': 640,\n",
       " '置物架': 641,\n",
       " '绣': 642,\n",
       " '18k': 643,\n",
       " '货到付款': 644,\n",
       " '味': 645,\n",
       " '移动': 646,\n",
       " '随机': 647,\n",
       " '脚蹬': 648,\n",
       " '茶几': 649,\n",
       " '男包': 650,\n",
       " '瓶': 651,\n",
       " 'rav4': 652,\n",
       " '玉石': 653,\n",
       " 'xxl': 654,\n",
       " '开关': 655,\n",
       " '珠宝': 656,\n",
       " '博世': 657,\n",
       " '机油': 658,\n",
       " '浪漫': 659,\n",
       " '雨刮器': 660,\n",
       " '高档': 661,\n",
       " '书': 662,\n",
       " '佛珠': 663,\n",
       " '双面': 664,\n",
       " '睡袋': 665,\n",
       " '翼': 666,\n",
       " '加绒': 667,\n",
       " '拼色': 668,\n",
       " '夏装': 669,\n",
       " '电动': 670,\n",
       " '链': 671,\n",
       " '超薄': 672,\n",
       " 'u': 673,\n",
       " '景德镇': 674,\n",
       " '短款': 675,\n",
       " '餐具': 676,\n",
       " '软底': 677,\n",
       " '沙滩': 678,\n",
       " '男式': 679,\n",
       " '秋冬季': 680,\n",
       " '考试': 681,\n",
       " '长裤': 682,\n",
       " '喷漆': 683,\n",
       " '镶': 684,\n",
       " '和田玉': 685,\n",
       " '提花': 686,\n",
       " '室内': 687,\n",
       " '加大': 688,\n",
       " '花瓶': 689,\n",
       " '全国': 690,\n",
       " '探路者': 691,\n",
       " '吉普': 692,\n",
       " '精品': 693,\n",
       " '荣耀': 694,\n",
       " '阳光': 695,\n",
       " '日': 696,\n",
       " '高帮': 697,\n",
       " '雪佛兰': 698,\n",
       " '保温杯': 699,\n",
       " '手套': 700,\n",
       " 'cd': 701,\n",
       " '护板': 702,\n",
       " '电源': 703,\n",
       " '哈弗': 704,\n",
       " '飞度': 705,\n",
       " '有': 706,\n",
       " '4g': 707,\n",
       " '户外运动': 708,\n",
       " '帮': 709,\n",
       " '一体机': 710,\n",
       " '夏凉': 711,\n",
       " '画': 712,\n",
       " '留言': 713,\n",
       " '45': 714,\n",
       " '后盖': 715,\n",
       " '帐篷': 716,\n",
       " '汉兰达': 717,\n",
       " '机': 718,\n",
       " '电子': 719,\n",
       " '皇冠': 720,\n",
       " '衣柜': 721,\n",
       " '网': 722,\n",
       " 'a6l': 723,\n",
       " 'pu': 724,\n",
       " '生日': 725,\n",
       " '上': 726,\n",
       " 'q5': 727,\n",
       " '世界': 728,\n",
       " '220': 729,\n",
       " '平': 730,\n",
       " '旅游': 731,\n",
       " '拼接': 732,\n",
       " '内饰': 733,\n",
       " '手镯': 734,\n",
       " '气质': 735,\n",
       " '面板': 736,\n",
       " '摆设': 737,\n",
       " 'dvd': 738,\n",
       " '帽': 739,\n",
       " '跨': 740,\n",
       " '灯泡': 741,\n",
       " '特价': 742,\n",
       " '鳄鱼': 743,\n",
       " '餐桌': 744,\n",
       " '磨毛': 745,\n",
       " '拍': 746,\n",
       " '木': 747,\n",
       " '长城': 748,\n",
       " '雪铁龙': 749,\n",
       " '杯子': 750,\n",
       " '元': 751,\n",
       " '支': 752,\n",
       " '动物': 753,\n",
       " '春': 754,\n",
       " '靠垫': 755,\n",
       " '罗马': 756,\n",
       " '玫红': 757,\n",
       " '夏天': 758,\n",
       " '防雨': 759,\n",
       " '早教': 760,\n",
       " '拉杆箱': 761,\n",
       " '其他': 762,\n",
       " '食品': 763,\n",
       " '干': 764,\n",
       " '男孩': 765,\n",
       " '鱼嘴': 766,\n",
       " '充电器': 767,\n",
       " '日常': 768,\n",
       " '头盔': 769,\n",
       " '毯': 770,\n",
       " '升级版': 771,\n",
       " '纯': 772,\n",
       " '罐': 773,\n",
       " '豪华': 774,\n",
       " '旅行包': 775,\n",
       " '泳衣': 776,\n",
       " 'x5': 777,\n",
       " '思域': 778,\n",
       " '嘴': 779,\n",
       " '实用': 780,\n",
       " '光盘': 781,\n",
       " '木质': 782,\n",
       " '升级': 783,\n",
       " '行李箱': 784,\n",
       " '趾': 785,\n",
       " '两件套': 786,\n",
       " 'e': 787,\n",
       " '单个': 788,\n",
       " '钢笔': 789,\n",
       " '马': 790,\n",
       " '凯越': 791,\n",
       " '支装': 792,\n",
       " '米奇': 793,\n",
       " '保温': 794,\n",
       " '抓': 795,\n",
       " '摩托车': 796,\n",
       " '款式': 797,\n",
       " 'ix35': 798,\n",
       " '男表': 799,\n",
       " '乐福鞋': 800,\n",
       " '正装': 801,\n",
       " '第': 802,\n",
       " '记录仪': 803,\n",
       " '黄金': 804,\n",
       " '网鞋': 805,\n",
       " '纸': 806,\n",
       " '连衣裙': 807,\n",
       " '片装': 808,\n",
       " '毛毯': 809,\n",
       " '划痕': 810,\n",
       " '宝来': 811,\n",
       " '居家': 812,\n",
       " '简易': 813,\n",
       " '男童': 814,\n",
       " '连体': 815,\n",
       " '230': 816,\n",
       " '内胆': 817,\n",
       " '大号': 818,\n",
       " '拼': 819,\n",
       " '皮带': 820,\n",
       " '拼装': 821,\n",
       " '之': 822,\n",
       " '松糕鞋': 823,\n",
       " '三合一': 824,\n",
       " '层': 825,\n",
       " '修身': 826,\n",
       " '儿童节': 827,\n",
       " '调光': 828,\n",
       " '附': 829,\n",
       " 'oppo': 830,\n",
       " '19': 831,\n",
       " '设计': 832,\n",
       " '我': 833,\n",
       " '包头': 834,\n",
       " '单件': 835,\n",
       " '阳台': 836,\n",
       " '编织': 837,\n",
       " '音箱': 838,\n",
       " '登山鞋': 839,\n",
       " '咖色': 840,\n",
       " '贡缎': 841,\n",
       " '镜': 842,\n",
       " '上海': 843,\n",
       " '安装': 844,\n",
       " '棉鞋': 845,\n",
       " '钓鱼': 846,\n",
       " '锐志': 847,\n",
       " '桌椅': 848,\n",
       " '训练': 849,\n",
       " '漆皮': 850,\n",
       " '宠物': 851,\n",
       " '游泳': 852,\n",
       " '仕': 853,\n",
       " '硬盘': 854,\n",
       " '骑行': 855,\n",
       " '逍客': 856,\n",
       " '索纳塔': 857,\n",
       " '漆': 858,\n",
       " '英语': 859,\n",
       " '故事': 860,\n",
       " '路': 861,\n",
       " '帽子': 862,\n",
       " '选': 863,\n",
       " '兔': 864,\n",
       " '方形': 865,\n",
       " '土豪': 866,\n",
       " '前后': 867,\n",
       " '开': 868,\n",
       " '约': 869,\n",
       " '摄像头': 870,\n",
       " 'a3': 871,\n",
       " '25': 872,\n",
       " '夜光': 873,\n",
       " '导航': 874,\n",
       " 'a4l': 875,\n",
       " '120': 876,\n",
       " '铆钉': 877,\n",
       " '睿': 878,\n",
       " '绣花': 879,\n",
       " '风格': 880,\n",
       " 'cc': 881,\n",
       " '学习': 882,\n",
       " '旅行箱': 883,\n",
       " '打火机': 884,\n",
       " '自': 885,\n",
       " '8g': 886,\n",
       " '东风': 887,\n",
       " '桑塔纳': 888,\n",
       " '整套': 889,\n",
       " 'h6': 890,\n",
       " '铝合金': 891,\n",
       " '软': 892,\n",
       " '长方形': 893,\n",
       " '仿古': 894,\n",
       " '扳手': 895,\n",
       " '貔貅': 896,\n",
       " '斯柯达': 897,\n",
       " '珍珠': 898,\n",
       " '行车': 899,\n",
       " 'd': 900,\n",
       " 's6': 901,\n",
       " '底': 902,\n",
       " '张': 903,\n",
       " '水洗': 904,\n",
       " '球': 905,\n",
       " '卷': 906,\n",
       " '点': 907,\n",
       " '速递': 908,\n",
       " '鼠标': 909,\n",
       " '咖': 910,\n",
       " '配饰': 911,\n",
       " '出版社': 912,\n",
       " '免': 913,\n",
       " '5mm': 914,\n",
       " '黑白': 915,\n",
       " '裙': 916,\n",
       " '图书': 917,\n",
       " '22': 918,\n",
       " '大红': 919,\n",
       " '车垫': 920,\n",
       " '宝骏': 921,\n",
       " '雕花': 922,\n",
       " '定做': 923,\n",
       " '盒装': 924,\n",
       " '电镀': 925,\n",
       " '好': 926,\n",
       " '超高': 927,\n",
       " '翻盖': 928,\n",
       " '音响': 929,\n",
       " '手绘': 930,\n",
       " '台灯': 931,\n",
       " '骆驼': 932,\n",
       " '圆领': 933,\n",
       " 'v': 934,\n",
       " '浅蓝色': 935,\n",
       " '古典': 936,\n",
       " '拉手': 937,\n",
       " '年级': 938,\n",
       " 'pro': 939,\n",
       " '酒杯': 940,\n",
       " '天': 941,\n",
       " '吊': 942,\n",
       " '台湾': 943,\n",
       " '品': 944,\n",
       " '笔记本电脑': 945,\n",
       " '镜头': 946,\n",
       " '绒': 947,\n",
       " '世家': 948,\n",
       " '精装': 949,\n",
       " '后视镜': 950,\n",
       " '指环': 951,\n",
       " '铜': 952,\n",
       " '幼儿园': 953,\n",
       " '生肖': 954,\n",
       " '烤': 955,\n",
       " '可折叠': 956,\n",
       " '网布': 957,\n",
       " '标签': 958,\n",
       " '朗逸': 959,\n",
       " '必备': 960,\n",
       " '边': 961,\n",
       " '共': 962,\n",
       " '魅族': 963,\n",
       " '电脑包': 964,\n",
       " '补漆笔': 965,\n",
       " '小熊': 966,\n",
       " '轻薄': 967,\n",
       " '静音': 968,\n",
       " '网络': 969,\n",
       " '泰迪熊': 970,\n",
       " '艺术': 971,\n",
       " '标准版': 972,\n",
       " '卡宴': 973,\n",
       " '腰包': 974,\n",
       " '文具': 975,\n",
       " '500ml': 976,\n",
       " '头枕': 977,\n",
       " '电视': 978,\n",
       " '风衣': 979,\n",
       " '男装': 980,\n",
       " '插座': 981,\n",
       " '年份': 982,\n",
       " '90': 983,\n",
       " '电视柜': 984,\n",
       " '刹车片': 985,\n",
       " '超': 986,\n",
       " '奶瓶': 987,\n",
       " '冬': 988,\n",
       " '昂科威': 989,\n",
       " '主机': 990,\n",
       " '富贵': 991,\n",
       " '自由': 992,\n",
       " '仪表': 993,\n",
       " '开业': 994,\n",
       " '工装': 995,\n",
       " '耳钉': 996,\n",
       " 'vivo': 997,\n",
       " '羊皮': 998,\n",
       " '书桌': 999,\n",
       " '机械': 1000,\n",
       " ...}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "274420"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tok.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = tok.texts_to_sequences(word_data_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = tok.texts_to_sequences(word_data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = sequence.pad_sequences(x_train, maxlen=10)\n",
    "x_test  = sequence.pad_sequences(x_test, maxlen=10) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[    4, 61541,   211, ...,    49,  3346,  1084],\n",
       "       [  961,  3674,  1660, ...,  1654,  1654,  1654],\n",
       "       [  680, 11975,     6, ...,   639,   178,   185],\n",
       "       ...,\n",
       "       [ 4370,  1205,   827, ..., 62083,  6929,   635],\n",
       "       [ 6127,  8618,  4373, ..., 20409,  8816, 16590],\n",
       "       [ 3182,  1310,   868, ...,    36,   234,   298]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## GRU Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.contrib import rnn\n",
    "import tensorflow.contrib.layers as layers\n",
    "class GruModel(object):\n",
    "    \"\"\"\n",
    "    A CNN for text classification.\n",
    "    Uses an embedding layer, followed by a convolutional, max-pooling and softmax layer.\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 sequence_length, \n",
    "                 num_classes, \n",
    "                 embedding_size,  \n",
    "                 num_filters, \n",
    "                 l2_reg_lambda=0.0, \n",
    "                 n_layer = 1, \n",
    "                 hidden_size = 64, \n",
    "                 batch_size = 256, \n",
    "                 vac_size = 27440):\n",
    "        \"\"\"\n",
    "        sequence_length : 一个句子的长度（词的个数）\n",
    "        embedding_size : 词向量的长度\n",
    "        num_classes : 三个标签的类别数\n",
    "        vac_size : 词的个数\n",
    "        \"\"\"\n",
    "        # Placeholders for input, output and dropout\n",
    "        self.input_x = tf.placeholder(tf.int32, [None, sequence_length], name=\"input_x\")\n",
    "        self.input_y1 = tf.placeholder(tf.float32, [None, num_classes[0]], name=\"input_y1\")\n",
    "        self.input_y2 = tf.placeholder(tf.float32, [None, num_classes[1]], name=\"input_y2\")\n",
    "        self.input_y3 = tf.placeholder(tf.float32, [None, num_classes[2]], name=\"input_y3\")\n",
    "        self.keep_prob = tf.placeholder(tf.float32, name=\"keep_prob\")\n",
    "        self.n_layer = n_layer\n",
    "        self.batch_size = batch_size\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        \n",
    "        with tf.name_scope(\"embedding\"):\n",
    "            W = tf.Variable(\n",
    "                tf.random_uniform([274420, embedding_size], -1.0, 1.0),\n",
    "                name=\"W\")\n",
    "            self.embedded_chars = tf.nn.embedding_lookup(W, self.input_x)\n",
    "\n",
    "        l2_loss = tf.constant(0.0)\n",
    "        \n",
    "        #gru模型\n",
    "        with tf.variable_scope('bigru_text'):\n",
    "            self.gru_output = self.bigru_inference(self.embedded_chars)\n",
    "            print(\"gru shape\", self.gru_output.shape)\n",
    "            \n",
    "        self.h_drop = self.gru_output\n",
    "        num_filters_total = self.h_drop.shape[1]\n",
    "        \n",
    "        with tf.name_scope(\"output1\"):\n",
    "            W1 = tf.get_variable(\n",
    "                \"W11\",\n",
    "                shape=[num_filters_total, 64],\n",
    "                initializer=tf.contrib.layers.xavier_initializer())\n",
    "            b1 = tf.Variable(tf.constant(0.1, shape=[64]), name=\"b11\")\n",
    "            self.output = tf.nn.xw_plus_b(self.h_drop, W1, b1, name=\"output1\")\n",
    "            self.output = tf.layers.batch_normalization(self.output, training=is_training)\n",
    "            self.output = tf.nn.relu(self.output)\n",
    "            self.output = tf.nn.dropout(self.output, self.keep_prob)\n",
    "            W2 = tf.get_variable(\n",
    "                \"W12\",\n",
    "                shape=[64, num_classes[0]],\n",
    "                initializer=tf.contrib.layers.xavier_initializer())\n",
    "            b2 = tf.Variable(tf.constant(0.1, shape=[num_classes[0]]), name=\"b12\")\n",
    "            l2_loss += tf.nn.l2_loss(W1)\n",
    "            l2_loss += tf.nn.l2_loss(b1)\n",
    "            self.scores1 = tf.nn.xw_plus_b(self.output, W2, b2, name=\"scores\")\n",
    "            self.predictions1 = tf.argmax(self.scores1, 1, name=\"predictions\")\n",
    "            \n",
    "        with tf.name_scope(\"output2\"):\n",
    "            W1 = tf.get_variable(\n",
    "                \"W21\",\n",
    "                shape=[num_filters_total, 128],\n",
    "                initializer=tf.contrib.layers.xavier_initializer())\n",
    "            b1 = tf.Variable(tf.constant(0.1, shape=[128]), name=\"b21\")\n",
    "            self.output = tf.nn.xw_plus_b(self.h_drop, W1, b1, name=\"output2\")\n",
    "            self.output = tf.layers.batch_normalization(self.output, training=is_training)\n",
    "            self.output = tf.nn.relu(self.output)\n",
    "            self.output = tf.nn.dropout(self.output, self.keep_prob)\n",
    "            W2 = tf.get_variable(\n",
    "                \"W22\",\n",
    "                shape=[128, num_classes[1]],\n",
    "                initializer=tf.contrib.layers.xavier_initializer())\n",
    "            b2 = tf.Variable(tf.constant(0.1, shape=[num_classes[1]]), name=\"b22\")\n",
    "            l2_loss += tf.nn.l2_loss(W1)\n",
    "            l2_loss += tf.nn.l2_loss(b1)\n",
    "            self.scores2 = tf.nn.xw_plus_b(self.output, W2, b2, name=\"scores\")\n",
    "            self.predictions2 = tf.argmax(self.scores2, 1, name=\"predictions\")\n",
    "            \n",
    "        with tf.name_scope(\"output3\"):\n",
    "            W1 = tf.get_variable(\n",
    "                \"W31\",\n",
    "                shape=[num_filters_total, 256],\n",
    "                initializer=tf.contrib.layers.xavier_initializer())\n",
    "            b1 = tf.Variable(tf.constant(0.1, shape=[256]), name=\"b31\")\n",
    "            self.output = tf.nn.xw_plus_b(self.h_drop, W1, b1, name=\"output3\")\n",
    "            self.output = tf.layers.batch_normalization(self.output, training=is_training)\n",
    "            self.output = tf.nn.relu(self.output)\n",
    "            self.output = tf.nn.dropout(self.output, self.keep_prob)\n",
    "            W2 = tf.get_variable(\n",
    "                \"W32\",\n",
    "                shape=[256, num_classes[2]],\n",
    "                initializer=tf.contrib.layers.xavier_initializer())\n",
    "            b2 = tf.Variable(tf.constant(0.1, shape=[num_classes[2]]), name=\"b32\")\n",
    "            l2_loss += tf.nn.l2_loss(W1)\n",
    "            l2_loss += tf.nn.l2_loss(b1)\n",
    "            self.scores3 = tf.nn.xw_plus_b(self.output, W2, b2, name=\"scores\")\n",
    "            self.predictions3 = tf.argmax(self.scores3, 1, name=\"predictions\")\n",
    "        # Calculate mean cross-entropy loss\n",
    "        \n",
    "        with tf.name_scope(\"loss\"):\n",
    "            losses1 = tf.nn.softmax_cross_entropy_with_logits(logits=self.scores1, labels=self.input_y1)\n",
    "            losses2 = tf.nn.softmax_cross_entropy_with_logits(logits=self.scores2, labels=self.input_y2)\n",
    "            losses3 = tf.nn.softmax_cross_entropy_with_logits(logits=self.scores3, labels=self.input_y3)\n",
    "            self.loss = 0.1 * tf.reduce_mean(losses1) + 0.2 * tf.reduce_mean(losses2) + 0.7*tf.reduce_mean(losses3) + l2_reg_lambda * l2_loss\n",
    "            tf.summary.scalar('loss1',tf.reduce_mean(losses1))\n",
    "            tf.summary.scalar('loss2',tf.reduce_mean(losses2))\n",
    "            tf.summary.scalar('loss3',tf.reduce_mean(losses3))\n",
    "            tf.summary.scalar('loss',self.loss)\n",
    "        # Accuracy\n",
    "        with tf.name_scope(\"accuracy\"):\n",
    "            correct_predictions1 = tf.equal(self.predictions1, tf.argmax(self.input_y1, 1))\n",
    "            correct_predictions2 = tf.equal(self.predictions2, tf.argmax(self.input_y2, 1))\n",
    "            correct_predictions3 = tf.equal(self.predictions3, tf.argmax(self.input_y3, 1))\n",
    "#           单独的准确率\n",
    "            self.accuracy1 = tf.reduce_mean(tf.cast(correct_predictions1, \"float\"), name=\"accuracy1\")\n",
    "            self.accuracy2 = tf.reduce_mean(tf.cast(correct_predictions2, \"float\"), name=\"accuracy2\")\n",
    "            self.accuracy3 = tf.reduce_mean(tf.cast(correct_predictions3, \"float\"), name=\"accuracy3\")\n",
    "#           一起的准确率\n",
    "            self.acc = tf.reduce_mean(tf.cast(correct_predictions1, \"float\")*tf.cast(correct_predictions2, \"float\")*tf.cast(correct_predictions3, \"float\"))\n",
    "            tf.summary.scalar('acc1',self.accuracy1)\n",
    "            tf.summary.scalar('acc2',self.accuracy2)\n",
    "            tf.summary.scalar('acc3',self.accuracy3)\n",
    "            tf.summary.scalar('acc',self.acc)\n",
    "            \n",
    "    def gru_cell(self):\n",
    "        with tf.name_scope('gru_cell'):\n",
    "            cell = rnn.GRUCell(self.hidden_size, reuse=tf.get_variable_scope().reuse)\n",
    "        return rnn.DropoutWrapper(cell, output_keep_prob=self.keep_prob)\n",
    "\n",
    "    def bi_gru(self, inputs):\n",
    "        \"\"\"build the bi-GRU network. 返回个所有层的隐含状态。\"\"\"\n",
    "        cells_fw = [self.gru_cell() for _ in range(self.n_layer)]\n",
    "        cells_bw = [self.gru_cell() for _ in range(self.n_layer)]\n",
    "        initial_states_fw = [cell_fw.zero_state(self.batch_size, tf.float32) for cell_fw in cells_fw]\n",
    "        initial_states_bw = [cell_bw.zero_state(self.batch_size, tf.float32) for cell_bw in cells_bw]\n",
    "        outputs, _, _ = rnn.stack_bidirectional_dynamic_rnn(cells_fw, cells_bw, inputs,\n",
    "                                                            initial_states_fw=initial_states_fw,\n",
    "                                                            initial_states_bw=initial_states_bw, dtype=tf.float32)\n",
    "        return outputs\n",
    "    \n",
    "    def bigru_inference(self, X_inputs):\n",
    "#         inputs = tf.nn.embedding_lookup(self.title_embedding, X_inputs)\n",
    "        output_bigru = self.bi_gru(X_inputs)\n",
    "        output_att = self.task_specific_attention(output_bigru, self.hidden_size*2)\n",
    "        return output_att\n",
    "    \n",
    "    def task_specific_attention(self, inputs, output_size,\n",
    "                                initializer=layers.xavier_initializer(),\n",
    "                                activation_fn=tf.tanh, scope=None):\n",
    "        \"\"\"\n",
    "        Performs task-specific attention reduction, using learned\n",
    "        attention context vector (constant within task of interest).\n",
    "        Args:\n",
    "            inputs: Tensor of shape [batch_size, units, input_size]\n",
    "                `input_size` must be static (known)\n",
    "                `units` axis will be attended over (reduced from output)\n",
    "                `batch_size` will be preserved\n",
    "            output_size: Size of output's inner (feature) dimension\n",
    "        Returns:\n",
    "           outputs: Tensor of shape [batch_size, output_dim].\n",
    "        \"\"\"\n",
    "#         assert len(inputs.get_shape()) == 3 and inputs.get_shape()[-1].value is not None\n",
    "        with tf.variable_scope(scope or 'attention') as scope:\n",
    "            # u_w, attention 向量\n",
    "            attention_context_vector = tf.get_variable(name='attention_context_vector', shape=[output_size],\n",
    "                                                       initializer=initializer, dtype=tf.float32)\n",
    "            # 全连接层，把 h_i 转为 u_i ， shape= [batch_size, units, input_size] -> [batch_size, units, output_size]\n",
    "            input_projection = layers.fully_connected(inputs, output_size, activation_fn=activation_fn, scope=scope)\n",
    "            # 输出 [batch_size, units]\n",
    "            vector_attn = tf.reduce_sum(tf.multiply(input_projection, attention_context_vector), axis=2, keep_dims=True)\n",
    "            attention_weights = tf.nn.softmax(vector_attn, dim=1)\n",
    "            tf.summary.histogram('attention_weigths', attention_weights)\n",
    "            weighted_projection = tf.multiply(inputs, attention_weights)\n",
    "            outputs = tf.reduce_sum(weighted_projection, axis=1)\n",
    "            return outputs  # 输出 [batch_size, hidden_size*2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# batch 生成函数\n",
    "import math\n",
    "def batch_iter(data, batch_size, num_epochs, shuffle=True):\n",
    "    \"\"\"\n",
    "    Generates a batch iterator for a dataset.\n",
    "    \"\"\"\n",
    "    data = np.array(data)\n",
    "    data_size = len(data)\n",
    "    num_batches_per_epoch = int((len(data)-1)/batch_size) + 1\n",
    "    for epoch in range(num_epochs):\n",
    "        # Shuffle the data at each epoch\n",
    "        if shuffle:\n",
    "            shuffle_indices = np.random.permutation(np.arange(data_size))\n",
    "            shuffled_data = data[shuffle_indices]\n",
    "        else:\n",
    "            shuffled_data = data\n",
    "  \n",
    "        for batch_num in range(num_batches_per_epoch):\n",
    "            start_index = batch_num * batch_size\n",
    "            end_index = np.min(((batch_num + 1) * batch_size, data_size))\n",
    "            yield shuffled_data[start_index:end_index]\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 特征数这里是第三级标签的特征数\n",
    "max_features = 1192\n",
    "\n",
    "# 句子填充的长度\n",
    "sequence_len = 10\n",
    "\n",
    "# batch 大小\n",
    "batch_size = 256 \n",
    "\n",
    "# 迭代次数\n",
    "epochs = 100\n",
    "\n",
    "is_training = True\n",
    "\n",
    "# 词向量长度\n",
    "embedding_dims = 32\n",
    "\n",
    "# gru  的filters\n",
    "num_filters = 64\n",
    "\n",
    "# filter 的大小\n",
    "filter_size = [1, 2, 3, 4, 5]\n",
    "\n",
    "# 三个标签的类数\n",
    "num_classes = [22, 191, 1192]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "from tensorflow.contrib import learn\n",
    "\n",
    "\n",
    "\n",
    "FLAGS = tf.flags.FLAGS\n",
    "\n",
    "def train(x_train, y_train, x_dev, y_dev):\n",
    "    # Training\n",
    "    # ==================================================\n",
    "\n",
    "    with tf.Graph().as_default():\n",
    "        session_conf = tf.ConfigProto()\n",
    "        sess = tf.Session(config=session_conf)\n",
    "        with sess.as_default():\n",
    "            cnn = GruModel(sequence_length = sequence_len, \n",
    "                          num_classes = num_classes, \n",
    "                          embedding_size = embedding_dims, \n",
    "                          num_filters = num_filters)\n",
    "\n",
    "            # Define Training procedure\n",
    "\n",
    "            global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "#           学习率衰减\n",
    "            learning_rate = tf.train.exponential_decay(\n",
    "                0.01,\n",
    "                global_step,\n",
    "                x_train.shape[0] / batch_size,\n",
    "                0.99,\n",
    "                staircase=True,)\n",
    "            optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "            grads_and_vars = optimizer.compute_gradients(cnn.loss)\n",
    "            train_op = optimizer.apply_gradients(grads_and_vars, global_step=global_step)\n",
    "\n",
    "            # Initialize all variables\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            \n",
    "            merged = tf.summary.merge_all()\n",
    "            \n",
    "            writer = tf.summary.FileWriter(\"fwwb\",sess.graph)\n",
    "            acc_sum = np.array([0.0,0.0,0.0,0.0])\n",
    "            loss_sum = 0\n",
    "            \n",
    "            def train_step(x_batch, y_batch1, y_batch2, y_batch3, loss_sum, acc_sum, interal = 10):\n",
    "  \n",
    "                \"\"\"\n",
    "                A single training step\n",
    "                \"\"\"\n",
    "                feed_dict = {\n",
    "                  cnn.input_x: x_batch,\n",
    "                  cnn.input_y1: y_batch1,\n",
    "                  cnn.input_y2: y_batch2,\n",
    "                  cnn.input_y3: y_batch3,\n",
    "                  cnn.keep_prob: 0.9\n",
    "                }\n",
    "\n",
    "#         三个标签单独的准确率， 一起的准确率共四个准确率\n",
    "                rs, _, step,  loss, accuracy1, accuracy2, accuracy3, acc = sess.run(\n",
    "                    [merged, train_op, global_step, cnn.loss, cnn.accuracy1, cnn.accuracy2, cnn.accuracy3, cnn.acc],\n",
    "                    feed_dict)\n",
    "                time_str = datetime.datetime.now().isoformat()\n",
    "                acc_sum += [accuracy1, accuracy2, accuracy3, acc]\n",
    "                loss_sum += loss\n",
    "                if step % interal == 0:\n",
    "                    print(\"{}: step {}, loss {:g}, acc {}\".format(time_str, step, loss_sum/interal, acc_sum/interal))\n",
    "                    loss_sum = 0\n",
    "                    acc_sum = np.array([0.0,0.0,0.0,0.0])\n",
    "                    writer.add_summary(rs, step)\n",
    "                return loss_sum, acc_sum\n",
    "            # 评估步骤\n",
    "            def dev_step(x_batch, y_batch):\n",
    "                \"\"\"\n",
    "                Evaluates model on a dev set\n",
    "                \"\"\"\n",
    "\n",
    "                dev_batches = batch_iter(\n",
    "                    list(zip(x_batch, y_batch[0], y_batch[1], y_batch[2])), batch_size, 1)\n",
    "                step = 0\n",
    "                loss = 0\n",
    "                accuracy = np.array([0.0, 0.0, 0.0,0.0])\n",
    "                for batch in dev_batches:\n",
    "                    x_batch_dev, y_batch1_dev, y_batch2_dev, y_batch3_dev = zip(*batch)\n",
    "                    feed_dict = {\n",
    "                      cnn.input_x: x_batch_dev,\n",
    "                      cnn.input_y1: y_batch1_dev,\n",
    "                      cnn.input_y2: y_batch2_dev,\n",
    "                      cnn.input_y3: y_batch3_dev,\n",
    "                      cnn.keep_prob: 1\n",
    "                    }\n",
    "                    if len(x_batch_dev) < batch_size:\n",
    "                        continue\n",
    "                    step,  temp_loss, temp_accuracy1, temp_accuracy2, temp_accuracy3 , acc= sess.run(\n",
    "                        [global_step,cnn.loss, cnn.accuracy1, cnn.accuracy2, cnn.accuracy3, cnn.acc],\n",
    "                        feed_dict)\n",
    "                    accuracy[0] += temp_accuracy1 * len(x_batch_dev)\n",
    "                    accuracy[1] += temp_accuracy2 * len(x_batch_dev)\n",
    "                    accuracy[2] += temp_accuracy3 * len(x_batch_dev)\n",
    "                    accuracy[3] += acc * len(x_batch_dev)\n",
    "                    loss += temp_loss * len(x_batch_dev)\n",
    "                accuracy /= x_batch.shape[0]\n",
    "                loss /= x_batch.shape[0]\n",
    "                time_str = datetime.datetime.now().isoformat()                    \n",
    "\n",
    "                print(\"{}: step {}, loss {:g}, acc {} \".format(time_str, step, loss, accuracy))\n",
    "\n",
    "\n",
    "            # Generate batches\n",
    "            batches = batch_iter(\n",
    "                list(zip(x_train, y_train[0],y_train[1], y_train[2] )), batch_size, 200)\n",
    "            # Training loop. For each batch...\n",
    "            for batch in batches:\n",
    "                x_batch, y_batch1, y_batch2, y_batch3 = zip(*batch)\n",
    "                if len(x_batch) < batch_size:\n",
    "                    continue\n",
    "                loss_sum, acc_sum = train_step(x_batch, y_batch1, y_batch2, y_batch3,  loss_sum, acc_sum, 40)\n",
    "                current_step = tf.train.global_step(sess, global_step)\n",
    "                if current_step % 200 == 0:\n",
    "                    print(\"\\nEvaluation:\")\n",
    "                    dev_step(x_dev, y_dev)\n",
    "                    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gru shape (256, 128)\n",
      "2019-02-25T17:25:43.966720: step 40, loss 4.22665, acc [0.38525391 0.30361328 0.21113281 0.15322266]\n",
      "2019-02-25T17:25:47.292803: step 80, loss 3.12802, acc [0.57822266 0.47978516 0.35449219 0.28369141]\n",
      "2019-02-25T17:25:50.542257: step 120, loss 2.6059, acc [0.671875   0.56503906 0.43935547 0.36445312]\n",
      "2019-02-25T17:25:53.730604: step 160, loss 2.38131, acc [0.70800781 0.59169922 0.47773437 0.40166016]\n",
      "2019-02-25T17:25:56.913549: step 200, loss 2.14369, acc [0.73681641 0.63203125 0.52558594 0.44980469]\n",
      "\n",
      "Evaluation:\n",
      "2019-02-25T17:26:14.560760: step 200, loss 2.00325, acc [0.76481895 0.65499705 0.5460361  0.47538768] \n",
      "\n",
      "2019-02-25T17:26:18.121350: step 240, loss 1.99718, acc [0.76708984 0.65478516 0.54423828 0.46992187]\n",
      "2019-02-25T17:26:21.426989: step 280, loss 1.88057, acc [0.78896484 0.67890625 0.56826172 0.50019531]\n",
      "2019-02-25T17:26:24.673660: step 320, loss 1.79321, acc [0.79375    0.68603516 0.57783203 0.50507813]\n",
      "2019-02-25T17:26:27.881642: step 360, loss 1.74533, acc [0.79912109 0.69755859 0.59355469 0.51640625]\n",
      "2019-02-25T17:26:31.029675: step 400, loss 1.70738, acc [0.803125   0.70126953 0.59726563 0.52773437]\n",
      "\n",
      "Evaluation:\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-38-87d756c70f63>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0my_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtrain_y_label1_one_hot\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrain_y_label2_one_hot\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrain_y_label3_one_hot\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0my_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtest_y_label1_one_hot\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtest_y_label2_one_hot\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtest_y_label3_one_hot\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-37-e00393d77016>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(x_train, y_train, x_dev, y_dev)\u001b[0m\n\u001b[0;32m    121\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mcurrent_step\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m200\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    122\u001b[0m                     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\\nEvaluation:\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 123\u001b[1;33m                     \u001b[0mdev_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_dev\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_dev\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    124\u001b[0m                     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-37-e00393d77016>\u001b[0m in \u001b[0;36mdev_step\u001b[1;34m(x_batch, y_batch)\u001b[0m\n\u001b[0;32m     96\u001b[0m                     step,  temp_loss, temp_accuracy1, temp_accuracy2, temp_accuracy3 , acc= sess.run(\n\u001b[0;32m     97\u001b[0m                         \u001b[1;33m[\u001b[0m\u001b[0mglobal_step\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maccuracy1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maccuracy2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maccuracy3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0macc\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 98\u001b[1;33m                         feed_dict)\n\u001b[0m\u001b[0;32m     99\u001b[0m                     \u001b[0maccuracy\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mtemp_accuracy1\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_batch_dev\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    100\u001b[0m                     \u001b[0maccuracy\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mtemp_accuracy2\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_batch_dev\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\richado\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    885\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    886\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 887\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    888\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    889\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\richado\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1108\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1109\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1110\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1111\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1112\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\richado\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1284\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1285\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1286\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1287\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1288\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\richado\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1290\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1291\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1292\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1293\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1294\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\richado\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1275\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1276\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m-> 1277\u001b[1;33m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1278\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1279\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\richado\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1365\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[0;32m   1366\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1367\u001b[1;33m         run_metadata)\n\u001b[0m\u001b[0;32m   1368\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1369\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "y_train = [train_y_label1_one_hot,train_y_label2_one_hot,train_y_label3_one_hot]\n",
    "y_test = [test_y_label1_one_hot,test_y_label2_one_hot,test_y_label3_one_hot]\n",
    "train(x_train, y_train,  x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_y_label3_one_hot.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y_label3_one_hot.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
